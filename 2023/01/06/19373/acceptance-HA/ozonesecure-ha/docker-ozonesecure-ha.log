Attaching to ozonesecure-ha_datanode1_1, ozonesecure-ha_recon_1, ozonesecure-ha_scm2.org_1, ozonesecure-ha_datanode3_1, ozonesecure-ha_s3g_1, ozonesecure-ha_kms_1, ozonesecure-ha_om1_1, ozonesecure-ha_om2_1, ozonesecure-ha_datanode2_1, ozonesecure-ha_scm3.org_1, ozonesecure-ha_scm1.org_1, ozonesecure-ha_kdc_1, ozonesecure-ha_om3_1
datanode1_1  | Sleeping for 5 seconds
datanode1_1  | Waiting for the service scm3.org:9894
datanode1_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode1_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode1_1  | 2023-01-06 03:51:31,424 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode1_1  | /************************************************************
datanode1_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode1_1  | STARTUP_MSG:   host = 98e33c1634d3/172.25.0.102
datanode1_1  | STARTUP_MSG:   args = []
datanode1_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode1_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode1_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/9d30c9a7ce375cd614e86022850c7c128723df6b ; compiled by 'runner' on 2023-01-06T03:18Z
datanode1_1  | STARTUP_MSG:   java = 11.0.14.1
datanode1_1  | ************************************************************/
datanode1_1  | 2023-01-06 03:51:31,496 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode1_1  | 2023-01-06 03:51:32,093 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode1_1  | 2023-01-06 03:51:33,073 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode1_1  | 2023-01-06 03:51:34,796 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode1_1  | 2023-01-06 03:51:34,802 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode1_1  | 2023-01-06 03:51:36,001 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:98e33c1634d3 ip:172.25.0.102
datanode1_1  | 2023-01-06 03:51:41,755 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/dn@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode1_1  | 2023-01-06 03:51:43,120 [main] INFO security.UserGroupInformation: Login successful for user dn/dn@EXAMPLE.COM using keytab file dn.keytab. Keytab auto renewal enabled : false
datanode1_1  | 2023-01-06 03:51:43,120 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode1_1  | 2023-01-06 03:51:46,822 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode1_1  | 2023-01-06 03:51:46,824 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode1_1  | 2023-01-06 03:51:46,832 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode1_1  | 2023-01-06 03:51:46,838 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode1_1  | 2023-01-06 03:51:56,326 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode1_1  | 2023-01-06 03:51:56,836 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.102,host:98e33c1634d3
datanode1_1  | 2023-01-06 03:51:56,839 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode1_1  | 2023-01-06 03:51:56,877 [main] ERROR client.DNCertificateClient: Invalid domain 98e33c1634d3
datanode1_1  | 2023-01-06 03:51:56,886 [main] INFO client.DNCertificateClient: Created csr for DN-> subject:dn@98e33c1634d3
datanode1_1  | 2023-01-06 03:52:04,281 [main] INFO client.DNCertificateClient: Loading certificate from location:/data/metadata/dn/certs.
datanode1_1  | 2023-01-06 03:52:04,482 [main] INFO client.DNCertificateClient: Added certificate [
datanode1_1  | [
datanode1_1  |   Version: V3
datanode1_1  |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
datanode1_1  |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
datanode1_1  | 
datanode1_1  |   Key:  Sun RSA public key, 2048 bits
datanode1_1  |   params: null
datanode1_1  |   modulus: 22321520092034904956367956777931957676969911078232901017895933698678468781753172695844393013092965970615602622357794023639048732305076160434218405355746465972426985604516863422070797512086951139731951669174461346685083874381071907314493811292043757517828199039562222504144931947786863316592495947302761724410796725398590551213642383206197662885192142834321323519855792963024540129867342534976672519842084316567886340381000959465926087773440860829229209280683297485436375405886176389600891889084968589860808245139826791285487072108592486434566834307409916351251604701981140849210029026671497960989540995516629779904417
datanode1_1  |   public exponent: 65537
datanode1_1  |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
datanode1_1  |                To: Mon Feb 14 00:00:00 UTC 2028]
datanode1_1  |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
datanode1_1  |   SerialNumber: [    01]
datanode1_1  | 
datanode1_1  | Certificate Extensions: 3
datanode3_1  | Sleeping for 5 seconds
datanode3_1  | Waiting for the service scm3.org:9894
datanode3_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode3_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode3_1  | 2023-01-06 03:51:31,568 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode3_1  | /************************************************************
datanode3_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode3_1  | STARTUP_MSG:   host = 03b09732740a/172.25.0.104
datanode3_1  | STARTUP_MSG:   args = []
datanode3_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode3_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode3_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/9d30c9a7ce375cd614e86022850c7c128723df6b ; compiled by 'runner' on 2023-01-06T03:18Z
datanode3_1  | STARTUP_MSG:   java = 11.0.14.1
datanode3_1  | ************************************************************/
datanode3_1  | 2023-01-06 03:51:31,694 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode3_1  | 2023-01-06 03:51:32,420 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode3_1  | 2023-01-06 03:51:33,213 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode3_1  | 2023-01-06 03:51:34,643 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode3_1  | 2023-01-06 03:51:34,643 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode3_1  | 2023-01-06 03:51:35,902 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:03b09732740a ip:172.25.0.104
datanode3_1  | 2023-01-06 03:51:41,472 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/dn@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode3_1  | 2023-01-06 03:51:42,949 [main] INFO security.UserGroupInformation: Login successful for user dn/dn@EXAMPLE.COM using keytab file dn.keytab. Keytab auto renewal enabled : false
datanode3_1  | 2023-01-06 03:51:42,949 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode3_1  | 2023-01-06 03:51:46,386 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode3_1  | 2023-01-06 03:51:46,397 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode3_1  | 2023-01-06 03:51:46,405 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode3_1  | 2023-01-06 03:51:46,468 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode3_1  | 2023-01-06 03:51:54,804 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode3_1  | 2023-01-06 03:51:55,126 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.104,host:03b09732740a
datanode3_1  | 2023-01-06 03:51:55,134 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode3_1  | 2023-01-06 03:51:55,163 [main] ERROR client.DNCertificateClient: Invalid domain 03b09732740a
datanode3_1  | 2023-01-06 03:51:55,174 [main] INFO client.DNCertificateClient: Created csr for DN-> subject:dn@03b09732740a
datanode3_1  | 2023-01-06 03:52:03,253 [main] INFO client.DNCertificateClient: Loading certificate from location:/data/metadata/dn/certs.
datanode3_1  | 2023-01-06 03:52:03,498 [main] INFO client.DNCertificateClient: Added certificate [
datanode3_1  | [
datanode3_1  |   Version: V3
datanode3_1  |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
datanode3_1  |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
datanode3_1  | 
datanode3_1  |   Key:  Sun RSA public key, 2048 bits
datanode3_1  |   params: null
datanode3_1  |   modulus: 22321520092034904956367956777931957676969911078232901017895933698678468781753172695844393013092965970615602622357794023639048732305076160434218405355746465972426985604516863422070797512086951139731951669174461346685083874381071907314493811292043757517828199039562222504144931947786863316592495947302761724410796725398590551213642383206197662885192142834321323519855792963024540129867342534976672519842084316567886340381000959465926087773440860829229209280683297485436375405886176389600891889084968589860808245139826791285487072108592486434566834307409916351251604701981140849210029026671497960989540995516629779904417
datanode3_1  |   public exponent: 65537
datanode3_1  |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
datanode3_1  |                To: Mon Feb 14 00:00:00 UTC 2028]
datanode3_1  |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
datanode3_1  |   SerialNumber: [    01]
datanode3_1  | 
datanode3_1  | Certificate Extensions: 3
datanode3_1  | [1]: ObjectId: 2.5.29.19 Criticality=true
datanode3_1  | BasicConstraints:[
datanode3_1  |   CA:true
datanode3_1  |   PathLen:2147483647
datanode3_1  | ]
datanode3_1  | 
datanode3_1  | [2]: ObjectId: 2.5.29.15 Criticality=true
datanode3_1  | KeyUsage [
datanode3_1  |   Key_CertSign
datanode3_1  |   Crl_Sign
datanode3_1  | ]
datanode3_1  | 
datanode3_1  | [3]: ObjectId: 2.5.29.17 Criticality=false
datanode3_1  | SubjectAlternativeName [
datanode3_1  |   IPAddress: 172.25.0.116
datanode3_1  |   DNSName: scm1.org
datanode3_1  | ]
datanode3_1  | 
datanode3_1  | ]
datanode3_1  |   Algorithm: [SHA256withRSA]
datanode3_1  |   Signature:
datanode3_1  | 0000: 21 07 40 68 BD F5 21 63   53 A4 1C 6D 61 63 C4 B6  !.@h..!cS..mac..
datanode3_1  | 0010: E3 BD 40 F3 9B FD 99 A0   FE 5C 7B FD 06 FB 50 8E  ..@......\....P.
datanode3_1  | 0020: E2 8C 95 3C 0B 0B 1A AD   6E 89 50 B3 9C 01 22 36  ...<....n.P..."6
datanode3_1  | 0030: 82 EA 04 5E 17 F6 FE 59   8A 78 83 34 58 E3 1A CF  ...^...Y.x.4X...
datanode3_1  | 0040: C0 08 87 80 9B 7D B8 33   3C E2 09 5B A0 BB 73 F9  .......3<..[..s.
datanode3_1  | 0050: 18 A1 8D D7 79 71 5D F0   D7 29 1C F6 F1 1A 40 0C  ....yq]..)....@.
datanode3_1  | 0060: DC B0 36 E8 EA 71 C3 CB   C9 B0 27 6D D7 9F 21 23  ..6..q....'m..!#
datanode3_1  | 0070: 40 5D F0 5B 0A B7 A0 74   F9 9F AD 3D 48 4A CC B0  @].[...t...=HJ..
datanode3_1  | 0080: 71 94 D1 27 C1 11 45 74   CC E9 49 95 45 B0 E7 F7  q..'..Et..I.E...
datanode3_1  | 0090: 9B 9F F3 43 1C 28 31 DC   D6 F4 73 3B 1B 52 1D 3F  ...C.(1...s;.R.?
datanode3_1  | 00A0: 44 76 E5 4C 63 7F 23 9B   17 64 90 51 18 89 86 33  Dv.Lc.#..d.Q...3
datanode3_1  | 00B0: CE 66 07 29 55 E3 3A 4D   BD 33 89 34 C3 A8 3C 51  .f.)U.:M.3.4..<Q
datanode3_1  | 00C0: 52 16 00 6D 7D 61 54 1E   1A 84 F4 F5 89 B2 A8 9C  R..m.aT.........
datanode3_1  | 00D0: 56 66 FE E2 2A 93 FE 19   10 10 C9 9C 0D 74 9B 01  Vf..*........t..
datanode2_1  | Sleeping for 5 seconds
datanode2_1  | Waiting for the service scm3.org:9894
datanode2_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode2_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode2_1  | 2023-01-06 03:51:32,318 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode2_1  | /************************************************************
datanode2_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode2_1  | STARTUP_MSG:   host = 47737eac8b7e/172.25.0.103
datanode2_1  | STARTUP_MSG:   args = []
datanode2_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode2_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode2_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/9d30c9a7ce375cd614e86022850c7c128723df6b ; compiled by 'runner' on 2023-01-06T03:18Z
datanode2_1  | STARTUP_MSG:   java = 11.0.14.1
datanode2_1  | ************************************************************/
datanode2_1  | 2023-01-06 03:51:32,386 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode2_1  | 2023-01-06 03:51:33,265 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode2_1  | 2023-01-06 03:51:34,397 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode2_1  | 2023-01-06 03:51:35,736 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode2_1  | 2023-01-06 03:51:35,736 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode2_1  | 2023-01-06 03:51:36,921 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:47737eac8b7e ip:172.25.0.103
datanode2_1  | 2023-01-06 03:51:42,567 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/dn@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode2_1  | 2023-01-06 03:51:43,900 [main] INFO security.UserGroupInformation: Login successful for user dn/dn@EXAMPLE.COM using keytab file dn.keytab. Keytab auto renewal enabled : false
datanode2_1  | 2023-01-06 03:51:43,902 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode2_1  | 2023-01-06 03:51:47,379 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode2_1  | 2023-01-06 03:51:47,386 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode2_1  | 2023-01-06 03:51:47,402 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode2_1  | 2023-01-06 03:51:47,410 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode2_1  | 2023-01-06 03:51:54,665 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode2_1  | 2023-01-06 03:51:55,015 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.103,host:47737eac8b7e
datanode2_1  | 2023-01-06 03:51:55,024 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode2_1  | 2023-01-06 03:51:55,053 [main] ERROR client.DNCertificateClient: Invalid domain 47737eac8b7e
datanode2_1  | 2023-01-06 03:51:55,061 [main] INFO client.DNCertificateClient: Created csr for DN-> subject:dn@47737eac8b7e
datanode2_1  | 2023-01-06 03:52:02,374 [main] INFO client.DNCertificateClient: Loading certificate from location:/data/metadata/dn/certs.
datanode2_1  | 2023-01-06 03:52:02,560 [main] INFO client.DNCertificateClient: Added certificate [
datanode2_1  | [
datanode2_1  |   Version: V3
datanode2_1  |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
datanode2_1  |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
datanode2_1  | 
datanode2_1  |   Key:  Sun RSA public key, 2048 bits
datanode2_1  |   params: null
datanode2_1  |   modulus: 22321520092034904956367956777931957676969911078232901017895933698678468781753172695844393013092965970615602622357794023639048732305076160434218405355746465972426985604516863422070797512086951139731951669174461346685083874381071907314493811292043757517828199039562222504144931947786863316592495947302761724410796725398590551213642383206197662885192142834321323519855792963024540129867342534976672519842084316567886340381000959465926087773440860829229209280683297485436375405886176389600891889084968589860808245139826791285487072108592486434566834307409916351251604701981140849210029026671497960989540995516629779904417
datanode2_1  |   public exponent: 65537
datanode2_1  |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
datanode2_1  |                To: Mon Feb 14 00:00:00 UTC 2028]
datanode2_1  |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
datanode2_1  |   SerialNumber: [    01]
datanode2_1  | 
datanode2_1  | Certificate Extensions: 3
datanode2_1  | [1]: ObjectId: 2.5.29.19 Criticality=true
datanode2_1  | BasicConstraints:[
datanode2_1  |   CA:true
datanode2_1  |   PathLen:2147483647
datanode2_1  | ]
datanode2_1  | 
datanode2_1  | [2]: ObjectId: 2.5.29.15 Criticality=true
datanode2_1  | KeyUsage [
datanode2_1  |   Key_CertSign
datanode2_1  |   Crl_Sign
datanode2_1  | ]
datanode2_1  | 
datanode2_1  | [3]: ObjectId: 2.5.29.17 Criticality=false
datanode2_1  | SubjectAlternativeName [
datanode2_1  |   IPAddress: 172.25.0.116
datanode2_1  |   DNSName: scm1.org
datanode2_1  | ]
datanode2_1  | 
datanode2_1  | ]
datanode2_1  |   Algorithm: [SHA256withRSA]
datanode2_1  |   Signature:
datanode2_1  | 0000: 21 07 40 68 BD F5 21 63   53 A4 1C 6D 61 63 C4 B6  !.@h..!cS..mac..
datanode2_1  | 0010: E3 BD 40 F3 9B FD 99 A0   FE 5C 7B FD 06 FB 50 8E  ..@......\....P.
datanode2_1  | 0020: E2 8C 95 3C 0B 0B 1A AD   6E 89 50 B3 9C 01 22 36  ...<....n.P..."6
datanode2_1  | 0030: 82 EA 04 5E 17 F6 FE 59   8A 78 83 34 58 E3 1A CF  ...^...Y.x.4X...
datanode2_1  | 0040: C0 08 87 80 9B 7D B8 33   3C E2 09 5B A0 BB 73 F9  .......3<..[..s.
datanode2_1  | 0050: 18 A1 8D D7 79 71 5D F0   D7 29 1C F6 F1 1A 40 0C  ....yq]..)....@.
datanode2_1  | 0060: DC B0 36 E8 EA 71 C3 CB   C9 B0 27 6D D7 9F 21 23  ..6..q....'m..!#
datanode2_1  | 0070: 40 5D F0 5B 0A B7 A0 74   F9 9F AD 3D 48 4A CC B0  @].[...t...=HJ..
datanode2_1  | 0080: 71 94 D1 27 C1 11 45 74   CC E9 49 95 45 B0 E7 F7  q..'..Et..I.E...
datanode2_1  | 0090: 9B 9F F3 43 1C 28 31 DC   D6 F4 73 3B 1B 52 1D 3F  ...C.(1...s;.R.?
datanode2_1  | 00A0: 44 76 E5 4C 63 7F 23 9B   17 64 90 51 18 89 86 33  Dv.Lc.#..d.Q...3
datanode2_1  | 00B0: CE 66 07 29 55 E3 3A 4D   BD 33 89 34 C3 A8 3C 51  .f.)U.:M.3.4..<Q
datanode2_1  | 00C0: 52 16 00 6D 7D 61 54 1E   1A 84 F4 F5 89 B2 A8 9C  R..m.aT.........
datanode2_1  | 00D0: 56 66 FE E2 2A 93 FE 19   10 10 C9 9C 0D 74 9B 01  Vf..*........t..
datanode2_1  | 00E0: 75 1D 1C AB 74 3A E3 B8   41 66 A1 BE 62 55 B9 60  u...t:..Af..bU.`
datanode2_1  | 00F0: F1 84 78 6F 50 A4 59 C4   78 19 02 34 DC BA 56 C3  ..xoP.Y.x..4..V.
datanode2_1  | 
datanode2_1  | ] from file:/data/metadata/dn/certs/ROOTCA-1.crt.
datanode2_1  | 2023-01-06 03:52:02,608 [main] INFO client.DNCertificateClient: Added certificate [
datanode2_1  | [
datanode2_1  |   Version: V3
datanode2_1  |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=dn@47737eac8b7e
datanode2_1  |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
datanode2_1  | 
datanode2_1  |   Key:  Sun RSA public key, 2048 bits
datanode2_1  |   params: null
datanode1_1  | [1]: ObjectId: 2.5.29.19 Criticality=true
datanode1_1  | BasicConstraints:[
datanode1_1  |   CA:true
datanode1_1  |   PathLen:2147483647
datanode1_1  | ]
datanode1_1  | 
datanode1_1  | [2]: ObjectId: 2.5.29.15 Criticality=true
datanode1_1  | KeyUsage [
datanode1_1  |   Key_CertSign
datanode1_1  |   Crl_Sign
datanode1_1  | ]
datanode1_1  | 
datanode1_1  | [3]: ObjectId: 2.5.29.17 Criticality=false
datanode1_1  | SubjectAlternativeName [
datanode1_1  |   IPAddress: 172.25.0.116
datanode1_1  |   DNSName: scm1.org
datanode1_1  | ]
datanode1_1  | 
datanode1_1  | ]
datanode1_1  |   Algorithm: [SHA256withRSA]
datanode1_1  |   Signature:
datanode1_1  | 0000: 21 07 40 68 BD F5 21 63   53 A4 1C 6D 61 63 C4 B6  !.@h..!cS..mac..
datanode1_1  | 0010: E3 BD 40 F3 9B FD 99 A0   FE 5C 7B FD 06 FB 50 8E  ..@......\....P.
datanode1_1  | 0020: E2 8C 95 3C 0B 0B 1A AD   6E 89 50 B3 9C 01 22 36  ...<....n.P..."6
datanode1_1  | 0030: 82 EA 04 5E 17 F6 FE 59   8A 78 83 34 58 E3 1A CF  ...^...Y.x.4X...
datanode1_1  | 0040: C0 08 87 80 9B 7D B8 33   3C E2 09 5B A0 BB 73 F9  .......3<..[..s.
datanode1_1  | 0050: 18 A1 8D D7 79 71 5D F0   D7 29 1C F6 F1 1A 40 0C  ....yq]..)....@.
datanode1_1  | 0060: DC B0 36 E8 EA 71 C3 CB   C9 B0 27 6D D7 9F 21 23  ..6..q....'m..!#
datanode1_1  | 0070: 40 5D F0 5B 0A B7 A0 74   F9 9F AD 3D 48 4A CC B0  @].[...t...=HJ..
datanode1_1  | 0080: 71 94 D1 27 C1 11 45 74   CC E9 49 95 45 B0 E7 F7  q..'..Et..I.E...
datanode1_1  | 0090: 9B 9F F3 43 1C 28 31 DC   D6 F4 73 3B 1B 52 1D 3F  ...C.(1...s;.R.?
datanode1_1  | 00A0: 44 76 E5 4C 63 7F 23 9B   17 64 90 51 18 89 86 33  Dv.Lc.#..d.Q...3
datanode1_1  | 00B0: CE 66 07 29 55 E3 3A 4D   BD 33 89 34 C3 A8 3C 51  .f.)U.:M.3.4..<Q
datanode1_1  | 00C0: 52 16 00 6D 7D 61 54 1E   1A 84 F4 F5 89 B2 A8 9C  R..m.aT.........
datanode1_1  | 00D0: 56 66 FE E2 2A 93 FE 19   10 10 C9 9C 0D 74 9B 01  Vf..*........t..
datanode1_1  | 00E0: 75 1D 1C AB 74 3A E3 B8   41 66 A1 BE 62 55 B9 60  u...t:..Af..bU.`
datanode1_1  | 00F0: F1 84 78 6F 50 A4 59 C4   78 19 02 34 DC BA 56 C3  ..xoP.Y.x..4..V.
datanode1_1  | 
datanode1_1  | ] from file:/data/metadata/dn/certs/ROOTCA-1.crt.
datanode1_1  | 2023-01-06 03:52:04,532 [main] INFO client.DNCertificateClient: Added certificate [
datanode1_1  | [
datanode1_1  |   Version: V3
datanode1_1  |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm-sub@scm1.org
datanode1_1  |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
datanode1_1  | 
datanode1_1  |   Key:  Sun RSA public key, 2048 bits
datanode1_1  |   params: null
kdc_1        | Jan 06 03:49:38 kdc krb5kdc[8](info): Loaded
kdc_1        | Jan 06 03:49:38 kdc krb5kdc[8](Error): preauth spake failed to initialize: No SPAKE preauth groups configured
kdc_1        | Jan 06 03:49:38 kdc krb5kdc[8](info): setting up network...
kdc_1        | Jan 06 03:49:38 kdc krb5kdc[8](info): setsockopt(8,IPV6_V6ONLY,1) worked
kdc_1        | Jan 06 03:49:38 kdc krb5kdc[8](info): setsockopt(10,IPV6_V6ONLY,1) worked
kdc_1        | Jan 06 03:49:38 kdc krb5kdc[8](info): set up 4 sockets
kdc_1        | Jan 06 03:49:38 kdc krb5kdc[8](info): commencing operation
kdc_1        | krb5kdc: starting...
kdc_1        | Jan 06 03:49:41 kdc krb5kdc[8](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1672976981, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 06 03:49:47 kdc krb5kdc[8](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1672976987, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 06 03:49:52 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.114: ISSUE: authtime 1672976992, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, s3g/s3g@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 06 03:49:56 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.115: ISSUE: authtime 1672976996, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, recon/recon@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 06 03:50:10 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.117: ISSUE: authtime 1672977010, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 06 03:50:17 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.116: ISSUE: authtime 1672977017, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 06 03:50:23 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1672977010, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 06 03:50:23 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.115: ISSUE: authtime 1672976996, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, recon/recon@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 06 03:50:25 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1672976987, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 06 03:50:36 kdc krb5kdc[8](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1672977036, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 06 03:50:39 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.117: ISSUE: authtime 1672977039, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 06 03:50:49 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1672977036, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 06 03:50:52 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1672977039, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 06 03:50:53 kdc krb5kdc[8](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1672977053, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 06 03:50:58 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.118: ISSUE: authtime 1672977058, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 06 03:50:59 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.118: ISSUE: authtime 1672977058, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 06 03:51:05 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1672977053, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
datanode1_1  |   modulus: 25512042468887604642064404093025249920280347815052024483618130467625890981963310751612908088931868680366166722736700966336351841536566461994206957959065592473127025545304172175201240100170923940025313585056297682798208555874810870376218207324432498120475419600056108682275459578022550441815608531770386354956257639186299780577671839142629796440640716929439466625583225667634032960664095389171765331267228308686758487244580763947638224999982475527976426192624582650657992140973107557451334337699303987990444760298981625629400711682432328660808877074618851496477253766349631046458008273474791424727251297086392264850331
datanode1_1  |   public exponent: 65537
datanode1_1  |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
datanode1_1  |                To: Mon Feb 14 00:00:00 UTC 2028]
datanode1_1  |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
datanode1_1  |   SerialNumber: [    01379773 48ab]
datanode1_1  | 
datanode1_1  | Certificate Extensions: 3
datanode1_1  | [1]: ObjectId: 2.5.29.19 Criticality=true
datanode1_1  | BasicConstraints:[
datanode1_1  |   CA:true
datanode1_1  |   PathLen:2147483647
datanode1_1  | ]
datanode1_1  | 
datanode1_1  | [2]: ObjectId: 2.5.29.15 Criticality=true
datanode1_1  | KeyUsage [
datanode1_1  |   DigitalSignature
datanode1_1  |   Key_Encipherment
datanode1_1  |   Data_Encipherment
datanode1_1  |   Key_Agreement
datanode1_1  |   Key_CertSign
datanode1_1  |   Crl_Sign
datanode1_1  | ]
datanode1_1  | 
datanode1_1  | [3]: ObjectId: 2.5.29.17 Criticality=false
datanode1_1  | SubjectAlternativeName [
datanode1_1  |   IPAddress: 172.25.0.116
datanode1_1  |   DNSName: scm1.org
datanode1_1  | ]
datanode1_1  | 
datanode1_1  | ]
datanode1_1  |   Algorithm: [SHA256withRSA]
datanode1_1  |   Signature:
datanode1_1  | 0000: 71 A1 97 4D DA 8D 52 3F   E5 06 22 8E 76 61 49 C0  q..M..R?..".vaI.
datanode1_1  | 0010: C6 DB 6D 93 9B E0 DF 39   D0 2C 5E B2 55 3C F2 BA  ..m....9.,^.U<..
datanode1_1  | 0020: 2B 11 C5 2A C7 34 78 58   C1 96 DE C2 09 D7 16 49  +..*.4xX.......I
datanode1_1  | 0030: 14 DD 81 0D 0D EC 2B 14   A8 C2 D5 DC D9 65 72 65  ......+......ere
datanode1_1  | 0040: AC 06 88 C6 35 48 D5 5F   88 26 38 54 A0 34 B7 6A  ....5H._.&8T.4.j
datanode1_1  | 0050: 80 D9 98 32 D0 A7 83 BC   EF F9 F3 B4 3E E3 DB 99  ...2........>...
datanode1_1  | 0060: C8 62 68 E3 13 33 55 58   91 03 33 A0 D4 E3 E8 C7  .bh..3UX..3.....
datanode1_1  | 0070: 11 6E E3 D9 21 80 E7 D3   FD D9 65 FA 44 9C 4A 18  .n..!.....e.D.J.
datanode1_1  | 0080: 0A F3 39 BC 62 9F 21 94   BE 66 EF E8 B3 BE 21 DA  ..9.b.!..f....!.
datanode1_1  | 0090: 04 60 36 57 8C 00 B7 0D   3C 31 30 4D 96 93 18 D6  .`6W....<10M....
datanode1_1  | 00A0: DE 9E 43 8C B8 02 39 06   65 33 4A E0 D2 C5 E1 C5  ..C...9.e3J.....
datanode1_1  | 00B0: EC 84 E7 E9 D8 7F 26 5A   7F EC C4 ED E0 DA 89 3A  ......&Z.......:
datanode1_1  | 00C0: 2A 12 BB 4E A2 53 59 0A   EB 4D CC C2 5B DB 3E 6C  *..N.SY..M..[.>l
datanode1_1  | 00D0: 79 1A 95 EA 9D F5 37 22   32 4C FC DF 91 B5 AD 85  y.....7"2L......
datanode1_1  | 00E0: 28 09 F5 E5 5B 53 76 DA   63 17 AE F5 80 A5 B1 AF  (...[Sv.c.......
datanode1_1  | 00F0: 1B 90 43 5F FF 80 DB 39   CC 0A 02 38 20 9A EB 4F  ..C_...9...8 ..O
datanode1_1  | 
datanode1_1  | ] from file:/data/metadata/dn/certs/CA-1338275743915.crt.
datanode1_1  | 2023-01-06 03:52:04,585 [main] INFO client.DNCertificateClient: Added certificate [
datanode1_1  | [
datanode1_1  |   Version: V3
datanode1_1  |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=dn@98e33c1634d3
datanode1_1  |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
datanode1_1  | 
datanode1_1  |   Key:  Sun RSA public key, 2048 bits
datanode1_1  |   params: null
datanode1_1  |   modulus: 22929424574322192276780902415520407132252482056118028327114519072629267919851037929813259448180123384954816463781344406286230719682690589235388025449324845595893687422413782694108966850502963279647618625391982377641141966072871552645922795510733473407595590473030674742579213742226799271622925449111884364558199722527449843020177229825500805489785384498313373027255935385002955219628314106448020503718998093788127484343176700258696069346828807593956342238683700216947882793972423063358796291974970371619946821884242197400941969082988604675244306152856081061519039413853185015594142691296031869238972626205256792870371
datanode1_1  |   public exponent: 65537
datanode1_1  |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
datanode1_1  |                To: Sat Jan 06 00:00:00 UTC 2024]
datanode1_1  |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm-sub@scm1.org
datanode1_1  |   SerialNumber: [    0152abad 3050]
datanode1_1  | 
datanode1_1  | Certificate Extensions: 2
datanode1_1  | [1]: ObjectId: 2.5.29.15 Criticality=true
datanode1_1  | KeyUsage [
datanode1_1  |   DigitalSignature
datanode1_1  |   Key_Encipherment
datanode1_1  |   Data_Encipherment
datanode1_1  |   Key_Agreement
datanode1_1  | ]
datanode1_1  | 
datanode1_1  | [2]: ObjectId: 2.5.29.17 Criticality=false
datanode3_1  | 00E0: 75 1D 1C AB 74 3A E3 B8   41 66 A1 BE 62 55 B9 60  u...t:..Af..bU.`
datanode3_1  | 00F0: F1 84 78 6F 50 A4 59 C4   78 19 02 34 DC BA 56 C3  ..xoP.Y.x..4..V.
datanode3_1  | 
datanode3_1  | ] from file:/data/metadata/dn/certs/ROOTCA-1.crt.
datanode3_1  | 2023-01-06 03:52:03,562 [main] INFO client.DNCertificateClient: Added certificate [
datanode3_1  | [
datanode3_1  |   Version: V3
datanode3_1  |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=dn@03b09732740a
datanode3_1  |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
datanode3_1  | 
datanode3_1  |   Key:  Sun RSA public key, 2048 bits
datanode3_1  |   params: null
datanode3_1  |   modulus: 24401127849855873968458113654717257053475355910746301224991878014413579423476718952645585082478701783301182963436010539127145626570845751106051958142479710458486491534747482108325580512641165808149049938263910280461079425226089520913868074830580045256695909135362779301141951229574651239158073459982165465147165275913529066981587062783630030801728945407724672020102221657439588325162083548436874524302807019093800519797022610345195287625107170667699607897092098966075603186654109495175504089474752275426509908826158498614157282212647030079987660280520835914002077659425671969921691018934285570254827890535819707586283
datanode3_1  |   public exponent: 65537
datanode3_1  |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
datanode3_1  |                To: Sat Jan 06 00:00:00 UTC 2024]
datanode3_1  |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm-sub@scm1.org
datanode3_1  |   SerialNumber: [    01526f9c 6813]
datanode3_1  | 
datanode3_1  | Certificate Extensions: 2
datanode3_1  | [1]: ObjectId: 2.5.29.15 Criticality=true
datanode3_1  | KeyUsage [
datanode3_1  |   DigitalSignature
datanode3_1  |   Key_Encipherment
datanode3_1  |   Data_Encipherment
datanode3_1  |   Key_Agreement
datanode3_1  | ]
datanode3_1  | 
datanode3_1  | [2]: ObjectId: 2.5.29.17 Criticality=false
datanode3_1  | SubjectAlternativeName [
datanode3_1  |   IPAddress: 172.25.0.104
datanode3_1  | ]
datanode3_1  | 
datanode3_1  | ]
datanode3_1  |   Algorithm: [SHA256withRSA]
datanode3_1  |   Signature:
datanode3_1  | 0000: 04 3B D9 AA 98 B8 C7 F7   6F CB F3 E0 39 7A 83 B7  .;......o...9z..
datanode3_1  | 0010: 07 F6 FB B0 B2 E4 0D 68   63 B7 F5 EE F8 46 66 91  .......hc....Ff.
datanode3_1  | 0020: B6 28 D7 AC 1C 59 AC 0F   94 18 B1 5F 79 4D 99 8B  .(...Y....._yM..
datanode3_1  | 0030: 20 5E AF 74 F2 9B BC FE   FC 94 82 B6 84 D7 86 9D   ^.t............
datanode3_1  | 0040: 72 11 AB 0C B8 63 8A 1D   61 CD 47 06 F5 1B C5 05  r....c..a.G.....
datanode3_1  | 0050: 92 0E C7 2A A0 5E 3B 94   C2 87 37 69 F7 62 35 A0  ...*.^;...7i.b5.
datanode3_1  | 0060: 67 29 F9 43 9C 3D 41 A3   B3 16 9A 31 7F 5F D0 95  g).C.=A....1._..
datanode3_1  | 0070: 5C 5C A8 64 99 B7 9E BD   C1 C8 15 CE 2D 6D 00 C1  \\.d........-m..
datanode3_1  | 0080: DF 5F 8A A4 1A D2 D1 12   33 AA 6A 6B 39 16 46 6F  ._......3.jk9.Fo
datanode3_1  | 0090: D4 6E BF 33 0B 9B 1E F8   04 50 1B 4A 45 F7 3E 8B  .n.3.....P.JE.>.
datanode3_1  | 00A0: AA 26 91 78 48 85 B0 86   49 AE 9B 73 FA 20 FE C8  .&.xH...I..s. ..
datanode3_1  | 00B0: C3 23 D5 66 B3 46 22 22   A7 E9 FA 36 90 5B B4 C4  .#.f.F""...6.[..
datanode3_1  | 00C0: EB 98 E6 5E 34 0C FE 2C   67 50 CA DB DC AE 73 B2  ...^4..,gP....s.
datanode3_1  | 00D0: 56 AA DE 29 2B D6 0F 99   65 55 EE 62 D1 6B F2 D7  V..)+...eU.b.k..
datanode3_1  | 00E0: A3 D1 3E 54 C4 FC 12 0B   5F 5C D1 07 10 D3 2B 40  ..>T...._\....+@
datanode3_1  | 00F0: 47 CB 74 27 A1 77 4C 29   CD 6C 57 00 DF 1D 7A 6D  G.t'.wL).lW...zm
datanode3_1  | 
datanode3_1  | ] from file:/data/metadata/dn/certs/1453571467283.crt.
datanode3_1  | 2023-01-06 03:52:03,616 [main] INFO client.DNCertificateClient: Added certificate [
datanode3_1  | [
datanode3_1  |   Version: V3
datanode3_1  |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm-sub@scm1.org
datanode3_1  |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
datanode3_1  | 
datanode3_1  |   Key:  Sun RSA public key, 2048 bits
datanode3_1  |   params: null
datanode3_1  |   modulus: 25512042468887604642064404093025249920280347815052024483618130467625890981963310751612908088931868680366166722736700966336351841536566461994206957959065592473127025545304172175201240100170923940025313585056297682798208555874810870376218207324432498120475419600056108682275459578022550441815608531770386354956257639186299780577671839142629796440640716929439466625583225667634032960664095389171765331267228308686758487244580763947638224999982475527976426192624582650657992140973107557451334337699303987990444760298981625629400711682432328660808877074618851496477253766349631046458008273474791424727251297086392264850331
datanode3_1  |   public exponent: 65537
datanode3_1  |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
datanode3_1  |                To: Mon Feb 14 00:00:00 UTC 2028]
datanode3_1  |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
datanode3_1  |   SerialNumber: [    01379773 48ab]
datanode3_1  | 
datanode3_1  | Certificate Extensions: 3
datanode3_1  | [1]: ObjectId: 2.5.29.19 Criticality=true
datanode3_1  | BasicConstraints:[
datanode3_1  |   CA:true
datanode3_1  |   PathLen:2147483647
datanode3_1  | ]
datanode3_1  | 
datanode3_1  | [2]: ObjectId: 2.5.29.15 Criticality=true
datanode3_1  | KeyUsage [
datanode3_1  |   DigitalSignature
datanode3_1  |   Key_Encipherment
datanode3_1  |   Data_Encipherment
datanode3_1  |   Key_Agreement
datanode3_1  |   Key_CertSign
datanode3_1  |   Crl_Sign
datanode3_1  | ]
datanode3_1  | 
datanode3_1  | [3]: ObjectId: 2.5.29.17 Criticality=false
datanode3_1  | SubjectAlternativeName [
datanode3_1  |   IPAddress: 172.25.0.116
datanode3_1  |   DNSName: scm1.org
datanode3_1  | ]
datanode3_1  | 
datanode3_1  | ]
datanode3_1  |   Algorithm: [SHA256withRSA]
datanode3_1  |   Signature:
datanode3_1  | 0000: 71 A1 97 4D DA 8D 52 3F   E5 06 22 8E 76 61 49 C0  q..M..R?..".vaI.
datanode3_1  | 0010: C6 DB 6D 93 9B E0 DF 39   D0 2C 5E B2 55 3C F2 BA  ..m....9.,^.U<..
datanode3_1  | 0020: 2B 11 C5 2A C7 34 78 58   C1 96 DE C2 09 D7 16 49  +..*.4xX.......I
datanode3_1  | 0030: 14 DD 81 0D 0D EC 2B 14   A8 C2 D5 DC D9 65 72 65  ......+......ere
datanode3_1  | 0040: AC 06 88 C6 35 48 D5 5F   88 26 38 54 A0 34 B7 6A  ....5H._.&8T.4.j
datanode3_1  | 0050: 80 D9 98 32 D0 A7 83 BC   EF F9 F3 B4 3E E3 DB 99  ...2........>...
datanode3_1  | 0060: C8 62 68 E3 13 33 55 58   91 03 33 A0 D4 E3 E8 C7  .bh..3UX..3.....
datanode3_1  | 0070: 11 6E E3 D9 21 80 E7 D3   FD D9 65 FA 44 9C 4A 18  .n..!.....e.D.J.
datanode3_1  | 0080: 0A F3 39 BC 62 9F 21 94   BE 66 EF E8 B3 BE 21 DA  ..9.b.!..f....!.
datanode3_1  | 0090: 04 60 36 57 8C 00 B7 0D   3C 31 30 4D 96 93 18 D6  .`6W....<10M....
datanode3_1  | 00A0: DE 9E 43 8C B8 02 39 06   65 33 4A E0 D2 C5 E1 C5  ..C...9.e3J.....
datanode3_1  | 00B0: EC 84 E7 E9 D8 7F 26 5A   7F EC C4 ED E0 DA 89 3A  ......&Z.......:
datanode3_1  | 00C0: 2A 12 BB 4E A2 53 59 0A   EB 4D CC C2 5B DB 3E 6C  *..N.SY..M..[.>l
datanode3_1  | 00D0: 79 1A 95 EA 9D F5 37 22   32 4C FC DF 91 B5 AD 85  y.....7"2L......
datanode3_1  | 00E0: 28 09 F5 E5 5B 53 76 DA   63 17 AE F5 80 A5 B1 AF  (...[Sv.c.......
datanode3_1  | 00F0: 1B 90 43 5F FF 80 DB 39   CC 0A 02 38 20 9A EB 4F  ..C_...9...8 ..O
datanode3_1  | 
datanode3_1  | ] from file:/data/metadata/dn/certs/CA-1338275743915.crt.
datanode3_1  | 2023-01-06 03:52:03,670 [main] INFO client.DNCertificateClient: CertificateLifetimeMonitor is started with first delay 29102876369 ms and interval 86400000 ms.
datanode3_1  | 2023-01-06 03:52:03,671 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode3_1  | 2023-01-06 03:52:03,918 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode3_1  | 2023-01-06 03:52:05,595 [main] INFO reflections.Reflections: Reflections took 1161 ms to scan 2 urls, producing 97 keys and 217 values 
datanode3_1  | 2023-01-06 03:52:06,258 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
datanode3_1  | 2023-01-06 03:52:08,230 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode3_1  | 2023-01-06 03:52:08,525 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode3_1  | 2023-01-06 03:52:08,569 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode3_1  | 2023-01-06 03:52:08,577 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode3_1  | 2023-01-06 03:52:08,871 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode2_1  |   modulus: 17987616994384489468919330479898195297137542849342652347483807975576053422446583953629163968048951547815676157060464078978429768639296094188914665566876872424315606957728346118582650821362054598685291312558878534266879356633703983395255334579761609343883361089790383036691315710335786668525825904771566454772767713388864682701972378450792628481010970388079479976823236817403943376256767783767388512964685513503931113765789923208625394162684032434016478446578634854826471463132068548421503917351073054453329366375602374185277019480711829364357461148629794085239704843935844080550599908780099949293834750608235721152307
datanode2_1  |   public exponent: 65537
datanode2_1  |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
datanode2_1  |                To: Sat Jan 06 00:00:00 UTC 2024]
datanode2_1  |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm-sub@scm1.org
datanode2_1  |   SerialNumber: [    015245d1 6498]
datanode2_1  | 
datanode2_1  | Certificate Extensions: 2
datanode2_1  | [1]: ObjectId: 2.5.29.15 Criticality=true
datanode2_1  | KeyUsage [
datanode2_1  |   DigitalSignature
datanode2_1  |   Key_Encipherment
datanode2_1  |   Data_Encipherment
datanode2_1  |   Key_Agreement
datanode2_1  | ]
datanode2_1  | 
datanode2_1  | [2]: ObjectId: 2.5.29.17 Criticality=false
datanode2_1  | SubjectAlternativeName [
datanode2_1  |   IPAddress: 172.25.0.103
datanode2_1  | ]
datanode2_1  | 
datanode2_1  | ]
datanode2_1  |   Algorithm: [SHA256withRSA]
datanode2_1  |   Signature:
datanode2_1  | 0000: 53 64 4F 45 74 E8 B0 91   68 26 9A BB 55 E4 97 1B  SdOEt...h&..U...
datanode2_1  | 0010: B6 EA 51 82 5E FB A3 C9   06 B9 3E 45 F4 AB 2B 12  ..Q.^.....>E..+.
datanode2_1  | 0020: EE 41 1B 10 99 EC 67 06   2E 59 F8 57 B0 FC DB E0  .A....g..Y.W....
datanode2_1  | 0030: 95 DB 5C 70 83 62 B2 19   79 8B B2 86 8F 28 F6 07  ..\p.b..y....(..
datanode2_1  | 0040: 1D 5B 58 AB 1A 1A 7C 7A   5E C5 D2 17 7B 5C 66 87  .[X....z^....\f.
datanode2_1  | 0050: 92 8A E9 79 40 A6 B6 91   0F 64 9B 66 89 88 6A D6  ...y@....d.f..j.
datanode2_1  | 0060: B2 A9 67 AA 3F B9 E6 55   93 56 36 21 6E 7C E2 62  ..g.?..U.V6!n..b
datanode2_1  | 0070: BF 32 F3 9F DD 6B 8F 25   88 02 6D DA 62 C6 03 7B  .2...k.%..m.b...
datanode2_1  | 0080: E4 FD FA 73 A8 F5 69 BB   0B AA BC 47 59 29 EC B5  ...s..i....GY)..
datanode2_1  | 0090: F6 BD 2F 6E 89 C6 81 41   D3 0E 73 B3 0B 00 B8 6B  ../n...A..s....k
datanode2_1  | 00A0: 92 03 E2 9D 4D 2E 58 45   5E CF 33 D5 62 13 DD 03  ....M.XE^.3.b...
datanode2_1  | 00B0: DA 9D AC 61 4F F1 C4 24   07 BB E9 2F 0A 31 87 37  ...aO..$.../.1.7
datanode2_1  | 00C0: 83 0F AB FB AE 93 D2 F7   A7 00 BA E4 60 59 33 77  ............`Y3w
datanode2_1  | 00D0: FB CA 94 2A 7B 47 F5 66   42 6B C8 F1 71 AF 63 C7  ...*.G.fBk..q.c.
datanode2_1  | 00E0: 56 3F 77 E2 8B 19 5B B9   8D 4E 29 16 E4 D8 F0 71  V?w...[..N)....q
datanode2_1  | 00F0: 9A F5 A4 0E 32 C1 A8 3B   AE A9 18 EB 18 9F 88 05  ....2..;........
datanode2_1  | 
datanode2_1  | ] from file:/data/metadata/dn/certs/1452870296728.crt.
datanode2_1  | 2023-01-06 03:52:02,658 [main] INFO client.DNCertificateClient: Added certificate [
datanode2_1  | [
datanode2_1  |   Version: V3
datanode2_1  |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm-sub@scm1.org
datanode2_1  |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
datanode2_1  | 
datanode2_1  |   Key:  Sun RSA public key, 2048 bits
datanode2_1  |   params: null
datanode2_1  |   modulus: 25512042468887604642064404093025249920280347815052024483618130467625890981963310751612908088931868680366166722736700966336351841536566461994206957959065592473127025545304172175201240100170923940025313585056297682798208555874810870376218207324432498120475419600056108682275459578022550441815608531770386354956257639186299780577671839142629796440640716929439466625583225667634032960664095389171765331267228308686758487244580763947638224999982475527976426192624582650657992140973107557451334337699303987990444760298981625629400711682432328660808877074618851496477253766349631046458008273474791424727251297086392264850331
datanode2_1  |   public exponent: 65537
datanode2_1  |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
datanode2_1  |                To: Mon Feb 14 00:00:00 UTC 2028]
datanode2_1  |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
datanode2_1  |   SerialNumber: [    01379773 48ab]
datanode2_1  | 
datanode2_1  | Certificate Extensions: 3
datanode2_1  | [1]: ObjectId: 2.5.29.19 Criticality=true
datanode2_1  | BasicConstraints:[
datanode2_1  |   CA:true
datanode2_1  |   PathLen:2147483647
datanode2_1  | ]
datanode2_1  | 
datanode2_1  | [2]: ObjectId: 2.5.29.15 Criticality=true
datanode2_1  | KeyUsage [
datanode2_1  |   DigitalSignature
datanode2_1  |   Key_Encipherment
datanode2_1  |   Data_Encipherment
datanode2_1  |   Key_Agreement
datanode2_1  |   Key_CertSign
datanode2_1  |   Crl_Sign
datanode2_1  | ]
datanode2_1  | 
datanode2_1  | [3]: ObjectId: 2.5.29.17 Criticality=false
datanode2_1  | SubjectAlternativeName [
datanode2_1  |   IPAddress: 172.25.0.116
datanode2_1  |   DNSName: scm1.org
datanode2_1  | ]
datanode2_1  | 
datanode2_1  | ]
datanode2_1  |   Algorithm: [SHA256withRSA]
datanode2_1  |   Signature:
datanode2_1  | 0000: 71 A1 97 4D DA 8D 52 3F   E5 06 22 8E 76 61 49 C0  q..M..R?..".vaI.
datanode2_1  | 0010: C6 DB 6D 93 9B E0 DF 39   D0 2C 5E B2 55 3C F2 BA  ..m....9.,^.U<..
datanode2_1  | 0020: 2B 11 C5 2A C7 34 78 58   C1 96 DE C2 09 D7 16 49  +..*.4xX.......I
datanode2_1  | 0030: 14 DD 81 0D 0D EC 2B 14   A8 C2 D5 DC D9 65 72 65  ......+......ere
datanode2_1  | 0040: AC 06 88 C6 35 48 D5 5F   88 26 38 54 A0 34 B7 6A  ....5H._.&8T.4.j
datanode2_1  | 0050: 80 D9 98 32 D0 A7 83 BC   EF F9 F3 B4 3E E3 DB 99  ...2........>...
datanode2_1  | 0060: C8 62 68 E3 13 33 55 58   91 03 33 A0 D4 E3 E8 C7  .bh..3UX..3.....
datanode2_1  | 0070: 11 6E E3 D9 21 80 E7 D3   FD D9 65 FA 44 9C 4A 18  .n..!.....e.D.J.
datanode2_1  | 0080: 0A F3 39 BC 62 9F 21 94   BE 66 EF E8 B3 BE 21 DA  ..9.b.!..f....!.
datanode2_1  | 0090: 04 60 36 57 8C 00 B7 0D   3C 31 30 4D 96 93 18 D6  .`6W....<10M....
datanode2_1  | 00A0: DE 9E 43 8C B8 02 39 06   65 33 4A E0 D2 C5 E1 C5  ..C...9.e3J.....
datanode2_1  | 00B0: EC 84 E7 E9 D8 7F 26 5A   7F EC C4 ED E0 DA 89 3A  ......&Z.......:
datanode2_1  | 00C0: 2A 12 BB 4E A2 53 59 0A   EB 4D CC C2 5B DB 3E 6C  *..N.SY..M..[.>l
datanode2_1  | 00D0: 79 1A 95 EA 9D F5 37 22   32 4C FC DF 91 B5 AD 85  y.....7"2L......
datanode2_1  | 00E0: 28 09 F5 E5 5B 53 76 DA   63 17 AE F5 80 A5 B1 AF  (...[Sv.c.......
datanode2_1  | 00F0: 1B 90 43 5F FF 80 DB 39   CC 0A 02 38 20 9A EB 4F  ..C_...9...8 ..O
datanode2_1  | 
datanode2_1  | ] from file:/data/metadata/dn/certs/CA-1338275743915.crt.
datanode2_1  | 2023-01-06 03:52:02,706 [main] INFO client.DNCertificateClient: CertificateLifetimeMonitor is started with first delay 29102877330 ms and interval 86400000 ms.
datanode2_1  | 2023-01-06 03:52:02,706 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode2_1  | 2023-01-06 03:52:02,916 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode2_1  | 2023-01-06 03:52:04,374 [main] INFO reflections.Reflections: Reflections took 1201 ms to scan 2 urls, producing 97 keys and 217 values 
datanode2_1  | 2023-01-06 03:52:05,269 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
datanode2_1  | 2023-01-06 03:52:07,075 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode2_1  | 2023-01-06 03:52:07,248 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode2_1  | 2023-01-06 03:52:07,281 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode2_1  | 2023-01-06 03:52:07,293 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode2_1  | 2023-01-06 03:52:07,708 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
kdc_1        | Jan 06 03:51:08 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.118: ISSUE: authtime 1672977068, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 06 03:51:09 kdc krb5kdc[8](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1672977069, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 06 03:51:21 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1672977069, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 06 03:51:21 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.118: ISSUE: authtime 1672977068, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 06 03:51:29 kdc krb5kdc[8](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1672977089, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 06 03:51:42 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.104: ISSUE: authtime 1672977102, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 06 03:51:42 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.102: ISSUE: authtime 1672977102, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 06 03:51:43 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.103: ISSUE: authtime 1672977103, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 06 03:51:49 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.113: ISSUE: authtime 1672977109, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 06 03:51:49 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.111: ISSUE: authtime 1672977109, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 06 03:51:50 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.112: ISSUE: authtime 1672977110, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 06 03:51:55 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.111: ISSUE: authtime 1672977109, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 06 03:51:55 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.113: ISSUE: authtime 1672977109, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 06 03:51:56 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.112: ISSUE: authtime 1672977110, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 06 03:52:00 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.103: ISSUE: authtime 1672977103, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 06 03:52:00 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.104: ISSUE: authtime 1672977102, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 06 03:52:01 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.102: ISSUE: authtime 1672977102, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 06 03:52:26 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1672977089, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 06 03:52:34 kdc krb5kdc[8](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1672977154, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 06 03:52:38 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.102: ISSUE: authtime 1672977102, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for recon/recon@EXAMPLE.COM
kdc_1        | Jan 06 03:52:38 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.104: ISSUE: authtime 1672977102, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for recon/recon@EXAMPLE.COM
kdc_1        | Jan 06 03:52:39 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.103: ISSUE: authtime 1672977103, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for recon/recon@EXAMPLE.COM
kdc_1        | Jan 06 03:52:43 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.111: ISSUE: authtime 1672977163, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 06 03:52:43 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.113: ISSUE: authtime 1672977163, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 06 03:52:45 kdc krb5kdc[8](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.112: ISSUE: authtime 1672977165, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 06 03:52:49 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.111: ISSUE: authtime 1672977163, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 06 03:52:49 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.113: ISSUE: authtime 1672977163, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 06 03:52:50 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.112: ISSUE: authtime 1672977165, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 06 03:53:08 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1672977154, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 06 03:53:13 kdc krb5kdc[8](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1672977193, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 06 03:53:23 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.115: ISSUE: authtime 1672976996, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, recon/recon@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jan 06 03:53:38 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.115: LOOKING_UP_SERVER: authtime 0, etypes {rep=UNSUPPORTED:(0)} recon/recon@EXAMPLE.COM for HTTP/om1@EXAMPLE.COM, Server not found in Kerberos database
kdc_1        | Jan 06 03:53:38 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.115: LOOKING_UP_SERVER: authtime 0, etypes {rep=UNSUPPORTED:(0)} recon/recon@EXAMPLE.COM for HTTP/om1@EXAMPLE.COM, Server not found in Kerberos database
kdc_1        | Jan 06 03:53:38 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.115: LOOKING_UP_SERVER: authtime 0, etypes {rep=UNSUPPORTED:(0)} recon/recon@EXAMPLE.COM for HTTP/om1@EXAMPLE.COM, Server not found in Kerberos database
kdc_1        | Jan 06 03:53:38 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.115: LOOKING_UP_SERVER: authtime 0, etypes {rep=UNSUPPORTED:(0)} recon/recon@EXAMPLE.COM for HTTP/om1@EXAMPLE.COM, Server not found in Kerberos database
kdc_1        | Jan 06 03:53:40 kdc krb5kdc[8](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1672977193, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kms_1        | Sleeping for 5 seconds
kms_1        | WARNING: /opt/hadoop/temp does not exist. Creating.
datanode1_1  | SubjectAlternativeName [
datanode1_1  |   IPAddress: 172.25.0.102
datanode1_1  | ]
datanode1_1  | 
datanode1_1  | ]
datanode1_1  |   Algorithm: [SHA256withRSA]
datanode1_1  |   Signature:
datanode1_1  | 0000: 69 3B 17 32 98 66 05 ED   85 9F 71 2A 65 80 C7 96  i;.2.f....q*e...
datanode1_1  | 0010: 23 8E 58 F0 F8 DD 6B 6A   EE C8 09 84 FE BE 8E DF  #.X...kj........
datanode1_1  | 0020: 28 CF 34 6A 9C 02 5E 2F   35 6E FF 51 8D 16 AF E0  (.4j..^/5n.Q....
datanode1_1  | 0030: A0 14 B1 B6 25 A0 98 CA   8F 98 E2 25 F8 67 E9 A6  ....%......%.g..
datanode1_1  | 0040: CD F0 CC B1 38 67 ED 3E   A1 BE 4B 5A 23 40 93 44  ....8g.>..KZ#@.D
datanode1_1  | 0050: D4 F0 A2 B3 66 25 FF C4   06 4E 38 37 E2 84 50 BA  ....f%...N87..P.
datanode1_1  | 0060: E7 A5 56 3D C0 70 3E 4D   50 FD 11 1A E2 8A F8 9F  ..V=.p>MP.......
datanode1_1  | 0070: 3F 3B 8C DE F3 D0 1A 88   E6 0C 01 0D AA 47 D4 61  ?;...........G.a
datanode1_1  | 0080: 36 E2 33 43 32 81 4A 5A   B5 85 B2 9F B4 96 90 E0  6.3C2.JZ........
datanode1_1  | 0090: CB 1E FF 3D D2 8D A4 3F   2F 62 C1 BB 1F 4A 82 31  ...=...?/b...J.1
datanode1_1  | 00A0: DA F6 67 79 47 11 DB 53   E9 65 93 B0 4A 96 C0 56  ..gyG..S.e..J..V
datanode1_1  | 00B0: AC A9 80 B9 53 E6 D2 76   5C 85 2A DA 9D 6C 16 CA  ....S..v\.*..l..
datanode1_1  | 00C0: 7F C3 CF 18 27 07 86 72   AD EF 73 7C 08 D1 D1 FA  ....'..r..s.....
datanode1_1  | 00D0: FB BA F0 47 1A 34 F5 A1   9E 28 11 E7 73 85 BE 62  ...G.4...(..s..b
datanode1_1  | 00E0: 38 45 58 ED 29 DB 08 35   B6 A5 40 DC 45 EB 40 85  8EX.)..5..@.E.@.
datanode1_1  | 00F0: 65 AC 28 0B 86 34 1E 06   23 CE BE 34 21 4D D7 1A  e.(..4..#..4!M..
datanode1_1  | 
datanode1_1  | ] from file:/data/metadata/dn/certs/1454579200080.crt.
datanode1_1  | 2023-01-06 03:52:04,640 [main] INFO client.DNCertificateClient: CertificateLifetimeMonitor is started with first delay 29102875383 ms and interval 86400000 ms.
datanode1_1  | 2023-01-06 03:52:04,641 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode1_1  | 2023-01-06 03:52:04,852 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode1_1  | 2023-01-06 03:52:06,421 [main] INFO reflections.Reflections: Reflections took 1234 ms to scan 2 urls, producing 97 keys and 217 values 
datanode1_1  | 2023-01-06 03:52:07,177 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
datanode1_1  | 2023-01-06 03:52:09,291 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode1_1  | 2023-01-06 03:52:09,576 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode1_1  | 2023-01-06 03:52:09,640 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode1_1  | 2023-01-06 03:52:09,654 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode1_1  | 2023-01-06 03:52:10,139 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode1_1  | 2023-01-06 03:52:10,314 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode1_1  | 2023-01-06 03:52:10,332 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode1_1  | 2023-01-06 03:52:10,335 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode1_1  | 2023-01-06 03:52:10,338 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode1_1  | 2023-01-06 03:52:10,340 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode1_1  | 2023-01-06 03:52:10,700 [Thread-8] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode1_1  | 2023-01-06 03:52:10,707 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode1_1  | 2023-01-06 03:52:16,977 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode1_1  | 2023-01-06 03:52:19,019 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
datanode1_1  | 2023-01-06 03:52:19,115 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig1-
datanode1_1  | 2023-01-06 03:52:19,405 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode1_1  | 2023-01-06 03:52:19,757 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode1_1  | 2023-01-06 03:52:20,298 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode1_1  | 2023-01-06 03:52:20,307 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode1_1  | 2023-01-06 03:52:20,311 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode1_1  | 2023-01-06 03:52:20,312 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode1_1  | 2023-01-06 03:52:20,313 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode1_1  | 2023-01-06 03:52:20,315 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode1_1  | 2023-01-06 03:52:20,318 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode1_1  | 2023-01-06 03:52:20,324 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-01-06 03:52:20,330 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode1_1  | 2023-01-06 03:52:20,337 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode1_1  | 2023-01-06 03:52:20,471 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode1_1  | 2023-01-06 03:52:20,569 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode1_1  | 2023-01-06 03:52:20,583 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode1_1  | 2023-01-06 03:52:27,141 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode1_1  | 2023-01-06 03:52:27,338 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode1_1  | 2023-01-06 03:52:27,345 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
datanode1_1  | 2023-01-06 03:52:27,352 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode1_1  | 2023-01-06 03:52:27,359 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode1_1  | 2023-01-06 03:52:27,391 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode1_1  | 2023-01-06 03:52:27,400 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode1_1  | 2023-01-06 03:52:27,470 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode1_1  | 2023-01-06 03:52:27,476 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
datanode1_1  | 2023-01-06 03:52:27,656 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode1_1  | 2023-01-06 03:52:27,658 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
datanode1_1  | 2023-01-06 03:52:28,051 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode1_1  | 2023-01-06 03:52:28,054 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode1_1  | 2023-01-06 03:52:28,065 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2023-01-06 03:52:28,065 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2023-01-06 03:52:28,120 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2023-01-06 03:52:28,265 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xac7fd765] REGISTERED
datanode1_1  | 2023-01-06 03:52:28,279 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xac7fd765] BIND: 0.0.0.0/0.0.0.0:0
datanode1_1  | 2023-01-06 03:52:28,304 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xac7fd765, L:/0.0.0.0:38749] ACTIVE
datanode1_1  | 2023-01-06 03:52:28,538 [main] INFO ssl.PemFileBasedKeyStoresFactory: SERVER KeyStore reloading at 60000 millis.
datanode1_1  | 2023-01-06 03:52:28,580 [main] INFO ssl.PemFileBasedKeyStoresFactory: SERVER TrustStore reloading at 60000 millis.
datanode1_1  | 2023-01-06 03:52:28,658 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode1_1  | 2023-01-06 03:52:30,198 [main] INFO token.OzoneBlockTokenSecretManager: Updating the current master key for generating tokens
datanode1_1  | 2023-01-06 03:52:30,227 [main] INFO token.ContainerTokenSecretManager: Updating the current master key for generating tokens
datanode1_1  | 2023-01-06 03:52:30,975 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode3_1  | 2023-01-06 03:52:09,098 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode3_1  | 2023-01-06 03:52:09,116 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode3_1  | 2023-01-06 03:52:09,118 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode3_1  | 2023-01-06 03:52:09,122 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode3_1  | 2023-01-06 03:52:09,123 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode3_1  | 2023-01-06 03:52:09,408 [Thread-8] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode3_1  | 2023-01-06 03:52:09,409 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode3_1  | 2023-01-06 03:52:16,437 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode3_1  | 2023-01-06 03:52:18,468 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
datanode3_1  | 2023-01-06 03:52:18,601 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig1-
datanode3_1  | 2023-01-06 03:52:19,251 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode3_1  | 2023-01-06 03:52:19,806 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode3_1  | 2023-01-06 03:52:20,469 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode3_1  | 2023-01-06 03:52:20,481 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode3_1  | 2023-01-06 03:52:20,488 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode3_1  | 2023-01-06 03:52:20,488 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode3_1  | 2023-01-06 03:52:20,493 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode3_1  | 2023-01-06 03:52:20,493 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode3_1  | 2023-01-06 03:52:20,497 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode3_1  | 2023-01-06 03:52:20,514 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-01-06 03:52:20,517 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode3_1  | 2023-01-06 03:52:20,524 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode3_1  | 2023-01-06 03:52:20,617 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode3_1  | 2023-01-06 03:52:20,655 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode3_1  | 2023-01-06 03:52:20,655 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode3_1  | 2023-01-06 03:52:27,777 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode3_1  | 2023-01-06 03:52:27,953 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode3_1  | 2023-01-06 03:52:27,961 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
datanode3_1  | 2023-01-06 03:52:27,965 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode3_1  | 2023-01-06 03:52:27,974 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode3_1  | 2023-01-06 03:52:27,997 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode3_1  | 2023-01-06 03:52:28,007 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode3_1  | 2023-01-06 03:52:28,077 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode3_1  | 2023-01-06 03:52:28,086 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
datanode3_1  | 2023-01-06 03:52:28,503 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode3_1  | 2023-01-06 03:52:28,509 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
datanode3_1  | 2023-01-06 03:52:28,930 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode3_1  | 2023-01-06 03:52:28,930 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode3_1  | 2023-01-06 03:52:28,930 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2023-01-06 03:52:28,934 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2023-01-06 03:52:29,038 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2023-01-06 03:52:29,060 [678ebbfa-a167-4ea6-8846-1b1f5c050dad-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x81c2de4f] REGISTERED
datanode3_1  | 2023-01-06 03:52:29,070 [678ebbfa-a167-4ea6-8846-1b1f5c050dad-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x81c2de4f] BIND: 0.0.0.0/0.0.0.0:0
datanode3_1  | 2023-01-06 03:52:29,104 [678ebbfa-a167-4ea6-8846-1b1f5c050dad-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x81c2de4f, L:/0.0.0.0:41441] ACTIVE
datanode3_1  | 2023-01-06 03:52:29,356 [main] INFO ssl.PemFileBasedKeyStoresFactory: SERVER KeyStore reloading at 60000 millis.
datanode3_1  | 2023-01-06 03:52:29,420 [main] INFO ssl.PemFileBasedKeyStoresFactory: SERVER TrustStore reloading at 60000 millis.
datanode3_1  | 2023-01-06 03:52:29,483 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode3_1  | 2023-01-06 03:52:30,663 [main] INFO token.OzoneBlockTokenSecretManager: Updating the current master key for generating tokens
datanode3_1  | 2023-01-06 03:52:30,696 [main] INFO token.ContainerTokenSecretManager: Updating the current master key for generating tokens
datanode3_1  | 2023-01-06 03:52:31,441 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode3_1  | 2023-01-06 03:52:31,442 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
datanode3_1  | 2023-01-06 03:52:31,444 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.datanode.http.auth.type = kerberos
datanode3_1  | 2023-01-06 03:52:31,799 [main] INFO util.log: Logging initialized @72235ms to org.eclipse.jetty.util.log.Slf4jLog
datanode3_1  | 2023-01-06 03:52:32,790 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode3_1  | 2023-01-06 03:52:32,869 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode3_1  | 2023-01-06 03:52:32,886 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode3_1  | 2023-01-06 03:52:32,888 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode3_1  | 2023-01-06 03:52:32,891 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode3_1  | 2023-01-06 03:52:32,915 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.datanode.http.auth.kerberos.principal keytabKey: hdds.datanode.http.auth.kerberos.keytab
datanode3_1  | 2023-01-06 03:52:33,465 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode3_1  | 2023-01-06 03:52:33,501 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode3_1  | 2023-01-06 03:52:34,200 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode3_1  | 2023-01-06 03:52:34,203 [main] INFO server.session: No SessionScavenger set, using defaults
datanode2_1  | 2023-01-06 03:52:07,941 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode2_1  | 2023-01-06 03:52:08,037 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode2_1  | 2023-01-06 03:52:08,087 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode2_1  | 2023-01-06 03:52:08,101 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
om1_1        | Sleeping for 5 seconds
om1_1        | Waiting for the service scm3.org:9894
om1_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1        | 2023-01-06 03:51:32,097 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1        | /************************************************************
om1_1        | STARTUP_MSG: Starting OzoneManager
om1_1        | STARTUP_MSG:   host = om1/172.25.0.111
om1_1        | STARTUP_MSG:   args = [--init]
om1_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om1_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om1_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/9d30c9a7ce375cd614e86022850c7c128723df6b ; compiled by 'runner' on 2023-01-06T03:19Z
om1_1        | STARTUP_MSG:   java = 11.0.14.1
om1_1        | ************************************************************/
om1_1        | 2023-01-06 03:51:32,238 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1        | 2023-01-06 03:51:41,469 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1        | 2023-01-06 03:51:46,091 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om1_1        | 2023-01-06 03:51:47,106 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1        | 2023-01-06 03:51:47,108 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om1: om1
om1_1        | 2023-01-06 03:51:47,110 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om1: om1
om1_1        | 2023-01-06 03:51:50,134 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om1_1        | 2023-01-06 03:51:50,135 [main] INFO om.OzoneManager: Ozone Manager login successful.
om1_1        | 2023-01-06 03:51:50,337 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-01-06 03:51:51,767 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om1_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3;layoutVersion=3
om1_1        | 2023-01-06 03:51:56,597 [main] INFO om.OzoneManager: OM storage initialized. Initializing security
om1_1        | 2023-01-06 03:51:56,597 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om1_1        | 2023-01-06 03:52:02,371 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om1_1        | value: 9862
om1_1        | ]
om1_1        | 2023-01-06 03:52:02,442 [main] ERROR security.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om1_1        | 2023-01-06 03:52:02,458 [main] INFO security.OMCertificateClient: Certificate client init case: 0
om1_1        | 2023-01-06 03:52:02,468 [main] INFO security.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om1_1        | 2023-01-06 03:52:10,517 [main] INFO om.OzoneManager: Init response: GETCERT
om1_1        | 2023-01-06 03:52:10,871 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.111,host:om1
om1_1        | 2023-01-06 03:52:10,878 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om1_1        | 2023-01-06 03:52:10,897 [main] ERROR security.OMCertificateClient: Invalid domain om1
om1_1        | 2023-01-06 03:52:10,904 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om1_1        | 2023-01-06 03:52:10,916 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1        | 2023-01-06 03:52:10,919 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om1: om1
om1_1        | 2023-01-06 03:52:10,956 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om1: om1
om1_1        | 2023-01-06 03:52:10,973 [main] INFO security.OMCertificateClient: Creating csr for OM->dns:om1,ip:172.25.0.111,scmId:e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674,clusterId:CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3,subject:om1
om1_1        | 2023-01-06 03:52:13,748 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
om1_1        | 2023-01-06 03:52:13,834 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om1_1        | /************************************************************
om1_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om1/172.25.0.111
om1_1        | ************************************************************/
om1_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode2_1  | 2023-01-06 03:52:08,105 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode2_1  | 2023-01-06 03:52:08,378 [Thread-8] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode2_1  | 2023-01-06 03:52:08,382 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode2_1  | 2023-01-06 03:52:15,261 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode2_1  | 2023-01-06 03:52:16,924 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
datanode2_1  | 2023-01-06 03:52:17,040 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig1-
datanode2_1  | 2023-01-06 03:52:17,523 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode2_1  | 2023-01-06 03:52:18,052 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode2_1  | 2023-01-06 03:52:19,328 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode2_1  | 2023-01-06 03:52:19,341 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode2_1  | 2023-01-06 03:52:19,404 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode2_1  | 2023-01-06 03:52:19,414 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode2_1  | 2023-01-06 03:52:19,422 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode2_1  | 2023-01-06 03:52:19,428 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode2_1  | 2023-01-06 03:52:19,446 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode2_1  | 2023-01-06 03:52:19,468 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-01-06 03:52:19,472 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode2_1  | 2023-01-06 03:52:19,480 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode2_1  | 2023-01-06 03:52:19,686 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode2_1  | 2023-01-06 03:52:19,715 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode2_1  | 2023-01-06 03:52:19,716 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode2_1  | 2023-01-06 03:52:27,964 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode2_1  | 2023-01-06 03:52:28,188 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode2_1  | 2023-01-06 03:52:28,210 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
datanode2_1  | 2023-01-06 03:52:28,219 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode2_1  | 2023-01-06 03:52:28,234 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode2_1  | 2023-01-06 03:52:28,421 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode2_1  | 2023-01-06 03:52:28,424 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode2_1  | 2023-01-06 03:52:28,517 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode2_1  | 2023-01-06 03:52:28,528 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
datanode2_1  | 2023-01-06 03:52:28,895 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode2_1  | 2023-01-06 03:52:28,907 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
datanode2_1  | 2023-01-06 03:52:29,260 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode2_1  | 2023-01-06 03:52:29,261 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode2_1  | 2023-01-06 03:52:29,261 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2023-01-06 03:52:29,265 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2023-01-06 03:52:29,351 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2023-01-06 03:52:29,441 [f7a31870-78ff-406d-a144-7fe17a627699-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xc8da4724] REGISTERED
datanode2_1  | 2023-01-06 03:52:29,466 [f7a31870-78ff-406d-a144-7fe17a627699-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xc8da4724] BIND: 0.0.0.0/0.0.0.0:0
datanode2_1  | 2023-01-06 03:52:29,508 [f7a31870-78ff-406d-a144-7fe17a627699-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xc8da4724, L:/0.0.0.0:38687] ACTIVE
datanode2_1  | 2023-01-06 03:52:29,805 [main] INFO ssl.PemFileBasedKeyStoresFactory: SERVER KeyStore reloading at 60000 millis.
datanode2_1  | 2023-01-06 03:52:29,848 [main] INFO ssl.PemFileBasedKeyStoresFactory: SERVER TrustStore reloading at 60000 millis.
datanode2_1  | 2023-01-06 03:52:29,928 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode2_1  | 2023-01-06 03:52:31,410 [main] INFO token.OzoneBlockTokenSecretManager: Updating the current master key for generating tokens
datanode2_1  | 2023-01-06 03:52:31,469 [main] INFO token.ContainerTokenSecretManager: Updating the current master key for generating tokens
datanode2_1  | 2023-01-06 03:52:31,967 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode2_1  | 2023-01-06 03:52:31,968 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
datanode2_1  | 2023-01-06 03:52:31,971 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.datanode.http.auth.type = kerberos
datanode2_1  | 2023-01-06 03:52:32,158 [main] INFO util.log: Logging initialized @71779ms to org.eclipse.jetty.util.log.Slf4jLog
datanode2_1  | 2023-01-06 03:52:32,918 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode2_1  | 2023-01-06 03:52:33,002 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode2_1  | 2023-01-06 03:52:33,012 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode2_1  | 2023-01-06 03:52:33,017 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode2_1  | 2023-01-06 03:52:33,024 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode2_1  | 2023-01-06 03:52:33,062 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.datanode.http.auth.kerberos.principal keytabKey: hdds.datanode.http.auth.kerberos.keytab
datanode2_1  | 2023-01-06 03:52:34,133 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode2_1  | 2023-01-06 03:52:34,163 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode2_1  | 2023-01-06 03:52:34,602 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode2_1  | 2023-01-06 03:52:34,609 [main] INFO server.session: No SessionScavenger set, using defaults
datanode1_1  | 2023-01-06 03:52:30,976 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
datanode1_1  | 2023-01-06 03:52:30,976 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.datanode.http.auth.type = kerberos
datanode1_1  | 2023-01-06 03:52:31,392 [main] INFO util.log: Logging initialized @71657ms to org.eclipse.jetty.util.log.Slf4jLog
datanode1_1  | 2023-01-06 03:52:32,353 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode1_1  | 2023-01-06 03:52:32,422 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode1_1  | 2023-01-06 03:52:32,430 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode1_1  | 2023-01-06 03:52:32,431 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode1_1  | 2023-01-06 03:52:32,431 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode1_1  | 2023-01-06 03:52:32,443 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.datanode.http.auth.kerberos.principal keytabKey: hdds.datanode.http.auth.kerberos.keytab
datanode1_1  | 2023-01-06 03:52:32,770 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode1_1  | 2023-01-06 03:52:32,783 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode1_1  | 2023-01-06 03:52:33,029 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode1_1  | 2023-01-06 03:52:33,029 [main] INFO server.session: No SessionScavenger set, using defaults
datanode1_1  | 2023-01-06 03:52:33,047 [main] INFO server.session: node0 Scavenging every 660000ms
datanode1_1  | 2023-01-06 03:52:33,762 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode1_1  | 2023-01-06 03:52:33,878 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@598f065f{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode1_1  | 2023-01-06 03:52:33,879 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5fd0235c{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode1_1  | 2023-01-06 03:52:34,764 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode1_1  | 2023-01-06 03:52:34,864 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2771e501{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-11531884890186108607/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode1_1  | 2023-01-06 03:52:34,948 [main] INFO server.AbstractConnector: Started ServerConnector@1915879c{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode1_1  | 2023-01-06 03:52:34,948 [main] INFO server.Server: Started @75217ms
datanode1_1  | 2023-01-06 03:52:34,966 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode1_1  | 2023-01-06 03:52:34,966 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode1_1  | 2023-01-06 03:52:34,978 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode1_1  | 2023-01-06 03:52:35,002 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode1_1  | 2023-01-06 03:52:35,564 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@59cb818e] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode1_1  | 2023-01-06 03:52:36,172 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.25.0.115:9891
datanode1_1  | 2023-01-06 03:52:36,245 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode1_1  | 2023-01-06 03:52:41,315 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3/DS-45ff6326-01c5-4555-825d-b49ff08138b5/container.db to cache
datanode1_1  | 2023-01-06 03:52:41,324 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3/DS-45ff6326-01c5-4555-825d-b49ff08138b5/container.db for volume DS-45ff6326-01c5-4555-825d-b49ff08138b5
datanode1_1  | 2023-01-06 03:52:41,327 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode1_1  | 2023-01-06 03:52:41,369 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode1_1  | 2023-01-06 03:52:41,465 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode1_1  | 2023-01-06 03:52:41,478 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86
datanode1_1  | 2023-01-06 03:52:41,781 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode1_1  | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:309)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:509)
datanode1_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode1_1  | Caused by: java.util.concurrent.TimeoutException
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode1_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode1_1  | 	... 1 more
datanode1_1  | 2023-01-06 03:52:41,918 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.RaftServer: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: start RPC server
datanode1_1  | 2023-01-06 03:52:41,966 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.GrpcService: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: GrpcService started, listening on 9858
datanode1_1  | 2023-01-06 03:52:41,982 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.GrpcService: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: GrpcService started, listening on 9856
datanode1_1  | 2023-01-06 03:52:41,983 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.GrpcService: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: GrpcService started, listening on 9857
datanode1_1  | 2023-01-06 03:52:42,034 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86 is started using port 9858 for RATIS
datanode1_1  | 2023-01-06 03:52:42,038 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86 is started using port 9857 for RATIS_ADMIN
datanode1_1  | 2023-01-06 03:52:42,038 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86 is started using port 9856 for RATIS_SERVER
datanode1_1  | 2023-01-06 03:52:42,039 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: Started
datanode1_1  | 2023-01-06 03:52:42,115 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode1_1  | 2023-01-06 03:52:42,115 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode3_1  | 2023-01-06 03:52:34,205 [main] INFO server.session: node0 Scavenging every 660000ms
datanode3_1  | 2023-01-06 03:52:34,509 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode3_1  | 2023-01-06 03:52:34,542 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4ceac22d{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode3_1  | 2023-01-06 03:52:34,553 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3868c92e{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode3_1  | 2023-01-06 03:52:35,351 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode3_1  | 2023-01-06 03:52:35,463 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@30b91493{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-6715728864081702971/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode3_1  | 2023-01-06 03:52:35,561 [main] INFO server.AbstractConnector: Started ServerConnector@7dc16d33{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode3_1  | 2023-01-06 03:52:35,562 [main] INFO server.Server: Started @75998ms
datanode3_1  | 2023-01-06 03:52:35,587 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode3_1  | 2023-01-06 03:52:35,588 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode3_1  | 2023-01-06 03:52:35,595 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode3_1  | 2023-01-06 03:52:35,603 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode3_1  | 2023-01-06 03:52:35,810 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2488cb71] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode3_1  | 2023-01-06 03:52:36,516 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.25.0.115:9891
datanode3_1  | 2023-01-06 03:52:36,580 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode3_1  | 2023-01-06 03:52:41,289 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3/DS-1c2dffc7-5213-4712-bd8f-cca21f0aa802/container.db to cache
datanode3_1  | 2023-01-06 03:52:41,289 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3/DS-1c2dffc7-5213-4712-bd8f-cca21f0aa802/container.db for volume DS-1c2dffc7-5213-4712-bd8f-cca21f0aa802
datanode3_1  | 2023-01-06 03:52:41,297 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode3_1  | 2023-01-06 03:52:41,309 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode3_1  | 2023-01-06 03:52:41,464 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode3_1  | 2023-01-06 03:52:41,502 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 678ebbfa-a167-4ea6-8846-1b1f5c050dad
datanode3_1  | 2023-01-06 03:52:41,874 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.RaftServer: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: start RPC server
datanode3_1  | 2023-01-06 03:52:41,906 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: GrpcService started, listening on 9858
datanode3_1  | 2023-01-06 03:52:41,964 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: GrpcService started, listening on 9856
datanode3_1  | 2023-01-06 03:52:42,033 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: GrpcService started, listening on 9857
datanode3_1  | 2023-01-06 03:52:42,085 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 678ebbfa-a167-4ea6-8846-1b1f5c050dad is started using port 9858 for RATIS
datanode3_1  | 2023-01-06 03:52:42,085 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 678ebbfa-a167-4ea6-8846-1b1f5c050dad is started using port 9857 for RATIS_ADMIN
datanode3_1  | 2023-01-06 03:52:42,090 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 678ebbfa-a167-4ea6-8846-1b1f5c050dad is started using port 9856 for RATIS_SERVER
datanode3_1  | 2023-01-06 03:52:42,093 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-678ebbfa-a167-4ea6-8846-1b1f5c050dad: Started
datanode3_1  | 2023-01-06 03:52:42,335 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode3_1  | 2023-01-06 03:52:42,335 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode3_1  | 2023-01-06 03:53:17,407 [Command processor thread] INFO server.RaftServer: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: addNew group-5839005B1100:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER] returns group-5839005B1100:java.util.concurrent.CompletableFuture@3df79aed[Not completed]
datanode3_1  | 2023-01-06 03:53:17,714 [pool-24-thread-1] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: new RaftServerImpl for group-5839005B1100:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode3_1  | 2023-01-06 03:53:17,732 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode3_1  | 2023-01-06 03:53:17,740 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode3_1  | 2023-01-06 03:53:17,742 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode3_1  | 2023-01-06 03:53:17,746 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2023-01-06 03:53:17,746 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2023-01-06 03:53:17,750 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om1_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1        | 2023-01-06 03:52:26,786 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1        | /************************************************************
om1_1        | STARTUP_MSG: Starting OzoneManager
om1_1        | STARTUP_MSG:   host = om1/172.25.0.111
om1_1        | STARTUP_MSG:   args = []
om1_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om1_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om1_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/9d30c9a7ce375cd614e86022850c7c128723df6b ; compiled by 'runner' on 2023-01-06T03:19Z
om1_1        | STARTUP_MSG:   java = 11.0.14.1
om1_1        | ************************************************************/
om1_1        | 2023-01-06 03:52:26,855 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1        | 2023-01-06 03:52:35,490 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1        | 2023-01-06 03:52:39,063 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om1_1        | 2023-01-06 03:52:39,529 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1        | 2023-01-06 03:52:39,529 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om1: om1
om1_1        | 2023-01-06 03:52:39,529 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om1: om1
om1_1        | 2023-01-06 03:52:39,641 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-01-06 03:52:39,942 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om1_1        | 2023-01-06 03:52:41,887 [main] INFO reflections.Reflections: Reflections took 1571 ms to scan 1 urls, producing 115 keys and 336 values [using 2 cores]
om1_1        | 2023-01-06 03:52:43,806 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om1_1        | 2023-01-06 03:52:43,807 [main] INFO om.OzoneManager: Ozone Manager login successful.
om1_1        | 2023-01-06 03:52:43,807 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-01-06 03:52:47,725 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om1_1        | 2023-01-06 03:52:48,374 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om1_1        | 2023-01-06 03:52:52,650 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om1_1        | value: 9862
om1_1        | ]
om1_1        | 2023-01-06 03:52:52,688 [main] INFO security.OMCertificateClient: Loading certificate from location:/data/metadata/om/certs.
om1_1        | 2023-01-06 03:52:53,378 [main] INFO security.OMCertificateClient: Added certificate [
om1_1        | [
om1_1        |   Version: V3
om1_1        |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
om1_1        |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
om1_1        | 
om1_1        |   Key:  Sun RSA public key, 2048 bits
om1_1        |   params: null
om1_1        |   modulus: 22321520092034904956367956777931957676969911078232901017895933698678468781753172695844393013092965970615602622357794023639048732305076160434218405355746465972426985604516863422070797512086951139731951669174461346685083874381071907314493811292043757517828199039562222504144931947786863316592495947302761724410796725398590551213642383206197662885192142834321323519855792963024540129867342534976672519842084316567886340381000959465926087773440860829229209280683297485436375405886176389600891889084968589860808245139826791285487072108592486434566834307409916351251604701981140849210029026671497960989540995516629779904417
om1_1        |   public exponent: 65537
om1_1        |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
om1_1        |                To: Mon Feb 14 00:00:00 UTC 2028]
om1_1        |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
om1_1        |   SerialNumber: [    01]
om1_1        | 
om1_1        | Certificate Extensions: 3
om1_1        | [1]: ObjectId: 2.5.29.19 Criticality=true
om1_1        | BasicConstraints:[
om1_1        |   CA:true
om1_1        |   PathLen:2147483647
om1_1        | ]
om1_1        | 
om1_1        | [2]: ObjectId: 2.5.29.15 Criticality=true
om1_1        | KeyUsage [
om1_1        |   Key_CertSign
om1_1        |   Crl_Sign
om1_1        | ]
om1_1        | 
om1_1        | [3]: ObjectId: 2.5.29.17 Criticality=false
om1_1        | SubjectAlternativeName [
om1_1        |   IPAddress: 172.25.0.116
om1_1        |   DNSName: scm1.org
om1_1        | ]
om1_1        | 
om1_1        | ]
om1_1        |   Algorithm: [SHA256withRSA]
om1_1        |   Signature:
om1_1        | 0000: 21 07 40 68 BD F5 21 63   53 A4 1C 6D 61 63 C4 B6  !.@h..!cS..mac..
om1_1        | 0010: E3 BD 40 F3 9B FD 99 A0   FE 5C 7B FD 06 FB 50 8E  ..@......\....P.
datanode2_1  | 2023-01-06 03:52:34,636 [main] INFO server.session: node0 Scavenging every 660000ms
datanode2_1  | 2023-01-06 03:52:34,843 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode2_1  | 2023-01-06 03:52:34,865 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@598f065f{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode2_1  | 2023-01-06 03:52:34,878 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5fd0235c{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode2_1  | 2023-01-06 03:52:35,856 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode2_1  | 2023-01-06 03:52:35,954 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2771e501{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-7670581680259330553/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode2_1  | 2023-01-06 03:52:36,018 [main] INFO server.AbstractConnector: Started ServerConnector@1915879c{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode2_1  | 2023-01-06 03:52:36,018 [main] INFO server.Server: Started @75638ms
datanode2_1  | 2023-01-06 03:52:36,042 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode2_1  | 2023-01-06 03:52:36,042 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode2_1  | 2023-01-06 03:52:36,053 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode2_1  | 2023-01-06 03:52:36,077 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode2_1  | 2023-01-06 03:52:36,462 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@6ee7298b] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode2_1  | 2023-01-06 03:52:37,007 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.25.0.115:9891
datanode2_1  | 2023-01-06 03:52:37,067 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode2_1  | 2023-01-06 03:52:41,396 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3/DS-baaa98e8-d5de-4f7a-b247-9675a5e9ed50/container.db to cache
datanode2_1  | 2023-01-06 03:52:41,407 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3/DS-baaa98e8-d5de-4f7a-b247-9675a5e9ed50/container.db for volume DS-baaa98e8-d5de-4f7a-b247-9675a5e9ed50
datanode2_1  | 2023-01-06 03:52:41,411 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode2_1  | 2023-01-06 03:52:41,443 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode2_1  | 2023-01-06 03:52:41,681 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode2_1  | 2023-01-06 03:52:41,702 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis f7a31870-78ff-406d-a144-7fe17a627699
datanode2_1  | 2023-01-06 03:52:42,031 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.RaftServer: f7a31870-78ff-406d-a144-7fe17a627699: start RPC server
datanode2_1  | 2023-01-06 03:52:42,055 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.GrpcService: f7a31870-78ff-406d-a144-7fe17a627699: GrpcService started, listening on 9858
datanode2_1  | 2023-01-06 03:52:42,077 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.GrpcService: f7a31870-78ff-406d-a144-7fe17a627699: GrpcService started, listening on 9856
datanode2_1  | 2023-01-06 03:52:42,098 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.GrpcService: f7a31870-78ff-406d-a144-7fe17a627699: GrpcService started, listening on 9857
datanode2_1  | 2023-01-06 03:52:42,134 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f7a31870-78ff-406d-a144-7fe17a627699 is started using port 9858 for RATIS
datanode2_1  | 2023-01-06 03:52:42,135 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f7a31870-78ff-406d-a144-7fe17a627699 is started using port 9857 for RATIS_ADMIN
datanode2_1  | 2023-01-06 03:52:42,135 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f7a31870-78ff-406d-a144-7fe17a627699 is started using port 9856 for RATIS_SERVER
datanode2_1  | 2023-01-06 03:52:42,135 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-f7a31870-78ff-406d-a144-7fe17a627699: Started
datanode2_1  | 2023-01-06 03:52:42,189 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode2_1  | 2023-01-06 03:52:42,189 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode2_1  | 2023-01-06 03:53:15,833 [Command processor thread] INFO server.RaftServer: f7a31870-78ff-406d-a144-7fe17a627699: addNew group-69810F3160D1:[f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER] returns group-69810F3160D1:java.util.concurrent.CompletableFuture@334b2791[Not completed]
datanode2_1  | 2023-01-06 03:53:16,101 [pool-24-thread-1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699: new RaftServerImpl for group-69810F3160D1:[f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode2_1  | 2023-01-06 03:53:16,111 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode2_1  | 2023-01-06 03:53:16,117 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode1_1  | 2023-01-06 03:53:17,438 [Command processor thread] INFO server.RaftServer: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: addNew group-5839005B1100:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER] returns group-5839005B1100:java.util.concurrent.CompletableFuture@23a2d17d[Not completed]
datanode1_1  | 2023-01-06 03:53:17,827 [pool-24-thread-1] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: new RaftServerImpl for group-5839005B1100:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode1_1  | 2023-01-06 03:53:17,828 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode1_1  | 2023-01-06 03:53:17,870 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode1_1  | 2023-01-06 03:53:17,871 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode1_1  | 2023-01-06 03:53:17,872 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2023-01-06 03:53:17,874 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2023-01-06 03:53:17,875 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode1_1  | 2023-01-06 03:53:17,984 [pool-24-thread-1] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100: ConfigurationManager, init=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode1_1  | 2023-01-06 03:53:18,000 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2023-01-06 03:53:18,092 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode1_1  | 2023-01-06 03:53:18,099 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode1_1  | 2023-01-06 03:53:18,240 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode1_1  | 2023-01-06 03:53:18,274 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode1_1  | 2023-01-06 03:53:18,278 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode1_1  | 2023-01-06 03:53:18,792 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2023-01-06 03:53:18,799 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode1_1  | 2023-01-06 03:53:18,805 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode1_1  | 2023-01-06 03:53:18,814 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode1_1  | 2023-01-06 03:53:18,820 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode1_1  | 2023-01-06 03:53:18,832 [pool-24-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/c742cece-fb27-47b9-b231-5839005b1100 does not exist. Creating ...
datanode1_1  | 2023-01-06 03:53:18,893 [pool-24-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/c742cece-fb27-47b9-b231-5839005b1100/in_use.lock acquired by nodename 7@98e33c1634d3
datanode1_1  | 2023-01-06 03:53:18,939 [pool-24-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/c742cece-fb27-47b9-b231-5839005b1100 has been successfully formatted.
datanode1_1  | 2023-01-06 03:53:18,987 [pool-24-thread-1] INFO ratis.ContainerStateMachine: group-5839005B1100: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode1_1  | 2023-01-06 03:53:19,046 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode1_1  | 2023-01-06 03:53:19,216 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode1_1  | 2023-01-06 03:53:19,218 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-01-06 03:53:19,231 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode1_1  | 2023-01-06 03:53:19,234 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode1_1  | 2023-01-06 03:53:19,267 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-01-06 03:53:19,310 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode2_1  | 2023-01-06 03:53:16,117 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode2_1  | 2023-01-06 03:53:16,119 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2023-01-06 03:53:16,120 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2023-01-06 03:53:16,122 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode2_1  | 2023-01-06 03:53:16,202 [pool-24-thread-1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1: ConfigurationManager, init=-1: peers:[f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode2_1  | 2023-01-06 03:53:16,205 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2023-01-06 03:53:16,260 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode3_1  | 2023-01-06 03:53:17,852 [pool-24-thread-1] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100: ConfigurationManager, init=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode3_1  | 2023-01-06 03:53:17,874 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2023-01-06 03:53:17,938 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode3_1  | 2023-01-06 03:53:17,947 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode3_1  | 2023-01-06 03:53:18,015 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode3_1  | 2023-01-06 03:53:18,067 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode3_1  | 2023-01-06 03:53:18,079 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode3_1  | 2023-01-06 03:53:18,484 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2023-01-06 03:53:18,484 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode3_1  | 2023-01-06 03:53:18,484 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode3_1  | 2023-01-06 03:53:18,485 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode3_1  | 2023-01-06 03:53:18,485 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode3_1  | 2023-01-06 03:53:18,494 [pool-24-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/c742cece-fb27-47b9-b231-5839005b1100 does not exist. Creating ...
datanode3_1  | 2023-01-06 03:53:18,542 [pool-24-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/c742cece-fb27-47b9-b231-5839005b1100/in_use.lock acquired by nodename 7@03b09732740a
datanode3_1  | 2023-01-06 03:53:18,598 [pool-24-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/c742cece-fb27-47b9-b231-5839005b1100 has been successfully formatted.
datanode3_1  | 2023-01-06 03:53:18,803 [pool-24-thread-1] INFO ratis.ContainerStateMachine: group-5839005B1100: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode3_1  | 2023-01-06 03:53:18,848 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode3_1  | 2023-01-06 03:53:19,082 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode3_1  | 2023-01-06 03:53:19,092 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-01-06 03:53:19,106 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode3_1  | 2023-01-06 03:53:19,110 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode3_1  | 2023-01-06 03:53:19,170 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-01-06 03:53:19,233 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode3_1  | 2023-01-06 03:53:19,233 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode3_1  | 2023-01-06 03:53:19,305 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: new 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/c742cece-fb27-47b9-b231-5839005b1100
datanode3_1  | 2023-01-06 03:53:19,314 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode3_1  | 2023-01-06 03:53:19,315 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode3_1  | 2023-01-06 03:53:19,324 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-01-06 03:53:19,324 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode3_1  | 2023-01-06 03:53:19,328 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode3_1  | 2023-01-06 03:53:19,336 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode3_1  | 2023-01-06 03:53:19,336 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode3_1  | 2023-01-06 03:53:19,336 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode3_1  | 2023-01-06 03:53:19,445 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode3_1  | 2023-01-06 03:53:19,457 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-01-06 03:53:19,853 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode3_1  | 2023-01-06 03:53:19,882 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode3_1  | 2023-01-06 03:53:19,898 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode3_1  | 2023-01-06 03:53:20,077 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-01-06 03:53:20,081 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-01-06 03:53:20,109 [pool-24-thread-1] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100: start as a follower, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-06 03:53:20,113 [pool-24-thread-1] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode3_1  | 2023-01-06 03:53:20,125 [pool-24-thread-1] INFO impl.RoleInfo: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: start 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState
om2_1        | Sleeping for 5 seconds
om2_1        | Waiting for the service scm3.org:9894
om2_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1        | 2023-01-06 03:51:34,520 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1        | /************************************************************
om2_1        | STARTUP_MSG: Starting OzoneManager
om2_1        | STARTUP_MSG:   host = om2/172.25.0.112
om2_1        | STARTUP_MSG:   args = [--init]
om2_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om2_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om2_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/9d30c9a7ce375cd614e86022850c7c128723df6b ; compiled by 'runner' on 2023-01-06T03:19Z
om2_1        | STARTUP_MSG:   java = 11.0.14.1
om2_1        | ************************************************************/
om2_1        | 2023-01-06 03:51:34,664 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1        | 2023-01-06 03:51:45,086 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1        | 2023-01-06 03:51:48,579 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om2_1        | 2023-01-06 03:51:49,197 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1        | 2023-01-06 03:51:49,201 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om2: om2
om2_1        | 2023-01-06 03:51:49,204 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om2: om2
om2_1        | 2023-01-06 03:51:51,496 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om2_1        | 2023-01-06 03:51:51,513 [main] INFO om.OzoneManager: Ozone Manager login successful.
om2_1        | 2023-01-06 03:51:51,670 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-01-06 03:51:52,947 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om2_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3;layoutVersion=3
om2_1        | 2023-01-06 03:51:57,438 [main] INFO om.OzoneManager: OM storage initialized. Initializing security
om2_1        | 2023-01-06 03:51:57,443 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om2_1        | 2023-01-06 03:52:02,386 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om2_1        | value: 9862
om2_1        | ]
om2_1        | 2023-01-06 03:52:02,449 [main] ERROR security.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om2_1        | 2023-01-06 03:52:02,458 [main] INFO security.OMCertificateClient: Certificate client init case: 0
om2_1        | 2023-01-06 03:52:02,473 [main] INFO security.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om2_1        | 2023-01-06 03:52:11,726 [main] INFO om.OzoneManager: Init response: GETCERT
om2_1        | 2023-01-06 03:52:12,204 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.112,host:om2
datanode2_1  | 2023-01-06 03:53:16,263 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode2_1  | 2023-01-06 03:53:16,336 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode2_1  | 2023-01-06 03:53:16,365 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode2_1  | 2023-01-06 03:53:16,367 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode2_1  | 2023-01-06 03:53:16,698 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2023-01-06 03:53:16,706 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode2_1  | 2023-01-06 03:53:16,707 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode2_1  | 2023-01-06 03:53:16,712 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode2_1  | 2023-01-06 03:53:16,714 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode2_1  | 2023-01-06 03:53:16,717 [pool-24-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5af4a3fc-47d6-497e-93af-69810f3160d1 does not exist. Creating ...
datanode2_1  | 2023-01-06 03:53:16,743 [pool-24-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5af4a3fc-47d6-497e-93af-69810f3160d1/in_use.lock acquired by nodename 7@47737eac8b7e
datanode2_1  | 2023-01-06 03:53:16,784 [pool-24-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5af4a3fc-47d6-497e-93af-69810f3160d1 has been successfully formatted.
datanode2_1  | 2023-01-06 03:53:16,833 [pool-24-thread-1] INFO ratis.ContainerStateMachine: group-69810F3160D1: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode2_1  | 2023-01-06 03:53:16,919 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode2_1  | 2023-01-06 03:53:17,177 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode2_1  | 2023-01-06 03:53:17,187 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-01-06 03:53:17,223 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode2_1  | 2023-01-06 03:53:17,236 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode2_1  | 2023-01-06 03:53:17,339 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-01-06 03:53:17,425 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode2_1  | 2023-01-06 03:53:17,436 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode2_1  | 2023-01-06 03:53:17,523 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: new f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5af4a3fc-47d6-497e-93af-69810f3160d1
datanode2_1  | 2023-01-06 03:53:17,527 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode2_1  | 2023-01-06 03:53:17,532 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode2_1  | 2023-01-06 03:53:17,540 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-01-06 03:53:17,545 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode2_1  | 2023-01-06 03:53:17,555 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode2_1  | 2023-01-06 03:53:17,563 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode2_1  | 2023-01-06 03:53:17,576 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode2_1  | 2023-01-06 03:53:17,577 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode2_1  | 2023-01-06 03:53:17,726 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode2_1  | 2023-01-06 03:53:17,735 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-01-06 03:53:17,939 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode2_1  | 2023-01-06 03:53:17,961 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode2_1  | 2023-01-06 03:53:17,972 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode2_1  | 2023-01-06 03:53:18,056 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-01-06 03:53:18,056 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-01-06 03:53:18,098 [pool-24-thread-1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1: start as a follower, conf=-1: peers:[f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-01-06 03:53:18,100 [pool-24-thread-1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode2_1  | 2023-01-06 03:53:18,108 [pool-24-thread-1] INFO impl.RoleInfo: f7a31870-78ff-406d-a144-7fe17a627699: start f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-FollowerState
datanode2_1  | 2023-01-06 03:53:18,148 [pool-24-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-69810F3160D1,id=f7a31870-78ff-406d-a144-7fe17a627699
datanode2_1  | 2023-01-06 03:53:18,161 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-01-06 03:53:18,163 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-01-06 03:53:18,164 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode2_1  | 2023-01-06 03:53:18,168 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode2_1  | 2023-01-06 03:53:18,171 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode2_1  | 2023-01-06 03:53:18,172 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode2_1  | 2023-01-06 03:53:18,268 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=5af4a3fc-47d6-497e-93af-69810f3160d1
datanode1_1  | 2023-01-06 03:53:19,312 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode1_1  | 2023-01-06 03:53:19,375 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: new d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/c742cece-fb27-47b9-b231-5839005b1100
datanode1_1  | 2023-01-06 03:53:19,376 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode1_1  | 2023-01-06 03:53:19,378 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode1_1  | 2023-01-06 03:53:19,390 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-01-06 03:53:19,391 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode1_1  | 2023-01-06 03:53:19,396 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode1_1  | 2023-01-06 03:53:19,402 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode1_1  | 2023-01-06 03:53:19,404 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode1_1  | 2023-01-06 03:53:19,404 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode1_1  | 2023-01-06 03:53:19,490 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode1_1  | 2023-01-06 03:53:19,493 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-01-06 03:53:19,541 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode1_1  | 2023-01-06 03:53:19,551 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode1_1  | 2023-01-06 03:53:19,552 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode1_1  | 2023-01-06 03:53:19,570 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-01-06 03:53:19,571 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-01-06 03:53:19,573 [pool-24-thread-1] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100: start as a follower, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-06 03:53:19,574 [pool-24-thread-1] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode1_1  | 2023-01-06 03:53:19,578 [pool-24-thread-1] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: start d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState
datanode1_1  | 2023-01-06 03:53:19,589 [pool-24-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-5839005B1100,id=d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86
datanode1_1  | 2023-01-06 03:53:19,601 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-01-06 03:53:19,602 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-01-06 03:53:19,601 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode1_1  | 2023-01-06 03:53:19,603 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode1_1  | 2023-01-06 03:53:19,604 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode1_1  | 2023-01-06 03:53:19,605 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode1_1  | 2023-01-06 03:53:19,634 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=c742cece-fb27-47b9-b231-5839005b1100
datanode1_1  | 2023-01-06 03:53:19,783 [Command processor thread] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode1_1  | 2023-01-06 03:53:24,616 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO impl.FollowerState: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5038010231ns, electionTimeout:5012ms
datanode1_1  | 2023-01-06 03:53:24,616 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: shutdown d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState
datanode1_1  | 2023-01-06 03:53:24,618 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode1_1  | 2023-01-06 03:53:24,653 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode1_1  | 2023-01-06 03:53:24,658 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: start d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection1
datanode1_1  | 2023-01-06 03:53:24,705 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection1] INFO impl.LeaderElection: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-06 03:53:24,776 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-01-06 03:53:24,807 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-01-06 03:53:24,836 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for f7a31870-78ff-406d-a144-7fe17a627699
datanode1_1  | 2023-01-06 03:53:24,836 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 678ebbfa-a167-4ea6-8846-1b1f5c050dad
datanode1_1  | 2023-01-06 03:53:28,465 [grpc-default-executor-1] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100: receive requestVote(ELECTION, 678ebbfa-a167-4ea6-8846-1b1f5c050dad, group-5839005B1100, 1, (t:0, i:0))
datanode1_1  | 2023-01-06 03:53:28,475 [grpc-default-executor-1] INFO impl.VoteContext: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-CANDIDATE: reject ELECTION from 678ebbfa-a167-4ea6-8846-1b1f5c050dad: already has voted for d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86 at current term 1
datanode1_1  | 2023-01-06 03:53:28,517 [grpc-default-executor-1] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100 replies to ELECTION vote request: 678ebbfa-a167-4ea6-8846-1b1f5c050dad<-d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86#0:FAIL-t1. Peer's state: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100:t1, leader=null, voted=d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86, raftlog=Memoized:d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-06 03:53:28,529 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection1] INFO impl.LeaderElection: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection1: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode1_1  | 2023-01-06 03:53:28,529 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection1] INFO impl.LeaderElection:   Response 0: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86<-678ebbfa-a167-4ea6-8846-1b1f5c050dad#0:FAIL-t1
datanode1_1  | 2023-01-06 03:53:28,531 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection1] INFO impl.LeaderElection: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection1 ELECTION round 0: result REJECTED
datanode1_1  | 2023-01-06 03:53:28,534 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection1] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode1_1  | 2023-01-06 03:53:28,534 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection1] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: shutdown d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection1
datanode1_1  | 2023-01-06 03:53:28,538 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection1] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: start d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState
datanode1_1  | 2023-01-06 03:53:28,574 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-01-06 03:53:28,575 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-01-06 03:53:29,667 [Command processor thread] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode1_1  | 2023-01-06 03:53:29,751 [grpc-default-executor-1] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100: receive requestVote(ELECTION, f7a31870-78ff-406d-a144-7fe17a627699, group-5839005B1100, 1, (t:0, i:0))
datanode1_1  | 2023-01-06 03:53:29,751 [grpc-default-executor-1] INFO impl.VoteContext: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FOLLOWER: reject ELECTION from f7a31870-78ff-406d-a144-7fe17a627699: already has voted for d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86 at current term 1
datanode1_1  | 2023-01-06 03:53:29,751 [grpc-default-executor-1] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100 replies to ELECTION vote request: f7a31870-78ff-406d-a144-7fe17a627699<-d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86#0:FAIL-t1. Peer's state: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100:t1, leader=null, voted=d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86, raftlog=Memoized:d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-06 03:53:31,282 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=c742cece-fb27-47b9-b231-5839005b1100.
datanode1_1  | 2023-01-06 03:53:31,283 [Command processor thread] INFO server.RaftServer: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: addNew group-507410C3658E:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER] returns group-507410C3658E:java.util.concurrent.CompletableFuture@3b50e85b[Not completed]
datanode1_1  | 2023-01-06 03:53:31,285 [pool-24-thread-1] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: new RaftServerImpl for group-507410C3658E:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode1_1  | 2023-01-06 03:53:31,289 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode1_1  | 2023-01-06 03:53:31,290 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode1_1  | 2023-01-06 03:53:31,290 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode1_1  | 2023-01-06 03:53:31,292 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2023-01-06 03:53:31,293 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2023-01-06 03:53:31,295 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode1_1  | 2023-01-06 03:53:31,295 [pool-24-thread-1] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E: ConfigurationManager, init=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode1_1  | 2023-01-06 03:53:31,296 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2023-01-06 03:53:31,297 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode1_1  | 2023-01-06 03:53:31,298 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode1_1  | 2023-01-06 03:53:31,302 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode1_1  | 2023-01-06 03:53:31,303 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode1_1  | 2023-01-06 03:53:31,303 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode1_1  | 2023-01-06 03:53:31,323 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2023-01-06 03:53:31,323 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode3_1  | 2023-01-06 03:53:20,134 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-01-06 03:53:20,175 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-01-06 03:53:20,176 [pool-24-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-5839005B1100,id=678ebbfa-a167-4ea6-8846-1b1f5c050dad
datanode3_1  | 2023-01-06 03:53:20,188 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode3_1  | 2023-01-06 03:53:20,188 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode3_1  | 2023-01-06 03:53:20,193 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode3_1  | 2023-01-06 03:53:20,204 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode3_1  | 2023-01-06 03:53:20,490 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=c742cece-fb27-47b9-b231-5839005b1100
datanode3_1  | 2023-01-06 03:53:20,755 [Command processor thread] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode3_1  | 2023-01-06 03:53:25,354 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState] INFO impl.FollowerState: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5231920561ns, electionTimeout:5178ms
datanode3_1  | 2023-01-06 03:53:25,355 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState] INFO impl.RoleInfo: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: shutdown 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState
datanode3_1  | 2023-01-06 03:53:25,361 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode3_1  | 2023-01-06 03:53:25,449 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode3_1  | 2023-01-06 03:53:25,449 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState] INFO impl.RoleInfo: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: start 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection1
datanode3_1  | 2023-01-06 03:53:25,551 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection1] INFO impl.LeaderElection: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-06 03:53:25,640 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-01-06 03:53:25,669 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-01-06 03:53:25,710 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for f7a31870-78ff-406d-a144-7fe17a627699
datanode3_1  | 2023-01-06 03:53:25,710 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86
datanode3_1  | 2023-01-06 03:53:28,218 [grpc-default-executor-2] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100: receive requestVote(ELECTION, d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86, group-5839005B1100, 1, (t:0, i:0))
datanode3_1  | 2023-01-06 03:53:28,234 [grpc-default-executor-2] INFO impl.VoteContext: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-CANDIDATE: reject ELECTION from d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: already has voted for 678ebbfa-a167-4ea6-8846-1b1f5c050dad at current term 1
datanode3_1  | 2023-01-06 03:53:28,308 [grpc-default-executor-2] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100 replies to ELECTION vote request: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86<-678ebbfa-a167-4ea6-8846-1b1f5c050dad#0:FAIL-t1. Peer's state: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100:t1, leader=null, voted=678ebbfa-a167-4ea6-8846-1b1f5c050dad, raftlog=Memoized:678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-06 03:53:29,275 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection1] INFO impl.LeaderElection: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection1: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode3_1  | 2023-01-06 03:53:29,275 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection1] INFO impl.LeaderElection:   Response 0: 678ebbfa-a167-4ea6-8846-1b1f5c050dad<-d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86#0:FAIL-t1
datanode3_1  | 2023-01-06 03:53:29,275 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection1] INFO impl.LeaderElection:   Response 1: 678ebbfa-a167-4ea6-8846-1b1f5c050dad<-f7a31870-78ff-406d-a144-7fe17a627699#0:FAIL-t1
datanode3_1  | 2023-01-06 03:53:29,276 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection1] INFO impl.LeaderElection: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection1 ELECTION round 0: result REJECTED
datanode3_1  | 2023-01-06 03:53:29,278 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection1] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode3_1  | 2023-01-06 03:53:29,278 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection1] INFO impl.RoleInfo: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: shutdown 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection1
datanode3_1  | 2023-01-06 03:53:29,279 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection1] INFO impl.RoleInfo: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: start 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState
datanode3_1  | 2023-01-06 03:53:29,296 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-01-06 03:53:29,296 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-01-06 03:53:29,752 [Command processor thread] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode3_1  | 2023-01-06 03:53:29,763 [grpc-default-executor-2] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100: receive requestVote(ELECTION, f7a31870-78ff-406d-a144-7fe17a627699, group-5839005B1100, 1, (t:0, i:0))
datanode3_1  | 2023-01-06 03:53:29,763 [grpc-default-executor-2] INFO impl.VoteContext: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FOLLOWER: reject ELECTION from f7a31870-78ff-406d-a144-7fe17a627699: already has voted for 678ebbfa-a167-4ea6-8846-1b1f5c050dad at current term 1
datanode3_1  | 2023-01-06 03:53:29,764 [grpc-default-executor-2] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100 replies to ELECTION vote request: f7a31870-78ff-406d-a144-7fe17a627699<-678ebbfa-a167-4ea6-8846-1b1f5c050dad#0:FAIL-t1. Peer's state: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100:t1, leader=null, voted=678ebbfa-a167-4ea6-8846-1b1f5c050dad, raftlog=Memoized:678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-06 03:53:30,472 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=c742cece-fb27-47b9-b231-5839005b1100.
datanode3_1  | 2023-01-06 03:53:30,484 [pool-24-thread-1] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: new RaftServerImpl for group-FC9DCBE94408:[678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode3_1  | 2023-01-06 03:53:30,487 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode3_1  | 2023-01-06 03:53:30,487 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode3_1  | 2023-01-06 03:53:30,487 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode3_1  | 2023-01-06 03:53:30,487 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2023-01-06 03:53:30,487 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2023-01-06 03:53:30,487 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode3_1  | 2023-01-06 03:53:30,487 [pool-24-thread-1] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408: ConfigurationManager, init=-1: peers:[678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode3_1  | 2023-01-06 03:53:30,488 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2023-01-06 03:53:30,492 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode3_1  | 2023-01-06 03:53:30,492 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode3_1  | 2023-01-06 03:53:30,492 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode3_1  | 2023-01-06 03:53:30,492 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode3_1  | 2023-01-06 03:53:30,492 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode3_1  | 2023-01-06 03:53:30,484 [Command processor thread] INFO server.RaftServer: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: addNew group-FC9DCBE94408:[678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER] returns group-FC9DCBE94408:java.util.concurrent.CompletableFuture@7788b19a[Not completed]
datanode3_1  | 2023-01-06 03:53:30,499 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2023-01-06 03:53:30,499 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode3_1  | 2023-01-06 03:53:30,499 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode3_1  | 2023-01-06 03:53:30,500 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode3_1  | 2023-01-06 03:53:30,500 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode3_1  | 2023-01-06 03:53:30,505 [pool-24-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d88fe7be-cf18-4d52-90b0-fc9dcbe94408 does not exist. Creating ...
datanode3_1  | 2023-01-06 03:53:30,513 [pool-24-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d88fe7be-cf18-4d52-90b0-fc9dcbe94408/in_use.lock acquired by nodename 7@03b09732740a
datanode3_1  | 2023-01-06 03:53:30,532 [pool-24-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d88fe7be-cf18-4d52-90b0-fc9dcbe94408 has been successfully formatted.
datanode3_1  | 2023-01-06 03:53:30,568 [pool-24-thread-1] INFO ratis.ContainerStateMachine: group-FC9DCBE94408: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode3_1  | 2023-01-06 03:53:30,570 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode3_1  | 2023-01-06 03:53:30,570 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode3_1  | 2023-01-06 03:53:30,604 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-01-06 03:53:30,604 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode3_1  | 2023-01-06 03:53:30,604 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode3_1  | 2023-01-06 03:53:30,618 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-01-06 03:53:30,629 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode3_1  | 2023-01-06 03:53:30,629 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode3_1  | 2023-01-06 03:53:30,629 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: new 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d88fe7be-cf18-4d52-90b0-fc9dcbe94408
datanode3_1  | 2023-01-06 03:53:30,685 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode3_1  | 2023-01-06 03:53:30,686 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
om1_1        | 0020: E2 8C 95 3C 0B 0B 1A AD   6E 89 50 B3 9C 01 22 36  ...<....n.P..."6
om1_1        | 0030: 82 EA 04 5E 17 F6 FE 59   8A 78 83 34 58 E3 1A CF  ...^...Y.x.4X...
om1_1        | 0040: C0 08 87 80 9B 7D B8 33   3C E2 09 5B A0 BB 73 F9  .......3<..[..s.
om1_1        | 0050: 18 A1 8D D7 79 71 5D F0   D7 29 1C F6 F1 1A 40 0C  ....yq]..)....@.
om1_1        | 0060: DC B0 36 E8 EA 71 C3 CB   C9 B0 27 6D D7 9F 21 23  ..6..q....'m..!#
om1_1        | 0070: 40 5D F0 5B 0A B7 A0 74   F9 9F AD 3D 48 4A CC B0  @].[...t...=HJ..
om1_1        | 0080: 71 94 D1 27 C1 11 45 74   CC E9 49 95 45 B0 E7 F7  q..'..Et..I.E...
om1_1        | 0090: 9B 9F F3 43 1C 28 31 DC   D6 F4 73 3B 1B 52 1D 3F  ...C.(1...s;.R.?
om1_1        | 00A0: 44 76 E5 4C 63 7F 23 9B   17 64 90 51 18 89 86 33  Dv.Lc.#..d.Q...3
om1_1        | 00B0: CE 66 07 29 55 E3 3A 4D   BD 33 89 34 C3 A8 3C 51  .f.)U.:M.3.4..<Q
om1_1        | 00C0: 52 16 00 6D 7D 61 54 1E   1A 84 F4 F5 89 B2 A8 9C  R..m.aT.........
om1_1        | 00D0: 56 66 FE E2 2A 93 FE 19   10 10 C9 9C 0D 74 9B 01  Vf..*........t..
om1_1        | 00E0: 75 1D 1C AB 74 3A E3 B8   41 66 A1 BE 62 55 B9 60  u...t:..Af..bU.`
om1_1        | 00F0: F1 84 78 6F 50 A4 59 C4   78 19 02 34 DC BA 56 C3  ..xoP.Y.x..4..V.
om1_1        | 
om1_1        | ] from file:/data/metadata/om/certs/ROOTCA-1.crt.
om1_1        | 2023-01-06 03:52:53,413 [main] INFO security.OMCertificateClient: Added certificate [
om1_1        | [
om1_1        |   Version: V3
om1_1        |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm-sub@scm1.org
om1_1        |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
om1_1        | 
om1_1        |   Key:  Sun RSA public key, 2048 bits
om1_1        |   params: null
om1_1        |   modulus: 25512042468887604642064404093025249920280347815052024483618130467625890981963310751612908088931868680366166722736700966336351841536566461994206957959065592473127025545304172175201240100170923940025313585056297682798208555874810870376218207324432498120475419600056108682275459578022550441815608531770386354956257639186299780577671839142629796440640716929439466625583225667634032960664095389171765331267228308686758487244580763947638224999982475527976426192624582650657992140973107557451334337699303987990444760298981625629400711682432328660808877074618851496477253766349631046458008273474791424727251297086392264850331
om1_1        |   public exponent: 65537
om1_1        |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
om1_1        |                To: Mon Feb 14 00:00:00 UTC 2028]
om1_1        |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
om1_1        |   SerialNumber: [    01379773 48ab]
om1_1        | 
om1_1        | Certificate Extensions: 3
om1_1        | [1]: ObjectId: 2.5.29.19 Criticality=true
om1_1        | BasicConstraints:[
om1_1        |   CA:true
om1_1        |   PathLen:2147483647
om1_1        | ]
om1_1        | 
om1_1        | [2]: ObjectId: 2.5.29.15 Criticality=true
om1_1        | KeyUsage [
om1_1        |   DigitalSignature
om1_1        |   Key_Encipherment
om1_1        |   Data_Encipherment
om1_1        |   Key_Agreement
om1_1        |   Key_CertSign
om1_1        |   Crl_Sign
om1_1        | ]
om1_1        | 
om1_1        | [3]: ObjectId: 2.5.29.17 Criticality=false
om1_1        | SubjectAlternativeName [
om1_1        |   IPAddress: 172.25.0.116
om1_1        |   DNSName: scm1.org
om1_1        | ]
om1_1        | 
om1_1        | ]
om1_1        |   Algorithm: [SHA256withRSA]
om1_1        |   Signature:
om1_1        | 0000: 71 A1 97 4D DA 8D 52 3F   E5 06 22 8E 76 61 49 C0  q..M..R?..".vaI.
om1_1        | 0010: C6 DB 6D 93 9B E0 DF 39   D0 2C 5E B2 55 3C F2 BA  ..m....9.,^.U<..
om1_1        | 0020: 2B 11 C5 2A C7 34 78 58   C1 96 DE C2 09 D7 16 49  +..*.4xX.......I
om1_1        | 0030: 14 DD 81 0D 0D EC 2B 14   A8 C2 D5 DC D9 65 72 65  ......+......ere
om1_1        | 0040: AC 06 88 C6 35 48 D5 5F   88 26 38 54 A0 34 B7 6A  ....5H._.&8T.4.j
om1_1        | 0050: 80 D9 98 32 D0 A7 83 BC   EF F9 F3 B4 3E E3 DB 99  ...2........>...
om1_1        | 0060: C8 62 68 E3 13 33 55 58   91 03 33 A0 D4 E3 E8 C7  .bh..3UX..3.....
om1_1        | 0070: 11 6E E3 D9 21 80 E7 D3   FD D9 65 FA 44 9C 4A 18  .n..!.....e.D.J.
om1_1        | 0080: 0A F3 39 BC 62 9F 21 94   BE 66 EF E8 B3 BE 21 DA  ..9.b.!..f....!.
om1_1        | 0090: 04 60 36 57 8C 00 B7 0D   3C 31 30 4D 96 93 18 D6  .`6W....<10M....
om1_1        | 00A0: DE 9E 43 8C B8 02 39 06   65 33 4A E0 D2 C5 E1 C5  ..C...9.e3J.....
om1_1        | 00B0: EC 84 E7 E9 D8 7F 26 5A   7F EC C4 ED E0 DA 89 3A  ......&Z.......:
om1_1        | 00C0: 2A 12 BB 4E A2 53 59 0A   EB 4D CC C2 5B DB 3E 6C  *..N.SY..M..[.>l
om1_1        | 00D0: 79 1A 95 EA 9D F5 37 22   32 4C FC DF 91 B5 AD 85  y.....7"2L......
om1_1        | 00E0: 28 09 F5 E5 5B 53 76 DA   63 17 AE F5 80 A5 B1 AF  (...[Sv.c.......
om1_1        | 00F0: 1B 90 43 5F FF 80 DB 39   CC 0A 02 38 20 9A EB 4F  ..C_...9...8 ..O
om1_1        | 
om1_1        | ] from file:/data/metadata/om/certs/CA-1338275743915.crt.
om1_1        | 2023-01-06 03:52:53,448 [main] INFO security.OMCertificateClient: Added certificate [
om1_1        | [
om1_1        |   Version: V3
om1_1        |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=om1
om1_1        |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
om1_1        | 
om1_1        |   Key:  Sun RSA public key, 2048 bits
om1_1        |   params: null
om1_1        |   modulus: 26387541250896957952506715038058974265442845815269302365886599436445211828204135722555845192080936784033085028644097272024863812794679839712741726474149200086709364353826786931242641749935012515496525690714507403438913964398980592977774770391532795391540345998114207101556847039277706793947710593389514931751178030826588223452249802193835871446631453813818129197091874471113237190760009417265454949169932341217115726238160831715208995606554865155347840169526942626894135641051511078375472713862862325425384539563677477580720619825291813551671726211787253486400831356181871934543738606237978613558579020482408233469917
om1_1        |   public exponent: 65537
om1_1        |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
om1_1        |                To: Sat Jan 06 00:00:00 UTC 2024]
om1_1        |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm-sub@scm1.org
om1_1        |   SerialNumber: [    01552cc3 adbb]
om1_1        | 
om1_1        | Certificate Extensions: 2
om1_1        | [1]: ObjectId: 2.5.29.15 Criticality=true
om1_1        | KeyUsage [
om1_1        |   DigitalSignature
om1_1        |   Key_Encipherment
om1_1        |   Data_Encipherment
om1_1        |   Key_Agreement
om1_1        | ]
om1_1        | 
om1_1        | [2]: ObjectId: 2.5.29.17 Criticality=false
om1_1        | SubjectAlternativeName [
om1_1        |   IPAddress: 172.25.0.111
om1_1        |   Other-Name: Unrecognized ObjectIdentifier: 2.16.840.1.113730.3.1.34
om1_1        | ]
om1_1        | 
om1_1        | ]
om1_1        |   Algorithm: [SHA256withRSA]
om1_1        |   Signature:
om1_1        | 0000: 2E FB F2 79 E7 7F A3 35   FB BA 8A 1B 91 08 10 F0  ...y...5........
om1_1        | 0010: DF AA E7 7E E6 04 CA 98   43 9D A9 22 55 DE E0 B1  ........C.."U...
om1_1        | 0020: 86 50 36 D3 44 AB 21 A2   79 EE E2 48 33 F3 44 9B  .P6.D.!.y..H3.D.
om1_1        | 0030: 7C E3 FF 1F 36 7B F2 4A   8C 05 15 C5 87 D3 2B 55  ....6..J......+U
om1_1        | 0040: CD 54 DF 04 B1 43 73 92   F2 6A 95 A4 2A 60 EE 1A  .T...Cs..j..*`..
om1_1        | 0050: 18 FA 0E 00 BB 88 8F 0D   1D 50 37 9B 1B 76 71 20  .........P7..vq 
om1_1        | 0060: 21 8D B4 06 4C 90 3E 96   A4 59 01 E9 77 95 7A 09  !...L.>..Y..w.z.
om1_1        | 0070: FB 07 22 C6 5E 3A 31 16   9A 3D 95 13 FA F1 71 B4  ..".^:1..=....q.
datanode2_1  | 2023-01-06 03:53:18,275 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=5af4a3fc-47d6-497e-93af-69810f3160d1.
datanode2_1  | 2023-01-06 03:53:18,282 [Command processor thread] INFO server.RaftServer: f7a31870-78ff-406d-a144-7fe17a627699: addNew group-5839005B1100:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER] returns group-5839005B1100:java.util.concurrent.CompletableFuture@61fffc9f[Not completed]
datanode2_1  | 2023-01-06 03:53:18,298 [pool-24-thread-1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699: new RaftServerImpl for group-5839005B1100:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode2_1  | 2023-01-06 03:53:18,298 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode2_1  | 2023-01-06 03:53:18,298 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode2_1  | 2023-01-06 03:53:18,298 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode2_1  | 2023-01-06 03:53:18,302 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2023-01-06 03:53:18,304 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2023-01-06 03:53:18,304 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode2_1  | 2023-01-06 03:53:18,305 [pool-24-thread-1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100: ConfigurationManager, init=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode2_1  | 2023-01-06 03:53:18,306 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2023-01-06 03:53:18,306 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode2_1  | 2023-01-06 03:53:18,306 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode2_1  | 2023-01-06 03:53:18,307 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode2_1  | 2023-01-06 03:53:18,308 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode2_1  | 2023-01-06 03:53:18,309 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode2_1  | 2023-01-06 03:53:18,316 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2023-01-06 03:53:18,316 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode2_1  | 2023-01-06 03:53:18,317 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode2_1  | 2023-01-06 03:53:18,320 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode2_1  | 2023-01-06 03:53:18,320 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode2_1  | 2023-01-06 03:53:18,320 [pool-24-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/c742cece-fb27-47b9-b231-5839005b1100 does not exist. Creating ...
datanode2_1  | 2023-01-06 03:53:18,324 [pool-24-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/c742cece-fb27-47b9-b231-5839005b1100/in_use.lock acquired by nodename 7@47737eac8b7e
datanode2_1  | 2023-01-06 03:53:18,344 [pool-24-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/c742cece-fb27-47b9-b231-5839005b1100 has been successfully formatted.
datanode2_1  | 2023-01-06 03:53:18,346 [pool-24-thread-1] INFO ratis.ContainerStateMachine: group-5839005B1100: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode2_1  | 2023-01-06 03:53:18,417 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode2_1  | 2023-01-06 03:53:18,421 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode2_1  | 2023-01-06 03:53:18,421 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-01-06 03:53:18,422 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode2_1  | 2023-01-06 03:53:18,422 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode2_1  | 2023-01-06 03:53:18,423 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-01-06 03:53:18,424 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode2_1  | 2023-01-06 03:53:18,425 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode2_1  | 2023-01-06 03:53:18,426 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: new f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/c742cece-fb27-47b9-b231-5839005b1100
datanode2_1  | 2023-01-06 03:53:18,429 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode2_1  | 2023-01-06 03:53:18,430 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode2_1  | 2023-01-06 03:53:18,431 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-01-06 03:53:18,431 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode2_1  | 2023-01-06 03:53:18,433 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode2_1  | 2023-01-06 03:53:18,434 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode2_1  | 2023-01-06 03:53:18,434 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode2_1  | 2023-01-06 03:53:18,435 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode2_1  | 2023-01-06 03:53:18,436 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode2_1  | 2023-01-06 03:53:18,444 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-01-06 03:53:19,829 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-f7a31870-78ff-406d-a144-7fe17a627699: Detected pause in JVM or host machine (eg GC): pause of approximately 1098604347ns.
datanode2_1  | GC pool 'ParNew' had collection(s): count=1 time=95ms
datanode2_1  | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1273ms
datanode2_1  | 2023-01-06 03:53:19,860 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@6ee7298b] INFO util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1243ms
datanode2_1  | GC pool 'ParNew' had collection(s): count=1 time=95ms
datanode2_1  | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1273ms
datanode2_1  | 2023-01-06 03:53:20,007 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode2_1  | 2023-01-06 03:53:20,011 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode2_1  | 2023-01-06 03:53:20,016 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode2_1  | 2023-01-06 03:53:20,022 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-01-06 03:53:20,027 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-01-06 03:53:20,046 [pool-24-thread-1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100: start as a follower, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-01-06 03:53:20,048 [pool-24-thread-1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode2_1  | 2023-01-06 03:53:20,049 [pool-24-thread-1] INFO impl.RoleInfo: f7a31870-78ff-406d-a144-7fe17a627699: start f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState
datanode2_1  | 2023-01-06 03:53:20,072 [pool-24-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-5839005B1100,id=f7a31870-78ff-406d-a144-7fe17a627699
datanode2_1  | 2023-01-06 03:53:20,074 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode2_1  | 2023-01-06 03:53:20,074 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode2_1  | 2023-01-06 03:53:20,075 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode2_1  | 2023-01-06 03:53:20,075 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode2_1  | 2023-01-06 03:53:20,079 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-01-06 03:53:20,101 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=c742cece-fb27-47b9-b231-5839005b1100
datanode2_1  | 2023-01-06 03:53:20,105 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-01-06 03:53:20,354 [Command processor thread] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode2_1  | 2023-01-06 03:53:23,340 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-FollowerState] INFO impl.FollowerState: f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5234155875ns, electionTimeout:5160ms
datanode2_1  | 2023-01-06 03:53:23,347 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-FollowerState] INFO impl.RoleInfo: f7a31870-78ff-406d-a144-7fe17a627699: shutdown f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-FollowerState
datanode2_1  | 2023-01-06 03:53:23,382 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-FollowerState] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode2_1  | 2023-01-06 03:53:23,398 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode2_1  | 2023-01-06 03:53:23,399 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-FollowerState] INFO impl.RoleInfo: f7a31870-78ff-406d-a144-7fe17a627699: start f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-LeaderElection1
datanode2_1  | 2023-01-06 03:53:23,522 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-LeaderElection1] INFO impl.LeaderElection: f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-01-06 03:53:23,562 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-LeaderElection1] INFO impl.LeaderElection: f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode2_1  | 2023-01-06 03:53:23,604 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-LeaderElection1] INFO impl.RoleInfo: f7a31870-78ff-406d-a144-7fe17a627699: shutdown f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-LeaderElection1
datanode2_1  | 2023-01-06 03:53:23,619 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-LeaderElection1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode2_1  | 2023-01-06 03:53:23,625 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-69810F3160D1 with new leaderId: f7a31870-78ff-406d-a144-7fe17a627699
datanode2_1  | 2023-01-06 03:53:23,661 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-LeaderElection1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1: change Leader from null to f7a31870-78ff-406d-a144-7fe17a627699 at term 1 for becomeLeader, leader elected after 7299ms
datanode2_1  | 2023-01-06 03:53:23,866 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode2_1  | 2023-01-06 03:53:24,011 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
om2_1        | 2023-01-06 03:52:12,207 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om2_1        | 2023-01-06 03:52:12,228 [main] ERROR security.OMCertificateClient: Invalid domain om2
om2_1        | 2023-01-06 03:52:12,229 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om2_1        | 2023-01-06 03:52:12,236 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1        | 2023-01-06 03:52:12,238 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om2: om2
om2_1        | 2023-01-06 03:52:12,239 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om2: om2
om2_1        | 2023-01-06 03:52:12,252 [main] INFO security.OMCertificateClient: Creating csr for OM->dns:om2,ip:172.25.0.112,scmId:e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674,clusterId:CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3,subject:om2
om2_1        | 2023-01-06 03:52:15,327 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
om2_1        | 2023-01-06 03:52:15,394 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om2_1        | /************************************************************
om2_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om2/172.25.0.112
om2_1        | ************************************************************/
om2_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1        | 2023-01-06 03:52:28,991 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1        | /************************************************************
om2_1        | STARTUP_MSG: Starting OzoneManager
om2_1        | STARTUP_MSG:   host = om2/172.25.0.112
om2_1        | STARTUP_MSG:   args = []
om2_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om2_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om2_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/9d30c9a7ce375cd614e86022850c7c128723df6b ; compiled by 'runner' on 2023-01-06T03:19Z
om2_1        | STARTUP_MSG:   java = 11.0.14.1
om2_1        | ************************************************************/
om2_1        | 2023-01-06 03:52:29,125 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1        | 2023-01-06 03:52:37,199 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1        | 2023-01-06 03:52:40,359 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om2_1        | 2023-01-06 03:52:40,967 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1        | 2023-01-06 03:52:40,973 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om2: om2
om2_1        | 2023-01-06 03:52:40,974 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om2: om2
om2_1        | 2023-01-06 03:52:41,084 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-01-06 03:52:41,538 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om2_1        | 2023-01-06 03:52:43,850 [main] INFO reflections.Reflections: Reflections took 1763 ms to scan 1 urls, producing 115 keys and 336 values [using 2 cores]
om2_1        | 2023-01-06 03:52:46,011 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om2_1        | 2023-01-06 03:52:46,011 [main] INFO om.OzoneManager: Ozone Manager login successful.
om2_1        | 2023-01-06 03:52:46,013 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-01-06 03:52:48,740 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om2_1        | 2023-01-06 03:52:49,133 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om2_1        | 2023-01-06 03:52:53,551 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om2_1        | value: 9862
om2_1        | ]
om2_1        | 2023-01-06 03:52:53,590 [main] INFO security.OMCertificateClient: Loading certificate from location:/data/metadata/om/certs.
om2_1        | 2023-01-06 03:52:54,337 [main] INFO security.OMCertificateClient: Added certificate [
om2_1        | [
om2_1        |   Version: V3
om2_1        |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
om2_1        |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
om2_1        | 
om2_1        |   Key:  Sun RSA public key, 2048 bits
om2_1        |   params: null
om2_1        |   modulus: 22321520092034904956367956777931957676969911078232901017895933698678468781753172695844393013092965970615602622357794023639048732305076160434218405355746465972426985604516863422070797512086951139731951669174461346685083874381071907314493811292043757517828199039562222504144931947786863316592495947302761724410796725398590551213642383206197662885192142834321323519855792963024540129867342534976672519842084316567886340381000959465926087773440860829229209280683297485436375405886176389600891889084968589860808245139826791285487072108592486434566834307409916351251604701981140849210029026671497960989540995516629779904417
om2_1        |   public exponent: 65537
om2_1        |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
om2_1        |                To: Mon Feb 14 00:00:00 UTC 2028]
om2_1        |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
om2_1        |   SerialNumber: [    01]
om2_1        | 
om2_1        | Certificate Extensions: 3
om2_1        | [1]: ObjectId: 2.5.29.19 Criticality=true
om2_1        | BasicConstraints:[
om2_1        |   CA:true
om2_1        |   PathLen:2147483647
om2_1        | ]
om2_1        | 
om2_1        | [2]: ObjectId: 2.5.29.15 Criticality=true
om2_1        | KeyUsage [
om2_1        |   Key_CertSign
om2_1        |   Crl_Sign
om2_1        | ]
om2_1        | 
om2_1        | [3]: ObjectId: 2.5.29.17 Criticality=false
om2_1        | SubjectAlternativeName [
om2_1        |   IPAddress: 172.25.0.116
om2_1        |   DNSName: scm1.org
om2_1        | ]
om2_1        | 
om2_1        | ]
om2_1        |   Algorithm: [SHA256withRSA]
om2_1        |   Signature:
om2_1        | 0000: 21 07 40 68 BD F5 21 63   53 A4 1C 6D 61 63 C4 B6  !.@h..!cS..mac..
om2_1        | 0010: E3 BD 40 F3 9B FD 99 A0   FE 5C 7B FD 06 FB 50 8E  ..@......\....P.
datanode1_1  | 2023-01-06 03:53:31,325 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode1_1  | 2023-01-06 03:53:31,325 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode1_1  | 2023-01-06 03:53:31,328 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode1_1  | 2023-01-06 03:53:31,329 [pool-24-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/7faae3af-cb79-4c3a-b244-507410c3658e does not exist. Creating ...
datanode1_1  | 2023-01-06 03:53:31,336 [pool-24-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/7faae3af-cb79-4c3a-b244-507410c3658e/in_use.lock acquired by nodename 7@98e33c1634d3
datanode1_1  | 2023-01-06 03:53:31,343 [pool-24-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/7faae3af-cb79-4c3a-b244-507410c3658e has been successfully formatted.
datanode1_1  | 2023-01-06 03:53:31,348 [pool-24-thread-1] INFO ratis.ContainerStateMachine: group-507410C3658E: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode1_1  | 2023-01-06 03:53:31,350 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode1_1  | 2023-01-06 03:53:31,351 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode1_1  | 2023-01-06 03:53:31,353 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-01-06 03:53:31,355 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode1_1  | 2023-01-06 03:53:31,370 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode1_1  | 2023-01-06 03:53:31,371 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-01-06 03:53:31,379 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode1_1  | 2023-01-06 03:53:31,385 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode1_1  | 2023-01-06 03:53:31,386 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: new d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/7faae3af-cb79-4c3a-b244-507410c3658e
datanode1_1  | 2023-01-06 03:53:31,388 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode1_1  | 2023-01-06 03:53:31,389 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode1_1  | 2023-01-06 03:53:31,390 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-01-06 03:53:31,391 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode1_1  | 2023-01-06 03:53:31,392 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode1_1  | 2023-01-06 03:53:31,392 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode1_1  | 2023-01-06 03:53:31,393 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode1_1  | 2023-01-06 03:53:31,394 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode1_1  | 2023-01-06 03:53:31,397 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode1_1  | 2023-01-06 03:53:31,408 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-01-06 03:53:31,648 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode1_1  | 2023-01-06 03:53:31,657 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode1_1  | 2023-01-06 03:53:31,659 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode1_1  | 2023-01-06 03:53:31,660 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-01-06 03:53:31,661 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-01-06 03:53:31,668 [pool-24-thread-1] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E: start as a follower, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-06 03:53:31,679 [pool-24-thread-1] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode1_1  | 2023-01-06 03:53:31,682 [pool-24-thread-1] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: start d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-FollowerState
datanode1_1  | 2023-01-06 03:53:31,684 [pool-24-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-507410C3658E,id=d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86
datanode1_1  | 2023-01-06 03:53:31,684 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode1_1  | 2023-01-06 03:53:31,684 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode1_1  | 2023-01-06 03:53:31,684 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode1_1  | 2023-01-06 03:53:31,684 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode1_1  | 2023-01-06 03:53:31,685 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-01-06 03:53:31,691 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=7faae3af-cb79-4c3a-b244-507410c3658e
datanode1_1  | 2023-01-06 03:53:31,700 [Command processor thread] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode1_1  | 2023-01-06 03:53:31,705 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-01-06 03:53:32,187 [Command processor thread] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode1_1  | 2023-01-06 03:53:32,972 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=7faae3af-cb79-4c3a-b244-507410c3658e.
om3_1        | Sleeping for 5 seconds
om3_1        | Waiting for the service scm3.org:9894
om3_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1        | 2023-01-06 03:51:32,600 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1        | /************************************************************
om3_1        | STARTUP_MSG: Starting OzoneManager
om3_1        | STARTUP_MSG:   host = om3/172.25.0.113
om3_1        | STARTUP_MSG:   args = [--init]
om3_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om3_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om3_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/9d30c9a7ce375cd614e86022850c7c128723df6b ; compiled by 'runner' on 2023-01-06T03:19Z
om3_1        | STARTUP_MSG:   java = 11.0.14.1
om3_1        | ************************************************************/
om3_1        | 2023-01-06 03:51:32,662 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1        | 2023-01-06 03:51:42,710 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1        | 2023-01-06 03:51:46,307 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om3_1        | 2023-01-06 03:51:47,262 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1        | 2023-01-06 03:51:47,270 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om3: om3
om3_1        | 2023-01-06 03:51:47,275 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om3: om3
om3_1        | 2023-01-06 03:51:50,131 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om3_1        | 2023-01-06 03:51:50,131 [main] INFO om.OzoneManager: Ozone Manager login successful.
om3_1        | 2023-01-06 03:51:50,295 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-01-06 03:51:51,468 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om3_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3;layoutVersion=3
om3_1        | 2023-01-06 03:51:57,103 [main] INFO om.OzoneManager: OM storage initialized. Initializing security
om3_1        | 2023-01-06 03:51:57,103 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om3_1        | 2023-01-06 03:52:03,302 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om3_1        | value: 9862
om3_1        | ]
om3_1        | 2023-01-06 03:52:03,435 [main] ERROR security.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om3_1        | 2023-01-06 03:52:03,448 [main] INFO security.OMCertificateClient: Certificate client init case: 0
om3_1        | 2023-01-06 03:52:03,459 [main] INFO security.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om3_1        | 2023-01-06 03:52:09,411 [main] INFO om.OzoneManager: Init response: GETCERT
om3_1        | 2023-01-06 03:52:09,759 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.113,host:om3
om3_1        | 2023-01-06 03:52:09,763 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om3_1        | 2023-01-06 03:52:09,783 [main] ERROR security.OMCertificateClient: Invalid domain om3
om3_1        | 2023-01-06 03:52:09,792 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om3_1        | 2023-01-06 03:52:09,805 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1        | 2023-01-06 03:52:09,805 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om3: om3
om3_1        | 2023-01-06 03:52:09,841 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om3: om3
om3_1        | 2023-01-06 03:52:09,861 [main] INFO security.OMCertificateClient: Creating csr for OM->dns:om3,ip:172.25.0.113,scmId:e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674,clusterId:CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3,subject:om3
om3_1        | 2023-01-06 03:52:13,404 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
om3_1        | 2023-01-06 03:52:13,501 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om3_1        | /************************************************************
om3_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om3/172.25.0.113
om3_1        | ************************************************************/
om3_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode3_1  | 2023-01-06 03:53:30,686 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-01-06 03:53:30,778 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode3_1  | 2023-01-06 03:53:30,778 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode3_1  | 2023-01-06 03:53:30,778 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode3_1  | 2023-01-06 03:53:30,779 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode3_1  | 2023-01-06 03:53:30,779 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode3_1  | 2023-01-06 03:53:30,803 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode3_1  | 2023-01-06 03:53:30,807 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-01-06 03:53:30,896 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode3_1  | 2023-01-06 03:53:30,896 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode3_1  | 2023-01-06 03:53:30,896 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode3_1  | 2023-01-06 03:53:30,898 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-01-06 03:53:30,898 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-01-06 03:53:30,898 [pool-24-thread-1] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408: start as a follower, conf=-1: peers:[678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-06 03:53:30,898 [pool-24-thread-1] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode3_1  | 2023-01-06 03:53:30,899 [pool-24-thread-1] INFO impl.RoleInfo: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: start 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-FollowerState
datanode3_1  | 2023-01-06 03:53:30,899 [pool-24-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-FC9DCBE94408,id=678ebbfa-a167-4ea6-8846-1b1f5c050dad
datanode3_1  | 2023-01-06 03:53:30,899 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode3_1  | 2023-01-06 03:53:30,899 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode3_1  | 2023-01-06 03:53:30,900 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode3_1  | 2023-01-06 03:53:30,900 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode3_1  | 2023-01-06 03:53:30,900 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-01-06 03:53:30,900 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-01-06 03:53:30,982 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=d88fe7be-cf18-4d52-90b0-fc9dcbe94408
datanode3_1  | 2023-01-06 03:53:30,983 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=d88fe7be-cf18-4d52-90b0-fc9dcbe94408.
datanode3_1  | 2023-01-06 03:53:30,989 [Command processor thread] INFO server.RaftServer: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: addNew group-507410C3658E:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER] returns group-507410C3658E:java.util.concurrent.CompletableFuture@baac18f[Not completed]
datanode3_1  | 2023-01-06 03:53:30,994 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-678ebbfa-a167-4ea6-8846-1b1f5c050dad: Detected pause in JVM or host machine (eg GC): pause of approximately 154449916ns. No GCs detected.
datanode3_1  | 2023-01-06 03:53:31,027 [pool-24-thread-1] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: new RaftServerImpl for group-507410C3658E:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode3_1  | 2023-01-06 03:53:31,033 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode3_1  | 2023-01-06 03:53:31,034 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode3_1  | 2023-01-06 03:53:31,039 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode3_1  | 2023-01-06 03:53:31,040 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2023-01-06 03:53:31,045 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2023-01-06 03:53:31,050 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode3_1  | 2023-01-06 03:53:31,204 [pool-24-thread-1] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E: ConfigurationManager, init=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode3_1  | 2023-01-06 03:53:31,204 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2023-01-06 03:53:31,204 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode3_1  | 2023-01-06 03:53:31,205 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode3_1  | 2023-01-06 03:53:31,211 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode3_1  | 2023-01-06 03:53:31,212 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode3_1  | 2023-01-06 03:53:31,212 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode3_1  | 2023-01-06 03:53:31,223 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2023-01-06 03:53:31,223 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode3_1  | 2023-01-06 03:53:31,223 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode3_1  | 2023-01-06 03:53:31,223 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode3_1  | 2023-01-06 03:53:31,231 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode3_1  | 2023-01-06 03:53:31,231 [pool-24-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/7faae3af-cb79-4c3a-b244-507410c3658e does not exist. Creating ...
datanode3_1  | 2023-01-06 03:53:31,245 [pool-24-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/7faae3af-cb79-4c3a-b244-507410c3658e/in_use.lock acquired by nodename 7@03b09732740a
datanode3_1  | 2023-01-06 03:53:31,254 [pool-24-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/7faae3af-cb79-4c3a-b244-507410c3658e has been successfully formatted.
datanode3_1  | 2023-01-06 03:53:31,259 [pool-24-thread-1] INFO ratis.ContainerStateMachine: group-507410C3658E: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode3_1  | 2023-01-06 03:53:31,263 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode3_1  | 2023-01-06 03:53:31,263 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode3_1  | 2023-01-06 03:53:31,263 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-01-06 03:53:31,263 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode3_1  | 2023-01-06 03:53:31,263 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode3_1  | 2023-01-06 03:53:31,263 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-01-06 03:53:31,292 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode3_1  | 2023-01-06 03:53:31,292 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode3_1  | 2023-01-06 03:53:31,292 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: new 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/7faae3af-cb79-4c3a-b244-507410c3658e
datanode3_1  | 2023-01-06 03:53:31,292 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode3_1  | 2023-01-06 03:53:31,292 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode3_1  | 2023-01-06 03:53:31,292 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-01-06 03:53:31,292 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode3_1  | 2023-01-06 03:53:31,292 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode3_1  | 2023-01-06 03:53:31,292 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode3_1  | 2023-01-06 03:53:31,316 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode3_1  | 2023-01-06 03:53:31,316 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode3_1  | 2023-01-06 03:53:31,378 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode3_1  | 2023-01-06 03:53:31,384 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-01-06 03:53:31,563 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode3_1  | 2023-01-06 03:53:31,563 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode3_1  | 2023-01-06 03:53:31,563 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode3_1  | 2023-01-06 03:53:31,564 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-01-06 03:53:31,564 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-01-06 03:53:31,621 [pool-24-thread-1] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E: start as a follower, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-06 03:53:31,621 [pool-24-thread-1] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode3_1  | 2023-01-06 03:53:31,622 [pool-24-thread-1] INFO impl.RoleInfo: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: start 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-FollowerState
datanode3_1  | 2023-01-06 03:53:31,726 [pool-24-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-507410C3658E,id=678ebbfa-a167-4ea6-8846-1b1f5c050dad
datanode3_1  | 2023-01-06 03:53:31,726 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode3_1  | 2023-01-06 03:53:31,726 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode3_1  | 2023-01-06 03:53:31,726 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode3_1  | 2023-01-06 03:53:31,727 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode3_1  | 2023-01-06 03:53:31,727 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-01-06 03:53:31,737 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-01-06 03:53:31,728 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=7faae3af-cb79-4c3a-b244-507410c3658e
datanode3_1  | 2023-01-06 03:53:31,746 [Command processor thread] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode3_1  | 2023-01-06 03:53:32,749 [Command processor thread] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode3_1  | 2023-01-06 03:53:33,369 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=7faae3af-cb79-4c3a-b244-507410c3658e.
datanode3_1  | 2023-01-06 03:53:33,727 [grpc-default-executor-2] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100: receive requestVote(ELECTION, d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86, group-5839005B1100, 2, (t:0, i:0))
datanode3_1  | 2023-01-06 03:53:33,728 [grpc-default-executor-2] INFO impl.VoteContext: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FOLLOWER: reject ELECTION from d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: our priority 1 > candidate's priority 0
datanode3_1  | 2023-01-06 03:53:33,728 [grpc-default-executor-2] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86
datanode3_1  | 2023-01-06 03:53:33,729 [grpc-default-executor-2] INFO impl.RoleInfo: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: shutdown 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState
datanode3_1  | 2023-01-06 03:53:33,730 [grpc-default-executor-2] INFO impl.RoleInfo: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: start 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState
datanode3_1  | 2023-01-06 03:53:33,730 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState] INFO impl.FollowerState: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState was interrupted
datanode3_1  | 2023-01-06 03:53:33,749 [grpc-default-executor-2] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100 replies to ELECTION vote request: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86<-678ebbfa-a167-4ea6-8846-1b1f5c050dad#0:FAIL-t2. Peer's state: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100:t2, leader=null, voted=null, raftlog=Memoized:678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-06 03:53:33,779 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-01-06 03:53:33,779 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-01-06 03:53:36,015 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-FollowerState] INFO impl.FollowerState: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5116203049ns, electionTimeout:5114ms
datanode3_1  | 2023-01-06 03:53:36,015 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-FollowerState] INFO impl.RoleInfo: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: shutdown 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-FollowerState
datanode3_1  | 2023-01-06 03:53:36,016 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-FollowerState] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode3_1  | 2023-01-06 03:53:36,016 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode3_1  | 2023-01-06 03:53:36,016 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-FollowerState] INFO impl.RoleInfo: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: start 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-LeaderElection2
datanode3_1  | 2023-01-06 03:53:36,028 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-LeaderElection2] INFO impl.LeaderElection: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-06 03:53:36,028 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-LeaderElection2] INFO impl.LeaderElection: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-LeaderElection2 ELECTION round 0: result PASSED (term=1)
datanode3_1  | 2023-01-06 03:53:36,029 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-LeaderElection2] INFO impl.RoleInfo: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: shutdown 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-LeaderElection2
datanode3_1  | 2023-01-06 03:53:36,030 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-LeaderElection2] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode3_1  | 2023-01-06 03:53:36,030 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-FC9DCBE94408 with new leaderId: 678ebbfa-a167-4ea6-8846-1b1f5c050dad
datanode3_1  | 2023-01-06 03:53:36,096 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-LeaderElection2] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408: change Leader from null to 678ebbfa-a167-4ea6-8846-1b1f5c050dad at term 1 for becomeLeader, leader elected after 5538ms
datanode2_1  | 2023-01-06 03:53:24,108 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode2_1  | 2023-01-06 03:53:24,271 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode2_1  | 2023-01-06 03:53:24,276 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode2_1  | 2023-01-06 03:53:24,284 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode2_1  | 2023-01-06 03:53:24,393 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode2_1  | 2023-01-06 03:53:24,447 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode2_1  | 2023-01-06 03:53:24,499 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-LeaderElection1] INFO impl.RoleInfo: f7a31870-78ff-406d-a144-7fe17a627699: start f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-LeaderStateImpl
datanode2_1  | 2023-01-06 03:53:24,792 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-SegmentedRaftLogWorker: Starting segment from index:0
datanode2_1  | 2023-01-06 03:53:25,109 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState] INFO impl.FollowerState: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5060680994ns, electionTimeout:5002ms
datanode2_1  | 2023-01-06 03:53:25,112 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState] INFO impl.RoleInfo: f7a31870-78ff-406d-a144-7fe17a627699: shutdown f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState
datanode2_1  | 2023-01-06 03:53:25,114 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode2_1  | 2023-01-06 03:53:25,164 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode2_1  | 2023-01-06 03:53:25,165 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState] INFO impl.RoleInfo: f7a31870-78ff-406d-a144-7fe17a627699: start f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection2
datanode2_1  | 2023-01-06 03:53:25,489 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection2] INFO impl.LeaderElection: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-01-06 03:53:25,673 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-01-06 03:53:25,675 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-01-06 03:53:25,734 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86
datanode2_1  | 2023-01-06 03:53:25,751 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 678ebbfa-a167-4ea6-8846-1b1f5c050dad
datanode2_1  | 2023-01-06 03:53:26,916 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-f7a31870-78ff-406d-a144-7fe17a627699: Detected pause in JVM or host machine (eg GC): pause of approximately 1031451286ns.
datanode2_1  | GC pool 'ParNew' had collection(s): count=1 time=1111ms
datanode2_1  | 2023-01-06 03:53:26,916 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@6ee7298b] INFO util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1031ms
datanode2_1  | GC pool 'ParNew' had collection(s): count=1 time=1111ms
datanode2_1  | 2023-01-06 03:53:25,734 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-LeaderElection1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1: set configuration 0: peers:[f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-01-06 03:53:28,397 [f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f7a31870-78ff-406d-a144-7fe17a627699@group-69810F3160D1-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5af4a3fc-47d6-497e-93af-69810f3160d1/current/log_inprogress_0
datanode2_1  | 2023-01-06 03:53:28,956 [grpc-default-executor-3] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100: receive requestVote(ELECTION, d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86, group-5839005B1100, 1, (t:0, i:0))
datanode2_1  | 2023-01-06 03:53:28,961 [grpc-default-executor-3] INFO impl.VoteContext: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-CANDIDATE: reject ELECTION from d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: already has voted for f7a31870-78ff-406d-a144-7fe17a627699 at current term 1
datanode2_1  | 2023-01-06 03:53:29,033 [grpc-default-executor-3] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100 replies to ELECTION vote request: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86<-f7a31870-78ff-406d-a144-7fe17a627699#0:FAIL-t1. Peer's state: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100:t1, leader=null, voted=f7a31870-78ff-406d-a144-7fe17a627699, raftlog=Memoized:f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-01-06 03:53:29,203 [grpc-default-executor-1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100: receive requestVote(ELECTION, 678ebbfa-a167-4ea6-8846-1b1f5c050dad, group-5839005B1100, 1, (t:0, i:0))
datanode2_1  | 2023-01-06 03:53:29,204 [grpc-default-executor-1] INFO impl.VoteContext: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-CANDIDATE: reject ELECTION from 678ebbfa-a167-4ea6-8846-1b1f5c050dad: already has voted for f7a31870-78ff-406d-a144-7fe17a627699 at current term 1
datanode2_1  | 2023-01-06 03:53:29,226 [grpc-default-executor-1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100 replies to ELECTION vote request: 678ebbfa-a167-4ea6-8846-1b1f5c050dad<-f7a31870-78ff-406d-a144-7fe17a627699#0:FAIL-t1. Peer's state: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100:t1, leader=null, voted=f7a31870-78ff-406d-a144-7fe17a627699, raftlog=Memoized:f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-01-06 03:53:29,868 [Command processor thread] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode2_1  | 2023-01-06 03:53:29,940 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection2] INFO impl.LeaderElection: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode2_1  | 2023-01-06 03:53:29,960 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection2] INFO impl.LeaderElection:   Response 0: f7a31870-78ff-406d-a144-7fe17a627699<-d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86#0:FAIL-t1
datanode2_1  | 2023-01-06 03:53:29,960 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection2] INFO impl.LeaderElection:   Response 1: f7a31870-78ff-406d-a144-7fe17a627699<-678ebbfa-a167-4ea6-8846-1b1f5c050dad#0:FAIL-t1
datanode2_1  | 2023-01-06 03:53:29,962 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection2] INFO impl.LeaderElection: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection2 ELECTION round 0: result REJECTED
datanode2_1  | 2023-01-06 03:53:29,963 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection2] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode2_1  | 2023-01-06 03:53:29,964 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection2] INFO impl.RoleInfo: f7a31870-78ff-406d-a144-7fe17a627699: shutdown f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection2
datanode2_1  | 2023-01-06 03:53:29,964 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection2] INFO impl.RoleInfo: f7a31870-78ff-406d-a144-7fe17a627699: start f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState
datanode2_1  | 2023-01-06 03:53:29,996 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-01-06 03:53:29,996 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-01-06 03:53:31,185 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=c742cece-fb27-47b9-b231-5839005b1100.
datanode2_1  | 2023-01-06 03:53:31,186 [Command processor thread] INFO server.RaftServer: f7a31870-78ff-406d-a144-7fe17a627699: addNew group-507410C3658E:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER] returns group-507410C3658E:java.util.concurrent.CompletableFuture@10310adc[Not completed]
datanode2_1  | 2023-01-06 03:53:31,204 [pool-24-thread-1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699: new RaftServerImpl for group-507410C3658E:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode2_1  | 2023-01-06 03:53:31,206 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode2_1  | 2023-01-06 03:53:31,207 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode2_1  | 2023-01-06 03:53:31,208 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode2_1  | 2023-01-06 03:53:31,208 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2023-01-06 03:53:31,208 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2023-01-06 03:53:31,209 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode2_1  | 2023-01-06 03:53:31,210 [pool-24-thread-1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E: ConfigurationManager, init=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode2_1  | 2023-01-06 03:53:31,211 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2023-01-06 03:53:31,213 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode2_1  | 2023-01-06 03:53:31,215 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode2_1  | 2023-01-06 03:53:31,215 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode2_1  | 2023-01-06 03:53:31,215 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode2_1  | 2023-01-06 03:53:31,216 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode2_1  | 2023-01-06 03:53:31,221 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2023-01-06 03:53:31,221 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode2_1  | 2023-01-06 03:53:31,224 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode2_1  | 2023-01-06 03:53:31,225 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode2_1  | 2023-01-06 03:53:31,228 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om2_1        | 0020: E2 8C 95 3C 0B 0B 1A AD   6E 89 50 B3 9C 01 22 36  ...<....n.P..."6
om2_1        | 0030: 82 EA 04 5E 17 F6 FE 59   8A 78 83 34 58 E3 1A CF  ...^...Y.x.4X...
om2_1        | 0040: C0 08 87 80 9B 7D B8 33   3C E2 09 5B A0 BB 73 F9  .......3<..[..s.
om2_1        | 0050: 18 A1 8D D7 79 71 5D F0   D7 29 1C F6 F1 1A 40 0C  ....yq]..)....@.
om2_1        | 0060: DC B0 36 E8 EA 71 C3 CB   C9 B0 27 6D D7 9F 21 23  ..6..q....'m..!#
om2_1        | 0070: 40 5D F0 5B 0A B7 A0 74   F9 9F AD 3D 48 4A CC B0  @].[...t...=HJ..
om2_1        | 0080: 71 94 D1 27 C1 11 45 74   CC E9 49 95 45 B0 E7 F7  q..'..Et..I.E...
om2_1        | 0090: 9B 9F F3 43 1C 28 31 DC   D6 F4 73 3B 1B 52 1D 3F  ...C.(1...s;.R.?
om2_1        | 00A0: 44 76 E5 4C 63 7F 23 9B   17 64 90 51 18 89 86 33  Dv.Lc.#..d.Q...3
om2_1        | 00B0: CE 66 07 29 55 E3 3A 4D   BD 33 89 34 C3 A8 3C 51  .f.)U.:M.3.4..<Q
om2_1        | 00C0: 52 16 00 6D 7D 61 54 1E   1A 84 F4 F5 89 B2 A8 9C  R..m.aT.........
om2_1        | 00D0: 56 66 FE E2 2A 93 FE 19   10 10 C9 9C 0D 74 9B 01  Vf..*........t..
om2_1        | 00E0: 75 1D 1C AB 74 3A E3 B8   41 66 A1 BE 62 55 B9 60  u...t:..Af..bU.`
om2_1        | 00F0: F1 84 78 6F 50 A4 59 C4   78 19 02 34 DC BA 56 C3  ..xoP.Y.x..4..V.
om2_1        | 
om2_1        | ] from file:/data/metadata/om/certs/ROOTCA-1.crt.
om2_1        | 2023-01-06 03:52:54,361 [main] INFO security.OMCertificateClient: Added certificate [
om2_1        | [
om2_1        |   Version: V3
om2_1        |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm-sub@scm1.org
om2_1        |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
om2_1        | 
om2_1        |   Key:  Sun RSA public key, 2048 bits
om2_1        |   params: null
om2_1        |   modulus: 25512042468887604642064404093025249920280347815052024483618130467625890981963310751612908088931868680366166722736700966336351841536566461994206957959065592473127025545304172175201240100170923940025313585056297682798208555874810870376218207324432498120475419600056108682275459578022550441815608531770386354956257639186299780577671839142629796440640716929439466625583225667634032960664095389171765331267228308686758487244580763947638224999982475527976426192624582650657992140973107557451334337699303987990444760298981625629400711682432328660808877074618851496477253766349631046458008273474791424727251297086392264850331
om2_1        |   public exponent: 65537
om2_1        |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
om2_1        |                To: Mon Feb 14 00:00:00 UTC 2028]
om2_1        |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
om2_1        |   SerialNumber: [    01379773 48ab]
om2_1        | 
om2_1        | Certificate Extensions: 3
om2_1        | [1]: ObjectId: 2.5.29.19 Criticality=true
om2_1        | BasicConstraints:[
om2_1        |   CA:true
om2_1        |   PathLen:2147483647
om2_1        | ]
om2_1        | 
om2_1        | [2]: ObjectId: 2.5.29.15 Criticality=true
om2_1        | KeyUsage [
om2_1        |   DigitalSignature
om2_1        |   Key_Encipherment
om2_1        |   Data_Encipherment
om2_1        |   Key_Agreement
om2_1        |   Key_CertSign
om2_1        |   Crl_Sign
om2_1        | ]
om2_1        | 
om2_1        | [3]: ObjectId: 2.5.29.17 Criticality=false
om2_1        | SubjectAlternativeName [
om2_1        |   IPAddress: 172.25.0.116
om2_1        |   DNSName: scm1.org
om2_1        | ]
om2_1        | 
om2_1        | ]
om2_1        |   Algorithm: [SHA256withRSA]
om2_1        |   Signature:
om2_1        | 0000: 71 A1 97 4D DA 8D 52 3F   E5 06 22 8E 76 61 49 C0  q..M..R?..".vaI.
om2_1        | 0010: C6 DB 6D 93 9B E0 DF 39   D0 2C 5E B2 55 3C F2 BA  ..m....9.,^.U<..
om2_1        | 0020: 2B 11 C5 2A C7 34 78 58   C1 96 DE C2 09 D7 16 49  +..*.4xX.......I
om2_1        | 0030: 14 DD 81 0D 0D EC 2B 14   A8 C2 D5 DC D9 65 72 65  ......+......ere
om2_1        | 0040: AC 06 88 C6 35 48 D5 5F   88 26 38 54 A0 34 B7 6A  ....5H._.&8T.4.j
om2_1        | 0050: 80 D9 98 32 D0 A7 83 BC   EF F9 F3 B4 3E E3 DB 99  ...2........>...
om2_1        | 0060: C8 62 68 E3 13 33 55 58   91 03 33 A0 D4 E3 E8 C7  .bh..3UX..3.....
om2_1        | 0070: 11 6E E3 D9 21 80 E7 D3   FD D9 65 FA 44 9C 4A 18  .n..!.....e.D.J.
om2_1        | 0080: 0A F3 39 BC 62 9F 21 94   BE 66 EF E8 B3 BE 21 DA  ..9.b.!..f....!.
om2_1        | 0090: 04 60 36 57 8C 00 B7 0D   3C 31 30 4D 96 93 18 D6  .`6W....<10M....
om2_1        | 00A0: DE 9E 43 8C B8 02 39 06   65 33 4A E0 D2 C5 E1 C5  ..C...9.e3J.....
om2_1        | 00B0: EC 84 E7 E9 D8 7F 26 5A   7F EC C4 ED E0 DA 89 3A  ......&Z.......:
om2_1        | 00C0: 2A 12 BB 4E A2 53 59 0A   EB 4D CC C2 5B DB 3E 6C  *..N.SY..M..[.>l
om2_1        | 00D0: 79 1A 95 EA 9D F5 37 22   32 4C FC DF 91 B5 AD 85  y.....7"2L......
om2_1        | 00E0: 28 09 F5 E5 5B 53 76 DA   63 17 AE F5 80 A5 B1 AF  (...[Sv.c.......
om2_1        | 00F0: 1B 90 43 5F FF 80 DB 39   CC 0A 02 38 20 9A EB 4F  ..C_...9...8 ..O
om2_1        | 
om2_1        | ] from file:/data/metadata/om/certs/CA-1338275743915.crt.
om2_1        | 2023-01-06 03:52:54,390 [main] INFO security.OMCertificateClient: Added certificate [
om2_1        | [
om2_1        |   Version: V3
om2_1        |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=om2
om2_1        |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
om2_1        | 
om2_1        |   Key:  Sun RSA public key, 2048 bits
om2_1        |   params: null
om2_1        |   modulus: 22794887316004889670150104615669674117163173589384476129399324335689846630590049639164930926509561151050386947671646605866495518467530635964886059923041635201580018426346779985546032422501583800778176210624314479962617912886308503580498598470754117641245928688549013079066432771119038316849467751959092235638220177835184463949331306334120212830666827666399783714407562401066257814784566925092253648424794138312510770283781566339707563490986478916995058675988587541140933025648093987425168733361056589724315204208367477282682158975928737019698132363623364863257439074144348448128798425514448615649428295991925838272653
om2_1        |   public exponent: 65537
om2_1        |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
om2_1        |                To: Sat Jan 06 00:00:00 UTC 2024]
om2_1        |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm-sub@scm1.org
om2_1        |   SerialNumber: [    01557e51 2134]
om2_1        | 
om2_1        | Certificate Extensions: 2
om2_1        | [1]: ObjectId: 2.5.29.15 Criticality=true
om2_1        | KeyUsage [
om2_1        |   DigitalSignature
om2_1        |   Key_Encipherment
om2_1        |   Data_Encipherment
om2_1        |   Key_Agreement
om2_1        | ]
om2_1        | 
om2_1        | [2]: ObjectId: 2.5.29.17 Criticality=false
om2_1        | SubjectAlternativeName [
om2_1        |   IPAddress: 172.25.0.112
om2_1        |   Other-Name: Unrecognized ObjectIdentifier: 2.16.840.1.113730.3.1.34
om2_1        | ]
om2_1        | 
om2_1        | ]
om2_1        |   Algorithm: [SHA256withRSA]
om2_1        |   Signature:
om2_1        | 0000: 9D E5 9C 76 01 A8 7C 15   8A 71 E6 46 98 09 58 17  ...v.....q.F..X.
om2_1        | 0010: B3 A3 81 03 7F 85 72 EC   B9 9C F4 88 A6 9D 0C 81  ......r.........
om2_1        | 0020: 77 A1 9D E0 AB ED 11 81   C5 33 0B AC B7 D8 4B 19  w........3....K.
om2_1        | 0030: 32 F9 FF 2D 65 66 A9 3E   91 BF 5C 95 84 C5 51 18  2..-ef.>..\...Q.
om2_1        | 0040: B5 AE 95 9D AE DF 6D 10   96 BE DF 03 9B 3F 32 BE  ......m......?2.
om2_1        | 0050: 9A A8 B0 1B 93 93 4B E4   4A 72 DA AC 6D 09 4D A7  ......K.Jr..m.M.
om2_1        | 0060: 9E BC 9D AA F5 FA 32 18   92 91 4F 5F 02 11 80 11  ......2...O_....
om2_1        | 0070: A0 C0 94 B8 17 18 62 3B   15 AA C5 67 4D DF 9E 76  ......b;...gM..v
om2_1        | 0080: E2 8E CB 75 8E F3 3A 57   02 1A 17 51 02 67 AC 21  ...u..:W...Q.g.!
om2_1        | 0090: DD 4A F0 AF 14 65 DA 0B   A3 5F 56 5E C2 6F 01 1B  .J...e..._V^.o..
om1_1        | 0080: 54 D3 7A 0F B9 54 43 F9   62 6B 4C DF 10 04 CA 21  T.z..TC.bkL....!
om1_1        | 0090: 46 11 40 74 AD E0 E5 1B   58 36 0C 20 BC 28 D9 16  F.@t....X6. .(..
om1_1        | 00A0: 54 1C 67 EF F4 36 C3 F8   4B 41 6C 0A 82 C6 B7 A1  T.g..6..KAl.....
om1_1        | 00B0: F1 C1 42 77 5F 01 7B C4   A9 6C 0C D4 1D E4 E4 37  ..Bw_....l.....7
om1_1        | 00C0: 8D 15 EC 62 61 63 42 A6   99 90 CC 52 54 D6 01 A6  ...bacB....RT...
om1_1        | 00D0: EC 1C 69 DB 0C C4 D8 34   9F 90 EE 9B EB 87 28 48  ..i....4......(H
om1_1        | 00E0: 88 10 9D 16 5F 12 6F C0   DD 96 9B 6E 3E EB 0C 68  ...._.o....n>..h
om1_1        | 00F0: F1 9F A6 45 D0 D3 41 0D   78 F3 79 31 33 F6 78 F4  ...E..A.x.y13.x.
om1_1        | 
om1_1        | ] from file:/data/metadata/om/certs/1465334869435.crt.
om1_1        | 2023-01-06 03:52:53,460 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor is started with first delay 29102826549 ms and interval 86400000 ms.
om1_1        | 2023-01-06 03:52:53,649 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-01-06 03:52:54,889 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om1_1        | 2023-01-06 03:52:54,907 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om1_1        | 2023-01-06 03:52:56,830 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om1_1        | 2023-01-06 03:52:56,916 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om1_1        | 2023-01-06 03:52:56,916 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om1_1        | 2023-01-06 03:52:57,727 [main] INFO om.OzoneManager: Created Volume s3v With Owner om required for S3Gateway operations.
om1_1        | 2023-01-06 03:52:58,114 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1        | 2023-01-06 03:52:58,116 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om1_1        | 2023-01-06 03:52:58,194 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om1_1        | 2023-01-06 03:52:58,887 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om1_1        | 2023-01-06 03:52:58,909 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1        | 2023-01-06 03:52:58,993 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: id1 and peers: om1:9872, om3:9872, om2:9872
om1_1        | 2023-01-06 03:52:59,036 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om1_1        | 2023-01-06 03:53:00,526 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om1_1        | 2023-01-06 03:53:00,558 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om1_1        | 2023-01-06 03:53:00,655 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om1_1        | 2023-01-06 03:53:00,966 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om1_1        | 2023-01-06 03:53:00,977 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om1_1        | 2023-01-06 03:53:00,978 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om1_1        | 2023-01-06 03:53:00,982 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om1_1        | 2023-01-06 03:53:00,982 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om1_1        | 2023-01-06 03:53:00,982 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om1_1        | 2023-01-06 03:53:00,985 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om1_1        | 2023-01-06 03:53:00,991 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-01-06 03:53:00,999 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om1_1        | 2023-01-06 03:53:00,999 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1        | 2023-01-06 03:53:01,057 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1        | 2023-01-06 03:53:01,074 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om1_1        | 2023-01-06 03:53:01,074 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om1_1        | 2023-01-06 03:53:03,406 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
om1_1        | 2023-01-06 03:53:03,509 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
om1_1        | 2023-01-06 03:53:03,514 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
om1_1        | 2023-01-06 03:53:03,517 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
om1_1        | 2023-01-06 03:53:03,524 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
om1_1        | 2023-01-06 03:53:03,531 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
om1_1        | 2023-01-06 03:53:03,532 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
om1_1        | 2023-01-06 03:53:03,569 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
om1_1        | 2023-01-06 03:53:03,588 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
om1_1        | 2023-01-06 03:53:03,965 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
om1_1        | 2023-01-06 03:53:03,966 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
om1_1        | 2023-01-06 03:53:04,206 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om1_1        | 2023-01-06 03:53:04,212 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om1_1        | 2023-01-06 03:53:04,221 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1        | 2023-01-06 03:53:04,222 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1        | 2023-01-06 03:53:04,265 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1        | 2023-01-06 03:53:04,301 [om1-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x19162e53] REGISTERED
om1_1        | 2023-01-06 03:53:04,322 [om1-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x19162e53] BIND: 0.0.0.0/0.0.0.0:0
om1_1        | 2023-01-06 03:53:04,324 [om1-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x19162e53, L:/0.0.0.0:35419] ACTIVE
om1_1        | 2023-01-06 03:53:04,308 [main] INFO server.RaftServer: om1: addNew group-562213E44849:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-562213E44849:java.util.concurrent.CompletableFuture@286e2b71[Not completed]
om1_1        | 2023-01-06 03:53:04,325 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om1_1        | 2023-01-06 03:53:04,446 [pool-28-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-562213E44849:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om3_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1        | 2023-01-06 03:52:26,901 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1        | /************************************************************
om3_1        | STARTUP_MSG: Starting OzoneManager
om3_1        | STARTUP_MSG:   host = om3/172.25.0.113
om3_1        | STARTUP_MSG:   args = []
om3_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om3_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om3_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/9d30c9a7ce375cd614e86022850c7c128723df6b ; compiled by 'runner' on 2023-01-06T03:19Z
om3_1        | STARTUP_MSG:   java = 11.0.14.1
om3_1        | ************************************************************/
om3_1        | 2023-01-06 03:52:27,001 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1        | 2023-01-06 03:52:35,677 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1        | 2023-01-06 03:52:38,949 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om3_1        | 2023-01-06 03:52:39,622 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1        | 2023-01-06 03:52:39,633 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om3: om3
om3_1        | 2023-01-06 03:52:39,634 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om3: om3
om3_1        | 2023-01-06 03:52:39,695 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-01-06 03:52:39,977 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om3_1        | 2023-01-06 03:52:42,223 [main] INFO reflections.Reflections: Reflections took 1960 ms to scan 1 urls, producing 115 keys and 336 values [using 2 cores]
om3_1        | 2023-01-06 03:52:43,968 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om3_1        | 2023-01-06 03:52:43,968 [main] INFO om.OzoneManager: Ozone Manager login successful.
om3_1        | 2023-01-06 03:52:43,968 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-01-06 03:52:47,643 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om3_1        | 2023-01-06 03:52:48,284 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om3_1        | 2023-01-06 03:52:52,970 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om3_1        | value: 9862
om3_1        | ]
om3_1        | 2023-01-06 03:52:53,017 [main] INFO security.OMCertificateClient: Loading certificate from location:/data/metadata/om/certs.
om3_1        | 2023-01-06 03:52:53,633 [main] INFO security.OMCertificateClient: Added certificate [
om3_1        | [
om3_1        |   Version: V3
om3_1        |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
om3_1        |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
om3_1        | 
om3_1        |   Key:  Sun RSA public key, 2048 bits
om3_1        |   params: null
om3_1        |   modulus: 22321520092034904956367956777931957676969911078232901017895933698678468781753172695844393013092965970615602622357794023639048732305076160434218405355746465972426985604516863422070797512086951139731951669174461346685083874381071907314493811292043757517828199039562222504144931947786863316592495947302761724410796725398590551213642383206197662885192142834321323519855792963024540129867342534976672519842084316567886340381000959465926087773440860829229209280683297485436375405886176389600891889084968589860808245139826791285487072108592486434566834307409916351251604701981140849210029026671497960989540995516629779904417
om3_1        |   public exponent: 65537
om3_1        |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
om3_1        |                To: Mon Feb 14 00:00:00 UTC 2028]
om3_1        |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
om3_1        |   SerialNumber: [    01]
om3_1        | 
om3_1        | Certificate Extensions: 3
om3_1        | [1]: ObjectId: 2.5.29.19 Criticality=true
om3_1        | BasicConstraints:[
om3_1        |   CA:true
om3_1        |   PathLen:2147483647
om3_1        | ]
om3_1        | 
om3_1        | [2]: ObjectId: 2.5.29.15 Criticality=true
om3_1        | KeyUsage [
om3_1        |   Key_CertSign
om3_1        |   Crl_Sign
om3_1        | ]
om3_1        | 
om3_1        | [3]: ObjectId: 2.5.29.17 Criticality=false
om3_1        | SubjectAlternativeName [
om3_1        |   IPAddress: 172.25.0.116
om3_1        |   DNSName: scm1.org
om3_1        | ]
om3_1        | 
om3_1        | ]
om3_1        |   Algorithm: [SHA256withRSA]
om3_1        |   Signature:
om3_1        | 0000: 21 07 40 68 BD F5 21 63   53 A4 1C 6D 61 63 C4 B6  !.@h..!cS..mac..
om3_1        | 0010: E3 BD 40 F3 9B FD 99 A0   FE 5C 7B FD 06 FB 50 8E  ..@......\....P.
datanode1_1  | 2023-01-06 03:53:32,977 [Command processor thread] INFO server.RaftServer: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: addNew group-BF9CF73BE954:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER] returns group-BF9CF73BE954:java.util.concurrent.CompletableFuture@3856f532[Not completed]
datanode1_1  | 2023-01-06 03:53:32,985 [pool-24-thread-1] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: new RaftServerImpl for group-BF9CF73BE954:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode1_1  | 2023-01-06 03:53:32,986 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode1_1  | 2023-01-06 03:53:32,987 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode1_1  | 2023-01-06 03:53:32,988 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode1_1  | 2023-01-06 03:53:32,989 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2023-01-06 03:53:32,989 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2023-01-06 03:53:32,989 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode1_1  | 2023-01-06 03:53:32,990 [pool-24-thread-1] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954: ConfigurationManager, init=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode1_1  | 2023-01-06 03:53:32,991 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2023-01-06 03:53:32,992 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode1_1  | 2023-01-06 03:53:32,993 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode1_1  | 2023-01-06 03:53:32,993 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode1_1  | 2023-01-06 03:53:32,994 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode1_1  | 2023-01-06 03:53:32,994 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode1_1  | 2023-01-06 03:53:32,999 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2023-01-06 03:53:33,000 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode1_1  | 2023-01-06 03:53:33,004 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode1_1  | 2023-01-06 03:53:33,004 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode1_1  | 2023-01-06 03:53:33,004 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode1_1  | 2023-01-06 03:53:33,006 [pool-24-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/88086403-84d0-4755-ada1-bf9cf73be954 does not exist. Creating ...
datanode1_1  | 2023-01-06 03:53:33,057 [pool-24-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/88086403-84d0-4755-ada1-bf9cf73be954/in_use.lock acquired by nodename 7@98e33c1634d3
datanode1_1  | 2023-01-06 03:53:33,070 [pool-24-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/88086403-84d0-4755-ada1-bf9cf73be954 has been successfully formatted.
datanode1_1  | 2023-01-06 03:53:33,072 [pool-24-thread-1] INFO ratis.ContainerStateMachine: group-BF9CF73BE954: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode1_1  | 2023-01-06 03:53:33,101 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode1_1  | 2023-01-06 03:53:33,101 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode1_1  | 2023-01-06 03:53:33,102 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-01-06 03:53:33,102 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode1_1  | 2023-01-06 03:53:33,102 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode1_1  | 2023-01-06 03:53:33,156 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-01-06 03:53:33,161 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode1_1  | 2023-01-06 03:53:33,163 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode1_1  | 2023-01-06 03:53:33,163 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: new d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/88086403-84d0-4755-ada1-bf9cf73be954
datanode1_1  | 2023-01-06 03:53:33,163 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode1_1  | 2023-01-06 03:53:33,164 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode1_1  | 2023-01-06 03:53:33,215 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-01-06 03:53:33,216 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode1_1  | 2023-01-06 03:53:33,216 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode1_1  | 2023-01-06 03:53:33,216 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode1_1  | 2023-01-06 03:53:33,217 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode1_1  | 2023-01-06 03:53:33,217 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode1_1  | 2023-01-06 03:53:33,219 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode1_1  | 2023-01-06 03:53:33,220 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-01-06 03:53:33,295 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode1_1  | 2023-01-06 03:53:33,295 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode1_1  | 2023-01-06 03:53:33,295 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode1_1  | 2023-01-06 03:53:33,296 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-01-06 03:53:33,296 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-01-06 03:53:33,297 [pool-24-thread-1] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954: start as a follower, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-06 03:53:33,297 [pool-24-thread-1] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode1_1  | 2023-01-06 03:53:33,298 [pool-24-thread-1] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: start d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-FollowerState
datanode1_1  | 2023-01-06 03:53:33,299 [pool-24-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-BF9CF73BE954,id=d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86
datanode1_1  | 2023-01-06 03:53:33,300 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode1_1  | 2023-01-06 03:53:33,300 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode1_1  | 2023-01-06 03:53:33,300 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode1_1  | 2023-01-06 03:53:33,301 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode1_1  | 2023-01-06 03:53:33,302 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-01-06 03:53:33,302 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-01-06 03:53:33,342 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=88086403-84d0-4755-ada1-bf9cf73be954
datanode1_1  | 2023-01-06 03:53:33,343 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=88086403-84d0-4755-ada1-bf9cf73be954.
datanode1_1  | 2023-01-06 03:53:33,714 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO impl.FollowerState: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5176403086ns, electionTimeout:5139ms
datanode1_1  | 2023-01-06 03:53:33,715 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: shutdown d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState
datanode1_1  | 2023-01-06 03:53:33,715 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode1_1  | 2023-01-06 03:53:33,715 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode1_1  | 2023-01-06 03:53:33,715 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: start d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection2
datanode1_1  | 2023-01-06 03:53:33,719 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection2] INFO impl.LeaderElection: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for -1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-06 03:53:33,737 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-01-06 03:53:33,761 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-01-06 03:53:33,766 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection2] INFO impl.LeaderElection: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection2: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode1_1  | 2023-01-06 03:53:33,768 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection2] INFO impl.LeaderElection:   Response 0: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86<-678ebbfa-a167-4ea6-8846-1b1f5c050dad#0:FAIL-t2
datanode1_1  | 2023-01-06 03:53:33,769 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection2] INFO impl.LeaderElection: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection2 ELECTION round 0: result REJECTED
datanode1_1  | 2023-01-06 03:53:33,769 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection2] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode1_1  | 2023-01-06 03:53:33,769 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection2] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: shutdown d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection2
datanode1_1  | 2023-01-06 03:53:33,769 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection2] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: start d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState
datanode1_1  | 2023-01-06 03:53:33,781 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-01-06 03:53:33,784 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-01-06 03:53:36,591 [grpc-default-executor-1] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E: receive requestVote(ELECTION, f7a31870-78ff-406d-a144-7fe17a627699, group-507410C3658E, 1, (t:0, i:0))
datanode1_1  | 2023-01-06 03:53:36,591 [grpc-default-executor-1] INFO impl.VoteContext: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-FOLLOWER: reject ELECTION from f7a31870-78ff-406d-a144-7fe17a627699: our priority 1 > candidate's priority 0
datanode1_1  | 2023-01-06 03:53:36,591 [grpc-default-executor-1] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:f7a31870-78ff-406d-a144-7fe17a627699
datanode1_1  | 2023-01-06 03:53:36,592 [grpc-default-executor-1] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: shutdown d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-FollowerState
datanode1_1  | 2023-01-06 03:53:36,592 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-FollowerState] INFO impl.FollowerState: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-FollowerState was interrupted
datanode1_1  | 2023-01-06 03:53:36,593 [grpc-default-executor-1] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: start d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-FollowerState
datanode3_1  | 2023-01-06 03:53:36,147 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode3_1  | 2023-01-06 03:53:36,184 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode3_1  | 2023-01-06 03:53:36,206 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode3_1  | 2023-01-06 03:53:36,500 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode3_1  | 2023-01-06 03:53:36,502 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode3_1  | 2023-01-06 03:53:36,504 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode3_1  | 2023-01-06 03:53:36,618 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode3_1  | 2023-01-06 03:53:36,662 [grpc-default-executor-2] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E: receive requestVote(ELECTION, f7a31870-78ff-406d-a144-7fe17a627699, group-507410C3658E, 1, (t:0, i:0))
recon_1      | Sleeping for 5 seconds
recon_1      | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1      | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1      | 2023-01-06 03:49:50,456 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1      | /************************************************************
recon_1      | STARTUP_MSG: Starting ReconServer
recon_1      | STARTUP_MSG:   host = recon/172.25.0.115
recon_1      | STARTUP_MSG:   args = []
recon_1      | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1      | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.23.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.3.23.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.23.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.23.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.23.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
recon_1      | STARTUP_MSG:   build = https://github.com/apache/ozone/9d30c9a7ce375cd614e86022850c7c128723df6b ; compiled by 'runner' on 2023-01-06T03:19Z
recon_1      | STARTUP_MSG:   java = 11.0.14.1
recon_1      | ************************************************************/
recon_1      | 2023-01-06 03:49:50,500 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1      | 2023-01-06 03:49:53,243 [main] INFO reflections.Reflections: Reflections took 394 ms to scan 1 urls, producing 16 keys and 51 values 
recon_1      | 2023-01-06 03:49:56,246 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1      | 2023-01-06 03:49:56,401 [main] INFO recon.ReconServer: Ozone security is enabled. Attempting login for Recon service. Principal: recon/recon@EXAMPLE.COM, keytab: /etc/security/keytabs/recon.keytab
recon_1      | 2023-01-06 03:49:57,045 [main] INFO security.UserGroupInformation: Login successful for user recon/recon@EXAMPLE.COM using keytab file recon.keytab. Keytab auto renewal enabled : false
recon_1      | 2023-01-06 03:49:57,045 [main] INFO recon.ReconServer: Recon login successful.
recon_1      | 2023-01-06 03:49:57,083 [main] INFO recon.ReconServer: ReconStorageConfig initialized.Initializing certificate.
recon_1      | 2023-01-06 03:49:57,084 [main] INFO recon.ReconServer: Initializing secure Recon.
recon_1      | 2023-01-06 03:49:58,562 [main] ERROR client.ReconCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
recon_1      | 2023-01-06 03:49:58,564 [main] INFO client.ReconCertificateClient: Certificate client init case: 0
recon_1      | 2023-01-06 03:49:58,567 [main] INFO client.ReconCertificateClient: Creating keypair for client as keypair and certificate not found.
recon_1      | 2023-01-06 03:50:01,777 [main] INFO recon.ReconServer: Init response: GETCERT
recon_1      | 2023-01-06 03:50:01,782 [main] INFO client.ReconCertificateClient: Creating CSR for Recon.
recon_1      | 2023-01-06 03:50:01,816 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.115,host:recon
recon_1      | 2023-01-06 03:50:01,816 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
recon_1      | 2023-01-06 03:50:01,823 [main] ERROR client.ReconCertificateClient: Invalid domain recon
recon_1      | 2023-01-06 03:50:05,055 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy39.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9961 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:50:07,058 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy39.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9961 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:50:09,060 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm1.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy39.submitRequest over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9961 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:50:11,062 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy39.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9961 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:50:13,064 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy39.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9961 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:50:15,066 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm1.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy39.submitRequest over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9961 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:50:17,068 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy39.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9961 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:50:19,070 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy39.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9961 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:50:24,039 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674 is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1      | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
recon_1      | 	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:93)
recon_1      | 	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:16080)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1      | , while invoking $Proxy39.submitRequest over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9961 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:50:26,041 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy39.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9961 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:50:28,043 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy39.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9961 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:50:30,505 [main] INFO client.ReconCertificateClient: Loading certificate from location:/data/metadata/recon/certs.
recon_1      | 2023-01-06 03:50:30,521 [main] INFO client.ReconCertificateClient: Added certificate [
om1_1        | 2023-01-06 03:53:04,479 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om1_1        | 2023-01-06 03:53:04,483 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om1_1        | 2023-01-06 03:53:04,483 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om1_1        | 2023-01-06 03:53:04,488 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1        | 2023-01-06 03:53:04,492 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1        | 2023-01-06 03:53:04,492 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om1_1        | 2023-01-06 03:53:04,516 [main] INFO om.OzoneManager: Creating RPC Server
om1_1        | 2023-01-06 03:53:04,584 [pool-28-thread-1] INFO server.RaftServer$Division: om1@group-562213E44849: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om1_1        | 2023-01-06 03:53:04,585 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1        | 2023-01-06 03:53:04,647 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om1_1        | 2023-01-06 03:53:04,772 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om1_1        | 2023-01-06 03:53:04,919 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om1_1        | 2023-01-06 03:53:04,953 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om1_1        | 2023-01-06 03:53:04,955 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om1_1        | 2023-01-06 03:53:05,622 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1        | 2023-01-06 03:53:05,627 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om1_1        | 2023-01-06 03:53:05,631 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om1_1        | 2023-01-06 03:53:05,667 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om1_1        | 2023-01-06 03:53:05,672 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om1_1        | 2023-01-06 03:53:07,739 [main] INFO reflections.Reflections: Reflections took 2612 ms to scan 8 urls, producing 23 keys and 540 values [using 2 cores]
om1_1        | 2023-01-06 03:53:09,117 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om1_1        | 2023-01-06 03:53:09,162 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om1_1        | 2023-01-06 03:53:14,314 [Listener at om1/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om1_1        | 2023-01-06 03:53:14,411 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om1_1        | 2023-01-06 03:53:14,416 [Listener at om1/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om1_1        | 2023-01-06 03:53:14,795 [Listener at om1/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om1/172.25.0.111:9862
om1_1        | 2023-01-06 03:53:14,795 [Listener at om1/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om1_1        | 2023-01-06 03:53:14,809 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 does not exist. Creating ...
om1_1        | 2023-01-06 03:53:14,822 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/in_use.lock acquired by nodename 7@om1
om1_1        | 2023-01-06 03:53:14,858 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 has been successfully formatted.
om1_1        | 2023-01-06 03:53:14,873 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om1_1        | 2023-01-06 03:53:14,987 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1        | 2023-01-06 03:53:14,987 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-01-06 03:53:14,999 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om1_1        | 2023-01-06 03:53:15,004 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om1_1        | 2023-01-06 03:53:15,031 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1        | 2023-01-06 03:53:15,072 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om1_1        | 2023-01-06 03:53:15,073 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1        | 2023-01-06 03:53:15,123 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-562213E44849-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849
om1_1        | 2023-01-06 03:53:15,124 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om1_1        | 2023-01-06 03:53:15,129 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om1_1        | 2023-01-06 03:53:15,135 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1        | 2023-01-06 03:53:15,135 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om1_1        | 2023-01-06 03:53:15,136 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1        | 2023-01-06 03:53:15,146 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om1_1        | 2023-01-06 03:53:15,148 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om1_1        | 2023-01-06 03:53:15,149 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om1_1        | 2023-01-06 03:53:15,207 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om1_1        | 2023-01-06 03:53:15,207 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-01-06 03:53:15,313 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om1_1        | 2023-01-06 03:53:15,316 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om1_1        | 2023-01-06 03:53:15,320 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om3_1        | 0020: E2 8C 95 3C 0B 0B 1A AD   6E 89 50 B3 9C 01 22 36  ...<....n.P..."6
om3_1        | 0030: 82 EA 04 5E 17 F6 FE 59   8A 78 83 34 58 E3 1A CF  ...^...Y.x.4X...
om3_1        | 0040: C0 08 87 80 9B 7D B8 33   3C E2 09 5B A0 BB 73 F9  .......3<..[..s.
om3_1        | 0050: 18 A1 8D D7 79 71 5D F0   D7 29 1C F6 F1 1A 40 0C  ....yq]..)....@.
om3_1        | 0060: DC B0 36 E8 EA 71 C3 CB   C9 B0 27 6D D7 9F 21 23  ..6..q....'m..!#
om3_1        | 0070: 40 5D F0 5B 0A B7 A0 74   F9 9F AD 3D 48 4A CC B0  @].[...t...=HJ..
om3_1        | 0080: 71 94 D1 27 C1 11 45 74   CC E9 49 95 45 B0 E7 F7  q..'..Et..I.E...
om3_1        | 0090: 9B 9F F3 43 1C 28 31 DC   D6 F4 73 3B 1B 52 1D 3F  ...C.(1...s;.R.?
om3_1        | 00A0: 44 76 E5 4C 63 7F 23 9B   17 64 90 51 18 89 86 33  Dv.Lc.#..d.Q...3
om3_1        | 00B0: CE 66 07 29 55 E3 3A 4D   BD 33 89 34 C3 A8 3C 51  .f.)U.:M.3.4..<Q
om3_1        | 00C0: 52 16 00 6D 7D 61 54 1E   1A 84 F4 F5 89 B2 A8 9C  R..m.aT.........
om3_1        | 00D0: 56 66 FE E2 2A 93 FE 19   10 10 C9 9C 0D 74 9B 01  Vf..*........t..
om3_1        | 00E0: 75 1D 1C AB 74 3A E3 B8   41 66 A1 BE 62 55 B9 60  u...t:..Af..bU.`
om3_1        | 00F0: F1 84 78 6F 50 A4 59 C4   78 19 02 34 DC BA 56 C3  ..xoP.Y.x..4..V.
om3_1        | 
om3_1        | ] from file:/data/metadata/om/certs/ROOTCA-1.crt.
om3_1        | 2023-01-06 03:52:53,666 [main] INFO security.OMCertificateClient: Added certificate [
om3_1        | [
om3_1        |   Version: V3
om3_1        |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm-sub@scm1.org
om3_1        |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
om3_1        | 
om3_1        |   Key:  Sun RSA public key, 2048 bits
om3_1        |   params: null
om3_1        |   modulus: 25512042468887604642064404093025249920280347815052024483618130467625890981963310751612908088931868680366166722736700966336351841536566461994206957959065592473127025545304172175201240100170923940025313585056297682798208555874810870376218207324432498120475419600056108682275459578022550441815608531770386354956257639186299780577671839142629796440640716929439466625583225667634032960664095389171765331267228308686758487244580763947638224999982475527976426192624582650657992140973107557451334337699303987990444760298981625629400711682432328660808877074618851496477253766349631046458008273474791424727251297086392264850331
om3_1        |   public exponent: 65537
om3_1        |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
om3_1        |                To: Mon Feb 14 00:00:00 UTC 2028]
om3_1        |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
om3_1        |   SerialNumber: [    01379773 48ab]
om3_1        | 
om3_1        | Certificate Extensions: 3
om3_1        | [1]: ObjectId: 2.5.29.19 Criticality=true
om3_1        | BasicConstraints:[
om3_1        |   CA:true
om3_1        |   PathLen:2147483647
om3_1        | ]
om3_1        | 
om3_1        | [2]: ObjectId: 2.5.29.15 Criticality=true
om3_1        | KeyUsage [
om3_1        |   DigitalSignature
om3_1        |   Key_Encipherment
om3_1        |   Data_Encipherment
om3_1        |   Key_Agreement
om3_1        |   Key_CertSign
om3_1        |   Crl_Sign
om3_1        | ]
om3_1        | 
om3_1        | [3]: ObjectId: 2.5.29.17 Criticality=false
om3_1        | SubjectAlternativeName [
om3_1        |   IPAddress: 172.25.0.116
om3_1        |   DNSName: scm1.org
om3_1        | ]
om3_1        | 
om3_1        | ]
om3_1        |   Algorithm: [SHA256withRSA]
om3_1        |   Signature:
om3_1        | 0000: 71 A1 97 4D DA 8D 52 3F   E5 06 22 8E 76 61 49 C0  q..M..R?..".vaI.
om3_1        | 0010: C6 DB 6D 93 9B E0 DF 39   D0 2C 5E B2 55 3C F2 BA  ..m....9.,^.U<..
om3_1        | 0020: 2B 11 C5 2A C7 34 78 58   C1 96 DE C2 09 D7 16 49  +..*.4xX.......I
om3_1        | 0030: 14 DD 81 0D 0D EC 2B 14   A8 C2 D5 DC D9 65 72 65  ......+......ere
om3_1        | 0040: AC 06 88 C6 35 48 D5 5F   88 26 38 54 A0 34 B7 6A  ....5H._.&8T.4.j
om3_1        | 0050: 80 D9 98 32 D0 A7 83 BC   EF F9 F3 B4 3E E3 DB 99  ...2........>...
om3_1        | 0060: C8 62 68 E3 13 33 55 58   91 03 33 A0 D4 E3 E8 C7  .bh..3UX..3.....
om3_1        | 0070: 11 6E E3 D9 21 80 E7 D3   FD D9 65 FA 44 9C 4A 18  .n..!.....e.D.J.
om3_1        | 0080: 0A F3 39 BC 62 9F 21 94   BE 66 EF E8 B3 BE 21 DA  ..9.b.!..f....!.
om3_1        | 0090: 04 60 36 57 8C 00 B7 0D   3C 31 30 4D 96 93 18 D6  .`6W....<10M....
om3_1        | 00A0: DE 9E 43 8C B8 02 39 06   65 33 4A E0 D2 C5 E1 C5  ..C...9.e3J.....
om3_1        | 00B0: EC 84 E7 E9 D8 7F 26 5A   7F EC C4 ED E0 DA 89 3A  ......&Z.......:
om3_1        | 00C0: 2A 12 BB 4E A2 53 59 0A   EB 4D CC C2 5B DB 3E 6C  *..N.SY..M..[.>l
om3_1        | 00D0: 79 1A 95 EA 9D F5 37 22   32 4C FC DF 91 B5 AD 85  y.....7"2L......
om3_1        | 00E0: 28 09 F5 E5 5B 53 76 DA   63 17 AE F5 80 A5 B1 AF  (...[Sv.c.......
om3_1        | 00F0: 1B 90 43 5F FF 80 DB 39   CC 0A 02 38 20 9A EB 4F  ..C_...9...8 ..O
om3_1        | 
om3_1        | ] from file:/data/metadata/om/certs/CA-1338275743915.crt.
om3_1        | 2023-01-06 03:52:53,696 [main] INFO security.OMCertificateClient: Added certificate [
om3_1        | [
om3_1        |   Version: V3
om3_1        |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=om3
om3_1        |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
om3_1        | 
om3_1        |   Key:  Sun RSA public key, 2048 bits
om3_1        |   params: null
om3_1        |   modulus: 29171023403752397543226298287939430417473046744892880389004232340855110246017241461780444268228369464393251069865160116860787369754616257408062813231852063600147290999793500439190832674079872789024680935452510462035396994921604385715661901070566481558841337098332683245859604245950164049473537093927399321751213136199691470433568638898133063160623254460951212426162044879536563211206266809832964804342442625893054588779700692829327836790998724399431830565677457989026749946036805303198054828757076222450256063100960899902219260467134965364569769830252495887016078353736554967431271903441456746138526072750269496705911
om3_1        |   public exponent: 65537
om3_1        |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
om3_1        |                To: Sat Jan 06 00:00:00 UTC 2024]
om3_1        |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm-sub@scm1.org
om3_1        |   SerialNumber: [    0155135e 3242]
om3_1        | 
om3_1        | Certificate Extensions: 2
om3_1        | [1]: ObjectId: 2.5.29.15 Criticality=true
om2_1        | 00A0: B2 69 AB B4 5D E4 9B 61   0C 14 72 20 9C EB 98 47  .i..]..a..r ...G
om2_1        | 00B0: 8D 57 B4 AD DE 2E F2 92   9E 16 97 DF EC 79 35 89  .W...........y5.
om2_1        | 00C0: 42 E0 8A A1 5A 8E 00 E1   EE 97 CE 7F 36 23 32 0C  B...Z.......6#2.
om2_1        | 00D0: 40 67 CE 5A 9A 35 A6 7D   14 2B 00 57 3F D3 46 68  @g.Z.5...+.W?.Fh
om2_1        | 00E0: B8 9E FF 4F F8 8B 3C 3C   F8 02 23 C1 D4 93 66 D7  ...O..<<..#...f.
om2_1        | 00F0: 55 0E 05 95 96 48 41 20   06 89 2B CA 1E 63 7D AB  U....HA ..+..c..
om2_1        | 
om2_1        | ] from file:/data/metadata/om/certs/1466703094068.crt.
om2_1        | 2023-01-06 03:52:54,418 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor is started with first delay 29102825597 ms and interval 86400000 ms.
om2_1        | 2023-01-06 03:52:54,652 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-01-06 03:52:55,729 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om2_1        | 2023-01-06 03:52:55,737 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om2_1        | 2023-01-06 03:52:57,305 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om2_1        | 2023-01-06 03:52:57,363 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om2_1        | 2023-01-06 03:52:57,363 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om2_1        | 2023-01-06 03:52:58,003 [main] INFO om.OzoneManager: Created Volume s3v With Owner om required for S3Gateway operations.
om2_1        | 2023-01-06 03:52:58,609 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1        | 2023-01-06 03:52:58,614 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om2_1        | 2023-01-06 03:52:58,718 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om2_1        | 2023-01-06 03:52:59,615 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om2_1        | 2023-01-06 03:52:59,655 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1        | 2023-01-06 03:52:59,813 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: id1 and peers: om2:9872, om1:9872, om3:9872
om2_1        | 2023-01-06 03:53:00,058 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om2_1        | 2023-01-06 03:53:01,946 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om2_1        | 2023-01-06 03:53:02,001 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om2_1        | 2023-01-06 03:53:02,148 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om2_1        | 2023-01-06 03:53:02,706 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om2_1        | 2023-01-06 03:53:02,719 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om2_1        | 2023-01-06 03:53:02,720 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om2_1        | 2023-01-06 03:53:02,724 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om2_1        | 2023-01-06 03:53:02,731 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om2_1        | 2023-01-06 03:53:02,733 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om2_1        | 2023-01-06 03:53:02,762 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om2_1        | 2023-01-06 03:53:02,764 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2023-01-06 03:53:02,773 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om2_1        | 2023-01-06 03:53:02,776 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1        | 2023-01-06 03:53:02,879 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1        | 2023-01-06 03:53:02,902 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om2_1        | 2023-01-06 03:53:02,905 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om2_1        | 2023-01-06 03:53:05,559 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
om2_1        | 2023-01-06 03:53:05,716 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
om2_1        | 2023-01-06 03:53:05,723 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
om2_1        | 2023-01-06 03:53:05,731 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
om2_1        | 2023-01-06 03:53:05,735 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
om2_1        | 2023-01-06 03:53:05,755 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
om2_1        | 2023-01-06 03:53:05,758 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
om2_1        | 2023-01-06 03:53:05,793 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
om2_1        | 2023-01-06 03:53:05,818 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
om2_1        | 2023-01-06 03:53:06,040 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
om2_1        | 2023-01-06 03:53:06,041 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
om2_1        | 2023-01-06 03:53:06,347 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om2_1        | 2023-01-06 03:53:06,348 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om2_1        | 2023-01-06 03:53:06,356 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1        | 2023-01-06 03:53:06,356 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1        | 2023-01-06 03:53:06,395 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1        | 2023-01-06 03:53:06,418 [om2-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xba40baba] REGISTERED
om2_1        | 2023-01-06 03:53:06,439 [om2-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xba40baba] BIND: 0.0.0.0/0.0.0.0:0
om2_1        | 2023-01-06 03:53:06,451 [om2-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xba40baba, L:/0.0.0.0:40167] ACTIVE
om2_1        | 2023-01-06 03:53:06,458 [main] INFO server.RaftServer: om2: addNew group-562213E44849:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-562213E44849:java.util.concurrent.CompletableFuture@5f092eaa[Not completed]
datanode3_1  | 2023-01-06 03:53:36,664 [grpc-default-executor-2] INFO impl.VoteContext: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-FOLLOWER: accept ELECTION from f7a31870-78ff-406d-a144-7fe17a627699: our priority 0 <= candidate's priority 0
datanode3_1  | 2023-01-06 03:53:36,665 [grpc-default-executor-2] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:f7a31870-78ff-406d-a144-7fe17a627699
datanode3_1  | 2023-01-06 03:53:36,668 [grpc-default-executor-2] INFO impl.RoleInfo: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: shutdown 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-FollowerState
datanode3_1  | 2023-01-06 03:53:36,670 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-FollowerState] INFO impl.FollowerState: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-FollowerState was interrupted
datanode3_1  | 2023-01-06 03:53:36,670 [grpc-default-executor-2] INFO impl.RoleInfo: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: start 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-FollowerState
datanode3_1  | 2023-01-06 03:53:36,726 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode3_1  | 2023-01-06 03:53:36,727 [grpc-default-executor-2] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E replies to ELECTION vote request: f7a31870-78ff-406d-a144-7fe17a627699<-678ebbfa-a167-4ea6-8846-1b1f5c050dad#0:OK-t1. Peer's state: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E:t1, leader=null, voted=f7a31870-78ff-406d-a144-7fe17a627699, raftlog=Memoized:678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-06 03:53:36,731 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-LeaderElection2] INFO impl.RoleInfo: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: start 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-LeaderStateImpl
datanode3_1  | 2023-01-06 03:53:36,728 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-01-06 03:53:36,771 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-01-06 03:53:36,791 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-SegmentedRaftLogWorker: Starting segment from index:0
datanode3_1  | 2023-01-06 03:53:36,854 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-LeaderElection2] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408: set configuration 0: peers:[678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-06 03:53:37,122 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-FC9DCBE94408-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d88fe7be-cf18-4d52-90b0-fc9dcbe94408/current/log_inprogress_0
datanode3_1  | 2023-01-06 03:53:38,879 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState] INFO impl.FollowerState: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5149163101ns, electionTimeout:5099ms
datanode3_1  | 2023-01-06 03:53:38,879 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState] INFO impl.RoleInfo: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: shutdown 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState
datanode3_1  | 2023-01-06 03:53:38,881 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode3_1  | 2023-01-06 03:53:38,881 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode3_1  | 2023-01-06 03:53:38,882 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState] INFO impl.RoleInfo: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: start 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection3
datanode3_1  | 2023-01-06 03:53:38,902 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection3] INFO impl.LeaderElection: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection3 ELECTION round 0: submit vote requests at term 3 for -1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-06 03:53:38,912 [grpc-default-executor-2] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100: receive requestVote(ELECTION, d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86, group-5839005B1100, 3, (t:0, i:0))
datanode3_1  | 2023-01-06 03:53:38,934 [grpc-default-executor-2] INFO impl.VoteContext: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-CANDIDATE: reject ELECTION from d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: already has voted for 678ebbfa-a167-4ea6-8846-1b1f5c050dad at current term 3
datanode3_1  | 2023-01-06 03:53:38,937 [grpc-default-executor-2] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100 replies to ELECTION vote request: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86<-678ebbfa-a167-4ea6-8846-1b1f5c050dad#0:FAIL-t3. Peer's state: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100:t3, leader=null, voted=678ebbfa-a167-4ea6-8846-1b1f5c050dad, raftlog=Memoized:678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-06 03:53:38,952 [grpc-default-executor-2] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100: receive requestVote(ELECTION, f7a31870-78ff-406d-a144-7fe17a627699, group-5839005B1100, 3, (t:0, i:0))
datanode2_1  | 2023-01-06 03:53:31,228 [pool-24-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/7faae3af-cb79-4c3a-b244-507410c3658e does not exist. Creating ...
datanode2_1  | 2023-01-06 03:53:31,236 [pool-24-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/7faae3af-cb79-4c3a-b244-507410c3658e/in_use.lock acquired by nodename 7@47737eac8b7e
datanode2_1  | 2023-01-06 03:53:31,248 [pool-24-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/7faae3af-cb79-4c3a-b244-507410c3658e has been successfully formatted.
datanode2_1  | 2023-01-06 03:53:31,262 [pool-24-thread-1] INFO ratis.ContainerStateMachine: group-507410C3658E: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode2_1  | 2023-01-06 03:53:31,274 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode2_1  | 2023-01-06 03:53:31,302 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode2_1  | 2023-01-06 03:53:31,304 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-01-06 03:53:31,305 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode2_1  | 2023-01-06 03:53:31,305 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode2_1  | 2023-01-06 03:53:31,305 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-01-06 03:53:31,321 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode2_1  | 2023-01-06 03:53:31,321 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode2_1  | 2023-01-06 03:53:31,322 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: new f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/7faae3af-cb79-4c3a-b244-507410c3658e
datanode2_1  | 2023-01-06 03:53:31,323 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode2_1  | 2023-01-06 03:53:31,325 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode2_1  | 2023-01-06 03:53:31,329 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-01-06 03:53:31,331 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode2_1  | 2023-01-06 03:53:31,334 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode2_1  | 2023-01-06 03:53:31,335 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode2_1  | 2023-01-06 03:53:31,337 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode2_1  | 2023-01-06 03:53:31,339 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode2_1  | 2023-01-06 03:53:31,351 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode2_1  | 2023-01-06 03:53:31,366 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-01-06 03:53:31,445 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode2_1  | 2023-01-06 03:53:31,451 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode2_1  | 2023-01-06 03:53:31,451 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode2_1  | 2023-01-06 03:53:31,452 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-01-06 03:53:31,457 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-01-06 03:53:31,458 [pool-24-thread-1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E: start as a follower, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-01-06 03:53:31,458 [pool-24-thread-1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode2_1  | 2023-01-06 03:53:31,463 [pool-24-thread-1] INFO impl.RoleInfo: f7a31870-78ff-406d-a144-7fe17a627699: start f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-FollowerState
datanode2_1  | 2023-01-06 03:53:31,465 [pool-24-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-507410C3658E,id=f7a31870-78ff-406d-a144-7fe17a627699
datanode2_1  | 2023-01-06 03:53:31,468 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode2_1  | 2023-01-06 03:53:31,470 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode2_1  | 2023-01-06 03:53:31,472 [f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-01-06 03:53:31,478 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode2_1  | 2023-01-06 03:53:31,478 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode2_1  | 2023-01-06 03:53:31,479 [f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-01-06 03:53:31,479 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=7faae3af-cb79-4c3a-b244-507410c3658e
datanode2_1  | 2023-01-06 03:53:31,488 [Command processor thread] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode2_1  | 2023-01-06 03:53:32,224 [Command processor thread] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode2_1  | 2023-01-06 03:53:32,822 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=7faae3af-cb79-4c3a-b244-507410c3658e.
datanode2_1  | 2023-01-06 03:53:33,753 [grpc-default-executor-1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100: receive requestVote(ELECTION, d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86, group-5839005B1100, 2, (t:0, i:0))
datanode2_1  | 2023-01-06 03:53:33,753 [grpc-default-executor-1] INFO impl.VoteContext: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FOLLOWER: accept ELECTION from d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: our priority 0 <= candidate's priority 0
datanode2_1  | 2023-01-06 03:53:33,754 [grpc-default-executor-1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86
datanode2_1  | 2023-01-06 03:53:33,754 [grpc-default-executor-1] INFO impl.RoleInfo: f7a31870-78ff-406d-a144-7fe17a627699: shutdown f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState
datanode2_1  | 2023-01-06 03:53:33,754 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState] INFO impl.FollowerState: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState was interrupted
datanode2_1  | 2023-01-06 03:53:33,755 [grpc-default-executor-1] INFO impl.RoleInfo: f7a31870-78ff-406d-a144-7fe17a627699: start f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState
datanode2_1  | 2023-01-06 03:53:33,756 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-01-06 03:53:33,757 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-01-06 03:53:33,762 [grpc-default-executor-1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100 replies to ELECTION vote request: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86<-f7a31870-78ff-406d-a144-7fe17a627699#0:OK-t2. Peer's state: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100:t2, leader=null, voted=d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86, raftlog=Memoized:f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-01-06 03:53:36,578 [f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-FollowerState] INFO impl.FollowerState: f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5115427515ns, electionTimeout:5099ms
datanode2_1  | 2023-01-06 03:53:36,579 [f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-FollowerState] INFO impl.RoleInfo: f7a31870-78ff-406d-a144-7fe17a627699: shutdown f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-FollowerState
datanode2_1  | 2023-01-06 03:53:36,579 [f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-FollowerState] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode2_1  | 2023-01-06 03:53:36,579 [f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode2_1  | 2023-01-06 03:53:36,580 [f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-FollowerState] INFO impl.RoleInfo: f7a31870-78ff-406d-a144-7fe17a627699: start f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-LeaderElection3
datanode2_1  | 2023-01-06 03:53:36,584 [f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-LeaderElection3] INFO impl.LeaderElection: f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-01-06 03:53:36,586 [f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-01-06 03:53:36,586 [f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-01-06 03:53:36,654 [f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-LeaderElection3] INFO impl.LeaderElection: f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-LeaderElection3: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode2_1  | 2023-01-06 03:53:36,654 [f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-LeaderElection3] INFO impl.LeaderElection:   Response 0: f7a31870-78ff-406d-a144-7fe17a627699<-d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86#0:FAIL-t1
datanode2_1  | 2023-01-06 03:53:36,654 [f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-LeaderElection3] INFO impl.LeaderElection: f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-LeaderElection3 ELECTION round 0: result REJECTED
datanode2_1  | 2023-01-06 03:53:36,654 [f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-LeaderElection3] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode2_1  | 2023-01-06 03:53:36,654 [f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-LeaderElection3] INFO impl.RoleInfo: f7a31870-78ff-406d-a144-7fe17a627699: shutdown f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-LeaderElection3
datanode2_1  | 2023-01-06 03:53:36,654 [f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-LeaderElection3] INFO impl.RoleInfo: f7a31870-78ff-406d-a144-7fe17a627699: start f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-FollowerState
datanode2_1  | 2023-01-06 03:53:36,688 [f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-01-06 03:53:36,690 [f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-01-06 03:53:38,929 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState] INFO impl.FollowerState: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5169517633ns, electionTimeout:5169ms
datanode2_1  | 2023-01-06 03:53:38,929 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState] INFO impl.RoleInfo: f7a31870-78ff-406d-a144-7fe17a627699: shutdown f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState
datanode2_1  | 2023-01-06 03:53:38,929 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode2_1  | 2023-01-06 03:53:38,930 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode2_1  | 2023-01-06 03:53:38,930 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState] INFO impl.RoleInfo: f7a31870-78ff-406d-a144-7fe17a627699: start f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection4
datanode1_1  | 2023-01-06 03:53:36,602 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-01-06 03:53:36,603 [grpc-default-executor-1] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E replies to ELECTION vote request: f7a31870-78ff-406d-a144-7fe17a627699<-d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86#0:FAIL-t1. Peer's state: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E:t1, leader=null, voted=null, raftlog=Memoized:d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-06 03:53:36,605 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-01-06 03:53:38,424 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-FollowerState] INFO impl.FollowerState: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5126247940ns, electionTimeout:5122ms
datanode1_1  | 2023-01-06 03:53:38,425 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-FollowerState] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: shutdown d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-FollowerState
datanode1_1  | 2023-01-06 03:53:38,425 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-FollowerState] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode1_1  | 2023-01-06 03:53:38,425 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode1_1  | 2023-01-06 03:53:38,425 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-FollowerState] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: start d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-LeaderElection3
datanode1_1  | 2023-01-06 03:53:38,436 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-LeaderElection3] INFO impl.LeaderElection: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-06 03:53:38,447 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-LeaderElection3] INFO impl.LeaderElection: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-LeaderElection3 ELECTION round 0: result PASSED (term=1)
datanode1_1  | 2023-01-06 03:53:38,447 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-LeaderElection3] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: shutdown d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-LeaderElection3
datanode1_1  | 2023-01-06 03:53:38,447 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-LeaderElection3] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode1_1  | 2023-01-06 03:53:38,447 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-BF9CF73BE954 with new leaderId: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86
datanode1_1  | 2023-01-06 03:53:38,447 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-LeaderElection3] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954: change Leader from null to d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86 at term 1 for becomeLeader, leader elected after 5453ms
datanode1_1  | 2023-01-06 03:53:38,483 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode1_1  | 2023-01-06 03:53:38,507 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode1_1  | 2023-01-06 03:53:38,509 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode1_1  | 2023-01-06 03:53:38,526 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode1_1  | 2023-01-06 03:53:38,527 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode1_1  | 2023-01-06 03:53:38,529 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode1_1  | 2023-01-06 03:53:38,551 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode1_1  | 2023-01-06 03:53:38,557 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode1_1  | 2023-01-06 03:53:38,566 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-LeaderElection3] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: start d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-LeaderStateImpl
datanode1_1  | 2023-01-06 03:53:38,607 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-SegmentedRaftLogWorker: Starting segment from index:0
datanode1_1  | 2023-01-06 03:53:38,673 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-LeaderElection3] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954: set configuration 0: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-06 03:53:38,847 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-BF9CF73BE954-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/88086403-84d0-4755-ada1-bf9cf73be954/current/log_inprogress_0
datanode1_1  | 2023-01-06 03:53:38,868 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO impl.FollowerState: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5098718225ns, electionTimeout:5084ms
datanode1_1  | 2023-01-06 03:53:38,868 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: shutdown d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState
datanode1_1  | 2023-01-06 03:53:38,869 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode1_1  | 2023-01-06 03:53:38,870 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode1_1  | 2023-01-06 03:53:38,872 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: start d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection4
datanode1_1  | 2023-01-06 03:53:38,878 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection4] INFO impl.LeaderElection: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection4 ELECTION round 0: submit vote requests at term 3 for -1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-06 03:53:38,916 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-01-06 03:53:38,955 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-01-06 03:53:38,961 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection4] INFO impl.LeaderElection: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection4: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode1_1  | 2023-01-06 03:53:38,961 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection4] INFO impl.LeaderElection:   Response 0: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86<-678ebbfa-a167-4ea6-8846-1b1f5c050dad#0:FAIL-t3
datanode1_1  | 2023-01-06 03:53:38,962 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection4] INFO impl.LeaderElection: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection4 ELECTION round 0: result REJECTED
datanode1_1  | 2023-01-06 03:53:38,962 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection4] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100: changes role from CANDIDATE to FOLLOWER at term 3 for REJECTED
datanode1_1  | 2023-01-06 03:53:38,964 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection4] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: shutdown d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection4
datanode1_1  | 2023-01-06 03:53:38,965 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection4] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: start d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState
datanode1_1  | 2023-01-06 03:53:38,997 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-01-06 03:53:38,997 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-01-06 03:53:39,018 [grpc-default-executor-1] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100: receive requestVote(ELECTION, 678ebbfa-a167-4ea6-8846-1b1f5c050dad, group-5839005B1100, 3, (t:0, i:0))
datanode1_1  | 2023-01-06 03:53:39,003 [grpc-default-executor-0] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100: receive requestVote(ELECTION, f7a31870-78ff-406d-a144-7fe17a627699, group-5839005B1100, 3, (t:0, i:0))
datanode1_1  | 2023-01-06 03:53:39,034 [grpc-default-executor-0] INFO impl.VoteContext: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FOLLOWER: reject ELECTION from f7a31870-78ff-406d-a144-7fe17a627699: already has voted for d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86 at current term 3
datanode1_1  | 2023-01-06 03:53:39,049 [grpc-default-executor-0] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100 replies to ELECTION vote request: f7a31870-78ff-406d-a144-7fe17a627699<-d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86#0:FAIL-t3. Peer's state: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100:t3, leader=null, voted=d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86, raftlog=Memoized:d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-06 03:53:39,049 [grpc-default-executor-1] INFO impl.VoteContext: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FOLLOWER: reject ELECTION from 678ebbfa-a167-4ea6-8846-1b1f5c050dad: already has voted for d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86 at current term 3
datanode1_1  | 2023-01-06 03:53:39,050 [grpc-default-executor-1] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100 replies to ELECTION vote request: 678ebbfa-a167-4ea6-8846-1b1f5c050dad<-d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86#0:FAIL-t3. Peer's state: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100:t3, leader=null, voted=d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86, raftlog=Memoized:d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-06 03:53:41,628 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-FollowerState] INFO impl.FollowerState: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5035550959ns, electionTimeout:5021ms
datanode1_1  | 2023-01-06 03:53:41,629 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-FollowerState] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: shutdown d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-FollowerState
datanode1_1  | 2023-01-06 03:53:41,630 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-FollowerState] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode1_1  | 2023-01-06 03:53:41,630 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode1_1  | 2023-01-06 03:53:41,630 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-FollowerState] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: start d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5
datanode3_1  | 2023-01-06 03:53:38,953 [grpc-default-executor-2] INFO impl.VoteContext: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-CANDIDATE: reject ELECTION from f7a31870-78ff-406d-a144-7fe17a627699: already has voted for 678ebbfa-a167-4ea6-8846-1b1f5c050dad at current term 3
datanode3_1  | 2023-01-06 03:53:38,953 [grpc-default-executor-2] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100 replies to ELECTION vote request: f7a31870-78ff-406d-a144-7fe17a627699<-678ebbfa-a167-4ea6-8846-1b1f5c050dad#0:FAIL-t3. Peer's state: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100:t3, leader=null, voted=678ebbfa-a167-4ea6-8846-1b1f5c050dad, raftlog=Memoized:678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-06 03:53:38,989 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-01-06 03:53:39,012 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-01-06 03:53:39,056 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection3] INFO impl.LeaderElection: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection3: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode3_1  | 2023-01-06 03:53:39,057 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection3] INFO impl.LeaderElection:   Response 0: 678ebbfa-a167-4ea6-8846-1b1f5c050dad<-d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86#0:FAIL-t3
datanode3_1  | 2023-01-06 03:53:39,058 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection3] INFO impl.LeaderElection:   Response 1: 678ebbfa-a167-4ea6-8846-1b1f5c050dad<-f7a31870-78ff-406d-a144-7fe17a627699#0:FAIL-t3
datanode3_1  | 2023-01-06 03:53:39,058 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection3] INFO impl.LeaderElection: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection3 ELECTION round 0: result REJECTED
datanode3_1  | 2023-01-06 03:53:39,058 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection3] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100: changes role from CANDIDATE to FOLLOWER at term 3 for REJECTED
datanode3_1  | 2023-01-06 03:53:39,063 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection3] INFO impl.RoleInfo: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: shutdown 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection3
datanode3_1  | 2023-01-06 03:53:39,064 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-LeaderElection3] INFO impl.RoleInfo: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: start 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState
datanode3_1  | 2023-01-06 03:53:39,085 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-01-06 03:53:39,086 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-01-06 03:53:41,648 [grpc-default-executor-2] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E: receive requestVote(ELECTION, d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86, group-507410C3658E, 2, (t:0, i:0))
datanode3_1  | 2023-01-06 03:53:41,648 [grpc-default-executor-2] INFO impl.VoteContext: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-FOLLOWER: accept ELECTION from d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: our priority 0 <= candidate's priority 1
datanode3_1  | 2023-01-06 03:53:41,649 [grpc-default-executor-2] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86
datanode3_1  | 2023-01-06 03:53:41,649 [grpc-default-executor-2] INFO impl.RoleInfo: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: shutdown 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-FollowerState
datanode3_1  | 2023-01-06 03:53:41,649 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-FollowerState] INFO impl.FollowerState: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-FollowerState was interrupted
datanode3_1  | 2023-01-06 03:53:41,649 [grpc-default-executor-2] INFO impl.RoleInfo: 678ebbfa-a167-4ea6-8846-1b1f5c050dad: start 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-FollowerState
datanode3_1  | 2023-01-06 03:53:41,652 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-01-06 03:53:41,653 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-01-06 03:53:41,655 [grpc-default-executor-2] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E replies to ELECTION vote request: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86<-678ebbfa-a167-4ea6-8846-1b1f5c050dad#0:OK-t2. Peer's state: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E:t2, leader=null, voted=d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86, raftlog=Memoized:678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-06 03:53:41,865 [678ebbfa-a167-4ea6-8846-1b1f5c050dad-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-507410C3658E with new leaderId: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86
datanode3_1  | 2023-01-06 03:53:41,865 [678ebbfa-a167-4ea6-8846-1b1f5c050dad-server-thread1] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E: change Leader from null to d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86 at term 2 for appendEntries, leader elected after 10653ms
datanode3_1  | 2023-01-06 03:53:41,881 [678ebbfa-a167-4ea6-8846-1b1f5c050dad-server-thread1] INFO server.RaftServer$Division: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E: set configuration 0: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1        | 2023-01-06 03:53:15,364 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om1_1        | 2023-01-06 03:53:15,365 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om1_1        | 2023-01-06 03:53:15,373 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-562213E44849: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1        | 2023-01-06 03:53:15,375 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-562213E44849: changes role from      null to FOLLOWER at term 0 for startAsFollower
om1_1        | 2023-01-06 03:53:15,389 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-562213E44849-FollowerState
om1_1        | 2023-01-06 03:53:15,403 [om1@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1        | 2023-01-06 03:53:15,406 [om1@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1        | 2023-01-06 03:53:15,426 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-562213E44849,id=om1
om1_1        | 2023-01-06 03:53:15,431 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om1_1        | 2023-01-06 03:53:15,433 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om1_1        | 2023-01-06 03:53:15,434 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om1_1        | 2023-01-06 03:53:15,439 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om1_1        | 2023-01-06 03:53:15,448 [Listener at om1/9862] INFO server.RaftServer: om1: start RPC server
om1_1        | 2023-01-06 03:53:15,567 [Listener at om1/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om1_1        | 2023-01-06 03:53:15,587 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om1_1        | 2023-01-06 03:53:15,587 [Listener at om1/9862] INFO om.OzoneManager: Starting OM block token secret manager
om1_1        | 2023-01-06 03:53:15,590 [Listener at om1/9862] INFO token.OzoneBlockTokenSecretManager: Updating the current master key for generating tokens
om1_1        | 2023-01-06 03:53:15,596 [Listener at om1/9862] INFO om.OzoneManager: Starting OM delegation token secret manager
om1_1        | 2023-01-06 03:53:15,597 [Listener at om1/9862] INFO security.OzoneDelegationTokenSecretManager: Updating the current master key for generating tokens
om1_1        | 2023-01-06 03:53:15,621 [Listener at om1/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om1_1        | 2023-01-06 03:53:15,657 [Thread[Thread-19,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
om1_1        | 2023-01-06 03:53:15,919 [Listener at om1/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om1_1        | 2023-01-06 03:53:15,922 [Listener at om1/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
om1_1        | 2023-01-06 03:53:15,930 [Listener at om1/9862] INFO http.BaseHttpServer: HttpAuthType: ozone.om.http.auth.type = kerberos
om1_1        | 2023-01-06 03:53:16,381 [Listener at om1/9862] INFO util.log: Logging initialized @61369ms to org.eclipse.jetty.util.log.Slf4jLog
om1_1        | 2023-01-06 03:53:18,156 [Listener at om1/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om1_1        | 2023-01-06 03:53:18,395 [Listener at om1/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om1_1        | 2023-01-06 03:53:18,402 [Listener at om1/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context ozoneManager
om1_1        | 2023-01-06 03:53:18,405 [Listener at om1/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om1_1        | 2023-01-06 03:53:18,409 [Listener at om1/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
om1_1        | 2023-01-06 03:53:18,468 [Listener at om1/9862] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.om.http.auth.kerberos.principal keytabKey: ozone.om.http.auth.kerberos.keytab
om1_1        | 2023-01-06 03:53:19,103 [Listener at om1/9862] INFO http.HttpServer2: Jetty bound to port 9874
om1_1        | 2023-01-06 03:53:19,145 [Listener at om1/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om1_1        | 2023-01-06 03:53:19,741 [Listener at om1/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om1_1        | 2023-01-06 03:53:19,741 [Listener at om1/9862] INFO server.session: No SessionScavenger set, using defaults
om1_1        | 2023-01-06 03:53:19,763 [Listener at om1/9862] INFO server.session: node0 Scavenging every 600000ms
om1_1        | 2023-01-06 03:53:19,902 [Listener at om1/9862] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om1_1        | 2023-01-06 03:53:19,930 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d3abf88{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om1_1        | 2023-01-06 03:53:19,938 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@e56c524{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om1_1        | 2023-01-06 03:53:20,572 [om1@group-562213E44849-FollowerState] INFO impl.FollowerState: om1@group-562213E44849-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5183132589ns, electionTimeout:5145ms
om1_1        | 2023-01-06 03:53:20,584 [om1@group-562213E44849-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-562213E44849-FollowerState
om1_1        | 2023-01-06 03:53:20,600 [om1@group-562213E44849-FollowerState] INFO server.RaftServer$Division: om1@group-562213E44849: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om1_1        | 2023-01-06 03:53:20,650 [om1@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om1_1        | 2023-01-06 03:53:20,652 [om1@group-562213E44849-FollowerState] INFO impl.RoleInfo: om1: start om1@group-562213E44849-LeaderElection1
om1_1        | 2023-01-06 03:53:20,807 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1        | 2023-01-06 03:53:21,407 [om1@group-562213E44849-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1        | 2023-01-06 03:53:21,407 [om1@group-562213E44849-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
s3g_1        | Sleeping for 5 seconds
s3g_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1        | 2023-01-06 03:49:52,438 [main] INFO security.UserGroupInformation: Login successful for user s3g/s3g@EXAMPLE.COM using keytab file s3g.keytab. Keytab auto renewal enabled : false
s3g_1        | 2023-01-06 03:49:52,438 [main] INFO s3.Gateway: S3Gateway login successful.
s3g_1        | 2023-01-06 03:49:52,797 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1        | 2023-01-06 03:49:52,797 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
s3g_1        | 2023-01-06 03:49:52,797 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.s3g.http.auth.type = kerberos
s3g_1        | 2023-01-06 03:49:52,974 [main] INFO util.log: Logging initialized @7147ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1        | 2023-01-06 03:49:53,486 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1        | 2023-01-06 03:49:53,514 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1        | 2023-01-06 03:49:53,515 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context s3gateway
s3g_1        | 2023-01-06 03:49:53,516 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
s3g_1        | 2023-01-06 03:49:53,516 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
s3g_1        | 2023-01-06 03:49:53,526 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.s3g.http.auth.kerberos.principal keytabKey: ozone.s3g.http.auth.kerberos.keytab
s3g_1        | 2023-01-06 03:49:53,913 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1        | /************************************************************
s3g_1        | STARTUP_MSG: Starting Gateway
s3g_1        | STARTUP_MSG:   host = s3g/172.25.0.114
s3g_1        | STARTUP_MSG:   args = []
s3g_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode3_1  | 2023-01-06 03:53:41,882 [678ebbfa-a167-4ea6-8846-1b1f5c050dad-server-thread1] INFO segmented.SegmentedRaftLogWorker: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-SegmentedRaftLogWorker: Starting segment from index:0
datanode3_1  | 2023-01-06 03:53:41,886 [678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 678ebbfa-a167-4ea6-8846-1b1f5c050dad@group-507410C3658E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/7faae3af-cb79-4c3a-b244-507410c3658e/current/log_inprogress_0
recon_1      | [
recon_1      |   Version: V3
recon_1      |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
recon_1      |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
recon_1      | 
recon_1      |   Key:  Sun RSA public key, 2048 bits
recon_1      |   params: null
datanode1_1  | 2023-01-06 03:53:41,639 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO impl.LeaderElection: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5 ELECTION round 0: submit vote requests at term 2 for -1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-06 03:53:41,640 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-01-06 03:53:41,640 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-01-06 03:53:41,660 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO impl.LeaderElection: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode1_1  | 2023-01-06 03:53:41,660 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO impl.LeaderElection:   Response 0: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86<-678ebbfa-a167-4ea6-8846-1b1f5c050dad#0:OK-t2
datanode1_1  | 2023-01-06 03:53:41,660 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO impl.LeaderElection: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5 ELECTION round 0: result PASSED
datanode1_1  | 2023-01-06 03:53:41,661 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: shutdown d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5
datanode1_1  | 2023-01-06 03:53:41,661 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
datanode1_1  | 2023-01-06 03:53:41,661 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-507410C3658E with new leaderId: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86
datanode1_1  | 2023-01-06 03:53:41,662 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E: change Leader from null to d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86 at term 2 for becomeLeader, leader elected after 10361ms
datanode1_1  | 2023-01-06 03:53:41,664 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode1_1  | 2023-01-06 03:53:41,665 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode1_1  | 2023-01-06 03:53:41,665 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode1_1  | 2023-01-06 03:53:41,666 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode1_1  | 2023-01-06 03:53:41,666 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode1_1  | 2023-01-06 03:53:41,667 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode1_1  | 2023-01-06 03:53:41,667 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode1_1  | 2023-01-06 03:53:41,714 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode1_1  | 2023-01-06 03:53:41,769 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode1_1  | 2023-01-06 03:53:41,769 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-01-06 03:53:41,769 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode1_1  | 2023-01-06 03:53:41,775 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode1_1  | 2023-01-06 03:53:41,777 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode1_1  | 2023-01-06 03:53:41,777 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2023-01-06 03:53:41,778 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode1_1  | 2023-01-06 03:53:41,784 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode1_1  | 2023-01-06 03:53:41,792 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode1_1  | 2023-01-06 03:53:41,792 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-01-06 03:53:41,792 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode1_1  | 2023-01-06 03:53:41,793 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode1_1  | 2023-01-06 03:53:41,794 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode1_1  | 2023-01-06 03:53:41,794 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2023-01-06 03:53:41,794 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode1_1  | 2023-01-06 03:53:41,795 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode1_1  | 2023-01-06 03:53:41,796 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: start d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderStateImpl
recon_1      |   modulus: 22321520092034904956367956777931957676969911078232901017895933698678468781753172695844393013092965970615602622357794023639048732305076160434218405355746465972426985604516863422070797512086951139731951669174461346685083874381071907314493811292043757517828199039562222504144931947786863316592495947302761724410796725398590551213642383206197662885192142834321323519855792963024540129867342534976672519842084316567886340381000959465926087773440860829229209280683297485436375405886176389600891889084968589860808245139826791285487072108592486434566834307409916351251604701981140849210029026671497960989540995516629779904417
recon_1      |   public exponent: 65537
recon_1      |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
recon_1      |                To: Mon Feb 14 00:00:00 UTC 2028]
recon_1      |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
recon_1      |   SerialNumber: [    01]
recon_1      | 
recon_1      | Certificate Extensions: 3
recon_1      | [1]: ObjectId: 2.5.29.19 Criticality=true
recon_1      | BasicConstraints:[
recon_1      |   CA:true
recon_1      |   PathLen:2147483647
recon_1      | ]
recon_1      | 
recon_1      | [2]: ObjectId: 2.5.29.15 Criticality=true
recon_1      | KeyUsage [
recon_1      |   Key_CertSign
recon_1      |   Crl_Sign
recon_1      | ]
recon_1      | 
recon_1      | [3]: ObjectId: 2.5.29.17 Criticality=false
recon_1      | SubjectAlternativeName [
recon_1      |   IPAddress: 172.25.0.116
recon_1      |   DNSName: scm1.org
recon_1      | ]
recon_1      | 
recon_1      | ]
recon_1      |   Algorithm: [SHA256withRSA]
recon_1      |   Signature:
recon_1      | 0000: 21 07 40 68 BD F5 21 63   53 A4 1C 6D 61 63 C4 B6  !.@h..!cS..mac..
recon_1      | 0010: E3 BD 40 F3 9B FD 99 A0   FE 5C 7B FD 06 FB 50 8E  ..@......\....P.
recon_1      | 0020: E2 8C 95 3C 0B 0B 1A AD   6E 89 50 B3 9C 01 22 36  ...<....n.P..."6
recon_1      | 0030: 82 EA 04 5E 17 F6 FE 59   8A 78 83 34 58 E3 1A CF  ...^...Y.x.4X...
recon_1      | 0040: C0 08 87 80 9B 7D B8 33   3C E2 09 5B A0 BB 73 F9  .......3<..[..s.
recon_1      | 0050: 18 A1 8D D7 79 71 5D F0   D7 29 1C F6 F1 1A 40 0C  ....yq]..)....@.
recon_1      | 0060: DC B0 36 E8 EA 71 C3 CB   C9 B0 27 6D D7 9F 21 23  ..6..q....'m..!#
recon_1      | 0070: 40 5D F0 5B 0A B7 A0 74   F9 9F AD 3D 48 4A CC B0  @].[...t...=HJ..
recon_1      | 0080: 71 94 D1 27 C1 11 45 74   CC E9 49 95 45 B0 E7 F7  q..'..Et..I.E...
recon_1      | 0090: 9B 9F F3 43 1C 28 31 DC   D6 F4 73 3B 1B 52 1D 3F  ...C.(1...s;.R.?
recon_1      | 00A0: 44 76 E5 4C 63 7F 23 9B   17 64 90 51 18 89 86 33  Dv.Lc.#..d.Q...3
recon_1      | 00B0: CE 66 07 29 55 E3 3A 4D   BD 33 89 34 C3 A8 3C 51  .f.)U.:M.3.4..<Q
recon_1      | 00C0: 52 16 00 6D 7D 61 54 1E   1A 84 F4 F5 89 B2 A8 9C  R..m.aT.........
recon_1      | 00D0: 56 66 FE E2 2A 93 FE 19   10 10 C9 9C 0D 74 9B 01  Vf..*........t..
recon_1      | 00E0: 75 1D 1C AB 74 3A E3 B8   41 66 A1 BE 62 55 B9 60  u...t:..Af..bU.`
recon_1      | 00F0: F1 84 78 6F 50 A4 59 C4   78 19 02 34 DC BA 56 C3  ..xoP.Y.x..4..V.
recon_1      | 
recon_1      | ] from file:/data/metadata/recon/certs/ROOTCA-1.crt.
recon_1      | 2023-01-06 03:50:30,526 [main] INFO client.ReconCertificateClient: Added certificate [
recon_1      | [
recon_1      |   Version: V3
recon_1      |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=recon@recon
recon_1      |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
recon_1      | 
recon_1      |   Key:  Sun RSA public key, 2048 bits
recon_1      |   params: null
recon_1      |   modulus: 28685222393305072714132156990567696558976751769650304776772482623921399195412427207776983453842013581060082897595838910324899010053133555527007064527947847873140074005979829516061060492561727366193787040694358965315772635665846515408279529880676153582482450825832757472341562561001468889156957789211229203650947753596392242818724569229184868093591493624252373146729856239715828829352353752289625570872293908595681470306818434831704499281939208841018592915019019757783262388217289271730593147410748812116626003201984469066900793095817334510991137968065986375574626668635253516264141763264286855046815547784304781912959
recon_1      |   public exponent: 65537
recon_1      |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
recon_1      |                To: Sat Jan 06 00:00:00 UTC 2024]
recon_1      |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm-sub@scm1.org
recon_1      |   SerialNumber: [    013d3655 9349]
recon_1      | 
recon_1      | Certificate Extensions: 2
recon_1      | [1]: ObjectId: 2.5.29.15 Criticality=true
recon_1      | KeyUsage [
recon_1      |   DigitalSignature
recon_1      |   Key_Encipherment
recon_1      |   Data_Encipherment
recon_1      |   Key_Agreement
recon_1      | ]
recon_1      | 
recon_1      | [2]: ObjectId: 2.5.29.17 Criticality=false
recon_1      | SubjectAlternativeName [
recon_1      |   IPAddress: 172.25.0.115
recon_1      | ]
recon_1      | 
recon_1      | ]
recon_1      |   Algorithm: [SHA256withRSA]
recon_1      |   Signature:
recon_1      | 0000: 8F D6 FC 66 36 95 C5 AA   E5 F6 6F 46 F9 B6 94 F5  ...f6.....oF....
recon_1      | 0010: C3 15 20 40 0D 33 6E F0   3A 90 57 25 92 E1 EE 4B  .. @.3n.:.W%...K
recon_1      | 0020: 5F E5 7C 2C 9C AF 8D AA   79 E7 8E A0 63 DA 36 C6  _..,....y...c.6.
recon_1      | 0030: 8E C8 99 A2 59 AE A1 C2   BE AC 9E 65 DE 26 3B DC  ....Y......e.&;.
recon_1      | 0040: C6 EE 88 20 3A 3B D0 3D   D9 DA D4 2C 4B 11 8B F5  ... :;.=...,K...
recon_1      | 0050: 2B 25 90 C8 C6 E8 0D 50   A0 44 EA 91 08 8F 7B EC  +%.....P.D......
recon_1      | 0060: 73 2B 88 0D 47 92 41 64   E9 A5 41 91 9E A4 44 CE  s+..G.Ad..A...D.
recon_1      | 0070: 4A A7 13 DB EC 25 5A 7E   43 AA 21 DF 42 30 EE DF  J....%Z.C.!.B0..
recon_1      | 0080: FF 24 AE EF E6 BA 30 7D   5F 9A 47 06 F3 E2 FB 37  .$....0._.G....7
recon_1      | 0090: 6C 37 82 F4 B5 EA 57 E1   77 86 04 AB 93 CB 39 94  l7....W.w.....9.
recon_1      | 00A0: BD 74 01 E3 40 33 22 83   EE A9 07 98 A2 87 55 5A  .t..@3".......UZ
recon_1      | 00B0: 33 C6 65 CD D5 6D F3 AC   A9 B1 97 A5 11 DF 48 89  3.e..m........H.
recon_1      | 00C0: 6D F5 BF D9 9F 6F 93 79   00 8F 97 2C 3D 4B 91 84  m....o.y...,=K..
recon_1      | 00D0: 6F 08 23 7A D4 CD 3C F6   82 B5 C4 15 03 C3 65 AF  o.#z..<.......e.
recon_1      | 00E0: B6 49 F5 CD 66 9E 37 FE   A7 5E 18 D0 3B 72 DF 2B  .I..f.7..^..;r.+
recon_1      | 00F0: EB 77 FD 35 A3 42 12 A7   FB 27 5C EF C3 B5 2C 49  .w.5.B...'\...,I
recon_1      | 
recon_1      | ] from file:/data/metadata/recon/certs/1362416210761.crt.
recon_1      | 2023-01-06 03:50:30,531 [main] INFO client.ReconCertificateClient: Added certificate [
recon_1      | [
recon_1      |   Version: V3
recon_1      |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm-sub@scm1.org
om1_1        | 2023-01-06 03:53:21,464 [om1@group-562213E44849-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
om1_1        | 2023-01-06 03:53:21,464 [om1@group-562213E44849-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om3
om1_1        | 2023-01-06 03:53:21,993 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-om1: Detected pause in JVM or host machine (eg GC): pause of approximately 315421359ns. No GCs detected.
om1_1        | 2023-01-06 03:53:22,007 [Listener at om1/9862] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om1_1        | 2023-01-06 03:53:22,208 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@26e5403{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-12835715956286216853/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om1_1        | 2023-01-06 03:53:22,471 [Listener at om1/9862] INFO server.AbstractConnector: Started ServerConnector@3fa0f38a{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om1_1        | 2023-01-06 03:53:22,471 [Listener at om1/9862] INFO server.Server: Started @67467ms
om1_1        | 2023-01-06 03:53:22,532 [Listener at om1/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om1_1        | 2023-01-06 03:53:22,533 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om1_1        | 2023-01-06 03:53:22,541 [Listener at om1/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om1_1        | 2023-01-06 03:53:22,554 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1        | 2023-01-06 03:53:22,571 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om1_1        | 2023-01-06 03:53:23,244 [Listener at om1/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om1_1        | 2023-01-06 03:53:24,262 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:37541
om1_1        | 2023-01-06 03:53:24,372 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2023-01-06 03:53:25,306 [Listener at om1/9862] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om1_1        | 2023-01-06 03:53:25,422 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@30779b35] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om1_1        | 2023-01-06 03:53:26,528 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1: ELECTION TIMEOUT received 0 response(s) and 0 exception(s):
om1_1        | 2023-01-06 03:53:26,531 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1 ELECTION round 0: result TIMEOUT
om1_1        | 2023-01-06 03:53:26,546 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1 ELECTION round 1: submit vote requests at term 2 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1        | 2023-01-06 03:53:26,642 [om1@group-562213E44849-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1        | 2023-01-06 03:53:26,650 [om1@group-562213E44849-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1        | 2023-01-06 03:53:29,719 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.949408919s. [buffered_nanos=3001887670, waiting_for_connection]
om1_1        | 2023-01-06 03:53:29,845 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.871355922s. [buffered_nanos=3128292122, waiting_for_connection]
om1_1        | 2023-01-06 03:53:29,845 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1: ELECTION REJECTED received 0 response(s) and 2 exception(s):
om1_1        | 2023-01-06 03:53:29,846 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.949408919s. [buffered_nanos=3001887670, waiting_for_connection]
om1_1        | 2023-01-06 03:53:29,846 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.871355922s. [buffered_nanos=3128292122, waiting_for_connection]
om1_1        | 2023-01-06 03:53:29,846 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1 ELECTION round 1: result REJECTED
om1_1        | 2023-01-06 03:53:29,855 [om1@group-562213E44849-LeaderElection1] INFO server.RaftServer$Division: om1@group-562213E44849: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
om1_1        | 2023-01-06 03:53:29,856 [om1@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-562213E44849-LeaderElection1
om1_1        | 2023-01-06 03:53:29,867 [om1@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-562213E44849-FollowerState
om1_1        | 2023-01-06 03:53:29,934 [om1@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1        | 2023-01-06 03:53:29,935 [om1@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1        | 2023-01-06 03:53:30,503 [grpc-default-executor-1] INFO server.RaftServer$Division: om1@group-562213E44849: receive requestVote(ELECTION, om2, group-562213E44849, 2, (t:0, i:~))
om1_1        | 2023-01-06 03:53:30,510 [grpc-default-executor-1] INFO impl.VoteContext: om1@group-562213E44849-FOLLOWER: reject ELECTION from om2: already has voted for om1 at current term 2
om1_1        | 2023-01-06 03:53:30,585 [grpc-default-executor-1] INFO server.RaftServer$Division: om1@group-562213E44849 replies to ELECTION vote request: om2<-om1#0:FAIL-t2. Peer's state: om1@group-562213E44849:t2, leader=null, voted=om1, raftlog=Memoized:om1@group-562213E44849-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1        | 2023-01-06 03:53:31,673 [grpc-default-executor-1] INFO server.RaftServer$Division: om1@group-562213E44849: receive requestVote(ELECTION, om3, group-562213E44849, 2, (t:0, i:~))
om1_1        | 2023-01-06 03:53:31,673 [grpc-default-executor-1] INFO impl.VoteContext: om1@group-562213E44849-FOLLOWER: reject ELECTION from om3: already has voted for om1 at current term 2
s3g_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
s3g_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/9d30c9a7ce375cd614e86022850c7c128723df6b ; compiled by 'runner' on 2023-01-06T03:19Z
s3g_1        | STARTUP_MSG:   java = 11.0.14.1
s3g_1        | ************************************************************/
s3g_1        | 2023-01-06 03:49:53,958 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1        | 2023-01-06 03:49:54,015 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1        | 2023-01-06 03:49:54,350 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1        | 2023-01-06 03:49:54,895 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1        | 2023-01-06 03:49:54,895 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1        | 2023-01-06 03:49:55,014 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1        | 2023-01-06 03:49:55,018 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
s3g_1        | 2023-01-06 03:49:55,261 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1        | 2023-01-06 03:49:55,261 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1        | 2023-01-06 03:49:55,264 [main] INFO server.session: node0 Scavenging every 660000ms
s3g_1        | 2023-01-06 03:49:55,325 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/s3g@EXAMPLE.COM
s3g_1        | 2023-01-06 03:49:55,352 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@f9b7332{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1        | 2023-01-06 03:49:55,358 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@b672aa8{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1        | WARNING: An illegal reflective access operation has occurred
s3g_1        | WARNING: Illegal reflective access by org.jboss.weld.util.reflection.Formats (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to constructor com.sun.org.apache.bcel.internal.classfile.ClassParser(java.io.InputStream,java.lang.String)
s3g_1        | WARNING: Please consider reporting this to the maintainers of org.jboss.weld.util.reflection.Formats
s3g_1        | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
om2_1        | 2023-01-06 03:53:06,459 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om2_1        | 2023-01-06 03:53:06,535 [pool-28-thread-1] INFO server.RaftServer$Division: om2: new RaftServerImpl for group-562213E44849:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om2_1        | 2023-01-06 03:53:06,565 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om2_1        | 2023-01-06 03:53:06,566 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om2_1        | 2023-01-06 03:53:06,566 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om2_1        | 2023-01-06 03:53:06,569 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1        | 2023-01-06 03:53:06,570 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1        | 2023-01-06 03:53:06,570 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1        | 2023-01-06 03:53:06,599 [main] INFO om.OzoneManager: Creating RPC Server
om2_1        | 2023-01-06 03:53:06,656 [pool-28-thread-1] INFO server.RaftServer$Division: om2@group-562213E44849: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om2_1        | 2023-01-06 03:53:06,671 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2023-01-06 03:53:41,797 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO segmented.SegmentedRaftLogWorker: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-SegmentedRaftLogWorker: Starting segment from index:0
datanode1_1  | 2023-01-06 03:53:41,800 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/7faae3af-cb79-4c3a-b244-507410c3658e/current/log_inprogress_0
datanode1_1  | 2023-01-06 03:53:41,841 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E-LeaderElection5] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-507410C3658E: set configuration 0: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-06 03:53:44,127 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO impl.FollowerState: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5161914856ns, electionTimeout:5063ms
datanode1_1  | 2023-01-06 03:53:44,127 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: shutdown d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState
datanode1_1  | 2023-01-06 03:53:44,127 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO server.RaftServer$Division: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
datanode1_1  | 2023-01-06 03:53:44,127 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode1_1  | 2023-01-06 03:53:44,127 [d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-FollowerState] INFO impl.RoleInfo: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: start d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86@group-5839005B1100-LeaderElection6
om2_1        | 2023-01-06 03:53:06,726 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1        | 2023-01-06 03:53:06,732 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om2_1        | 2023-01-06 03:53:06,896 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om2_1        | 2023-01-06 03:53:06,915 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om2_1        | 2023-01-06 03:53:06,915 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om2_1        | 2023-01-06 03:53:07,396 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1        | 2023-01-06 03:53:07,405 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om2_1        | 2023-01-06 03:53:07,410 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om2_1        | 2023-01-06 03:53:07,415 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om2_1        | 2023-01-06 03:53:07,420 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om2_1        | 2023-01-06 03:53:09,143 [main] INFO reflections.Reflections: Reflections took 1957 ms to scan 8 urls, producing 23 keys and 540 values [using 2 cores]
om2_1        | 2023-01-06 03:53:10,565 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om2_1        | 2023-01-06 03:53:10,584 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om2_1        | 2023-01-06 03:53:15,781 [Listener at om2/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om2_1        | 2023-01-06 03:53:15,877 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om2_1        | 2023-01-06 03:53:15,877 [Listener at om2/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om2_1        | 2023-01-06 03:53:16,398 [Listener at om2/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om2/172.25.0.112:9862
om2_1        | 2023-01-06 03:53:16,407 [Listener at om2/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om2 at port 9872
om2_1        | 2023-01-06 03:53:16,422 [om2-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 does not exist. Creating ...
om2_1        | 2023-01-06 03:53:16,458 [om2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/in_use.lock acquired by nodename 7@om2
om2_1        | 2023-01-06 03:53:16,520 [om2-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 has been successfully formatted.
om2_1        | 2023-01-06 03:53:16,542 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om2_1        | 2023-01-06 03:53:16,599 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om2_1        | 2023-01-06 03:53:16,604 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2023-01-06 03:53:16,611 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om2_1        | 2023-01-06 03:53:16,619 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om2_1        | 2023-01-06 03:53:16,632 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1        | 2023-01-06 03:53:16,668 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om2_1        | 2023-01-06 03:53:16,670 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om2_1        | 2023-01-06 03:53:16,705 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om2@group-562213E44849-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849
om2_1        | 2023-01-06 03:53:16,707 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om2_1        | 2023-01-06 03:53:16,709 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om2_1        | 2023-01-06 03:53:16,719 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1        | 2023-01-06 03:53:16,723 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om2_1        | 2023-01-06 03:53:16,726 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om2_1        | 2023-01-06 03:53:16,735 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om2_1        | 2023-01-06 03:53:16,746 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om2_1        | 2023-01-06 03:53:16,748 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om2_1        | 2023-01-06 03:53:16,827 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om2_1        | 2023-01-06 03:53:16,828 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2023-01-06 03:53:16,940 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om2_1        | 2023-01-06 03:53:16,943 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om2_1        | 2023-01-06 03:53:16,945 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om2_1        | 2023-01-06 03:53:17,013 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om3_1        | KeyUsage [
om3_1        |   DigitalSignature
om3_1        |   Key_Encipherment
om3_1        |   Data_Encipherment
om3_1        |   Key_Agreement
om3_1        | ]
om3_1        | 
om3_1        | [2]: ObjectId: 2.5.29.17 Criticality=false
om3_1        | SubjectAlternativeName [
om3_1        |   IPAddress: 172.25.0.113
om3_1        |   Other-Name: Unrecognized ObjectIdentifier: 2.16.840.1.113730.3.1.34
om3_1        | ]
om3_1        | 
om3_1        | ]
om3_1        |   Algorithm: [SHA256withRSA]
om3_1        |   Signature:
om3_1        | 0000: 5E 8B 21 14 44 27 C9 04   C7 68 A8 9B C0 D1 6B AE  ^.!.D'...h....k.
om3_1        | 0010: 35 0E 4F 62 8E 1A 84 0B   17 1B 51 23 38 A5 E9 FF  5.Ob......Q#8...
om3_1        | 0020: AB 56 46 2D D5 AF 23 17   F9 F6 A9 12 F8 56 08 A9  .VF-..#......V..
om3_1        | 0030: B4 00 98 0F 35 EE 0A B3   E9 BB 2F 23 FA 38 DE FC  ....5...../#.8..
om3_1        | 0040: 3C C4 8A E5 B1 5D 9A 36   07 54 1F 91 81 EB 0D 40  <....].6.T.....@
om3_1        | 0050: 25 08 B5 68 B6 D6 D4 67   E7 F8 3F 67 14 82 78 D7  %..h...g..?g..x.
om3_1        | 0060: AF 97 63 D3 75 66 70 08   25 6C F6 10 8E 50 76 A5  ..c.ufp.%l...Pv.
om3_1        | 0070: 8A C9 42 9C 9E 39 31 8A   CC 73 68 80 F0 59 F7 CD  ..B..91..sh..Y..
om3_1        | 0080: E5 33 83 38 43 49 B9 6B   BC 54 FA 1F FC 81 D8 4F  .3.8CI.k.T.....O
om3_1        | 0090: 66 81 53 57 47 60 2A DD   01 6E 45 33 B3 AE E1 CD  f.SWG`*..nE3....
om3_1        | 00A0: A4 22 D4 57 25 A9 DF 57   36 A9 F6 81 73 24 E4 39  .".W%..W6...s$.9
om3_1        | 00B0: 32 D0 D1 AD 86 1C F4 88   0C A4 8A 55 43 7D 98 4A  2..........UC..J
om3_1        | 00C0: FE 04 38 C1 12 91 18 66   8A CB 40 1C 82 EA BE C3  ..8....f..@.....
om3_1        | 00D0: F8 1E 56 F3 39 4E 14 39   CF D4 ED BA C9 92 5B DE  ..V.9N.9......[.
om3_1        | 00E0: FC 50 8E EA EE 10 54 53   83 C0 69 34 93 9E 99 F5  .P....TS..i4....
om3_1        | 00F0: E5 50 64 81 8A 02 A8 4A   D6 26 51 99 57 22 D9 B8  .Pd....J.&Q.W"..
om3_1        | 
om3_1        | ] from file:/data/metadata/om/certs/1464908788290.crt.
om3_1        | 2023-01-06 03:52:53,707 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor is started with first delay 29102826300 ms and interval 86400000 ms.
om3_1        | 2023-01-06 03:52:53,927 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-01-06 03:52:55,239 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om3_1        | 2023-01-06 03:52:55,263 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om3_1        | 2023-01-06 03:52:57,124 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om3_1        | 2023-01-06 03:52:57,179 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om3_1        | 2023-01-06 03:52:57,179 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om3_1        | 2023-01-06 03:52:57,956 [main] INFO om.OzoneManager: Created Volume s3v With Owner om required for S3Gateway operations.
om3_1        | 2023-01-06 03:52:58,664 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1        | 2023-01-06 03:52:58,668 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om3_1        | 2023-01-06 03:52:58,734 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om3_1        | 2023-01-06 03:52:59,575 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om3_1        | 2023-01-06 03:52:59,626 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1        | 2023-01-06 03:52:59,850 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: id1 and peers: om3:9872, om1:9872, om2:9872
om3_1        | 2023-01-06 03:52:59,901 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om3_1        | 2023-01-06 03:53:01,203 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om1_1        | 2023-01-06 03:53:31,674 [grpc-default-executor-1] INFO server.RaftServer$Division: om1@group-562213E44849 replies to ELECTION vote request: om3<-om1#0:FAIL-t2. Peer's state: om1@group-562213E44849:t2, leader=null, voted=om1, raftlog=Memoized:om1@group-562213E44849-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1        | 2023-01-06 03:53:35,109 [om1@group-562213E44849-FollowerState] INFO impl.FollowerState: om1@group-562213E44849-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5242505657ns, electionTimeout:5174ms
om1_1        | 2023-01-06 03:53:35,110 [om1@group-562213E44849-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-562213E44849-FollowerState
om1_1        | 2023-01-06 03:53:35,110 [om1@group-562213E44849-FollowerState] INFO server.RaftServer$Division: om1@group-562213E44849: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
om1_1        | 2023-01-06 03:53:35,110 [om1@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om1_1        | 2023-01-06 03:53:35,110 [om1@group-562213E44849-FollowerState] INFO impl.RoleInfo: om1: start om1@group-562213E44849-LeaderElection2
om1_1        | 2023-01-06 03:53:35,120 [om1@group-562213E44849-LeaderElection2] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection2 ELECTION round 0: submit vote requests at term 3 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1        | 2023-01-06 03:53:35,126 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1        | 2023-01-06 03:53:35,127 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1        | 2023-01-06 03:53:35,174 [om1@group-562213E44849-LeaderElection2] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
om1_1        | 2023-01-06 03:53:35,175 [om1@group-562213E44849-LeaderElection2] INFO impl.LeaderElection:   Response 0: om1<-om2#0:OK-t3
om1_1        | 2023-01-06 03:53:35,175 [om1@group-562213E44849-LeaderElection2] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection2 ELECTION round 0: result PASSED
om1_1        | 2023-01-06 03:53:35,176 [om1@group-562213E44849-LeaderElection2] INFO impl.RoleInfo: om1: shutdown om1@group-562213E44849-LeaderElection2
om1_1        | 2023-01-06 03:53:35,177 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServer$Division: om1@group-562213E44849: changes role from CANDIDATE to LEADER at term 3 for changeToLeader
om1_1        | 2023-01-06 03:53:35,178 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServer$Division: om1@group-562213E44849: change Leader from null to om1 at term 3 for becomeLeader, leader elected after 30258ms
om1_1        | 2023-01-06 03:53:35,213 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om1_1        | 2023-01-06 03:53:35,239 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om1_1        | 2023-01-06 03:53:35,241 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om1_1        | 2023-01-06 03:53:35,255 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om1_1        | 2023-01-06 03:53:35,257 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om1_1        | 2023-01-06 03:53:35,257 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om1_1        | 2023-01-06 03:53:35,265 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om1_1        | 2023-01-06 03:53:35,268 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om1_1        | 2023-01-06 03:53:35,322 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om1_1        | 2023-01-06 03:53:35,324 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-01-06 03:53:35,325 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om1_1        | 2023-01-06 03:53:35,333 [om1@group-562213E44849-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om1_1        | 2023-01-06 03:53:35,334 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1        | 2023-01-06 03:53:35,334 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1        | 2023-01-06 03:53:35,334 [om1@group-562213E44849-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1        | 2023-01-06 03:53:35,335 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om1_1        | 2023-01-06 03:53:35,343 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om1_1        | 2023-01-06 03:53:35,343 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-01-06 03:53:35,343 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om1_1        | 2023-01-06 03:53:35,344 [om1@group-562213E44849-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om1_1        | 2023-01-06 03:53:35,344 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1        | 2023-01-06 03:53:35,346 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1        | 2023-01-06 03:53:35,346 [om1@group-562213E44849-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1        | 2023-01-06 03:53:35,347 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om1_1        | 2023-01-06 03:53:35,353 [om1@group-562213E44849-LeaderElection2] INFO impl.RoleInfo: om1: start om1@group-562213E44849-LeaderStateImpl
om1_1        | 2023-01-06 03:53:35,405 [om1@group-562213E44849-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: Starting segment from index:0
om1_1        | 2023-01-06 03:53:35,539 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServer$Division: om1@group-562213E44849: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1        | 2023-01-06 03:53:35,950 [om1@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_0
s3g_1        | WARNING: All illegal access operations will be denied in a future release
s3g_1        | 2023-01-06 03:50:01,535 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/s3g@EXAMPLE.COM
s3g_1        | Jan 06, 2023 3:50:04 AM org.glassfish.jersey.internal.Errors logErrors
s3g_1        | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1        | 
s3g_1        | 2023-01-06 03:50:05,003 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7c4ca87c{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-5115896077459158836/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1        | 2023-01-06 03:50:05,026 [main] INFO server.AbstractConnector: Started ServerConnector@551a20d6{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1        | 2023-01-06 03:50:05,026 [main] INFO server.Server: Started @19208ms
s3g_1        | 2023-01-06 03:50:05,041 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1        | 2023-01-06 03:50:05,041 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1        | 2023-01-06 03:50:05,047 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
datanode2_1  | 2023-01-06 03:53:38,933 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection4] INFO impl.LeaderElection: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection4 ELECTION round 0: submit vote requests at term 3 for -1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-01-06 03:53:38,935 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-01-06 03:53:38,935 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-01-06 03:53:38,960 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection4] INFO impl.LeaderElection: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection4: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode2_1  | 2023-01-06 03:53:38,960 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection4] INFO impl.LeaderElection:   Response 0: f7a31870-78ff-406d-a144-7fe17a627699<-678ebbfa-a167-4ea6-8846-1b1f5c050dad#0:FAIL-t3
datanode2_1  | 2023-01-06 03:53:38,960 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection4] INFO impl.LeaderElection: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection4 ELECTION round 0: result REJECTED
datanode2_1  | 2023-01-06 03:53:38,963 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection4] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100: changes role from CANDIDATE to FOLLOWER at term 3 for REJECTED
datanode2_1  | 2023-01-06 03:53:38,963 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection4] INFO impl.RoleInfo: f7a31870-78ff-406d-a144-7fe17a627699: shutdown f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection4
datanode2_1  | 2023-01-06 03:53:38,964 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-LeaderElection4] INFO impl.RoleInfo: f7a31870-78ff-406d-a144-7fe17a627699: start f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState
datanode2_1  | 2023-01-06 03:53:38,980 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-01-06 03:53:38,983 [f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-01-06 03:53:39,006 [grpc-default-executor-1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100: receive requestVote(ELECTION, d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86, group-5839005B1100, 3, (t:0, i:0))
datanode2_1  | 2023-01-06 03:53:39,007 [grpc-default-executor-1] INFO impl.VoteContext: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FOLLOWER: reject ELECTION from d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: already has voted for f7a31870-78ff-406d-a144-7fe17a627699 at current term 3
datanode2_1  | 2023-01-06 03:53:39,007 [grpc-default-executor-1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100 replies to ELECTION vote request: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86<-f7a31870-78ff-406d-a144-7fe17a627699#0:FAIL-t3. Peer's state: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100:t3, leader=null, voted=f7a31870-78ff-406d-a144-7fe17a627699, raftlog=Memoized:f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-01-06 03:53:39,026 [grpc-default-executor-1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100: receive requestVote(ELECTION, 678ebbfa-a167-4ea6-8846-1b1f5c050dad, group-5839005B1100, 3, (t:0, i:0))
datanode2_1  | 2023-01-06 03:53:39,027 [grpc-default-executor-1] INFO impl.VoteContext: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-FOLLOWER: reject ELECTION from 678ebbfa-a167-4ea6-8846-1b1f5c050dad: already has voted for f7a31870-78ff-406d-a144-7fe17a627699 at current term 3
datanode2_1  | 2023-01-06 03:53:39,027 [grpc-default-executor-1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100 replies to ELECTION vote request: 678ebbfa-a167-4ea6-8846-1b1f5c050dad<-f7a31870-78ff-406d-a144-7fe17a627699#0:FAIL-t3. Peer's state: f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100:t3, leader=null, voted=f7a31870-78ff-406d-a144-7fe17a627699, raftlog=Memoized:f7a31870-78ff-406d-a144-7fe17a627699@group-5839005B1100-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-01-06 03:53:41,646 [grpc-default-executor-1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E: receive requestVote(ELECTION, d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86, group-507410C3658E, 2, (t:0, i:0))
datanode2_1  | 2023-01-06 03:53:41,646 [grpc-default-executor-1] INFO impl.VoteContext: f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-FOLLOWER: accept ELECTION from d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86: our priority 0 <= candidate's priority 1
datanode2_1  | 2023-01-06 03:53:41,647 [grpc-default-executor-1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86
datanode2_1  | 2023-01-06 03:53:41,647 [grpc-default-executor-1] INFO impl.RoleInfo: f7a31870-78ff-406d-a144-7fe17a627699: shutdown f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-FollowerState
datanode2_1  | 2023-01-06 03:53:41,647 [f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-FollowerState] INFO impl.FollowerState: f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-FollowerState was interrupted
datanode2_1  | 2023-01-06 03:53:41,650 [grpc-default-executor-1] INFO impl.RoleInfo: f7a31870-78ff-406d-a144-7fe17a627699: start f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-FollowerState
datanode2_1  | 2023-01-06 03:53:41,651 [f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-01-06 03:53:41,652 [f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-01-06 03:53:41,653 [grpc-default-executor-1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E replies to ELECTION vote request: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86<-f7a31870-78ff-406d-a144-7fe17a627699#0:OK-t2. Peer's state: f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E:t2, leader=null, voted=d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86, raftlog=Memoized:f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-01-06 03:53:41,931 [f7a31870-78ff-406d-a144-7fe17a627699-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-507410C3658E with new leaderId: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86
datanode2_1  | 2023-01-06 03:53:41,932 [f7a31870-78ff-406d-a144-7fe17a627699-server-thread1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E: change Leader from null to d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86 at term 2 for appendEntries, leader elected after 10716ms
datanode2_1  | 2023-01-06 03:53:41,951 [f7a31870-78ff-406d-a144-7fe17a627699-server-thread1] INFO server.RaftServer$Division: f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E: set configuration 0: peers:[d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER, 678ebbfa-a167-4ea6-8846-1b1f5c050dad|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, f7a31870-78ff-406d-a144-7fe17a627699|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-01-06 03:53:41,958 [f7a31870-78ff-406d-a144-7fe17a627699-server-thread1] INFO segmented.SegmentedRaftLogWorker: f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-SegmentedRaftLogWorker: Starting segment from index:0
datanode2_1  | 2023-01-06 03:53:41,961 [f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f7a31870-78ff-406d-a144-7fe17a627699@group-507410C3658E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/7faae3af-cb79-4c3a-b244-507410c3658e/current/log_inprogress_0
om3_1        | 2023-01-06 03:53:01,229 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om3_1        | 2023-01-06 03:53:01,322 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om3_1        | 2023-01-06 03:53:01,660 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om3_1        | 2023-01-06 03:53:01,665 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om3_1        | 2023-01-06 03:53:01,666 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om3_1        | 2023-01-06 03:53:01,670 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om3_1        | 2023-01-06 03:53:01,671 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om3_1        | 2023-01-06 03:53:01,671 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om3_1        | 2023-01-06 03:53:01,674 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om3_1        | 2023-01-06 03:53:01,699 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2023-01-06 03:53:01,703 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om3_1        | 2023-01-06 03:53:01,704 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1        | 2023-01-06 03:53:01,763 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om3_1        | 2023-01-06 03:53:01,781 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om3_1        | 2023-01-06 03:53:01,786 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om3_1        | 2023-01-06 03:53:04,240 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
om3_1        | 2023-01-06 03:53:04,526 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
om3_1        | 2023-01-06 03:53:04,534 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
om3_1        | 2023-01-06 03:53:04,544 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
om3_1        | 2023-01-06 03:53:04,554 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
om3_1        | 2023-01-06 03:53:04,564 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
om3_1        | 2023-01-06 03:53:04,566 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
om3_1        | 2023-01-06 03:53:04,584 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
om3_1        | 2023-01-06 03:53:04,607 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
om3_1        | 2023-01-06 03:53:04,787 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
om3_1        | 2023-01-06 03:53:04,789 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
om3_1        | 2023-01-06 03:53:05,164 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om3_1        | 2023-01-06 03:53:05,175 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om3_1        | 2023-01-06 03:53:05,180 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1        | 2023-01-06 03:53:05,188 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1        | 2023-01-06 03:53:05,232 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1        | 2023-01-06 03:53:05,279 [om3-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x0cc0eff0] REGISTERED
om3_1        | 2023-01-06 03:53:05,320 [om3-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x0cc0eff0] BIND: 0.0.0.0/0.0.0.0:0
om3_1        | 2023-01-06 03:53:05,326 [om3-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x0cc0eff0, L:/0.0.0.0:36317] ACTIVE
om3_1        | 2023-01-06 03:53:05,327 [main] INFO server.RaftServer: om3: addNew group-562213E44849:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-562213E44849:java.util.concurrent.CompletableFuture@264e0b8e[Not completed]
om3_1        | 2023-01-06 03:53:05,327 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om3_1        | 2023-01-06 03:53:05,450 [pool-28-thread-1] INFO server.RaftServer$Division: om3: new RaftServerImpl for group-562213E44849:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om3_1        | 2023-01-06 03:53:05,476 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om3_1        | 2023-01-06 03:53:05,481 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om3_1        | 2023-01-06 03:53:05,481 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om3_1        | 2023-01-06 03:53:05,481 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1        | 2023-01-06 03:53:05,481 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1        | 2023-01-06 03:53:05,481 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om3_1        | 2023-01-06 03:53:05,599 [main] INFO om.OzoneManager: Creating RPC Server
om3_1        | 2023-01-06 03:53:05,601 [pool-28-thread-1] INFO server.RaftServer$Division: om3@group-562213E44849: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om3_1        | 2023-01-06 03:53:05,601 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1        | 2023-01-06 03:53:05,650 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om3_1        | 2023-01-06 03:53:05,654 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om3_1        | 2023-01-06 03:53:05,724 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om3_1        | 2023-01-06 03:53:05,843 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om3_1        | 2023-01-06 03:53:05,844 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om3_1        | 2023-01-06 03:53:06,326 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1        | 2023-01-06 03:53:06,345 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om3_1        | 2023-01-06 03:53:06,355 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om3_1        | 2023-01-06 03:53:06,355 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om3_1        | 2023-01-06 03:53:06,356 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om3_1        | 2023-01-06 03:53:08,661 [main] INFO reflections.Reflections: Reflections took 2430 ms to scan 8 urls, producing 23 keys and 540 values [using 2 cores]
om3_1        | 2023-01-06 03:53:10,260 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1        | 2023-01-06 03:53:10,298 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om3_1        | 2023-01-06 03:53:16,465 [Listener at om3/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om3_1        | 2023-01-06 03:53:16,571 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om3_1        | 2023-01-06 03:53:16,573 [Listener at om3/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om3_1        | 2023-01-06 03:53:17,166 [Listener at om3/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om3/172.25.0.113:9862
om3_1        | 2023-01-06 03:53:17,171 [Listener at om3/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om3 at port 9872
om3_1        | 2023-01-06 03:53:17,205 [om3-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 does not exist. Creating ...
om3_1        | 2023-01-06 03:53:17,265 [om3-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/in_use.lock acquired by nodename 6@om3
om3_1        | 2023-01-06 03:53:17,366 [om3-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 has been successfully formatted.
om3_1        | 2023-01-06 03:53:17,450 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om3_1        | 2023-01-06 03:53:17,555 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om3_1        | 2023-01-06 03:53:17,556 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2023-01-06 03:53:17,581 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om3_1        | 2023-01-06 03:53:17,595 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om3_1        | 2023-01-06 03:53:17,638 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1        | 2023-01-06 03:53:17,720 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om3_1        | 2023-01-06 03:53:17,725 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om3_1        | 2023-01-06 03:53:17,830 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om3@group-562213E44849-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849
om3_1        | 2023-01-06 03:53:17,853 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om2_1        | 2023-01-06 03:53:17,018 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om2_1        | 2023-01-06 03:53:17,029 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-562213E44849: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1        | 2023-01-06 03:53:17,050 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-562213E44849: changes role from      null to FOLLOWER at term 0 for startAsFollower
om2_1        | 2023-01-06 03:53:17,053 [om2-impl-thread1] INFO impl.RoleInfo: om2: start om2@group-562213E44849-FollowerState
om2_1        | 2023-01-06 03:53:17,060 [om2@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1        | 2023-01-06 03:53:17,092 [om2@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1        | 2023-01-06 03:53:17,104 [om2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-562213E44849,id=om2
om2_1        | 2023-01-06 03:53:17,137 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om2_1        | 2023-01-06 03:53:17,139 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om2_1        | 2023-01-06 03:53:17,147 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om2_1        | 2023-01-06 03:53:17,166 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om2_1        | 2023-01-06 03:53:17,238 [Listener at om2/9862] INFO server.RaftServer: om2: start RPC server
om2_1        | 2023-01-06 03:53:17,594 [Listener at om2/9862] INFO server.GrpcService: om2: GrpcService started, listening on 9872
om2_1        | 2023-01-06 03:53:17,683 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om2: Started
om2_1        | 2023-01-06 03:53:17,694 [Listener at om2/9862] INFO om.OzoneManager: Starting OM block token secret manager
om2_1        | 2023-01-06 03:53:17,696 [Listener at om2/9862] INFO token.OzoneBlockTokenSecretManager: Updating the current master key for generating tokens
om2_1        | 2023-01-06 03:53:17,699 [Listener at om2/9862] INFO om.OzoneManager: Starting OM delegation token secret manager
om2_1        | 2023-01-06 03:53:17,714 [Listener at om2/9862] INFO security.OzoneDelegationTokenSecretManager: Updating the current master key for generating tokens
om2_1        | 2023-01-06 03:53:17,763 [Listener at om2/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om2_1        | 2023-01-06 03:53:17,784 [Thread[Thread-19,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
om2_1        | 2023-01-06 03:53:18,424 [Listener at om2/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om2_1        | 2023-01-06 03:53:18,429 [Listener at om2/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
om2_1        | 2023-01-06 03:53:18,429 [Listener at om2/9862] INFO http.BaseHttpServer: HttpAuthType: ozone.om.http.auth.type = kerberos
om2_1        | 2023-01-06 03:53:18,807 [Listener at om2/9862] INFO util.log: Logging initialized @61548ms to org.eclipse.jetty.util.log.Slf4jLog
om2_1        | 2023-01-06 03:53:19,950 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-om2: Detected pause in JVM or host machine (eg GC): pause of approximately 174046537ns.
om2_1        | GC pool 'ParNew' had collection(s): count=1 time=180ms
om2_1        | 2023-01-06 03:53:20,473 [Listener at om2/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om2_1        | 2023-01-06 03:53:20,536 [Listener at om2/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om2_1        | 2023-01-06 03:53:20,553 [Listener at om2/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context ozoneManager
om2_1        | 2023-01-06 03:53:20,555 [Listener at om2/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om2_1        | 2023-01-06 03:53:20,557 [Listener at om2/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
om2_1        | 2023-01-06 03:53:20,576 [Listener at om2/9862] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.om.http.auth.kerberos.principal keytabKey: ozone.om.http.auth.kerberos.keytab
om2_1        | 2023-01-06 03:53:21,064 [Listener at om2/9862] INFO http.HttpServer2: Jetty bound to port 9874
om2_1        | 2023-01-06 03:53:21,083 [Listener at om2/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om2_1        | 2023-01-06 03:53:21,520 [Listener at om2/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om2_1        | 2023-01-06 03:53:21,522 [Listener at om2/9862] INFO server.session: No SessionScavenger set, using defaults
om2_1        | 2023-01-06 03:53:21,551 [Listener at om2/9862] INFO server.session: node0 Scavenging every 660000ms
om2_1        | 2023-01-06 03:53:21,708 [Listener at om2/9862] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om2_1        | 2023-01-06 03:53:21,731 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7b87ff85{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om2_1        | 2023-01-06 03:53:21,737 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c7537f6{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om2_1        | 2023-01-06 03:53:22,284 [om2@group-562213E44849-FollowerState] INFO impl.FollowerState: om2@group-562213E44849-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5231285942ns, electionTimeout:5189ms
om2_1        | 2023-01-06 03:53:22,287 [om2@group-562213E44849-FollowerState] INFO impl.RoleInfo: om2: shutdown om2@group-562213E44849-FollowerState
om2_1        | 2023-01-06 03:53:22,297 [om2@group-562213E44849-FollowerState] INFO server.RaftServer$Division: om2@group-562213E44849: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om2_1        | 2023-01-06 03:53:22,331 [om2@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om2_1        | 2023-01-06 03:53:22,331 [om2@group-562213E44849-FollowerState] INFO impl.RoleInfo: om2: start om2@group-562213E44849-LeaderElection1
om2_1        | 2023-01-06 03:53:22,378 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1        | 2023-01-06 03:53:22,805 [Listener at om2/9862] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
scm1.org_1   | Sleeping for 5 seconds
scm1.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1.org_1   | 2023-01-06 03:49:53,935 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm1.org_1   | /************************************************************
scm1.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm1.org_1   | STARTUP_MSG:   host = scm1.org/172.25.0.116
scm1.org_1   | STARTUP_MSG:   args = [--init]
scm1.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm1.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm1.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/9d30c9a7ce375cd614e86022850c7c128723df6b ; compiled by 'runner' on 2023-01-06T03:18Z
scm1.org_1   | STARTUP_MSG:   java = 11.0.14.1
scm1.org_1   | ************************************************************/
scm1.org_1   | 2023-01-06 03:49:54,052 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1.org_1   | 2023-01-06 03:49:54,629 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-01-06 03:49:54,992 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1.org_1   | 2023-01-06 03:49:55,127 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm1.org_1   | 2023-01-06 03:49:55,500 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1.org:9894 and Ratis port: 9894
scm1.org_1   | 2023-01-06 03:49:55,526 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1.org
scm1.org_1   | 2023-01-06 03:49:55,619 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
scm1.org_1   | 2023-01-06 03:49:58,994 [main] ERROR client.SCMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
scm1.org_1   | 2023-01-06 03:49:58,995 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
scm1.org_1   | 2023-01-06 03:49:59,006 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
scm1.org_1   | 2023-01-06 03:50:04,059 [main] INFO ha.HASecurityUtils: Init response: GETCERT
scm1.org_1   | 2023-01-06 03:50:05,473 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.116,host:scm1.org
scm1.org_1   | 2023-01-06 03:50:05,473 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm1.org_1   | 2023-01-06 03:50:05,724 [main] INFO utils.SelfSignedCertificate: Certificate 1 is issued by CN=scm@scm1.org,OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674,O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3 to CN=scm@scm1.org,OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674,O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, valid from Fri Jan 06 00:00:00 UTC 2023 to Mon Feb 14 00:00:00 UTC 2028
scm1.org_1   | 2023-01-06 03:50:05,816 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.116,host:scm1.org
scm1.org_1   | 2023-01-06 03:50:05,822 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm1.org_1   | 2023-01-06 03:50:05,827 [main] INFO ha.HASecurityUtils: Creating csr for SCM->hostName:scm1.org,scmId:e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674,clusterId:CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3,subject:scm-sub@scm1.org
scm1.org_1   | 2023-01-06 03:50:05,980 [main] INFO ha.HASecurityUtils: Successfully stored SCM signed certificate.
scm1.org_1   | 2023-01-06 03:50:06,225 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1.org_1   | 2023-01-06 03:50:06,435 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm1.org_1   | 2023-01-06 03:50:06,449 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm1.org_1   | 2023-01-06 03:50:06,451 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm1.org_1   | 2023-01-06 03:50:06,461 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm1.org_1   | 2023-01-06 03:50:06,462 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm1.org_1   | 2023-01-06 03:50:06,462 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1.org_1   | 2023-01-06 03:50:06,463 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm1.org_1   | 2023-01-06 03:50:06,473 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-01-06 03:50:06,474 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1.org_1   | 2023-01-06 03:50:06,475 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-01-06 03:50:06,493 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1.org_1   | 2023-01-06 03:50:06,496 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm1.org_1   | 2023-01-06 03:50:06,497 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm1.org_1   | 2023-01-06 03:50:06,948 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
scm1.org_1   | 2023-01-06 03:50:06,960 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1.org_1   | 2023-01-06 03:50:07,157 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
scm1.org_1   | 2023-01-06 03:50:07,158 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
scm1.org_1   | 2023-01-06 03:50:07,159 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
scm1.org_1   | 2023-01-06 03:50:07,161 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
scm1.org_1   | 2023-01-06 03:50:07,167 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
scm1.org_1   | 2023-01-06 03:50:07,168 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
scm1.org_1   | 2023-01-06 03:50:07,173 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
scm1.org_1   | 2023-01-06 03:50:07,174 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = null (default)
scm1.org_1   | 2023-01-06 03:50:07,176 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
scm1.org_1   | 2023-01-06 03:50:07,176 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
scm1.org_1   | 2023-01-06 03:50:07,296 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm1.org_1   | 2023-01-06 03:50:07,313 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm1.org_1   | 2023-01-06 03:50:07,314 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2023-01-06 03:50:07,314 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1      |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
recon_1      | 
recon_1      |   Key:  Sun RSA public key, 2048 bits
recon_1      |   params: null
recon_1      |   modulus: 25512042468887604642064404093025249920280347815052024483618130467625890981963310751612908088931868680366166722736700966336351841536566461994206957959065592473127025545304172175201240100170923940025313585056297682798208555874810870376218207324432498120475419600056108682275459578022550441815608531770386354956257639186299780577671839142629796440640716929439466625583225667634032960664095389171765331267228308686758487244580763947638224999982475527976426192624582650657992140973107557451334337699303987990444760298981625629400711682432328660808877074618851496477253766349631046458008273474791424727251297086392264850331
recon_1      |   public exponent: 65537
recon_1      |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
recon_1      |                To: Mon Feb 14 00:00:00 UTC 2028]
recon_1      |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
recon_1      |   SerialNumber: [    01379773 48ab]
recon_1      | 
recon_1      | Certificate Extensions: 3
recon_1      | [1]: ObjectId: 2.5.29.19 Criticality=true
recon_1      | BasicConstraints:[
recon_1      |   CA:true
recon_1      |   PathLen:2147483647
recon_1      | ]
recon_1      | 
recon_1      | [2]: ObjectId: 2.5.29.15 Criticality=true
recon_1      | KeyUsage [
recon_1      |   DigitalSignature
recon_1      |   Key_Encipherment
recon_1      |   Data_Encipherment
recon_1      |   Key_Agreement
recon_1      |   Key_CertSign
recon_1      |   Crl_Sign
recon_1      | ]
recon_1      | 
recon_1      | [3]: ObjectId: 2.5.29.17 Criticality=false
recon_1      | SubjectAlternativeName [
recon_1      |   IPAddress: 172.25.0.116
recon_1      |   DNSName: scm1.org
recon_1      | ]
recon_1      | 
recon_1      | ]
recon_1      |   Algorithm: [SHA256withRSA]
recon_1      |   Signature:
recon_1      | 0000: 71 A1 97 4D DA 8D 52 3F   E5 06 22 8E 76 61 49 C0  q..M..R?..".vaI.
recon_1      | 0010: C6 DB 6D 93 9B E0 DF 39   D0 2C 5E B2 55 3C F2 BA  ..m....9.,^.U<..
recon_1      | 0020: 2B 11 C5 2A C7 34 78 58   C1 96 DE C2 09 D7 16 49  +..*.4xX.......I
recon_1      | 0030: 14 DD 81 0D 0D EC 2B 14   A8 C2 D5 DC D9 65 72 65  ......+......ere
recon_1      | 0040: AC 06 88 C6 35 48 D5 5F   88 26 38 54 A0 34 B7 6A  ....5H._.&8T.4.j
recon_1      | 0050: 80 D9 98 32 D0 A7 83 BC   EF F9 F3 B4 3E E3 DB 99  ...2........>...
recon_1      | 0060: C8 62 68 E3 13 33 55 58   91 03 33 A0 D4 E3 E8 C7  .bh..3UX..3.....
recon_1      | 0070: 11 6E E3 D9 21 80 E7 D3   FD D9 65 FA 44 9C 4A 18  .n..!.....e.D.J.
recon_1      | 0080: 0A F3 39 BC 62 9F 21 94   BE 66 EF E8 B3 BE 21 DA  ..9.b.!..f....!.
recon_1      | 0090: 04 60 36 57 8C 00 B7 0D   3C 31 30 4D 96 93 18 D6  .`6W....<10M....
recon_1      | 00A0: DE 9E 43 8C B8 02 39 06   65 33 4A E0 D2 C5 E1 C5  ..C...9.e3J.....
recon_1      | 00B0: EC 84 E7 E9 D8 7F 26 5A   7F EC C4 ED E0 DA 89 3A  ......&Z.......:
recon_1      | 00C0: 2A 12 BB 4E A2 53 59 0A   EB 4D CC C2 5B DB 3E 6C  *..N.SY..M..[.>l
recon_1      | 00D0: 79 1A 95 EA 9D F5 37 22   32 4C FC DF 91 B5 AD 85  y.....7"2L......
recon_1      | 00E0: 28 09 F5 E5 5B 53 76 DA   63 17 AE F5 80 A5 B1 AF  (...[Sv.c.......
recon_1      | 00F0: 1B 90 43 5F FF 80 DB 39   CC 0A 02 38 20 9A EB 4F  ..C_...9...8 ..O
recon_1      | 
recon_1      | ] from file:/data/metadata/recon/certs/CA-1338275743915.crt.
recon_1      | 2023-01-06 03:50:30,536 [main] INFO client.ReconCertificateClient: CertificateLifetimeMonitor is started with first delay 29102969467 ms and interval 86400000 ms.
recon_1      | 2023-01-06 03:50:30,536 [main] INFO recon.ReconServer: Successfully stored SCM signed certificate, case:GETCERT.
recon_1      | 2023-01-06 03:50:31,148 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1      | 2023-01-06 03:50:33,891 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1      | WARNING: An illegal reflective access operation has occurred
recon_1      | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
recon_1      | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
recon_1      | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1      | WARNING: All illegal access operations will be denied in a future release
recon_1      | 2023-01-06 03:50:34,576 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1      | 2023-01-06 03:50:34,579 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.002 seconds to initialized 0 records to KEY_CONTAINER table
recon_1      | 2023-01-06 03:50:34,592 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1      | 2023-01-06 03:50:34,647 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1      | 2023-01-06 03:50:34,649 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1      | 2023-01-06 03:50:37,851 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1      | 2023-01-06 03:50:37,852 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
recon_1      | 2023-01-06 03:50:37,854 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.recon.http.auth.type = kerberos
recon_1      | 2023-01-06 03:50:37,897 [main] INFO util.log: Logging initialized @52787ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1      | 2023-01-06 03:50:38,272 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1      | 2023-01-06 03:50:38,288 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1      | 2023-01-06 03:50:38,295 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context recon
recon_1      | 2023-01-06 03:50:38,295 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
recon_1      | 2023-01-06 03:50:38,295 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
recon_1      | 2023-01-06 03:50:38,298 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.recon.http.auth.kerberos.principal keytabKey: ozone.recon.http.auth.kerberos.keytab
recon_1      | 2023-01-06 03:50:38,452 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
om1_1        | 2023-01-06 03:53:37,313 [om1@group-562213E44849-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om1_1        | [id: "om1"
om1_1        | address: "om1:9872"
om1_1        | startupRole: FOLLOWER
om1_1        | , id: "om3"
om1_1        | address: "om3:9872"
om1_1        | startupRole: FOLLOWER
om1_1        | , id: "om2"
om1_1        | address: "om2:9872"
om1_1        | startupRole: FOLLOWER
om1_1        | ]
scm3.org_1   | Sleeping for 5 seconds
scm3.org_1   | Waiting for the service scm2.org:9894
scm3.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm3.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm3.org_1   | 2023-01-06 03:50:57,479 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3.org_1   | /************************************************************
scm3.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm3.org_1   | STARTUP_MSG:   host = scm3.org/172.25.0.118
scm3.org_1   | STARTUP_MSG:   args = [--bootstrap]
scm3.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm3.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm3.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/9d30c9a7ce375cd614e86022850c7c128723df6b ; compiled by 'runner' on 2023-01-06T03:18Z
scm3.org_1   | STARTUP_MSG:   java = 11.0.14.1
scm3.org_1   | ************************************************************/
scm3.org_1   | 2023-01-06 03:50:57,536 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm3.org_1   | 2023-01-06 03:50:57,776 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2023-01-06 03:50:57,863 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm3.org_1   | 2023-01-06 03:50:57,864 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3.org_1   | 2023-01-06 03:50:57,980 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3.org:9894 and Ratis port: 9894
scm3.org_1   | 2023-01-06 03:50:57,983 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3.org
scm3.org_1   | 2023-01-06 03:50:58,497 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm3.org_1   | 2023-01-06 03:50:58,498 [main] INFO server.StorageContainerManager: SCM login successful.
scm3.org_1   | 2023-01-06 03:50:58,698 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863]
scm3.org_1   | 2023-01-06 03:50:59,864 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
scm3.org_1   | 2023-01-06 03:51:01,372 [main] ERROR client.SCMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
scm3.org_1   | 2023-01-06 03:51:01,374 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
scm3.org_1   | 2023-01-06 03:51:01,388 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
scm3.org_1   | 2023-01-06 03:51:03,209 [main] INFO ha.HASecurityUtils: Init response: GETCERT
scm3.org_1   | 2023-01-06 03:51:03,289 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.118,host:scm3.org
scm3.org_1   | 2023-01-06 03:51:03,289 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm3.org_1   | 2023-01-06 03:51:03,294 [main] INFO ha.HASecurityUtils: Creating csr for SCM->hostName:scm3.org,scmId:962229bc-cc46-47e6-82cc-1ac6b63780ff,clusterId:CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3,subject:scm-sub@scm3.org
scm3.org_1   | 2023-01-06 03:51:04,332 [main] INFO ha.HASecurityUtils: Successfully stored SCM signed certificate.
scm3.org_1   | 2023-01-06 03:51:04,369 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, SCMID 962229bc-cc46-47e6-82cc-1ac6b63780ff
scm3.org_1   | 2023-01-06 03:51:04,369 [main] INFO server.StorageContainerManager: Primary SCM Node ID e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674
scm3.org_1   | 2023-01-06 03:51:04,412 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm3.org_1   | /************************************************************
scm3.org_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm3.org/172.25.0.118
scm3.org_1   | ************************************************************/
scm3.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm3.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm3.org_1   | 2023-01-06 03:51:07,379 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3.org_1   | /************************************************************
scm3.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm3.org_1   | STARTUP_MSG:   host = scm3.org/172.25.0.118
scm3.org_1   | STARTUP_MSG:   args = []
scm3.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm2.org_1   | Sleeping for 5 seconds
scm2.org_1   | Waiting for the service scm1.org:9894
scm2.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm2.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2.org_1   | 2023-01-06 03:50:10,459 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm2.org_1   | /************************************************************
scm2.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm2.org_1   | STARTUP_MSG:   host = scm2.org/172.25.0.117
scm2.org_1   | STARTUP_MSG:   args = [--bootstrap]
scm2.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm2.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm2.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/9d30c9a7ce375cd614e86022850c7c128723df6b ; compiled by 'runner' on 2023-01-06T03:18Z
scm2.org_1   | STARTUP_MSG:   java = 11.0.14.1
scm2.org_1   | ************************************************************/
scm2.org_1   | 2023-01-06 03:50:10,469 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2.org_1   | 2023-01-06 03:50:10,538 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2023-01-06 03:50:10,594 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm2.org_1   | 2023-01-06 03:50:10,595 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm2.org_1   | 2023-01-06 03:50:10,664 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2.org:9894 and Ratis port: 9894
scm2.org_1   | 2023-01-06 03:50:10,665 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2.org
scm2.org_1   | 2023-01-06 03:50:10,851 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm2.org_1   | 2023-01-06 03:50:10,851 [main] INFO server.StorageContainerManager: SCM login successful.
scm2.org_1   | 2023-01-06 03:50:10,906 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
scm2.org_1   | 2023-01-06 03:50:13,107 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2023-01-06 03:50:15,109 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm1.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2023-01-06 03:50:17,111 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2023-01-06 03:50:19,113 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm1.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2023-01-06 03:50:21,128 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2023-01-06 03:50:24,000 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674 is not the leader. Could not determine the leader node.
scm2.org_1   | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
scm2.org_1   | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
scm2.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
scm2.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
scm2.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
scm2.org_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
scm2.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
scm2.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2.org_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2.org_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2.org_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2.org_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2.org_1   | , while invoking $Proxy15.send over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2023-01-06 03:50:26,002 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2023-01-06 03:50:28,216 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
om2_1        | 2023-01-06 03:53:22,820 [om2@group-562213E44849-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1        | 2023-01-06 03:53:22,820 [om2@group-562213E44849-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1        | 2023-01-06 03:53:22,902 [om2@group-562213E44849-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
om2_1        | 2023-01-06 03:53:22,903 [om2@group-562213E44849-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om3
om2_1        | 2023-01-06 03:53:23,094 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7993ec00{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-885348219308722762/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om2_1        | 2023-01-06 03:53:23,202 [Listener at om2/9862] INFO server.AbstractConnector: Started ServerConnector@70ebf6d8{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om2_1        | 2023-01-06 03:53:23,202 [Listener at om2/9862] INFO server.Server: Started @65944ms
om2_1        | 2023-01-06 03:53:23,267 [Listener at om2/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om2_1        | 2023-01-06 03:53:23,267 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om2_1        | 2023-01-06 03:53:23,277 [Listener at om2/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om2_1        | 2023-01-06 03:53:23,293 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om2_1        | 2023-01-06 03:53:23,399 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om2_1        | 2023-01-06 03:53:24,488 [Listener at om2/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om2_1        | 2023-01-06 03:53:25,481 [Listener at om2/9862] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om2_1        | 2023-01-06 03:53:25,640 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@10218fdf] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om2_1        | 2023-01-06 03:53:28,033 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection1: ELECTION TIMEOUT received 0 response(s) and 0 exception(s):
om2_1        | 2023-01-06 03:53:28,034 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection1 ELECTION round 0: result TIMEOUT
om2_1        | 2023-01-06 03:53:28,038 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection1 ELECTION round 1: submit vote requests at term 2 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1        | 2023-01-06 03:53:28,098 [om2@group-562213E44849-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1        | 2023-01-06 03:53:28,135 [om2@group-562213E44849-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1        | 2023-01-06 03:53:28,098 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:38229
om2_1        | 2023-01-06 03:53:28,246 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om2_1        | 2023-01-06 03:53:31,141 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.999828898s. [buffered_nanos=2586161497, remote_addr=om3/172.25.0.113:9872]
om2_1        | 2023-01-06 03:53:31,141 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection1: ELECTION REJECTED received 1 response(s) and 1 exception(s):
om2_1        | 2023-01-06 03:53:31,144 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Response 0: om2<-om1#0:FAIL-t2
om2_1        | 2023-01-06 03:53:31,144 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.999828898s. [buffered_nanos=2586161497, remote_addr=om3/172.25.0.113:9872]
om2_1        | 2023-01-06 03:53:31,145 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection1 ELECTION round 1: result REJECTED
om2_1        | 2023-01-06 03:53:31,146 [om2@group-562213E44849-LeaderElection1] INFO server.RaftServer$Division: om2@group-562213E44849: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
om2_1        | 2023-01-06 03:53:31,146 [om2@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om2: shutdown om2@group-562213E44849-LeaderElection1
om2_1        | 2023-01-06 03:53:31,146 [om2@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om2: start om2@group-562213E44849-FollowerState
om2_1        | 2023-01-06 03:53:31,168 [om2@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1        | 2023-01-06 03:53:31,168 [om2@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1        | 2023-01-06 03:53:31,933 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-562213E44849: receive requestVote(ELECTION, om3, group-562213E44849, 2, (t:0, i:~))
om2_1        | 2023-01-06 03:53:31,943 [grpc-default-executor-0] INFO impl.VoteContext: om2@group-562213E44849-FOLLOWER: reject ELECTION from om3: already has voted for om2 at current term 2
om2_1        | 2023-01-06 03:53:31,949 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-562213E44849 replies to ELECTION vote request: om3<-om2#0:FAIL-t2. Peer's state: om2@group-562213E44849:t2, leader=null, voted=om2, raftlog=Memoized:om2@group-562213E44849-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1        | 2023-01-06 03:53:35,140 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-562213E44849: receive requestVote(ELECTION, om1, group-562213E44849, 3, (t:0, i:~))
om2_1        | 2023-01-06 03:53:35,143 [grpc-default-executor-0] INFO impl.VoteContext: om2@group-562213E44849-FOLLOWER: accept ELECTION from om1: our priority 0 <= candidate's priority 0
om2_1        | 2023-01-06 03:53:35,143 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-562213E44849: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:om1
om2_1        | 2023-01-06 03:53:35,143 [grpc-default-executor-0] INFO impl.RoleInfo: om2: shutdown om2@group-562213E44849-FollowerState
om2_1        | 2023-01-06 03:53:35,144 [om2@group-562213E44849-FollowerState] INFO impl.FollowerState: om2@group-562213E44849-FollowerState was interrupted
om2_1        | 2023-01-06 03:53:35,145 [grpc-default-executor-0] INFO impl.RoleInfo: om2: start om2@group-562213E44849-FollowerState
om3_1        | 2023-01-06 03:53:17,855 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om3_1        | 2023-01-06 03:53:17,863 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1        | 2023-01-06 03:53:17,871 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om3_1        | 2023-01-06 03:53:17,874 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om3_1        | 2023-01-06 03:53:17,877 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om3_1        | 2023-01-06 03:53:17,895 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om3_1        | 2023-01-06 03:53:17,902 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om3_1        | 2023-01-06 03:53:18,062 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om3_1        | 2023-01-06 03:53:18,076 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2023-01-06 03:53:18,325 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om3_1        | 2023-01-06 03:53:18,326 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om3_1        | 2023-01-06 03:53:18,328 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om3_1        | 2023-01-06 03:53:18,362 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om3_1        | 2023-01-06 03:53:18,362 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om3_1        | 2023-01-06 03:53:18,381 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-562213E44849: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1        | 2023-01-06 03:53:18,382 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-562213E44849: changes role from      null to FOLLOWER at term 0 for startAsFollower
om3_1        | 2023-01-06 03:53:18,385 [om3-impl-thread1] INFO impl.RoleInfo: om3: start om3@group-562213E44849-FollowerState
om3_1        | 2023-01-06 03:53:18,406 [om3@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1        | 2023-01-06 03:53:18,426 [om3@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1        | 2023-01-06 03:53:18,427 [om3-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-562213E44849,id=om3
om3_1        | 2023-01-06 03:53:18,470 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om3_1        | 2023-01-06 03:53:18,477 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om3_1        | 2023-01-06 03:53:18,480 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om3_1        | 2023-01-06 03:53:18,489 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om3_1        | 2023-01-06 03:53:18,519 [Listener at om3/9862] INFO server.RaftServer: om3: start RPC server
om3_1        | 2023-01-06 03:53:18,621 [Listener at om3/9862] INFO server.GrpcService: om3: GrpcService started, listening on 9872
om3_1        | 2023-01-06 03:53:18,637 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om3: Started
om3_1        | 2023-01-06 03:53:18,639 [Listener at om3/9862] INFO om.OzoneManager: Starting OM block token secret manager
om3_1        | 2023-01-06 03:53:18,644 [Listener at om3/9862] INFO token.OzoneBlockTokenSecretManager: Updating the current master key for generating tokens
om3_1        | 2023-01-06 03:53:18,665 [Listener at om3/9862] INFO om.OzoneManager: Starting OM delegation token secret manager
om3_1        | 2023-01-06 03:53:18,665 [Listener at om3/9862] INFO security.OzoneDelegationTokenSecretManager: Updating the current master key for generating tokens
om3_1        | 2023-01-06 03:53:18,700 [Listener at om3/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om3_1        | 2023-01-06 03:53:18,717 [Thread[Thread-19,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
om3_1        | 2023-01-06 03:53:19,175 [Listener at om3/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om3_1        | 2023-01-06 03:53:19,180 [Listener at om3/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
om3_1        | 2023-01-06 03:53:19,180 [Listener at om3/9862] INFO http.BaseHttpServer: HttpAuthType: ozone.om.http.auth.type = kerberos
om3_1        | 2023-01-06 03:53:19,605 [Listener at om3/9862] INFO util.log: Logging initialized @64869ms to org.eclipse.jetty.util.log.Slf4jLog
om3_1        | 2023-01-06 03:53:21,628 [Listener at om3/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om3_1        | 2023-01-06 03:53:21,724 [Listener at om3/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om3_1        | 2023-01-06 03:53:21,756 [Listener at om3/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context ozoneManager
om3_1        | 2023-01-06 03:53:21,756 [Listener at om3/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om3_1        | 2023-01-06 03:53:21,757 [Listener at om3/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
om3_1        | 2023-01-06 03:53:21,809 [Listener at om3/9862] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.om.http.auth.kerberos.principal keytabKey: ozone.om.http.auth.kerberos.keytab
om3_1        | 2023-01-06 03:53:22,360 [Listener at om3/9862] INFO http.HttpServer2: Jetty bound to port 9874
om3_1        | 2023-01-06 03:53:22,371 [Listener at om3/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om3_1        | 2023-01-06 03:53:22,909 [Listener at om3/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om3_1        | 2023-01-06 03:53:22,919 [Listener at om3/9862] INFO server.session: No SessionScavenger set, using defaults
om3_1        | 2023-01-06 03:53:22,923 [Listener at om3/9862] INFO server.session: node0 Scavenging every 660000ms
om3_1        | 2023-01-06 03:53:23,129 [Listener at om3/9862] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om3_1        | 2023-01-06 03:53:23,167 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@198f3677{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om3_1        | 2023-01-06 03:53:23,183 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@779e0b46{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om3_1        | 2023-01-06 03:53:23,560 [om3@group-562213E44849-FollowerState] INFO impl.FollowerState: om3@group-562213E44849-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5175269100ns, electionTimeout:5116ms
om3_1        | 2023-01-06 03:53:23,576 [om3@group-562213E44849-FollowerState] INFO impl.RoleInfo: om3: shutdown om3@group-562213E44849-FollowerState
om3_1        | 2023-01-06 03:53:23,582 [om3@group-562213E44849-FollowerState] INFO server.RaftServer$Division: om3@group-562213E44849: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om3_1        | 2023-01-06 03:53:23,611 [om3@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om3_1        | 2023-01-06 03:53:23,612 [om3@group-562213E44849-FollowerState] INFO impl.RoleInfo: om3: start om3@group-562213E44849-LeaderElection1
om3_1        | 2023-01-06 03:53:23,676 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1        | 2023-01-06 03:53:24,577 [om3@group-562213E44849-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1        | 2023-01-06 03:53:24,577 [om3@group-562213E44849-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1        | 2023-01-06 03:53:35,160 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-562213E44849 replies to ELECTION vote request: om1<-om2#0:OK-t3. Peer's state: om2@group-562213E44849:t3, leader=null, voted=om1, raftlog=Memoized:om2@group-562213E44849-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1        | 2023-01-06 03:53:35,167 [om2@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1        | 2023-01-06 03:53:35,168 [om2@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1        | 2023-01-06 03:53:35,972 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-562213E44849: change Leader from null to om1 at term 3 for appendEntries, leader elected after 29076ms
om2_1        | 2023-01-06 03:53:36,246 [om2-server-thread3] INFO server.RaftServer$Division: om2@group-562213E44849: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1        | 2023-01-06 03:53:36,305 [om2-server-thread3] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: Starting segment from index:0
om2_1        | 2023-01-06 03:53:36,903 [om2@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_0
om2_1        | 2023-01-06 03:53:39,838 [om2@group-562213E44849-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om2_1        | [id: "om1"
om2_1        | address: "om1:9872"
om2_1        | startupRole: FOLLOWER
om2_1        | , id: "om3"
om2_1        | address: "om3:9872"
om2_1        | startupRole: FOLLOWER
om2_1        | , id: "om2"
om2_1        | address: "om2:9872"
om2_1        | startupRole: FOLLOWER
om2_1        | ]
recon_1      | 2023-01-06 03:50:39,070 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1      | 2023-01-06 03:50:39,089 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1      | 2023-01-06 03:50:39,106 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1      | 2023-01-06 03:50:39,226 [main] INFO ozone.OmUtils: Using OzoneManager ServiceID 'id1'.
recon_1      | 2023-01-06 03:50:40,908 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2023-01-06 03:50:41,604 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2023-01-06 03:50:41,806 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1      | 2023-01-06 03:50:41,810 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1      | 2023-01-06 03:50:42,090 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2023-01-06 03:50:42,543 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
recon_1      | 2023-01-06 03:50:42,773 [main] INFO reflections.Reflections: Reflections took 202 ms to scan 3 urls, producing 121 keys and 272 values 
recon_1      | 2023-01-06 03:50:42,961 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1      | 2023-01-06 03:50:43,144 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1      | 2023-01-06 03:50:43,161 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1      | 2023-01-06 03:50:43,193 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1      | 2023-01-06 03:50:43,301 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1      | 2023-01-06 03:50:43,411 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1      | 2023-01-06 03:50:43,455 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1      | 2023-01-06 03:50:43,628 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1      | 2023-01-06 03:50:43,941 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
recon_1      | 2023-01-06 03:50:43,941 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1      | 2023-01-06 03:50:44,174 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1      | 2023-01-06 03:50:44,219 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1      | 2023-01-06 03:50:44,219 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1      | 2023-01-06 03:50:44,984 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1      | 2023-01-06 03:50:44,999 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
recon_1      | 2023-01-06 03:50:45,171 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1      | 2023-01-06 03:50:45,171 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1      | 2023-01-06 03:50:45,184 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 660000ms
recon_1      | 2023-01-06 03:50:45,246 [Listener at 0.0.0.0/9891] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/recon.keytab, for principal HTTP/recon@EXAMPLE.COM
recon_1      | 2023-01-06 03:50:45,250 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@381d7c4c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1      | 2023-01-06 03:50:45,258 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7acbd3d0{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1      | 2023-01-06 03:50:46,627 [Listener at 0.0.0.0/9891] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/recon.keytab, for principal HTTP/recon@EXAMPLE.COM
recon_1      | 2023-01-06 03:50:46,639 [Listener at 0.0.0.0/9891] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/recon.keytab, for principal HTTP/recon@EXAMPLE.COM
recon_1      | 2023-01-06 03:50:50,864 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@38b24746{recon,/,file:///tmp/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-11434815910616176384/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
recon_1      | 2023-01-06 03:50:50,887 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@2cbd855e{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1      | 2023-01-06 03:50:50,887 [Listener at 0.0.0.0/9891] INFO server.Server: Started @65778ms
recon_1      | 2023-01-06 03:50:50,897 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1      | 2023-01-06 03:50:50,897 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1      | 2023-01-06 03:50:50,899 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1      | 2023-01-06 03:50:50,899 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1      | 2023-01-06 03:50:50,916 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1      | 2023-01-06 03:50:50,931 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1      | 2023-01-06 03:50:50,932 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1      | 2023-01-06 03:50:50,932 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2023-01-06 03:50:50,932 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1      | 2023-01-06 03:50:50,939 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1      | 2023-01-06 03:50:51,534 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 0 pipelines from SCM.
recon_1      | 2023-01-06 03:50:51,534 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1      | 2023-01-06 03:50:51,534 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1      | 2023-01-06 03:50:51,541 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1      | 2023-01-06 03:50:51,551 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1      | 2023-01-06 03:50:51,569 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1      | 2023-01-06 03:50:51,679 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1      | 2023-01-06 03:50:51,679 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1      | 2023-01-06 03:50:51,693 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1      | 2023-01-06 03:50:51,693 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1      | 2023-01-06 03:50:51,709 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1      | 2023-01-06 03:50:51,714 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 19 milliseconds.
recon_1      | 2023-01-06 03:50:51,875 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 171 milliseconds to process 0 existing database records.
recon_1      | 2023-01-06 03:50:51,903 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 26 milliseconds for processing 0 containers.
recon_1      | 2023-01-06 03:51:10,934 [pool-30-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1      | 2023-01-06 03:51:10,935 [pool-30-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1      | 2023-01-06 03:51:11,154 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 1 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:11,167 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:51:13,170 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 3 failover attempts. Trying to failover immediately.
scm2.org_1   | 2023-01-06 03:50:28,783 [main] ERROR client.SCMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
scm2.org_1   | 2023-01-06 03:50:28,785 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
scm2.org_1   | 2023-01-06 03:50:28,786 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
scm2.org_1   | 2023-01-06 03:50:29,305 [main] INFO ha.HASecurityUtils: Init response: GETCERT
scm2.org_1   | 2023-01-06 03:50:29,351 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.117,host:scm2.org
scm2.org_1   | 2023-01-06 03:50:29,351 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm2.org_1   | 2023-01-06 03:50:29,356 [main] INFO ha.HASecurityUtils: Creating csr for SCM->hostName:scm2.org,scmId:aea2b8cb-487c-4369-b11b-87f4d7694103,clusterId:CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3,subject:scm-sub@scm2.org
scm2.org_1   | 2023-01-06 03:50:31,767 [main] INFO ha.HASecurityUtils: Successfully stored SCM signed certificate.
scm2.org_1   | 2023-01-06 03:50:31,801 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, SCMID aea2b8cb-487c-4369-b11b-87f4d7694103
scm2.org_1   | 2023-01-06 03:50:31,802 [main] INFO server.StorageContainerManager: Primary SCM Node ID e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674
scm2.org_1   | 2023-01-06 03:50:31,855 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm2.org_1   | /************************************************************
scm2.org_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm2.org/172.25.0.117
scm2.org_1   | ************************************************************/
scm2.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm2.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2.org_1   | 2023-01-06 03:50:35,932 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm2.org_1   | /************************************************************
scm2.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm2.org_1   | STARTUP_MSG:   host = scm2.org/172.25.0.117
scm2.org_1   | STARTUP_MSG:   args = []
scm2.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm2.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm2.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/9d30c9a7ce375cd614e86022850c7c128723df6b ; compiled by 'runner' on 2023-01-06T03:18Z
scm2.org_1   | STARTUP_MSG:   java = 11.0.14.1
scm2.org_1   | ************************************************************/
scm2.org_1   | 2023-01-06 03:50:35,993 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2.org_1   | 2023-01-06 03:50:36,330 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2023-01-06 03:50:36,523 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm2.org_1   | 2023-01-06 03:50:36,547 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm2.org_1   | 2023-01-06 03:50:36,668 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2.org:9894 and Ratis port: 9894
scm2.org_1   | 2023-01-06 03:50:36,669 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2.org
scm2.org_1   | 2023-01-06 03:50:38,292 [main] INFO client.SCMCertificateClient: Loading certificate from location:/data/metadata/scm/sub-ca/certs.
scm2.org_1   | 2023-01-06 03:50:38,837 [main] INFO client.SCMCertificateClient: Added certificate [
scm2.org_1   | [
scm2.org_1   |   Version: V3
scm2.org_1   |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=aea2b8cb-487c-4369-b11b-87f4d7694103, CN=scm-sub@scm2.org
scm2.org_1   |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
scm2.org_1   | 
scm2.org_1   |   Key:  Sun RSA public key, 2048 bits
scm2.org_1   |   params: null
scm2.org_1   |   modulus: 19778687731656783772902830138794289191672000448805848329103884768919417809288510074901844159535406711588697773789918091190180188081234478858440340972086773861034019073695059359075079030277481784868461999684352064578062828793989761167257543645975096970706355374322054880501576319849373121843095991424965125439656676571281789245182074680401674312259974497266585520686914893620314882138483272129225136754739166002360094272522613050167208491314474059999343859379487390204773389386141154611105399439274668810213999193236564345819598451717566758467633730773824047022548834010030804952760683971758717927916279117889616455907
scm2.org_1   |   public exponent: 65537
scm2.org_1   |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
scm2.org_1   |                To: Mon Feb 14 00:00:00 UTC 2028]
scm2.org_1   |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
scm2.org_1   |   SerialNumber: [    013d2b98 fd1f]
scm2.org_1   | 
scm2.org_1   | Certificate Extensions: 3
scm2.org_1   | [1]: ObjectId: 2.5.29.19 Criticality=true
scm2.org_1   | BasicConstraints:[
scm2.org_1   |   CA:true
scm2.org_1   |   PathLen:2147483647
scm2.org_1   | ]
scm2.org_1   | 
scm2.org_1   | [2]: ObjectId: 2.5.29.15 Criticality=true
scm2.org_1   | KeyUsage [
scm2.org_1   |   DigitalSignature
scm2.org_1   |   Key_Encipherment
scm2.org_1   |   Data_Encipherment
scm2.org_1   |   Key_Agreement
scm2.org_1   |   Key_CertSign
scm2.org_1   |   Crl_Sign
scm2.org_1   | ]
scm2.org_1   | 
scm2.org_1   | [3]: ObjectId: 2.5.29.17 Criticality=false
scm2.org_1   | SubjectAlternativeName [
scm2.org_1   |   IPAddress: 172.25.0.117
scm2.org_1   |   DNSName: scm2.org
scm2.org_1   | ]
scm2.org_1   | 
scm2.org_1   | ]
scm2.org_1   |   Algorithm: [SHA256withRSA]
scm2.org_1   |   Signature:
scm2.org_1   | 0000: 8E E7 B6 EA 4B 46 C1 E3   39 F3 F6 EA A0 98 E3 BF  ....KF..9.......
scm2.org_1   | 0010: 32 05 D0 D2 48 B1 72 63   30 7E E3 E7 52 CB AE A8  2...H.rc0...R...
scm2.org_1   | 0020: 01 21 E0 9B B3 9A E7 24   05 67 E7 50 19 56 86 D4  .!.....$.g.P.V..
scm2.org_1   | 0030: C9 98 71 08 EA 3E 0A 6F   9D 9E 14 EF A9 95 1F A6  ..q..>.o........
scm2.org_1   | 0040: 41 BB 17 DC F7 DA 27 FF   3D D4 3B F0 1A 20 EB 64  A.....'.=.;.. .d
scm2.org_1   | 0050: 45 8A 5A 48 E7 F0 CA D4   03 F9 8B AB 6C F3 D9 02  E.ZH........l...
scm2.org_1   | 0060: 8F 68 B9 53 0F 74 D4 90   88 FB 7A B0 A8 61 D3 D1  .h.S.t....z..a..
scm2.org_1   | 0070: 85 6C 89 4C 29 F2 67 32   D1 5F F0 1C 50 F6 59 DE  .l.L).g2._..P.Y.
scm2.org_1   | 0080: 5B A5 84 9D 81 0A 20 5D   18 83 06 0F 01 BE 5E E9  [..... ]......^.
scm2.org_1   | 0090: BD 2B A2 35 52 CA 29 B0   2D 7F FC 05 1F 59 68 C9  .+.5R.).-....Yh.
scm2.org_1   | 00A0: 08 18 BB A6 1B D1 63 5E   04 0D DA 17 9F A1 E3 DF  ......c^........
scm2.org_1   | 00B0: 36 B8 48 7F 2A C7 15 87   52 9B B4 68 8D 46 B7 7F  6.H.*...R..h.F..
scm2.org_1   | 00C0: BB 06 03 BC F5 AE B7 BB   9F A1 68 01 9C 9D A1 0D  ..........h.....
scm2.org_1   | 00D0: F0 F0 A4 FC 03 E7 E5 DF   CE 97 B4 B5 5F 99 C7 89  ............_...
scm2.org_1   | 00E0: 06 D0 E7 A3 D6 49 77 6E   E4 5C A5 86 01 C5 C9 C1  .....Iwn.\......
scm2.org_1   | 00F0: 9E 9D AB 55 4C A9 BE 9D   C1 41 4F 33 CD 32 7A EE  ...UL....AO3.2z.
scm2.org_1   | 
scm2.org_1   | ] from file:/data/metadata/scm/sub-ca/certs/1362236079391.crt.
scm2.org_1   | 2023-01-06 03:50:38,863 [main] INFO client.SCMCertificateClient: Added certificate [
scm2.org_1   | [
scm2.org_1   |   Version: V3
scm2.org_1   |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=aea2b8cb-487c-4369-b11b-87f4d7694103, CN=scm-sub@scm2.org
scm2.org_1   |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
scm2.org_1   | 
scm2.org_1   |   Key:  Sun RSA public key, 2048 bits
scm2.org_1   |   params: null
scm1.org_1   | 2023-01-06 03:50:07,317 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2023-01-06 03:50:07,352 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x6e5bf74c] REGISTERED
scm1.org_1   | 2023-01-06 03:50:07,358 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x6e5bf74c] BIND: 0.0.0.0/0.0.0.0:0
scm1.org_1   | 2023-01-06 03:50:07,361 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x6e5bf74c, L:/0.0.0.0:36235] ACTIVE
scm1.org_1   | 2023-01-06 03:50:07,371 [main] INFO server.RaftServer: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: addNew group-D75CE56C2BC3:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER] returns group-D75CE56C2BC3:java.util.concurrent.CompletableFuture@4fa4f485[Not completed]
scm1.org_1   | 2023-01-06 03:50:07,462 [pool-2-thread-1] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: new RaftServerImpl for group-D75CE56C2BC3:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
scm1.org_1   | 2023-01-06 03:50:07,464 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm1.org_1   | 2023-01-06 03:50:07,464 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1.org_1   | 2023-01-06 03:50:07,465 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1.org_1   | 2023-01-06 03:50:07,465 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2023-01-06 03:50:07,465 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2023-01-06 03:50:07,465 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1.org_1   | 2023-01-06 03:50:07,475 [pool-2-thread-1] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3: ConfigurationManager, init=-1: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm1.org_1   | 2023-01-06 03:50:07,476 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2023-01-06 03:50:07,487 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1.org_1   | 2023-01-06 03:50:07,487 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1.org_1   | 2023-01-06 03:50:07,507 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1.org_1   | 2023-01-06 03:50:07,510 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1.org_1   | 2023-01-06 03:50:07,510 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1.org_1   | 2023-01-06 03:50:07,590 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2023-01-06 03:50:07,597 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1.org_1   | 2023-01-06 03:50:07,598 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1.org_1   | 2023-01-06 03:50:07,598 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1.org_1   | 2023-01-06 03:50:07,599 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm1.org_1   | 2023-01-06 03:50:07,600 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3 does not exist. Creating ...
scm1.org_1   | 2023-01-06 03:50:07,607 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3/in_use.lock acquired by nodename 91@scm1.org
scm1.org_1   | 2023-01-06 03:50:07,615 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3 has been successfully formatted.
scm1.org_1   | 2023-01-06 03:50:07,639 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1.org_1   | 2023-01-06 03:50:07,656 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1.org_1   | 2023-01-06 03:50:07,656 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-01-06 03:50:07,658 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1.org_1   | 2023-01-06 03:50:07,660 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1.org_1   | 2023-01-06 03:50:07,662 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2023-01-06 03:50:07,671 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1.org_1   | 2023-01-06 03:50:07,672 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1.org_1   | 2023-01-06 03:50:07,684 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3
scm1.org_1   | 2023-01-06 03:50:07,685 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2023-01-06 03:50:07,685 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1.org_1   | 2023-01-06 03:50:07,686 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2023-01-06 03:50:07,687 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1.org_1   | 2023-01-06 03:50:07,687 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1.org_1   | 2023-01-06 03:50:07,688 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1.org_1   | 2023-01-06 03:50:07,689 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1.org_1   | 2023-01-06 03:50:07,689 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1.org_1   | 2023-01-06 03:50:07,714 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1.org_1   | 2023-01-06 03:50:07,714 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-01-06 03:50:07,728 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1.org_1   | 2023-01-06 03:50:07,729 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1.org_1   | 2023-01-06 03:50:07,730 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1.org_1   | 2023-01-06 03:50:07,758 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1.org_1   | 2023-01-06 03:50:07,758 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1.org_1   | 2023-01-06 03:50:07,767 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3: start as a follower, conf=-1: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-01-06 03:50:07,774 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm1.org_1   | 2023-01-06 03:50:07,775 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO impl.RoleInfo: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: start e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-FollowerState
scm1.org_1   | 2023-01-06 03:50:07,782 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1.org_1   | 2023-01-06 03:50:07,782 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1.org_1   | 2023-01-06 03:50:07,810 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D75CE56C2BC3,id=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674
scm1.org_1   | 2023-01-06 03:50:07,812 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1.org_1   | 2023-01-06 03:50:07,813 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1.org_1   | 2023-01-06 03:50:07,813 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1.org_1   | 2023-01-06 03:50:07,814 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1.org_1   | 2023-01-06 03:50:07,834 [main] INFO server.RaftServer: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: start RPC server
scm1.org_1   | 2023-01-06 03:50:07,891 [main] INFO server.GrpcService: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: GrpcService started, listening on 9894
scm1.org_1   | 2023-01-06 03:50:07,898 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: Started
scm1.org_1   | 2023-01-06 03:50:12,917 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-FollowerState] INFO impl.FollowerState: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5141940888ns, electionTimeout:5133ms
scm1.org_1   | 2023-01-06 03:50:12,924 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-FollowerState] INFO impl.RoleInfo: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: shutdown e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-FollowerState
scm1.org_1   | 2023-01-06 03:50:12,924 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-FollowerState] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm1.org_1   | 2023-01-06 03:50:12,931 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm1.org_1   | 2023-01-06 03:50:12,931 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-FollowerState] INFO impl.RoleInfo: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: start e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1
scm1.org_1   | 2023-01-06 03:50:12,939 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO impl.LeaderElection: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-01-06 03:50:12,940 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO impl.LeaderElection: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm1.org_1   | 2023-01-06 03:50:12,940 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO impl.RoleInfo: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: shutdown e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1
scm1.org_1   | 2023-01-06 03:50:12,941 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm1.org_1   | 2023-01-06 03:50:12,941 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3: change Leader from null to e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674 at term 1 for becomeLeader, leader elected after 5434ms
scm1.org_1   | 2023-01-06 03:50:12,946 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1.org_1   | 2023-01-06 03:50:12,951 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2023-01-06 03:50:12,951 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2023-01-06 03:50:12,978 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm1.org_1   | 2023-01-06 03:50:12,978 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1.org_1   | 2023-01-06 03:50:12,979 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1.org_1   | 2023-01-06 03:50:12,984 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2023-01-06 03:50:12,985 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm3.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm3.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/9d30c9a7ce375cd614e86022850c7c128723df6b ; compiled by 'runner' on 2023-01-06T03:18Z
scm3.org_1   | STARTUP_MSG:   java = 11.0.14.1
scm3.org_1   | ************************************************************/
scm3.org_1   | 2023-01-06 03:51:07,390 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm3.org_1   | 2023-01-06 03:51:07,456 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2023-01-06 03:51:07,493 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm3.org_1   | 2023-01-06 03:51:07,523 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3.org_1   | 2023-01-06 03:51:07,614 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3.org:9894 and Ratis port: 9894
scm3.org_1   | 2023-01-06 03:51:07,615 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3.org
scm3.org_1   | 2023-01-06 03:51:08,303 [main] INFO client.SCMCertificateClient: Loading certificate from location:/data/metadata/scm/sub-ca/certs.
scm3.org_1   | 2023-01-06 03:51:08,462 [main] INFO client.SCMCertificateClient: Added certificate [
scm3.org_1   | [
scm3.org_1   |   Version: V3
scm3.org_1   |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=962229bc-cc46-47e6-82cc-1ac6b63780ff, CN=scm-sub@scm3.org
scm3.org_1   |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
scm3.org_1   | 
scm3.org_1   |   Key:  Sun RSA public key, 2048 bits
scm3.org_1   |   params: null
scm3.org_1   |   modulus: 21968972585243363200145839441107242306421141690747826357550116627195106037173631559016116383035242755760596859240338503149155134410520528141167358255636917251693971486541040991152648926548501342404834210432043236266971754607869771253727810871686847251687640666185595296415674349447456298242281500081518390056685663602185436255556765932006343661875885282542727265855622507989489386085139753359261912491509565305694951962190738685991557846396268392496177215981162263434684781455922049067476887707632605876967385270021300967376758037548721884122567757453274542666103956086933602860733799935125471997730900950428794496103
scm3.org_1   |   public exponent: 65537
scm3.org_1   |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
scm3.org_1   |                To: Mon Feb 14 00:00:00 UTC 2028]
scm3.org_1   |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
scm3.org_1   |   SerialNumber: [    0145110b 53fe]
scm3.org_1   | 
scm3.org_1   | Certificate Extensions: 3
scm3.org_1   | [1]: ObjectId: 2.5.29.19 Criticality=true
scm3.org_1   | BasicConstraints:[
scm3.org_1   |   CA:true
scm3.org_1   |   PathLen:2147483647
scm3.org_1   | ]
scm3.org_1   | 
scm3.org_1   | [2]: ObjectId: 2.5.29.15 Criticality=true
scm3.org_1   | KeyUsage [
scm3.org_1   |   DigitalSignature
scm3.org_1   |   Key_Encipherment
scm3.org_1   |   Data_Encipherment
scm3.org_1   |   Key_Agreement
scm3.org_1   |   Key_CertSign
scm3.org_1   |   Crl_Sign
scm3.org_1   | ]
scm3.org_1   | 
scm3.org_1   | [3]: ObjectId: 2.5.29.17 Criticality=false
scm3.org_1   | SubjectAlternativeName [
scm3.org_1   |   IPAddress: 172.25.0.118
scm3.org_1   |   DNSName: scm3.org
scm3.org_1   | ]
scm3.org_1   | 
scm3.org_1   | ]
scm3.org_1   |   Algorithm: [SHA256withRSA]
scm3.org_1   |   Signature:
scm3.org_1   | 0000: 0E F8 44 CF FA 04 46 8F   F4 10 24 A9 1F D6 FA 1A  ..D...F...$.....
scm3.org_1   | 0010: 39 2C 6C 27 46 24 EA DF   A2 31 02 6B 1D 52 BF D2  9,l'F$...1.k.R..
scm3.org_1   | 0020: 82 89 74 3A A2 44 41 94   40 2A 62 BF A3 72 52 44  ..t:.DA.@*b..rRD
scm3.org_1   | 0030: 05 D1 58 10 54 E2 D3 58   7F 2C 3B AB 2F 69 13 E3  ..X.T..X.,;./i..
scm3.org_1   | 0040: 91 54 BE 19 00 F8 80 C1   EF 6B 16 21 FA E7 27 9C  .T.......k.!..'.
scm3.org_1   | 0050: C5 15 79 A7 11 7B BF 1D   3D 35 DD 70 A6 8A D1 0D  ..y.....=5.p....
scm3.org_1   | 0060: 47 38 F0 D4 64 AB 5B 87   88 E2 58 35 FC B2 31 DA  G8..d.[...X5..1.
scm3.org_1   | 0070: C1 DD 8E 21 BB 17 FE 11   E4 DD D3 04 74 1E C4 3B  ...!........t..;
scm3.org_1   | 0080: A2 E0 FC 14 DD 34 5A AA   9E 27 61 E2 1C B0 FC 75  .....4Z..'a....u
scm3.org_1   | 0090: 66 52 3A 11 59 F8 0E 3B   9C 19 AA CC 67 5E A5 E0  fR:.Y..;....g^..
scm3.org_1   | 00A0: 01 22 21 CF 44 7A 43 22   1C 10 A5 4E 47 2A 77 0B  ."!.DzC"...NG*w.
scm3.org_1   | 00B0: 9E 44 A9 BC 64 DC C1 A0   2B F4 95 81 AD CE 83 F8  .D..d...+.......
scm3.org_1   | 00C0: E4 83 63 B4 D8 CF B4 81   F7 24 0D 37 63 F9 03 82  ..c......$.7c...
scm3.org_1   | 00D0: C6 A0 9E 27 ED 29 AE 58   65 88 3B AB 5D B1 B2 1B  ...'.).Xe.;.]...
scm3.org_1   | 00E0: EC 2C F5 04 42 B2 01 B6   87 AB E0 00 A6 24 FE 17  .,..B........$..
scm3.org_1   | 00F0: 25 5F 2A B7 5C 4B 2E 73   ED 5C 84 75 43 1A 4E 51  %_*.\K.s.\.uC.NQ
scm3.org_1   | 
scm3.org_1   | ] from file:/data/metadata/scm/sub-ca/certs/certificate.crt.
scm3.org_1   | 2023-01-06 03:51:08,481 [main] INFO client.SCMCertificateClient: Added certificate [
scm3.org_1   | [
scm3.org_1   |   Version: V3
scm3.org_1   |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
scm3.org_1   |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
scm3.org_1   | 
scm3.org_1   |   Key:  Sun RSA public key, 2048 bits
scm3.org_1   |   params: null
scm3.org_1   |   modulus: 22321520092034904956367956777931957676969911078232901017895933698678468781753172695844393013092965970615602622357794023639048732305076160434218405355746465972426985604516863422070797512086951139731951669174461346685083874381071907314493811292043757517828199039562222504144931947786863316592495947302761724410796725398590551213642383206197662885192142834321323519855792963024540129867342534976672519842084316567886340381000959465926087773440860829229209280683297485436375405886176389600891889084968589860808245139826791285487072108592486434566834307409916351251604701981140849210029026671497960989540995516629779904417
scm3.org_1   |   public exponent: 65537
scm3.org_1   |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
scm3.org_1   |                To: Mon Feb 14 00:00:00 UTC 2028]
scm3.org_1   |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
scm3.org_1   |   SerialNumber: [    01]
scm3.org_1   | 
scm3.org_1   | Certificate Extensions: 3
scm3.org_1   | [1]: ObjectId: 2.5.29.19 Criticality=true
scm3.org_1   | BasicConstraints:[
scm3.org_1   |   CA:true
scm3.org_1   |   PathLen:2147483647
scm3.org_1   | ]
scm3.org_1   | 
scm3.org_1   | [2]: ObjectId: 2.5.29.15 Criticality=true
scm3.org_1   | KeyUsage [
scm3.org_1   |   Key_CertSign
scm3.org_1   |   Crl_Sign
scm3.org_1   | ]
scm3.org_1   | 
scm3.org_1   | [3]: ObjectId: 2.5.29.17 Criticality=false
scm3.org_1   | SubjectAlternativeName [
scm3.org_1   |   IPAddress: 172.25.0.116
scm3.org_1   |   DNSName: scm1.org
scm3.org_1   | ]
scm3.org_1   | 
scm3.org_1   | ]
scm3.org_1   |   Algorithm: [SHA256withRSA]
scm3.org_1   |   Signature:
scm3.org_1   | 0000: 21 07 40 68 BD F5 21 63   53 A4 1C 6D 61 63 C4 B6  !.@h..!cS..mac..
scm3.org_1   | 0010: E3 BD 40 F3 9B FD 99 A0   FE 5C 7B FD 06 FB 50 8E  ..@......\....P.
scm3.org_1   | 0020: E2 8C 95 3C 0B 0B 1A AD   6E 89 50 B3 9C 01 22 36  ...<....n.P..."6
scm3.org_1   | 0030: 82 EA 04 5E 17 F6 FE 59   8A 78 83 34 58 E3 1A CF  ...^...Y.x.4X...
scm3.org_1   | 0040: C0 08 87 80 9B 7D B8 33   3C E2 09 5B A0 BB 73 F9  .......3<..[..s.
scm3.org_1   | 0050: 18 A1 8D D7 79 71 5D F0   D7 29 1C F6 F1 1A 40 0C  ....yq]..)....@.
scm3.org_1   | 0060: DC B0 36 E8 EA 71 C3 CB   C9 B0 27 6D D7 9F 21 23  ..6..q....'m..!#
scm3.org_1   | 0070: 40 5D F0 5B 0A B7 A0 74   F9 9F AD 3D 48 4A CC B0  @].[...t...=HJ..
scm3.org_1   | 0080: 71 94 D1 27 C1 11 45 74   CC E9 49 95 45 B0 E7 F7  q..'..Et..I.E...
scm3.org_1   | 0090: 9B 9F F3 43 1C 28 31 DC   D6 F4 73 3B 1B 52 1D 3F  ...C.(1...s;.R.?
scm3.org_1   | 00A0: 44 76 E5 4C 63 7F 23 9B   17 64 90 51 18 89 86 33  Dv.Lc.#..d.Q...3
scm3.org_1   | 00B0: CE 66 07 29 55 E3 3A 4D   BD 33 89 34 C3 A8 3C 51  .f.)U.:M.3.4..<Q
scm3.org_1   | 00C0: 52 16 00 6D 7D 61 54 1E   1A 84 F4 F5 89 B2 A8 9C  R..m.aT.........
scm3.org_1   | 00D0: 56 66 FE E2 2A 93 FE 19   10 10 C9 9C 0D 74 9B 01  Vf..*........t..
scm3.org_1   | 00E0: 75 1D 1C AB 74 3A E3 B8   41 66 A1 BE 62 55 B9 60  u...t:..Af..bU.`
scm3.org_1   | 00F0: F1 84 78 6F 50 A4 59 C4   78 19 02 34 DC BA 56 C3  ..xoP.Y.x..4..V.
scm3.org_1   | 
scm3.org_1   | ] from file:/data/metadata/scm/sub-ca/certs/CA-1.crt.
scm3.org_1   | 2023-01-06 03:51:08,491 [main] INFO client.SCMCertificateClient: Added certificate [
scm3.org_1   | [
scm3.org_1   |   Version: V3
scm3.org_1   |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=962229bc-cc46-47e6-82cc-1ac6b63780ff, CN=scm-sub@scm3.org
scm3.org_1   |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
scm3.org_1   | 
scm3.org_1   |   Key:  Sun RSA public key, 2048 bits
scm3.org_1   |   params: null
scm3.org_1   |   modulus: 21968972585243363200145839441107242306421141690747826357550116627195106037173631559016116383035242755760596859240338503149155134410520528141167358255636917251693971486541040991152648926548501342404834210432043236266971754607869771253727810871686847251687640666185595296415674349447456298242281500081518390056685663602185436255556765932006343661875885282542727265855622507989489386085139753359261912491509565305694951962190738685991557846396268392496177215981162263434684781455922049067476887707632605876967385270021300967376758037548721884122567757453274542666103956086933602860733799935125471997730900950428794496103
scm3.org_1   |   public exponent: 65537
scm3.org_1   |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
scm3.org_1   |                To: Mon Feb 14 00:00:00 UTC 2028]
scm3.org_1   |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
scm3.org_1   |   SerialNumber: [    0145110b 53fe]
scm3.org_1   | 
scm3.org_1   | Certificate Extensions: 3
scm3.org_1   | [1]: ObjectId: 2.5.29.19 Criticality=true
scm3.org_1   | BasicConstraints:[
scm3.org_1   |   CA:true
scm3.org_1   |   PathLen:2147483647
scm3.org_1   | ]
scm3.org_1   | 
scm3.org_1   | [2]: ObjectId: 2.5.29.15 Criticality=true
scm3.org_1   | KeyUsage [
scm3.org_1   |   DigitalSignature
scm3.org_1   |   Key_Encipherment
scm3.org_1   |   Data_Encipherment
scm3.org_1   |   Key_Agreement
scm3.org_1   |   Key_CertSign
scm3.org_1   |   Crl_Sign
scm3.org_1   | ]
scm3.org_1   | 
scm1.org_1   | 2023-01-06 03:50:12,987 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO impl.RoleInfo: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: start e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderStateImpl
scm1.org_1   | 2023-01-06 03:50:13,007 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-SegmentedRaftLogWorker: Starting segment from index:0
scm1.org_1   | 2023-01-06 03:50:13,033 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3: set configuration 0: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-01-06 03:50:13,098 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3/current/log_inprogress_0
scm1.org_1   | 2023-01-06 03:50:13,901 [main] INFO server.RaftServer: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: close
scm1.org_1   | 2023-01-06 03:50:13,901 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3: shutdown
scm1.org_1   | 2023-01-06 03:50:13,902 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-D75CE56C2BC3,id=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674
scm1.org_1   | 2023-01-06 03:50:13,902 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO impl.RoleInfo: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: shutdown e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderStateImpl
scm1.org_1   | 2023-01-06 03:50:13,906 [main] INFO server.GrpcService: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: shutdown server GrpcServerProtocolService now
scm1.org_1   | 2023-01-06 03:50:13,908 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO impl.PendingRequests: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-PendingRequests: sendNotLeaderResponses
scm1.org_1   | 2023-01-06 03:50:13,915 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO impl.StateMachineUpdater: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater: set stopIndex = 0
scm1.org_1   | 2023-01-06 03:50:13,916 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO impl.StateMachineUpdater: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater: Took a snapshot at index 0
scm1.org_1   | 2023-01-06 03:50:13,923 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO impl.StateMachineUpdater: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm1.org_1   | 2023-01-06 03:50:13,927 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3: closes. applyIndex: 0
scm1.org_1   | 2023-01-06 03:50:13,928 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
scm1.org_1   | 2023-01-06 03:50:13,929 [main] INFO server.GrpcService: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: shutdown server GrpcServerProtocolService successfully
scm1.org_1   | 2023-01-06 03:50:13,930 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x6e5bf74c, L:/0.0.0.0:36235] CLOSE
scm1.org_1   | 2023-01-06 03:50:13,930 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x6e5bf74c, L:/0.0.0.0:36235] INACTIVE
scm1.org_1   | 2023-01-06 03:50:13,930 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x6e5bf74c, L:/0.0.0.0:36235] UNREGISTERED
scm1.org_1   | 2023-01-06 03:50:13,948 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-SegmentedRaftLogWorker close()
scm1.org_1   | 2023-01-06 03:50:13,961 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: Stopped
scm1.org_1   | 2023-01-06 03:50:13,961 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-01-06 03:50:13,964 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3; layoutVersion=4; scmId=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674
scm1.org_1   | 2023-01-06 03:50:14,966 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm1.org_1   | /************************************************************
scm1.org_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm1.org/172.25.0.116
scm1.org_1   | ************************************************************/
scm1.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1.org_1   | 2023-01-06 03:50:16,552 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm1.org_1   | /************************************************************
scm1.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm1.org_1   | STARTUP_MSG:   host = scm1.org/172.25.0.116
scm1.org_1   | STARTUP_MSG:   args = []
scm1.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm1.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm1.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/9d30c9a7ce375cd614e86022850c7c128723df6b ; compiled by 'runner' on 2023-01-06T03:18Z
scm1.org_1   | STARTUP_MSG:   java = 11.0.14.1
scm1.org_1   | ************************************************************/
scm1.org_1   | 2023-01-06 03:50:16,564 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1.org_1   | 2023-01-06 03:50:16,660 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-01-06 03:50:16,712 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1.org_1   | 2023-01-06 03:50:16,736 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm1.org_1   | 2023-01-06 03:50:16,785 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1.org:9894 and Ratis port: 9894
scm1.org_1   | 2023-01-06 03:50:16,785 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1.org
scm1.org_1   | 2023-01-06 03:50:17,395 [main] INFO client.SCMCertificateClient: Loading certificate from location:/data/metadata/scm/sub-ca/certs.
scm1.org_1   | 2023-01-06 03:50:17,572 [main] INFO client.SCMCertificateClient: Added certificate [
scm1.org_1   | [
scm1.org_1   |   Version: V3
scm1.org_1   |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm-sub@scm1.org
scm1.org_1   |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
scm1.org_1   | 
scm1.org_1   |   Key:  Sun RSA public key, 2048 bits
scm1.org_1   |   params: null
om3_1        | 2023-01-06 03:53:24,625 [om3@group-562213E44849-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
om3_1        | 2023-01-06 03:53:24,625 [om3@group-562213E44849-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
om3_1        | 2023-01-06 03:53:25,505 [Listener at om3/9862] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om3_1        | 2023-01-06 03:53:25,815 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@58e3c906{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-3769354933669146219/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om3_1        | 2023-01-06 03:53:25,980 [Listener at om3/9862] INFO server.AbstractConnector: Started ServerConnector@7fda0e89{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om3_1        | 2023-01-06 03:53:25,986 [Listener at om3/9862] INFO server.Server: Started @71249ms
om3_1        | 2023-01-06 03:53:26,036 [Listener at om3/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om3_1        | 2023-01-06 03:53:26,036 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om3_1        | 2023-01-06 03:53:26,047 [Listener at om3/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om3_1        | 2023-01-06 03:53:26,048 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om3_1        | 2023-01-06 03:53:26,092 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om3_1        | 2023-01-06 03:53:27,824 [Listener at om3/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om3_1        | 2023-01-06 03:53:29,372 [Listener at om3/9862] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om3_1        | 2023-01-06 03:53:29,585 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@3fc7b289] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om3_1        | 2023-01-06 03:53:29,700 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1: ELECTION TIMEOUT received 0 response(s) and 0 exception(s):
om3_1        | 2023-01-06 03:53:29,703 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1 ELECTION round 0: result TIMEOUT
om3_1        | 2023-01-06 03:53:29,772 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1 ELECTION round 1: submit vote requests at term 2 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1        | 2023-01-06 03:53:29,862 [om3@group-562213E44849-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1        | 2023-01-06 03:53:29,862 [om3@group-562213E44849-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1        | 2023-01-06 03:53:30,504 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:41099
om3_1        | 2023-01-06 03:53:30,705 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om3_1        | 2023-01-06 03:53:31,742 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-562213E44849: receive requestVote(ELECTION, om2, group-562213E44849, 2, (t:0, i:~))
om3_1        | 2023-01-06 03:53:31,825 [grpc-default-executor-1] INFO impl.VoteContext: om3@group-562213E44849-CANDIDATE: reject ELECTION from om2: already has voted for om3 at current term 2
om3_1        | 2023-01-06 03:53:31,852 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-562213E44849 replies to ELECTION vote request: om2<-om3#0:FAIL-t2. Peer's state: om3@group-562213E44849:t2, leader=null, voted=om3, raftlog=Memoized:om3@group-562213E44849-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1        | 2023-01-06 03:53:32,086 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1: ELECTION REJECTED received 2 response(s) and 0 exception(s):
om3_1        | 2023-01-06 03:53:32,086 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Response 0: om3<-om1#0:FAIL-t2
om3_1        | 2023-01-06 03:53:32,086 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Response 1: om3<-om2#0:FAIL-t2
om3_1        | 2023-01-06 03:53:32,087 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1 ELECTION round 1: result REJECTED
om3_1        | 2023-01-06 03:53:32,089 [om3@group-562213E44849-LeaderElection1] INFO server.RaftServer$Division: om3@group-562213E44849: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
om3_1        | 2023-01-06 03:53:32,100 [om3@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om3: shutdown om3@group-562213E44849-LeaderElection1
om3_1        | 2023-01-06 03:53:32,108 [om3@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om3: start om3@group-562213E44849-FollowerState
om3_1        | 2023-01-06 03:53:32,132 [om3@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1        | 2023-01-06 03:53:32,148 [om3@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1        | 2023-01-06 03:53:35,155 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-562213E44849: receive requestVote(ELECTION, om1, group-562213E44849, 3, (t:0, i:~))
om3_1        | 2023-01-06 03:53:35,157 [grpc-default-executor-1] INFO impl.VoteContext: om3@group-562213E44849-FOLLOWER: accept ELECTION from om1: our priority 0 <= candidate's priority 0
om3_1        | 2023-01-06 03:53:35,158 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-562213E44849: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:om1
om3_1        | 2023-01-06 03:53:35,158 [grpc-default-executor-1] INFO impl.RoleInfo: om3: shutdown om3@group-562213E44849-FollowerState
om3_1        | 2023-01-06 03:53:35,159 [om3@group-562213E44849-FollowerState] INFO impl.FollowerState: om3@group-562213E44849-FollowerState was interrupted
om3_1        | 2023-01-06 03:53:35,163 [grpc-default-executor-1] INFO impl.RoleInfo: om3: start om3@group-562213E44849-FollowerState
om3_1        | 2023-01-06 03:53:35,185 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-562213E44849 replies to ELECTION vote request: om1<-om3#0:OK-t3. Peer's state: om3@group-562213E44849:t3, leader=null, voted=om1, raftlog=Memoized:om3@group-562213E44849-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1        | 2023-01-06 03:53:35,201 [om3@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1        | 2023-01-06 03:53:35,201 [om3@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1        | 2023-01-06 03:53:35,816 [om3-server-thread1] INFO server.RaftServer$Division: om3@group-562213E44849: change Leader from null to om1 at term 3 for appendEntries, leader elected after 30095ms
om3_1        | 2023-01-06 03:53:35,846 [om3-server-thread1] INFO server.RaftServer$Division: om3@group-562213E44849: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1        | 2023-01-06 03:53:35,876 [om3-server-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: Starting segment from index:0
om3_1        | 2023-01-06 03:53:36,570 [om3@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_0
om3_1        | 2023-01-06 03:53:39,524 [om3@group-562213E44849-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om3_1        | [id: "om1"
om3_1        | address: "om1:9872"
om3_1        | startupRole: FOLLOWER
om3_1        | , id: "om3"
om3_1        | address: "om3:9872"
om3_1        | startupRole: FOLLOWER
om3_1        | , id: "om2"
om3_1        | address: "om2:9872"
om3_1        | startupRole: FOLLOWER
om3_1        | ]
scm1.org_1   |   modulus: 25512042468887604642064404093025249920280347815052024483618130467625890981963310751612908088931868680366166722736700966336351841536566461994206957959065592473127025545304172175201240100170923940025313585056297682798208555874810870376218207324432498120475419600056108682275459578022550441815608531770386354956257639186299780577671839142629796440640716929439466625583225667634032960664095389171765331267228308686758487244580763947638224999982475527976426192624582650657992140973107557451334337699303987990444760298981625629400711682432328660808877074618851496477253766349631046458008273474791424727251297086392264850331
scm1.org_1   |   public exponent: 65537
scm1.org_1   |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
scm1.org_1   |                To: Mon Feb 14 00:00:00 UTC 2028]
scm1.org_1   |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
scm1.org_1   |   SerialNumber: [    01379773 48ab]
scm1.org_1   | 
scm1.org_1   | Certificate Extensions: 3
scm1.org_1   | [1]: ObjectId: 2.5.29.19 Criticality=true
scm1.org_1   | BasicConstraints:[
scm1.org_1   |   CA:true
scm1.org_1   |   PathLen:2147483647
scm1.org_1   | ]
scm1.org_1   | 
scm1.org_1   | [2]: ObjectId: 2.5.29.15 Criticality=true
scm1.org_1   | KeyUsage [
scm1.org_1   |   DigitalSignature
scm1.org_1   |   Key_Encipherment
scm1.org_1   |   Data_Encipherment
scm1.org_1   |   Key_Agreement
scm1.org_1   |   Key_CertSign
scm1.org_1   |   Crl_Sign
scm1.org_1   | ]
scm1.org_1   | 
scm1.org_1   | [3]: ObjectId: 2.5.29.17 Criticality=false
scm1.org_1   | SubjectAlternativeName [
scm1.org_1   |   IPAddress: 172.25.0.116
scm1.org_1   |   DNSName: scm1.org
scm1.org_1   | ]
scm1.org_1   | 
scm1.org_1   | ]
scm1.org_1   |   Algorithm: [SHA256withRSA]
scm1.org_1   |   Signature:
scm1.org_1   | 0000: 71 A1 97 4D DA 8D 52 3F   E5 06 22 8E 76 61 49 C0  q..M..R?..".vaI.
scm1.org_1   | 0010: C6 DB 6D 93 9B E0 DF 39   D0 2C 5E B2 55 3C F2 BA  ..m....9.,^.U<..
scm1.org_1   | 0020: 2B 11 C5 2A C7 34 78 58   C1 96 DE C2 09 D7 16 49  +..*.4xX.......I
scm1.org_1   | 0030: 14 DD 81 0D 0D EC 2B 14   A8 C2 D5 DC D9 65 72 65  ......+......ere
scm1.org_1   | 0040: AC 06 88 C6 35 48 D5 5F   88 26 38 54 A0 34 B7 6A  ....5H._.&8T.4.j
scm1.org_1   | 0050: 80 D9 98 32 D0 A7 83 BC   EF F9 F3 B4 3E E3 DB 99  ...2........>...
scm1.org_1   | 0060: C8 62 68 E3 13 33 55 58   91 03 33 A0 D4 E3 E8 C7  .bh..3UX..3.....
scm1.org_1   | 0070: 11 6E E3 D9 21 80 E7 D3   FD D9 65 FA 44 9C 4A 18  .n..!.....e.D.J.
scm1.org_1   | 0080: 0A F3 39 BC 62 9F 21 94   BE 66 EF E8 B3 BE 21 DA  ..9.b.!..f....!.
scm1.org_1   | 0090: 04 60 36 57 8C 00 B7 0D   3C 31 30 4D 96 93 18 D6  .`6W....<10M....
scm1.org_1   | 00A0: DE 9E 43 8C B8 02 39 06   65 33 4A E0 D2 C5 E1 C5  ..C...9.e3J.....
scm1.org_1   | 00B0: EC 84 E7 E9 D8 7F 26 5A   7F EC C4 ED E0 DA 89 3A  ......&Z.......:
scm1.org_1   | 00C0: 2A 12 BB 4E A2 53 59 0A   EB 4D CC C2 5B DB 3E 6C  *..N.SY..M..[.>l
scm1.org_1   | 00D0: 79 1A 95 EA 9D F5 37 22   32 4C FC DF 91 B5 AD 85  y.....7"2L......
scm1.org_1   | 00E0: 28 09 F5 E5 5B 53 76 DA   63 17 AE F5 80 A5 B1 AF  (...[Sv.c.......
scm1.org_1   | 00F0: 1B 90 43 5F FF 80 DB 39   CC 0A 02 38 20 9A EB 4F  ..C_...9...8 ..O
scm1.org_1   | 
scm1.org_1   | ] from file:/data/metadata/scm/sub-ca/certs/1338275743915.crt.
scm1.org_1   | 2023-01-06 03:50:17,578 [main] INFO client.SCMCertificateClient: Added certificate [
scm1.org_1   | [
scm1.org_1   |   Version: V3
scm1.org_1   |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm-sub@scm1.org
scm1.org_1   |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
scm1.org_1   | 
scm1.org_1   |   Key:  Sun RSA public key, 2048 bits
scm1.org_1   |   params: null
scm1.org_1   |   modulus: 25512042468887604642064404093025249920280347815052024483618130467625890981963310751612908088931868680366166722736700966336351841536566461994206957959065592473127025545304172175201240100170923940025313585056297682798208555874810870376218207324432498120475419600056108682275459578022550441815608531770386354956257639186299780577671839142629796440640716929439466625583225667634032960664095389171765331267228308686758487244580763947638224999982475527976426192624582650657992140973107557451334337699303987990444760298981625629400711682432328660808877074618851496477253766349631046458008273474791424727251297086392264850331
scm1.org_1   |   public exponent: 65537
scm1.org_1   |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
scm1.org_1   |                To: Mon Feb 14 00:00:00 UTC 2028]
scm1.org_1   |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
scm1.org_1   |   SerialNumber: [    01379773 48ab]
scm1.org_1   | 
scm1.org_1   | Certificate Extensions: 3
scm1.org_1   | [1]: ObjectId: 2.5.29.19 Criticality=true
scm1.org_1   | BasicConstraints:[
scm1.org_1   |   CA:true
scm1.org_1   |   PathLen:2147483647
scm1.org_1   | ]
scm1.org_1   | 
scm1.org_1   | [2]: ObjectId: 2.5.29.15 Criticality=true
scm1.org_1   | KeyUsage [
scm1.org_1   |   DigitalSignature
scm1.org_1   |   Key_Encipherment
scm1.org_1   |   Data_Encipherment
scm1.org_1   |   Key_Agreement
scm1.org_1   |   Key_CertSign
scm1.org_1   |   Crl_Sign
scm3.org_1   | [3]: ObjectId: 2.5.29.17 Criticality=false
scm3.org_1   | SubjectAlternativeName [
scm3.org_1   |   IPAddress: 172.25.0.118
scm3.org_1   |   DNSName: scm3.org
scm3.org_1   | ]
scm3.org_1   | 
scm3.org_1   | ]
scm3.org_1   |   Algorithm: [SHA256withRSA]
scm3.org_1   |   Signature:
scm3.org_1   | 0000: 0E F8 44 CF FA 04 46 8F   F4 10 24 A9 1F D6 FA 1A  ..D...F...$.....
scm3.org_1   | 0010: 39 2C 6C 27 46 24 EA DF   A2 31 02 6B 1D 52 BF D2  9,l'F$...1.k.R..
scm3.org_1   | 0020: 82 89 74 3A A2 44 41 94   40 2A 62 BF A3 72 52 44  ..t:.DA.@*b..rRD
scm3.org_1   | 0030: 05 D1 58 10 54 E2 D3 58   7F 2C 3B AB 2F 69 13 E3  ..X.T..X.,;./i..
scm3.org_1   | 0040: 91 54 BE 19 00 F8 80 C1   EF 6B 16 21 FA E7 27 9C  .T.......k.!..'.
scm3.org_1   | 0050: C5 15 79 A7 11 7B BF 1D   3D 35 DD 70 A6 8A D1 0D  ..y.....=5.p....
scm3.org_1   | 0060: 47 38 F0 D4 64 AB 5B 87   88 E2 58 35 FC B2 31 DA  G8..d.[...X5..1.
scm3.org_1   | 0070: C1 DD 8E 21 BB 17 FE 11   E4 DD D3 04 74 1E C4 3B  ...!........t..;
scm3.org_1   | 0080: A2 E0 FC 14 DD 34 5A AA   9E 27 61 E2 1C B0 FC 75  .....4Z..'a....u
scm3.org_1   | 0090: 66 52 3A 11 59 F8 0E 3B   9C 19 AA CC 67 5E A5 E0  fR:.Y..;....g^..
scm3.org_1   | 00A0: 01 22 21 CF 44 7A 43 22   1C 10 A5 4E 47 2A 77 0B  ."!.DzC"...NG*w.
scm3.org_1   | 00B0: 9E 44 A9 BC 64 DC C1 A0   2B F4 95 81 AD CE 83 F8  .D..d...+.......
scm3.org_1   | 00C0: E4 83 63 B4 D8 CF B4 81   F7 24 0D 37 63 F9 03 82  ..c......$.7c...
scm3.org_1   | 00D0: C6 A0 9E 27 ED 29 AE 58   65 88 3B AB 5D B1 B2 1B  ...'.).Xe.;.]...
scm3.org_1   | 00E0: EC 2C F5 04 42 B2 01 B6   87 AB E0 00 A6 24 FE 17  .,..B........$..
scm3.org_1   | 00F0: 25 5F 2A B7 5C 4B 2E 73   ED 5C 84 75 43 1A 4E 51  %_*.\K.s.\.uC.NQ
scm3.org_1   | 
scm3.org_1   | ] from file:/data/metadata/scm/sub-ca/certs/1396150326270.crt.
scm3.org_1   | 2023-01-06 03:51:08,508 [main] INFO client.SCMCertificateClient: CertificateLifetimeMonitor is started with first delay 158702931505 ms and interval 86400000 ms.
scm3.org_1   | 2023-01-06 03:51:08,963 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm3.org_1   | 2023-01-06 03:51:08,963 [main] INFO server.StorageContainerManager: SCM login successful.
scm3.org_1   | 2023-01-06 03:51:09,114 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2023-01-06 03:51:09,584 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2023-01-06 03:51:10,154 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm3.org_1   | 2023-01-06 03:51:10,158 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm3.org_1   | 2023-01-06 03:51:10,320 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm3.org_1   | 2023-01-06 03:51:10,381 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:962229bc-cc46-47e6-82cc-1ac6b63780ff
scm3.org_1   | 2023-01-06 03:51:10,457 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm3.org_1   | 2023-01-06 03:51:10,477 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm3.org_1   | 2023-01-06 03:51:10,598 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm3.org_1   | 2023-01-06 03:51:10,741 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm3.org_1   | 2023-01-06 03:51:10,746 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm3.org_1   | 2023-01-06 03:51:10,750 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm3.org_1   | 2023-01-06 03:51:10,750 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm3.org_1   | 2023-01-06 03:51:10,752 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm3.org_1   | 2023-01-06 03:51:10,752 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm3.org_1   | 2023-01-06 03:51:10,753 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm3.org_1   | 2023-01-06 03:51:10,755 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2023-01-06 03:51:10,759 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm3.org_1   | 2023-01-06 03:51:10,760 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm3.org_1   | 2023-01-06 03:51:10,780 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm3.org_1   | 2023-01-06 03:51:10,794 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm3.org_1   | 2023-01-06 03:51:10,795 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm3.org_1   | 2023-01-06 03:51:11,978 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
scm3.org_1   | 2023-01-06 03:51:12,180 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
scm3.org_1   | 2023-01-06 03:51:12,180 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
scm3.org_1   | 2023-01-06 03:51:12,181 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
scm3.org_1   | 2023-01-06 03:51:12,183 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
scm3.org_1   | 2023-01-06 03:51:12,190 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
scm3.org_1   | 2023-01-06 03:51:12,190 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
scm3.org_1   | 2023-01-06 03:51:12,202 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
scm3.org_1   | 2023-01-06 03:51:12,205 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
scm3.org_1   | 2023-01-06 03:51:12,296 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
scm3.org_1   | 2023-01-06 03:51:12,298 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
scm3.org_1   | 2023-01-06 03:51:12,401 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm3.org_1   | 2023-01-06 03:51:12,401 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm3.org_1   | 2023-01-06 03:51:12,403 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3.org_1   | 2023-01-06 03:51:12,403 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2.org_1   |   modulus: 19778687731656783772902830138794289191672000448805848329103884768919417809288510074901844159535406711588697773789918091190180188081234478858440340972086773861034019073695059359075079030277481784868461999684352064578062828793989761167257543645975096970706355374322054880501576319849373121843095991424965125439656676571281789245182074680401674312259974497266585520686914893620314882138483272129225136754739166002360094272522613050167208491314474059999343859379487390204773389386141154611105399439274668810213999193236564345819598451717566758467633730773824047022548834010030804952760683971758717927916279117889616455907
scm2.org_1   |   public exponent: 65537
scm2.org_1   |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
scm2.org_1   |                To: Mon Feb 14 00:00:00 UTC 2028]
scm2.org_1   |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
scm2.org_1   |   SerialNumber: [    013d2b98 fd1f]
scm2.org_1   | 
scm2.org_1   | Certificate Extensions: 3
scm2.org_1   | [1]: ObjectId: 2.5.29.19 Criticality=true
scm2.org_1   | BasicConstraints:[
scm2.org_1   |   CA:true
scm2.org_1   |   PathLen:2147483647
scm2.org_1   | ]
scm2.org_1   | 
scm2.org_1   | [2]: ObjectId: 2.5.29.15 Criticality=true
scm2.org_1   | KeyUsage [
scm2.org_1   |   DigitalSignature
scm2.org_1   |   Key_Encipherment
scm2.org_1   |   Data_Encipherment
scm2.org_1   |   Key_Agreement
scm2.org_1   |   Key_CertSign
scm2.org_1   |   Crl_Sign
scm2.org_1   | ]
scm2.org_1   | 
scm2.org_1   | [3]: ObjectId: 2.5.29.17 Criticality=false
scm2.org_1   | SubjectAlternativeName [
scm2.org_1   |   IPAddress: 172.25.0.117
scm2.org_1   |   DNSName: scm2.org
scm2.org_1   | ]
scm2.org_1   | 
scm2.org_1   | ]
scm2.org_1   |   Algorithm: [SHA256withRSA]
scm2.org_1   |   Signature:
scm2.org_1   | 0000: 8E E7 B6 EA 4B 46 C1 E3   39 F3 F6 EA A0 98 E3 BF  ....KF..9.......
scm2.org_1   | 0010: 32 05 D0 D2 48 B1 72 63   30 7E E3 E7 52 CB AE A8  2...H.rc0...R...
scm2.org_1   | 0020: 01 21 E0 9B B3 9A E7 24   05 67 E7 50 19 56 86 D4  .!.....$.g.P.V..
scm2.org_1   | 0030: C9 98 71 08 EA 3E 0A 6F   9D 9E 14 EF A9 95 1F A6  ..q..>.o........
scm2.org_1   | 0040: 41 BB 17 DC F7 DA 27 FF   3D D4 3B F0 1A 20 EB 64  A.....'.=.;.. .d
scm2.org_1   | 0050: 45 8A 5A 48 E7 F0 CA D4   03 F9 8B AB 6C F3 D9 02  E.ZH........l...
scm2.org_1   | 0060: 8F 68 B9 53 0F 74 D4 90   88 FB 7A B0 A8 61 D3 D1  .h.S.t....z..a..
scm2.org_1   | 0070: 85 6C 89 4C 29 F2 67 32   D1 5F F0 1C 50 F6 59 DE  .l.L).g2._..P.Y.
scm2.org_1   | 0080: 5B A5 84 9D 81 0A 20 5D   18 83 06 0F 01 BE 5E E9  [..... ]......^.
scm2.org_1   | 0090: BD 2B A2 35 52 CA 29 B0   2D 7F FC 05 1F 59 68 C9  .+.5R.).-....Yh.
scm2.org_1   | 00A0: 08 18 BB A6 1B D1 63 5E   04 0D DA 17 9F A1 E3 DF  ......c^........
scm2.org_1   | 00B0: 36 B8 48 7F 2A C7 15 87   52 9B B4 68 8D 46 B7 7F  6.H.*...R..h.F..
scm2.org_1   | 00C0: BB 06 03 BC F5 AE B7 BB   9F A1 68 01 9C 9D A1 0D  ..........h.....
scm2.org_1   | 00D0: F0 F0 A4 FC 03 E7 E5 DF   CE 97 B4 B5 5F 99 C7 89  ............_...
scm2.org_1   | 00E0: 06 D0 E7 A3 D6 49 77 6E   E4 5C A5 86 01 C5 C9 C1  .....Iwn.\......
scm2.org_1   | 00F0: 9E 9D AB 55 4C A9 BE 9D   C1 41 4F 33 CD 32 7A EE  ...UL....AO3.2z.
scm2.org_1   | 
scm2.org_1   | ] from file:/data/metadata/scm/sub-ca/certs/certificate.crt.
scm2.org_1   | 2023-01-06 03:50:38,877 [main] INFO client.SCMCertificateClient: Added certificate [
scm2.org_1   | [
scm2.org_1   |   Version: V3
scm2.org_1   |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
scm2.org_1   |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
scm2.org_1   | 
scm2.org_1   |   Key:  Sun RSA public key, 2048 bits
scm2.org_1   |   params: null
scm2.org_1   |   modulus: 22321520092034904956367956777931957676969911078232901017895933698678468781753172695844393013092965970615602622357794023639048732305076160434218405355746465972426985604516863422070797512086951139731951669174461346685083874381071907314493811292043757517828199039562222504144931947786863316592495947302761724410796725398590551213642383206197662885192142834321323519855792963024540129867342534976672519842084316567886340381000959465926087773440860829229209280683297485436375405886176389600891889084968589860808245139826791285487072108592486434566834307409916351251604701981140849210029026671497960989540995516629779904417
scm2.org_1   |   public exponent: 65537
scm2.org_1   |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
scm2.org_1   |                To: Mon Feb 14 00:00:00 UTC 2028]
scm2.org_1   |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
scm2.org_1   |   SerialNumber: [    01]
scm2.org_1   | 
scm2.org_1   | Certificate Extensions: 3
scm2.org_1   | [1]: ObjectId: 2.5.29.19 Criticality=true
scm2.org_1   | BasicConstraints:[
scm2.org_1   |   CA:true
scm2.org_1   |   PathLen:2147483647
scm2.org_1   | ]
scm2.org_1   | 
scm2.org_1   | [2]: ObjectId: 2.5.29.15 Criticality=true
scm2.org_1   | KeyUsage [
scm2.org_1   |   Key_CertSign
scm2.org_1   |   Crl_Sign
scm2.org_1   | ]
scm2.org_1   | 
scm2.org_1   | [3]: ObjectId: 2.5.29.17 Criticality=false
scm3.org_1   | 2023-01-06 03:51:12,411 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3.org_1   | 2023-01-06 03:51:12,446 [main] INFO server.RaftServer: 962229bc-cc46-47e6-82cc-1ac6b63780ff: addNew group-D75CE56C2BC3:[] returns group-D75CE56C2BC3:java.util.concurrent.CompletableFuture@514377fc[Not completed]
scm3.org_1   | 2023-01-06 03:51:12,441 [962229bc-cc46-47e6-82cc-1ac6b63780ff-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xf229f5f8] REGISTERED
scm3.org_1   | 2023-01-06 03:51:12,449 [962229bc-cc46-47e6-82cc-1ac6b63780ff-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xf229f5f8] BIND: 0.0.0.0/0.0.0.0:0
scm3.org_1   | 2023-01-06 03:51:12,455 [962229bc-cc46-47e6-82cc-1ac6b63780ff-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xf229f5f8, L:/0.0.0.0:46277] ACTIVE
scm3.org_1   | 2023-01-06 03:51:12,493 [pool-17-thread-1] INFO server.RaftServer$Division: 962229bc-cc46-47e6-82cc-1ac6b63780ff: new RaftServerImpl for group-D75CE56C2BC3:[] with SCMStateMachine:uninitialized
scm3.org_1   | 2023-01-06 03:51:12,497 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm3.org_1   | 2023-01-06 03:51:12,497 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm3.org_1   | 2023-01-06 03:51:12,498 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm3.org_1   | 2023-01-06 03:51:12,498 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3.org_1   | 2023-01-06 03:51:12,498 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3.org_1   | 2023-01-06 03:51:12,498 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm3.org_1   | 2023-01-06 03:51:12,516 [pool-17-thread-1] INFO server.RaftServer$Division: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm3.org_1   | 2023-01-06 03:51:12,516 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3.org_1   | 2023-01-06 03:51:12,527 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm3.org_1   | 2023-01-06 03:51:12,528 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm3.org_1   | 2023-01-06 03:51:12,546 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm3.org_1   | 2023-01-06 03:51:12,551 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm3.org_1   | 2023-01-06 03:51:12,552 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm3.org_1   | 2023-01-06 03:51:12,658 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3.org_1   | 2023-01-06 03:51:12,659 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm3.org_1   | 2023-01-06 03:51:12,659 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm3.org_1   | 2023-01-06 03:51:12,659 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm3.org_1   | 2023-01-06 03:51:12,661 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm3.org_1   | 2023-01-06 03:51:12,665 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm3.org_1   | 2023-01-06 03:51:12,665 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm3.org_1   | 2023-01-06 03:51:12,674 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm3.org_1   | 2023-01-06 03:51:13,116 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
scm3.org_1   | 2023-01-06 03:51:13,451 [main] INFO reflections.Reflections: Reflections took 290 ms to scan 3 urls, producing 121 keys and 272 values 
scm3.org_1   | 2023-01-06 03:51:13,669 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm3.org_1   | 2023-01-06 03:51:13,670 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm3.org_1   | 2023-01-06 03:51:13,682 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm3.org_1   | 2023-01-06 03:51:13,685 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm3.org_1   | 2023-01-06 03:51:13,951 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm3.org_1   | 2023-01-06 03:51:13,994 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm3.org_1   | 2023-01-06 03:51:14,002 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm3.org_1   | 2023-01-06 03:51:14,045 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm3.org_1   | 2023-01-06 03:51:14,242 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm3.org_1   | 2023-01-06 03:51:14,243 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm3.org_1   | 2023-01-06 03:51:14,266 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
recon_1      | 2023-01-06 03:51:13,172 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 4 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:13,174 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:51:15,176 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 6 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:15,178 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 7 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:15,179 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:51:17,180 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 9 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:17,183 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 10 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:17,203 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:51:19,204 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 12 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:19,206 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 13 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:19,208 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:51:21,211 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 15 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:21,212 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 16 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:21,213 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 17 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:51:23,215 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 18 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:23,216 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 19 failover attempts. Trying to failover immediately.
scm3.org_1   | 2023-01-06 03:51:14,266 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm3.org_1   | 2023-01-06 03:51:14,281 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm3.org_1   | 2023-01-06 03:51:14,281 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm3.org_1   | 2023-01-06 03:51:14,296 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm3.org_1   | 2023-01-06 03:51:14,297 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm3.org_1   | 2023-01-06 03:51:14,419 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm3.org_1   | 2023-01-06 03:51:14,490 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm3.org_1   | 2023-01-06 03:51:14,734 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm3.org_1   | 2023-01-06 03:51:14,783 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm3.org_1   | 2023-01-06 03:51:14,784 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm3.org_1   | 2023-01-06 03:51:14,825 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm3.org_1   | 2023-01-06 03:51:14,834 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-06 03:51:14,836 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3.org_1   | 2023-01-06 03:51:14,905 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm3.org_1   | 2023-01-06 03:51:14,976 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2023-01-06 03:51:15,047 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm3.org_1   | 2023-01-06 03:51:16,852 [Listener at 0.0.0.0/9961] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3.org_1   | 2023-01-06 03:51:16,891 [Listener at 0.0.0.0/9961] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | SubjectAlternativeName [
scm2.org_1   |   IPAddress: 172.25.0.116
scm2.org_1   |   DNSName: scm1.org
scm2.org_1   | ]
scm2.org_1   | 
scm2.org_1   | ]
scm2.org_1   |   Algorithm: [SHA256withRSA]
scm2.org_1   |   Signature:
scm2.org_1   | 0000: 21 07 40 68 BD F5 21 63   53 A4 1C 6D 61 63 C4 B6  !.@h..!cS..mac..
scm2.org_1   | 0010: E3 BD 40 F3 9B FD 99 A0   FE 5C 7B FD 06 FB 50 8E  ..@......\....P.
scm2.org_1   | 0020: E2 8C 95 3C 0B 0B 1A AD   6E 89 50 B3 9C 01 22 36  ...<....n.P..."6
scm2.org_1   | 0030: 82 EA 04 5E 17 F6 FE 59   8A 78 83 34 58 E3 1A CF  ...^...Y.x.4X...
scm2.org_1   | 0040: C0 08 87 80 9B 7D B8 33   3C E2 09 5B A0 BB 73 F9  .......3<..[..s.
scm2.org_1   | 0050: 18 A1 8D D7 79 71 5D F0   D7 29 1C F6 F1 1A 40 0C  ....yq]..)....@.
scm2.org_1   | 0060: DC B0 36 E8 EA 71 C3 CB   C9 B0 27 6D D7 9F 21 23  ..6..q....'m..!#
scm2.org_1   | 0070: 40 5D F0 5B 0A B7 A0 74   F9 9F AD 3D 48 4A CC B0  @].[...t...=HJ..
scm2.org_1   | 0080: 71 94 D1 27 C1 11 45 74   CC E9 49 95 45 B0 E7 F7  q..'..Et..I.E...
scm2.org_1   | 0090: 9B 9F F3 43 1C 28 31 DC   D6 F4 73 3B 1B 52 1D 3F  ...C.(1...s;.R.?
scm2.org_1   | 00A0: 44 76 E5 4C 63 7F 23 9B   17 64 90 51 18 89 86 33  Dv.Lc.#..d.Q...3
scm2.org_1   | 00B0: CE 66 07 29 55 E3 3A 4D   BD 33 89 34 C3 A8 3C 51  .f.)U.:M.3.4..<Q
scm2.org_1   | 00C0: 52 16 00 6D 7D 61 54 1E   1A 84 F4 F5 89 B2 A8 9C  R..m.aT.........
scm2.org_1   | 00D0: 56 66 FE E2 2A 93 FE 19   10 10 C9 9C 0D 74 9B 01  Vf..*........t..
scm2.org_1   | 00E0: 75 1D 1C AB 74 3A E3 B8   41 66 A1 BE 62 55 B9 60  u...t:..Af..bU.`
scm2.org_1   | 00F0: F1 84 78 6F 50 A4 59 C4   78 19 02 34 DC BA 56 C3  ..xoP.Y.x..4..V.
scm2.org_1   | 
scm2.org_1   | ] from file:/data/metadata/scm/sub-ca/certs/CA-1.crt.
scm2.org_1   | 2023-01-06 03:50:38,901 [main] INFO client.SCMCertificateClient: CertificateLifetimeMonitor is started with first delay 158702961109 ms and interval 86400000 ms.
scm2.org_1   | 2023-01-06 03:50:39,390 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm2.org_1   | 2023-01-06 03:50:39,396 [main] INFO server.StorageContainerManager: SCM login successful.
scm2.org_1   | 2023-01-06 03:50:39,565 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2023-01-06 03:50:40,093 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2023-01-06 03:50:40,937 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm2.org_1   | 2023-01-06 03:50:40,938 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm2.org_1   | 2023-01-06 03:50:41,194 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm2.org_1   | 2023-01-06 03:50:41,307 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:aea2b8cb-487c-4369-b11b-87f4d7694103
scm2.org_1   | 2023-01-06 03:50:41,425 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm2.org_1   | 2023-01-06 03:50:41,448 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm2.org_1   | 2023-01-06 03:50:41,662 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm2.org_1   | 2023-01-06 03:50:41,862 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm2.org_1   | 2023-01-06 03:50:41,865 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm2.org_1   | 2023-01-06 03:50:41,865 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm2.org_1   | 2023-01-06 03:50:41,867 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm2.org_1   | 2023-01-06 03:50:41,867 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm2.org_1   | 2023-01-06 03:50:41,867 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm2.org_1   | 2023-01-06 03:50:41,874 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm2.org_1   | 2023-01-06 03:50:41,881 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2023-01-06 03:50:41,881 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm2.org_1   | 2023-01-06 03:50:41,883 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2.org_1   | 2023-01-06 03:50:41,918 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm2.org_1   | 2023-01-06 03:50:41,931 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm2.org_1   | 2023-01-06 03:50:41,931 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm2.org_1   | 2023-01-06 03:50:43,446 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
scm2.org_1   | 2023-01-06 03:50:43,711 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
scm2.org_1   | 2023-01-06 03:50:43,717 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
scm2.org_1   | 2023-01-06 03:50:43,722 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
scm2.org_1   | 2023-01-06 03:50:43,725 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
scm2.org_1   | 2023-01-06 03:50:43,731 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
scm2.org_1   | 2023-01-06 03:50:43,740 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
scm2.org_1   | 2023-01-06 03:50:43,751 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
scm2.org_1   | 2023-01-06 03:50:43,753 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
scm2.org_1   | 2023-01-06 03:50:43,929 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
scm2.org_1   | 2023-01-06 03:50:43,932 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
scm2.org_1   | 2023-01-06 03:50:44,104 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm2.org_1   | 2023-01-06 03:50:44,110 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm2.org_1   | 2023-01-06 03:50:44,112 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2.org_1   | 2023-01-06 03:50:44,112 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2.org_1   | 2023-01-06 03:50:44,133 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2.org_1   | 2023-01-06 03:50:44,169 [aea2b8cb-487c-4369-b11b-87f4d7694103-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xa4c92727] REGISTERED
scm2.org_1   | 2023-01-06 03:50:44,189 [aea2b8cb-487c-4369-b11b-87f4d7694103-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xa4c92727] BIND: 0.0.0.0/0.0.0.0:0
scm2.org_1   | 2023-01-06 03:50:44,199 [main] INFO server.RaftServer: aea2b8cb-487c-4369-b11b-87f4d7694103: addNew group-D75CE56C2BC3:[] returns group-D75CE56C2BC3:java.util.concurrent.CompletableFuture@514377fc[Not completed]
scm2.org_1   | 2023-01-06 03:50:44,201 [aea2b8cb-487c-4369-b11b-87f4d7694103-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xa4c92727, L:/0.0.0.0:34737] ACTIVE
scm2.org_1   | 2023-01-06 03:50:44,295 [pool-17-thread-1] INFO server.RaftServer$Division: aea2b8cb-487c-4369-b11b-87f4d7694103: new RaftServerImpl for group-D75CE56C2BC3:[] with SCMStateMachine:uninitialized
scm2.org_1   | 2023-01-06 03:50:44,304 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm2.org_1   | 2023-01-06 03:50:44,305 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm2.org_1   | 2023-01-06 03:50:44,309 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
recon_1      | 2023-01-06 03:51:23,216 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 20 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:51:25,218 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 21 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:25,219 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 22 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:25,222 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 23 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:51:27,223 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 24 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:27,225 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 25 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:27,246 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 26 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:51:29,255 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 27 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:29,256 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 28 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:29,258 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 29 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:51:31,260 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 30 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:31,261 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 31 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:31,261 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 32 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:51:33,263 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 33 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:33,267 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 34 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:33,270 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 35 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:51:35,272 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 36 failover attempts. Trying to failover immediately.
scm1.org_1   | ]
scm1.org_1   | 
scm1.org_1   | [3]: ObjectId: 2.5.29.17 Criticality=false
scm1.org_1   | SubjectAlternativeName [
scm1.org_1   |   IPAddress: 172.25.0.116
scm1.org_1   |   DNSName: scm1.org
scm1.org_1   | ]
scm1.org_1   | 
scm1.org_1   | ]
scm1.org_1   |   Algorithm: [SHA256withRSA]
scm1.org_1   |   Signature:
scm1.org_1   | 0000: 71 A1 97 4D DA 8D 52 3F   E5 06 22 8E 76 61 49 C0  q..M..R?..".vaI.
scm1.org_1   | 0010: C6 DB 6D 93 9B E0 DF 39   D0 2C 5E B2 55 3C F2 BA  ..m....9.,^.U<..
scm1.org_1   | 0020: 2B 11 C5 2A C7 34 78 58   C1 96 DE C2 09 D7 16 49  +..*.4xX.......I
scm1.org_1   | 0030: 14 DD 81 0D 0D EC 2B 14   A8 C2 D5 DC D9 65 72 65  ......+......ere
scm1.org_1   | 0040: AC 06 88 C6 35 48 D5 5F   88 26 38 54 A0 34 B7 6A  ....5H._.&8T.4.j
scm1.org_1   | 0050: 80 D9 98 32 D0 A7 83 BC   EF F9 F3 B4 3E E3 DB 99  ...2........>...
scm1.org_1   | 0060: C8 62 68 E3 13 33 55 58   91 03 33 A0 D4 E3 E8 C7  .bh..3UX..3.....
scm1.org_1   | 0070: 11 6E E3 D9 21 80 E7 D3   FD D9 65 FA 44 9C 4A 18  .n..!.....e.D.J.
scm1.org_1   | 0080: 0A F3 39 BC 62 9F 21 94   BE 66 EF E8 B3 BE 21 DA  ..9.b.!..f....!.
scm1.org_1   | 0090: 04 60 36 57 8C 00 B7 0D   3C 31 30 4D 96 93 18 D6  .`6W....<10M....
scm1.org_1   | 00A0: DE 9E 43 8C B8 02 39 06   65 33 4A E0 D2 C5 E1 C5  ..C...9.e3J.....
scm1.org_1   | 00B0: EC 84 E7 E9 D8 7F 26 5A   7F EC C4 ED E0 DA 89 3A  ......&Z.......:
scm1.org_1   | 00C0: 2A 12 BB 4E A2 53 59 0A   EB 4D CC C2 5B DB 3E 6C  *..N.SY..M..[.>l
scm1.org_1   | 00D0: 79 1A 95 EA 9D F5 37 22   32 4C FC DF 91 B5 AD 85  y.....7"2L......
scm1.org_1   | 00E0: 28 09 F5 E5 5B 53 76 DA   63 17 AE F5 80 A5 B1 AF  (...[Sv.c.......
scm1.org_1   | 00F0: 1B 90 43 5F FF 80 DB 39   CC 0A 02 38 20 9A EB 4F  ..C_...9...8 ..O
scm1.org_1   | 
scm1.org_1   | ] from file:/data/metadata/scm/sub-ca/certs/certificate.crt.
scm1.org_1   | 2023-01-06 03:50:17,584 [main] INFO client.SCMCertificateClient: Added certificate [
scm1.org_1   | [
scm1.org_1   |   Version: V3
scm1.org_1   |   Subject: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
scm1.org_1   |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
scm1.org_1   | 
scm1.org_1   |   Key:  Sun RSA public key, 2048 bits
scm1.org_1   |   params: null
scm1.org_1   |   modulus: 22321520092034904956367956777931957676969911078232901017895933698678468781753172695844393013092965970615602622357794023639048732305076160434218405355746465972426985604516863422070797512086951139731951669174461346685083874381071907314493811292043757517828199039562222504144931947786863316592495947302761724410796725398590551213642383206197662885192142834321323519855792963024540129867342534976672519842084316567886340381000959465926087773440860829229209280683297485436375405886176389600891889084968589860808245139826791285487072108592486434566834307409916351251604701981140849210029026671497960989540995516629779904417
scm1.org_1   |   public exponent: 65537
scm1.org_1   |   Validity: [From: Fri Jan 06 00:00:00 UTC 2023,
scm1.org_1   |                To: Mon Feb 14 00:00:00 UTC 2028]
scm1.org_1   |   Issuer: O=CID-2e7972fd-840e-47e3-acc1-d75ce56c2bc3, OU=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674, CN=scm@scm1.org
scm1.org_1   |   SerialNumber: [    01]
scm1.org_1   | 
scm1.org_1   | Certificate Extensions: 3
scm1.org_1   | [1]: ObjectId: 2.5.29.19 Criticality=true
scm1.org_1   | BasicConstraints:[
scm1.org_1   |   CA:true
scm1.org_1   |   PathLen:2147483647
scm1.org_1   | ]
scm1.org_1   | 
scm1.org_1   | [2]: ObjectId: 2.5.29.15 Criticality=true
scm1.org_1   | KeyUsage [
scm1.org_1   |   Key_CertSign
scm1.org_1   |   Crl_Sign
scm1.org_1   | ]
scm1.org_1   | 
scm1.org_1   | [3]: ObjectId: 2.5.29.17 Criticality=false
scm1.org_1   | SubjectAlternativeName [
scm1.org_1   |   IPAddress: 172.25.0.116
scm1.org_1   |   DNSName: scm1.org
scm1.org_1   | ]
scm1.org_1   | 
scm1.org_1   | ]
scm1.org_1   |   Algorithm: [SHA256withRSA]
scm1.org_1   |   Signature:
scm1.org_1   | 0000: 21 07 40 68 BD F5 21 63   53 A4 1C 6D 61 63 C4 B6  !.@h..!cS..mac..
scm1.org_1   | 0010: E3 BD 40 F3 9B FD 99 A0   FE 5C 7B FD 06 FB 50 8E  ..@......\....P.
scm1.org_1   | 0020: E2 8C 95 3C 0B 0B 1A AD   6E 89 50 B3 9C 01 22 36  ...<....n.P..."6
scm1.org_1   | 0030: 82 EA 04 5E 17 F6 FE 59   8A 78 83 34 58 E3 1A CF  ...^...Y.x.4X...
scm1.org_1   | 0040: C0 08 87 80 9B 7D B8 33   3C E2 09 5B A0 BB 73 F9  .......3<..[..s.
scm1.org_1   | 0050: 18 A1 8D D7 79 71 5D F0   D7 29 1C F6 F1 1A 40 0C  ....yq]..)....@.
scm1.org_1   | 0060: DC B0 36 E8 EA 71 C3 CB   C9 B0 27 6D D7 9F 21 23  ..6..q....'m..!#
scm1.org_1   | 0070: 40 5D F0 5B 0A B7 A0 74   F9 9F AD 3D 48 4A CC B0  @].[...t...=HJ..
scm1.org_1   | 0080: 71 94 D1 27 C1 11 45 74   CC E9 49 95 45 B0 E7 F7  q..'..Et..I.E...
scm1.org_1   | 0090: 9B 9F F3 43 1C 28 31 DC   D6 F4 73 3B 1B 52 1D 3F  ...C.(1...s;.R.?
scm1.org_1   | 00A0: 44 76 E5 4C 63 7F 23 9B   17 64 90 51 18 89 86 33  Dv.Lc.#..d.Q...3
scm1.org_1   | 00B0: CE 66 07 29 55 E3 3A 4D   BD 33 89 34 C3 A8 3C 51  .f.)U.:M.3.4..<Q
recon_1      | 2023-01-06 03:51:35,274 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 37 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:35,276 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 38 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:51:37,277 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 39 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:37,281 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 40 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:37,282 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 41 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:51:39,284 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 42 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:39,287 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 43 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:39,297 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 44 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:51:41,299 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 45 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:41,302 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 46 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:41,305 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 47 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:51:43,306 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 48 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:43,308 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 49 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:43,309 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 50 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:51:45,311 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 51 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:45,312 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 52 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:45,313 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 53 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:51:47,315 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 54 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:47,317 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 55 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:47,318 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 56 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:51:49,320 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 57 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:49,324 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 58 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:49,332 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 59 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:51:51,335 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 60 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:51,337 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 61 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:51,338 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 62 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2023-01-06 03:50:44,314 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2.org_1   | 2023-01-06 03:50:44,314 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2.org_1   | 2023-01-06 03:50:44,315 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm2.org_1   | 2023-01-06 03:50:44,338 [pool-17-thread-1] INFO server.RaftServer$Division: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm2.org_1   | 2023-01-06 03:50:44,340 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2.org_1   | 2023-01-06 03:50:44,363 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm2.org_1   | 2023-01-06 03:50:44,364 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm2.org_1   | 2023-01-06 03:50:44,394 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm2.org_1   | 2023-01-06 03:50:44,409 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm2.org_1   | 2023-01-06 03:50:44,417 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm2.org_1   | 2023-01-06 03:50:44,564 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2.org_1   | 2023-01-06 03:50:44,567 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm2.org_1   | 2023-01-06 03:50:44,568 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm2.org_1   | 2023-01-06 03:50:44,570 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm2.org_1   | 2023-01-06 03:50:44,571 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm2.org_1   | 2023-01-06 03:50:44,579 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm2.org_1   | 2023-01-06 03:50:44,580 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm2.org_1   | 2023-01-06 03:50:44,581 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm2.org_1   | 2023-01-06 03:50:45,076 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
scm2.org_1   | 2023-01-06 03:50:45,678 [main] INFO reflections.Reflections: Reflections took 500 ms to scan 3 urls, producing 121 keys and 272 values 
scm2.org_1   | 2023-01-06 03:50:45,955 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm2.org_1   | 2023-01-06 03:50:45,956 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm2.org_1   | 2023-01-06 03:50:45,974 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm2.org_1   | 2023-01-06 03:50:45,979 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm2.org_1   | 2023-01-06 03:50:46,348 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm2.org_1   | 2023-01-06 03:50:46,385 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm2.org_1   | 2023-01-06 03:50:46,405 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm2.org_1   | 2023-01-06 03:50:46,455 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm2.org_1   | 2023-01-06 03:50:46,560 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm2.org_1   | 2023-01-06 03:50:46,564 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm2.org_1   | 2023-01-06 03:50:46,584 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm2.org_1   | 2023-01-06 03:50:46,594 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm2.org_1   | 2023-01-06 03:50:46,616 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm2.org_1   | 2023-01-06 03:50:46,626 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm2.org_1   | 2023-01-06 03:50:46,669 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm2.org_1   | 2023-01-06 03:50:46,685 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm2.org_1   | 2023-01-06 03:50:46,845 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm2.org_1   | 2023-01-06 03:50:46,987 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm2.org_1   | 2023-01-06 03:50:47,228 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm2.org_1   | 2023-01-06 03:50:47,302 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm2.org_1   | 2023-01-06 03:50:47,307 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm2.org_1   | 2023-01-06 03:50:47,344 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm2.org_1   | 2023-01-06 03:50:47,360 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-06 03:50:47,374 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm2.org_1   | 2023-01-06 03:50:47,499 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm2.org_1   | 2023-01-06 03:50:47,628 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-01-06 03:50:47,782 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm2.org_1   | 2023-01-06 03:50:50,194 [Listener at 0.0.0.0/9961] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2.org_1   | 2023-01-06 03:50:50,256 [Listener at 0.0.0.0/9961] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-01-06 03:50:50,263 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm2.org_1   | 2023-01-06 03:50:50,374 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3.org_1   | 2023-01-06 03:51:16,898 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm3.org_1   | 2023-01-06 03:51:17,017 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3.org_1   | 2023-01-06 03:51:17,033 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2023-01-06 03:51:17,036 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm3.org_1   | 2023-01-06 03:51:17,214 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3.org_1   | 2023-01-06 03:51:17,250 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2023-01-06 03:51:17,252 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm3.org_1   | 2023-01-06 03:51:17,556 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm3.org_1   | 2023-01-06 03:51:17,558 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm3.org_1   | Container Balancer status:
scm3.org_1   | Key                            Value
scm3.org_1   | Running                        true
scm3.org_1   | Container Balancer Configuration values:
scm3.org_1   | Key                                                Value
scm3.org_1   | Threshold                                          10
scm3.org_1   | Max Datanodes to Involve per Iteration(percent)    20
scm3.org_1   | Max Size to Move per Iteration                     500GB
scm3.org_1   | Max Size Entering Target per Iteration             26GB
scm3.org_1   | Max Size Leaving Source per Iteration              26GB
scm3.org_1   | 
scm3.org_1   | 2023-01-06 03:51:17,558 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm3.org_1   | 2023-01-06 03:51:17,570 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm3.org_1   | 2023-01-06 03:51:17,584 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm3.org_1   | 2023-01-06 03:51:17,587 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3 does not exist. Creating ...
scm3.org_1   | 2023-01-06 03:51:17,610 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3/in_use.lock acquired by nodename 8@scm3.org
scm3.org_1   | 2023-01-06 03:51:17,638 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3 has been successfully formatted.
scm3.org_1   | 2023-01-06 03:51:17,650 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm3.org_1   | 2023-01-06 03:51:17,683 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm3.org_1   | 2023-01-06 03:51:17,683 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2023-01-06 03:51:17,689 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm3.org_1   | 2023-01-06 03:51:17,691 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm3.org_1   | 2023-01-06 03:51:17,699 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3.org_1   | 2023-01-06 03:51:17,720 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm3.org_1   | 2023-01-06 03:51:17,720 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm3.org_1   | 2023-01-06 03:51:17,820 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3
scm3.org_1   | 2023-01-06 03:51:17,821 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm3.org_1   | 2023-01-06 03:51:17,821 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm3.org_1   | 2023-01-06 03:51:17,824 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3.org_1   | 2023-01-06 03:51:17,826 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm3.org_1   | 2023-01-06 03:51:17,826 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm3.org_1   | 2023-01-06 03:51:17,827 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm3.org_1   | 2023-01-06 03:51:17,827 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3.org_1   | 2023-01-06 03:51:17,828 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3.org_1   | 2023-01-06 03:51:17,850 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm3.org_1   | 2023-01-06 03:51:17,850 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2023-01-06 03:51:18,351 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3.org_1   | 2023-01-06 03:51:18,353 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm3.org_1   | 2023-01-06 03:51:18,354 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm3.org_1   | 2023-01-06 03:51:18,375 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm3.org_1   | 2023-01-06 03:51:18,375 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm3.org_1   | 2023-01-06 03:51:18,388 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServer$Division: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3: start with initializing state, conf=-1: peers:[]|listeners:[], old=null
scm1.org_1   | 00C0: 52 16 00 6D 7D 61 54 1E   1A 84 F4 F5 89 B2 A8 9C  R..m.aT.........
scm1.org_1   | 00D0: 56 66 FE E2 2A 93 FE 19   10 10 C9 9C 0D 74 9B 01  Vf..*........t..
scm1.org_1   | 00E0: 75 1D 1C AB 74 3A E3 B8   41 66 A1 BE 62 55 B9 60  u...t:..Af..bU.`
scm1.org_1   | 00F0: F1 84 78 6F 50 A4 59 C4   78 19 02 34 DC BA 56 C3  ..xoP.Y.x..4..V.
scm1.org_1   | 
scm1.org_1   | ] from file:/data/metadata/scm/sub-ca/certs/CA-1.crt.
scm1.org_1   | 2023-01-06 03:50:17,593 [main] INFO client.SCMCertificateClient: CertificateLifetimeMonitor is started with first delay 158702982412 ms and interval 86400000 ms.
scm1.org_1   | 2023-01-06 03:50:17,711 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm1.org_1   | 2023-01-06 03:50:17,711 [main] INFO server.StorageContainerManager: SCM login successful.
scm1.org_1   | 2023-01-06 03:50:17,754 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-01-06 03:50:17,942 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-01-06 03:50:18,292 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm1.org_1   | 2023-01-06 03:50:18,293 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm1.org_1   | 2023-01-06 03:50:18,373 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1.org_1   | 2023-01-06 03:50:18,410 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674
scm1.org_1   | 2023-01-06 03:50:18,459 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm1.org_1   | 2023-01-06 03:50:18,489 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm1.org_1   | 2023-01-06 03:50:18,543 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1.org_1   | 2023-01-06 03:50:18,601 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm1.org_1   | 2023-01-06 03:50:18,602 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm1.org_1   | 2023-01-06 03:50:18,603 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm1.org_1   | 2023-01-06 03:50:18,604 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm1.org_1   | 2023-01-06 03:50:18,604 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm1.org_1   | 2023-01-06 03:50:18,604 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1.org_1   | 2023-01-06 03:50:18,605 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm1.org_1   | 2023-01-06 03:50:18,607 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-01-06 03:50:18,607 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1.org_1   | 2023-01-06 03:50:18,608 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-01-06 03:50:18,617 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1.org_1   | 2023-01-06 03:50:18,620 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm1.org_1   | 2023-01-06 03:50:18,620 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm1.org_1   | 2023-01-06 03:50:19,186 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
scm1.org_1   | 2023-01-06 03:50:19,331 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
scm1.org_1   | 2023-01-06 03:50:19,336 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
scm1.org_1   | 2023-01-06 03:50:19,337 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
scm1.org_1   | 2023-01-06 03:50:19,338 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
recon_1      | 2023-01-06 03:51:53,340 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 63 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:53,343 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 64 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:53,353 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 65 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:51:55,357 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 66 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:55,358 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 67 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:55,359 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 68 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:51:57,360 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 69 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:57,364 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 70 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:57,366 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 71 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:51:59,374 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 72 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:59,376 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 73 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:51:59,377 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 74 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:01,379 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 75 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:01,383 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 76 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:01,387 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 77 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:03,389 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 78 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:03,390 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 79 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:03,391 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 80 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:05,393 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 81 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:05,394 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 82 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:05,395 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 83 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:07,396 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 84 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:07,397 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 85 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:07,398 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 86 failover attempts. Trying to failover after sleeping for 2000ms.
scm3.org_1   | 2023-01-06 03:51:18,388 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServer$Division: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3: changes role from      null to FOLLOWER at term 0 for startInitializing
scm3.org_1   | 2023-01-06 03:51:18,393 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D75CE56C2BC3,id=962229bc-cc46-47e6-82cc-1ac6b63780ff
scm3.org_1   | 2023-01-06 03:51:18,400 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3.org_1   | 2023-01-06 03:51:18,401 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm3.org_1   | 2023-01-06 03:51:18,404 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm3.org_1   | 2023-01-06 03:51:18,407 [962229bc-cc46-47e6-82cc-1ac6b63780ff-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm3.org_1   | 2023-01-06 03:51:18,422 [Listener at 0.0.0.0/9860] INFO server.RaftServer: 962229bc-cc46-47e6-82cc-1ac6b63780ff: start RPC server
scm3.org_1   | 2023-01-06 03:51:18,444 [Listener at 0.0.0.0/9860] INFO server.GrpcService: 962229bc-cc46-47e6-82cc-1ac6b63780ff: GrpcService started, listening on 9894
scm3.org_1   | 2023-01-06 03:51:18,462 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-962229bc-cc46-47e6-82cc-1ac6b63780ff: Started
scm3.org_1   | 2023-01-06 03:51:18,501 [Listener at 0.0.0.0/9860] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863]
scm3.org_1   | 2023-01-06 03:51:25,410 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3: receive installSnapshot: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674->962229bc-cc46-47e6-82cc-1ac6b63780ff#0-t2,notify:(t:1, i:0)
scm3.org_1   | 2023-01-06 03:51:25,432 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm3.org_1   | 2023-01-06 03:51:25,432 [grpc-default-executor-0] INFO server.RaftServer$Division: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3: change Leader from null to e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674 at term 2 for installSnapshot, leader elected after 12888ms
scm3.org_1   | 2023-01-06 03:51:25,475 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3: Received notification to install snapshot at index 0
scm3.org_1   | 2023-01-06 03:51:25,486 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3: InstallSnapshot notification result: ALREADY_INSTALLED, current snapshot index: -1
scm3.org_1   | 2023-01-06 03:51:26,994 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3: set new configuration index: 9
scm3.org_1   | configurationEntry {
scm3.org_1   |   peers {
scm3.org_1   |     id: "e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674"
scm3.org_1   |     address: "scm1.org:9894"
scm3.org_1   |     startupRole: FOLLOWER
scm3.org_1   |   }
scm3.org_1   |   peers {
scm3.org_1   |     id: "aea2b8cb-487c-4369-b11b-87f4d7694103"
scm3.org_1   |     address: "scm2.org:9894"
scm3.org_1   |     startupRole: FOLLOWER
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   |  from snapshot
scm3.org_1   | 2023-01-06 03:51:27,012 [grpc-default-executor-0] INFO server.RaftServer$Division: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3: set configuration 9: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-01-06 03:51:27,092 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3: reply installSnapshot: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674<-962229bc-cc46-47e6-82cc-1ac6b63780ff#0:OK-t0,ALREADY_INSTALLED
scm3.org_1   | 2023-01-06 03:51:27,226 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 962229bc-cc46-47e6-82cc-1ac6b63780ff: Completed INSTALL_SNAPSHOT, lastRequest: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674->962229bc-cc46-47e6-82cc-1ac6b63780ff#0-t2,notify:(t:1, i:0)
scm3.org_1   | 2023-01-06 03:51:27,546 [962229bc-cc46-47e6-82cc-1ac6b63780ff-server-thread1] INFO impl.RoleInfo: 962229bc-cc46-47e6-82cc-1ac6b63780ff: start 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-FollowerState
scm3.org_1   | 2023-01-06 03:51:27,568 [962229bc-cc46-47e6-82cc-1ac6b63780ff-server-thread1] INFO server.RaftServer$Division: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm3.org_1   | 2023-01-06 03:51:27,584 [962229bc-cc46-47e6-82cc-1ac6b63780ff-server-thread1] INFO server.RaftServer$Division: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3: inconsistency entries. Reply:e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674<-962229bc-cc46-47e6-82cc-1ac6b63780ff#1:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm3.org_1   | 2023-01-06 03:51:27,590 [962229bc-cc46-47e6-82cc-1ac6b63780ff-server-thread2] INFO server.RaftServer$Division: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm3.org_1   | 2023-01-06 03:51:27,590 [962229bc-cc46-47e6-82cc-1ac6b63780ff-server-thread2] INFO server.RaftServer$Division: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3: inconsistency entries. Reply:e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674<-962229bc-cc46-47e6-82cc-1ac6b63780ff#0:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm3.org_1   | 2023-01-06 03:51:27,726 [962229bc-cc46-47e6-82cc-1ac6b63780ff-server-thread1] INFO server.RaftServer$Division: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3: set configuration 0: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-01-06 03:51:27,728 [962229bc-cc46-47e6-82cc-1ac6b63780ff-server-thread1] INFO server.RaftServer$Division: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3: set configuration 1: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-01-06 03:51:27,734 [962229bc-cc46-47e6-82cc-1ac6b63780ff-server-thread1] INFO server.RaftServer$Division: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3: set configuration 7: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-01-06 03:51:27,740 [962229bc-cc46-47e6-82cc-1ac6b63780ff-server-thread1] INFO server.RaftServer$Division: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3: set configuration 9: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-01-06 03:51:27,814 [962229bc-cc46-47e6-82cc-1ac6b63780ff-server-thread1] INFO segmented.SegmentedRaftLogWorker: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-SegmentedRaftLogWorker: Starting segment from index:0
scm2.org_1   | 2023-01-06 03:50:50,411 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-01-06 03:50:50,418 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm2.org_1   | 2023-01-06 03:50:50,589 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2.org_1   | 2023-01-06 03:50:50,645 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-01-06 03:50:50,646 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm2.org_1   | 2023-01-06 03:50:50,921 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm2.org_1   | 2023-01-06 03:50:50,925 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm2.org_1   | Container Balancer status:
scm2.org_1   | Key                            Value
scm2.org_1   | Running                        true
scm2.org_1   | Container Balancer Configuration values:
scm2.org_1   | Key                                                Value
scm2.org_1   | Threshold                                          10
scm2.org_1   | Max Datanodes to Involve per Iteration(percent)    20
scm2.org_1   | Max Size to Move per Iteration                     500GB
scm2.org_1   | Max Size Entering Target per Iteration             26GB
scm2.org_1   | Max Size Leaving Source per Iteration              26GB
scm2.org_1   | 
scm2.org_1   | 2023-01-06 03:50:50,926 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm2.org_1   | 2023-01-06 03:50:50,938 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm2.org_1   | 2023-01-06 03:50:50,948 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm2.org_1   | 2023-01-06 03:50:50,951 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3 does not exist. Creating ...
scm2.org_1   | 2023-01-06 03:50:50,963 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3/in_use.lock acquired by nodename 6@scm2.org
scm2.org_1   | 2023-01-06 03:50:50,979 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3 has been successfully formatted.
scm2.org_1   | 2023-01-06 03:50:50,988 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm2.org_1   | 2023-01-06 03:50:51,027 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm2.org_1   | 2023-01-06 03:50:51,029 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2023-01-06 03:50:51,036 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm2.org_1   | 2023-01-06 03:50:51,041 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm2.org_1   | 2023-01-06 03:50:51,055 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2.org_1   | 2023-01-06 03:50:51,121 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm2.org_1   | 2023-01-06 03:50:51,121 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm2.org_1   | 2023-01-06 03:50:51,166 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3
scm2.org_1   | 2023-01-06 03:50:51,166 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm2.org_1   | 2023-01-06 03:50:51,166 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm2.org_1   | 2023-01-06 03:50:51,171 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2.org_1   | 2023-01-06 03:50:51,191 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm2.org_1   | 2023-01-06 03:50:51,192 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm2.org_1   | 2023-01-06 03:50:51,195 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm2.org_1   | 2023-01-06 03:50:51,195 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm2.org_1   | 2023-01-06 03:50:51,202 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm2.org_1   | 2023-01-06 03:50:51,226 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm2.org_1   | 2023-01-06 03:50:51,226 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2023-01-06 03:50:51,552 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm2.org_1   | 2023-01-06 03:50:51,553 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm2.org_1   | 2023-01-06 03:50:51,557 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm2.org_1   | 2023-01-06 03:50:51,581 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO segmented.SegmentedRaftLogWorker: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm2.org_1   | 2023-01-06 03:50:51,581 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO segmented.SegmentedRaftLogWorker: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm2.org_1   | 2023-01-06 03:50:51,590 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServer$Division: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3: start with initializing state, conf=-1: peers:[]|listeners:[], old=null
scm2.org_1   | 2023-01-06 03:50:51,591 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServer$Division: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3: changes role from      null to FOLLOWER at term 0 for startInitializing
scm2.org_1   | 2023-01-06 03:50:51,594 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D75CE56C2BC3,id=aea2b8cb-487c-4369-b11b-87f4d7694103
scm2.org_1   | 2023-01-06 03:50:51,597 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
recon_1      | 2023-01-06 03:52:09,400 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 87 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:09,401 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 88 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:09,404 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 89 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:11,406 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 90 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:11,408 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 91 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:11,422 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 92 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:13,426 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 93 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:13,427 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 94 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:13,428 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 95 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:15,430 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 96 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:15,431 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 97 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:15,431 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 98 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:17,433 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 99 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:17,434 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 100 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:17,436 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 101 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:19,440 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 102 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:19,441 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 103 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:19,442 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 104 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:21,443 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 105 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:21,445 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 106 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:21,446 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 107 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:23,447 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 108 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:23,448 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 109 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:23,449 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 110 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:25,451 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 111 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:25,452 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 112 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:25,453 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 113 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:27,454 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 114 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:27,455 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 115 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:27,456 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 116 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:29,458 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 117 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:29,459 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 118 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:29,460 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 119 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:31,461 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 120 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:31,462 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 121 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:31,463 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 122 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:33,467 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 123 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:33,482 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 124 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:33,490 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 125 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:35,492 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 126 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:35,494 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 127 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:35,497 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 128 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:37,503 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 129 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:37,504 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 130 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:37,505 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 131 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:38,276 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:56344
recon_1      | 2023-01-06 03:52:38,342 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-01-06 03:52:38,610 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:32992
recon_1      | 2023-01-06 03:52:38,723 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-01-06 03:52:39,142 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:54536
recon_1      | 2023-01-06 03:52:39,251 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-01-06 03:52:39,514 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 132 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:39,515 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 133 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:39,533 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 134 failover attempts. Trying to failover after sleeping for 2000ms.
scm1.org_1   | 2023-01-06 03:50:19,341 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
scm1.org_1   | 2023-01-06 03:50:19,341 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
scm1.org_1   | 2023-01-06 03:50:19,347 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
scm1.org_1   | 2023-01-06 03:50:19,348 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
scm1.org_1   | 2023-01-06 03:50:19,382 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
scm1.org_1   | 2023-01-06 03:50:19,383 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
scm1.org_1   | 2023-01-06 03:50:19,427 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm1.org_1   | 2023-01-06 03:50:19,428 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm1.org_1   | 2023-01-06 03:50:19,428 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2023-01-06 03:50:19,429 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2023-01-06 03:50:19,433 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2023-01-06 03:50:19,451 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServer: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: found a subdirectory /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3
scm1.org_1   | 2023-01-06 03:50:19,461 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x811817ec] REGISTERED
scm1.org_1   | 2023-01-06 03:50:19,462 [main] INFO server.RaftServer: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: addNew group-D75CE56C2BC3:[] returns group-D75CE56C2BC3:java.util.concurrent.CompletableFuture@514377fc[Not completed]
scm1.org_1   | 2023-01-06 03:50:19,462 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x811817ec] BIND: 0.0.0.0/0.0.0.0:0
scm1.org_1   | 2023-01-06 03:50:19,474 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x811817ec, L:/0.0.0.0:40809] ACTIVE
scm1.org_1   | 2023-01-06 03:50:19,520 [pool-17-thread-1] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: new RaftServerImpl for group-D75CE56C2BC3:[] with SCMStateMachine:uninitialized
scm1.org_1   | 2023-01-06 03:50:19,524 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm1.org_1   | 2023-01-06 03:50:19,525 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1.org_1   | 2023-01-06 03:50:19,526 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1.org_1   | 2023-01-06 03:50:19,526 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2023-01-06 03:50:19,527 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2023-01-06 03:50:19,527 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1.org_1   | 2023-01-06 03:50:19,535 [pool-17-thread-1] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm1.org_1   | 2023-01-06 03:50:19,535 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2023-01-06 03:50:19,540 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1.org_1   | 2023-01-06 03:50:19,541 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm3.org_1   | 2023-01-06 03:51:28,080 [962229bc-cc46-47e6-82cc-1ac6b63780ff-server-thread1] INFO segmented.SegmentedRaftLogWorker: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm3.org_1   | 2023-01-06 03:51:28,163 [962229bc-cc46-47e6-82cc-1ac6b63780ff-server-thread2] INFO server.RaftServer$Division: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3: set configuration 0: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-01-06 03:51:28,165 [962229bc-cc46-47e6-82cc-1ac6b63780ff-server-thread2] INFO server.RaftServer$Division: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3: set configuration 1: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-01-06 03:51:28,165 [962229bc-cc46-47e6-82cc-1ac6b63780ff-server-thread2] INFO server.RaftServer$Division: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3: set configuration 7: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-01-06 03:51:28,165 [962229bc-cc46-47e6-82cc-1ac6b63780ff-server-thread2] INFO server.RaftServer$Division: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3: set configuration 9: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-01-06 03:51:28,682 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-962229bc-cc46-47e6-82cc-1ac6b63780ff: Detected pause in JVM or host machine (eg GC): pause of approximately 106900589ns. No GCs detected.
scm3.org_1   | 2023-01-06 03:51:29,739 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3/current/log_inprogress_0
scm3.org_1   | 2023-01-06 03:51:29,841 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3/current/log_inprogress_0 to /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3/current/log_0-0
scm3.org_1   | 2023-01-06 03:51:30,173 [962229bc-cc46-47e6-82cc-1ac6b63780ff-server-thread1] INFO server.RaftServer$Division: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3: set configuration 13: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 962229bc-cc46-47e6-82cc-1ac6b63780ff|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-01-06 03:51:30,269 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3/current/log_inprogress_1
scm3.org_1   | 2023-01-06 03:51:30,312 [962229bc-cc46-47e6-82cc-1ac6b63780ff-server-thread2] INFO server.RaftServer$Division: 962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3: set configuration 15: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 962229bc-cc46-47e6-82cc-1ac6b63780ff|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-01-06 03:51:30,449 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-06 03:51:30,477 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm3.org_1   | 2023-01-06 03:51:30,482 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3.org_1   | 2023-01-06 03:51:30,483 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm3.org_1   | 2023-01-06 03:51:30,563 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2023-01-06 03:51:30,565 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm3.org_1   | 2023-01-06 03:51:31,488 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl: Successfully added SCM scm3 to group group-D75CE56C2BC3:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 962229bc-cc46-47e6-82cc-1ac6b63780ff|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm3.org_1   | 2023-01-06 03:51:31,489 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm3.org_1   | 2023-01-06 03:51:31,604 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: Starting token manager
scm3.org_1   | 2023-01-06 03:51:31,613 [Listener at 0.0.0.0/9860] INFO token.ContainerTokenSecretManager: Updating the current master key for generating tokens
scm3.org_1   | 2023-01-06 03:51:32,442 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-06 03:51:32,454 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm3.org_1   | 2023-01-06 03:51:32,456 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm3.org_1   | 2023-01-06 03:51:32,666 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-06 03:51:32,736 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-06 03:51:33,785 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm3.org_1   | 2023-01-06 03:51:34,143 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm3.org_1   | 2023-01-06 03:51:34,175 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm3.org_1   | 2023-01-06 03:51:38,370 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm3.org_1   | 2023-01-06 03:51:38,394 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2023-01-06 03:51:38,395 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm3.org_1   | 2023-01-06 03:51:38,964 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-962229bc-cc46-47e6-82cc-1ac6b63780ff: Detected pause in JVM or host machine (eg GC): pause of approximately 280627282ns.
scm3.org_1   | GC pool 'ParNew' had collection(s): count=1 time=203ms
scm3.org_1   | 2023-01-06 03:51:39,061 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm3.org_1   | 2023-01-06 03:51:39,076 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm3.org_1   | 2023-01-06 03:51:39,087 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2023-01-06 03:51:39,088 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm3.org_1   | 2023-01-06 03:51:39,556 [Listener at 0.0.0.0/9860] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm3.org_1   | 2023-01-06 03:51:39,579 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2023-01-06 03:51:39,581 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm3.org_1   | 2023-01-06 03:51:39,590 [Listener at 0.0.0.0/9860] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
scm3.org_1   | 2023-01-06 03:51:40,573 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.client.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.client.port appended with serviceId and nodeId
scm3.org_1   | 2023-01-06 03:51:40,579 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.block.client.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.block.client.port appended with serviceId and nodeId
scm3.org_1   | 2023-01-06 03:51:40,581 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.datanode.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.datanode.port appended with serviceId and nodeId
scm3.org_1   | 2023-01-06 03:51:42,669 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: Persist certificate serialId 1 on Scm Bootstrap Node 962229bc-cc46-47e6-82cc-1ac6b63780ff
scm3.org_1   | 2023-01-06 03:51:42,765 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: Persist certificate serialId 1338275743915 on Scm Bootstrap Node 962229bc-cc46-47e6-82cc-1ac6b63780ff
scm3.org_1   | 2023-01-06 03:51:42,899 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@414fd9f2] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm3.org_1   | 2023-01-06 03:51:43,294 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm3.org_1   | 2023-01-06 03:51:43,297 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
scm3.org_1   | 2023-01-06 03:51:43,325 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
scm3.org_1   | 2023-01-06 03:51:43,721 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @38659ms to org.eclipse.jetty.util.log.Slf4jLog
scm3.org_1   | 2023-01-06 03:51:45,679 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-962229bc-cc46-47e6-82cc-1ac6b63780ff: Detected pause in JVM or host machine (eg GC): pause of approximately 176506390ns.
scm3.org_1   | GC pool 'ParNew' had collection(s): count=1 time=533ms
scm3.org_1   | 2023-01-06 03:51:45,928 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm3.org_1   | 2023-01-06 03:51:45,968 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm3.org_1   | 2023-01-06 03:51:45,978 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
scm3.org_1   | 2023-01-06 03:51:45,980 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm3.org_1   | 2023-01-06 03:51:45,982 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
scm3.org_1   | 2023-01-06 03:51:45,995 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
scm3.org_1   | 2023-01-06 03:51:46,398 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm3.org_1   | 2023-01-06 03:51:46,422 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm3.org_1   | 2023-01-06 03:51:47,079 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm3.org_1   | 2023-01-06 03:51:47,081 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm3.org_1   | 2023-01-06 03:51:47,096 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
scm3.org_1   | 2023-01-06 03:51:47,386 [Listener at 0.0.0.0/9860] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm3.org_1   | 2023-01-06 03:51:47,427 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@31f151ff{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm3.org_1   | 2023-01-06 03:51:47,440 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6c861b25{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm3.org_1   | 2023-01-06 03:51:49,093 [Listener at 0.0.0.0/9860] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm3.org_1   | 2023-01-06 03:51:49,327 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@42ec6e3{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-7436365157974748052/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm3.org_1   | 2023-01-06 03:51:49,466 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@59d64b60{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm3.org_1   | 2023-01-06 03:51:49,471 [Listener at 0.0.0.0/9860] INFO server.Server: Started @44409ms
scm3.org_1   | 2023-01-06 03:51:49,504 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm3.org_1   | 2023-01-06 03:51:49,513 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm3.org_1   | 2023-01-06 03:51:49,529 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm3.org_1   | 2023-01-06 03:52:01,074 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
recon_1      | 2023-01-06 03:52:41,542 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 135 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:41,552 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 136 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:41,555 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 137 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:42,701 [IPC Server handler 18 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86
recon_1      | 2023-01-06 03:52:42,733 [IPC Server handler 66 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/678ebbfa-a167-4ea6-8846-1b1f5c050dad
recon_1      | 2023-01-06 03:52:42,782 [IPC Server handler 18 on default port 9891] INFO node.SCMNodeManager: Registered Data node : d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1454579200080, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-01-06 03:52:42,782 [IPC Server handler 66 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1453571467283, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-01-06 03:52:43,157 [IPC Server handler 17 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f7a31870-78ff-406d-a144-7fe17a627699
recon_1      | 2023-01-06 03:52:43,158 [IPC Server handler 17 on default port 9891] INFO node.SCMNodeManager: Registered Data node : f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1452870296728, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-01-06 03:52:43,326 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86 to Node DB.
recon_1      | 2023-01-06 03:52:43,366 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 678ebbfa-a167-4ea6-8846-1b1f5c050dad to Node DB.
recon_1      | 2023-01-06 03:52:43,380 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node f7a31870-78ff-406d-a144-7fe17a627699 to Node DB.
recon_1      | 2023-01-06 03:52:43,619 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 138 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:43,666 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 139 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:43,753 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 140 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:44,174 [IPC Server handler 17 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net
recon_1      | 2023-01-06 03:52:44,176 [IPC Server handler 21 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net
recon_1      | 2023-01-06 03:52:44,632 [IPC Server handler 19 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net
recon_1      | 2023-01-06 03:52:45,763 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 141 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:45,772 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 142 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:45,773 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 143 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:47,781 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 144 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:47,782 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 145 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:47,783 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 146 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:49,785 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 147 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:49,786 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 148 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:49,786 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 149 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:51,788 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 150 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:51,790 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 151 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:51,791 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 152 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:53,792 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 153 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:53,793 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 154 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:53,801 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 155 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:55,803 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 156 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:55,804 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 157 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:55,805 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 158 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:52:57,806 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 159 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:57,807 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 160 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:57,808 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 161 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2023-01-06 03:50:51,598 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm2.org_1   | 2023-01-06 03:50:51,599 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm2.org_1   | 2023-01-06 03:50:51,601 [aea2b8cb-487c-4369-b11b-87f4d7694103-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm2.org_1   | 2023-01-06 03:50:51,614 [Listener at 0.0.0.0/9860] INFO server.RaftServer: aea2b8cb-487c-4369-b11b-87f4d7694103: start RPC server
scm2.org_1   | 2023-01-06 03:50:51,642 [Listener at 0.0.0.0/9860] INFO server.GrpcService: aea2b8cb-487c-4369-b11b-87f4d7694103: GrpcService started, listening on 9894
scm2.org_1   | 2023-01-06 03:50:51,660 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-aea2b8cb-487c-4369-b11b-87f4d7694103: Started
scm2.org_1   | 2023-01-06 03:50:51,706 [Listener at 0.0.0.0/9860] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
scm2.org_1   | 2023-01-06 03:50:54,057 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3: receive installSnapshot: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674->aea2b8cb-487c-4369-b11b-87f4d7694103#0-t2,notify:(t:1, i:0)
scm2.org_1   | 2023-01-06 03:50:54,069 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm2.org_1   | 2023-01-06 03:50:54,069 [grpc-default-executor-0] INFO server.RaftServer$Division: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3: change Leader from null to e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674 at term 2 for installSnapshot, leader elected after 9675ms
scm2.org_1   | 2023-01-06 03:50:54,079 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3: Received notification to install snapshot at index 0
scm2.org_1   | 2023-01-06 03:50:54,080 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3: InstallSnapshot notification result: ALREADY_INSTALLED, current snapshot index: -1
scm2.org_1   | 2023-01-06 03:50:54,515 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3: set new configuration index: 1
scm2.org_1   | configurationEntry {
scm2.org_1   |   peers {
scm2.org_1   |     id: "e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674"
scm2.org_1   |     address: "scm1.org:9894"
scm2.org_1   |     startupRole: FOLLOWER
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   |  from snapshot
scm2.org_1   | 2023-01-06 03:50:54,528 [grpc-default-executor-0] INFO server.RaftServer$Division: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3: set configuration 1: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-01-06 03:50:54,548 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3: reply installSnapshot: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674<-aea2b8cb-487c-4369-b11b-87f4d7694103#0:OK-t0,ALREADY_INSTALLED
scm2.org_1   | 2023-01-06 03:50:54,606 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: aea2b8cb-487c-4369-b11b-87f4d7694103: Completed INSTALL_SNAPSHOT, lastRequest: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674->aea2b8cb-487c-4369-b11b-87f4d7694103#0-t2,notify:(t:1, i:0)
scm2.org_1   | 2023-01-06 03:50:54,816 [aea2b8cb-487c-4369-b11b-87f4d7694103-server-thread1] INFO impl.RoleInfo: aea2b8cb-487c-4369-b11b-87f4d7694103: start aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-FollowerState
scm2.org_1   | 2023-01-06 03:50:54,858 [aea2b8cb-487c-4369-b11b-87f4d7694103-server-thread1] INFO server.RaftServer$Division: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm2.org_1   | 2023-01-06 03:50:54,860 [aea2b8cb-487c-4369-b11b-87f4d7694103-server-thread1] INFO server.RaftServer$Division: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3: inconsistency entries. Reply:e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674<-aea2b8cb-487c-4369-b11b-87f4d7694103#0:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm2.org_1   | 2023-01-06 03:50:54,872 [aea2b8cb-487c-4369-b11b-87f4d7694103-server-thread1] INFO server.RaftServer$Division: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm2.org_1   | 2023-01-06 03:50:54,873 [aea2b8cb-487c-4369-b11b-87f4d7694103-server-thread1] INFO server.RaftServer$Division: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3: inconsistency entries. Reply:e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674<-aea2b8cb-487c-4369-b11b-87f4d7694103#1:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm2.org_1   | 2023-01-06 03:50:54,961 [aea2b8cb-487c-4369-b11b-87f4d7694103-server-thread1] INFO server.RaftServer$Division: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3: set configuration 0: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-01-06 03:50:54,961 [aea2b8cb-487c-4369-b11b-87f4d7694103-server-thread1] INFO server.RaftServer$Division: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3: set configuration 1: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-01-06 03:50:54,975 [aea2b8cb-487c-4369-b11b-87f4d7694103-server-thread1] INFO segmented.SegmentedRaftLogWorker: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-SegmentedRaftLogWorker: Starting segment from index:0
scm2.org_1   | 2023-01-06 03:50:55,068 [aea2b8cb-487c-4369-b11b-87f4d7694103-server-thread1] INFO segmented.SegmentedRaftLogWorker: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm2.org_1   | 2023-01-06 03:50:55,398 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3/current/log_inprogress_0
scm2.org_1   | 2023-01-06 03:50:55,411 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3/current/log_inprogress_0 to /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3/current/log_0-0
scm2.org_1   | 2023-01-06 03:50:55,445 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3/current/log_inprogress_1
scm2.org_1   | 2023-01-06 03:50:55,485 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-06 03:50:55,488 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm2.org_1   | 2023-01-06 03:50:55,488 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm2.org_1   | 2023-01-06 03:50:55,489 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm2.org_1   | 2023-01-06 03:50:55,511 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2023-01-06 03:50:55,522 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm2.org_1   | 2023-01-06 03:50:55,629 [aea2b8cb-487c-4369-b11b-87f4d7694103-server-thread1] INFO server.RaftServer$Division: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3: set configuration 7: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm2.org_1   | 2023-01-06 03:50:55,667 [aea2b8cb-487c-4369-b11b-87f4d7694103-server-thread1] INFO server.RaftServer$Division: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3: set configuration 9: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-01-06 03:50:55,946 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl: Successfully added SCM scm2 to group group-D75CE56C2BC3:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm2.org_1   | 2023-01-06 03:50:55,959 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm2.org_1   | 2023-01-06 03:50:55,962 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: Starting token manager
scm2.org_1   | 2023-01-06 03:50:55,964 [Listener at 0.0.0.0/9860] INFO token.ContainerTokenSecretManager: Updating the current master key for generating tokens
scm2.org_1   | 2023-01-06 03:50:56,009 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-06 03:50:56,014 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm2.org_1   | 2023-01-06 03:50:56,014 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm2.org_1   | 2023-01-06 03:50:56,105 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-06 03:50:56,344 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm2.org_1   | 2023-01-06 03:50:56,387 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm2.org_1   | 2023-01-06 03:50:56,387 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm2.org_1   | 2023-01-06 03:50:57,386 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm2.org_1   | 2023-01-06 03:50:57,429 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2023-01-06 03:50:57,430 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm2.org_1   | 2023-01-06 03:50:57,604 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm2.org_1   | 2023-01-06 03:50:57,612 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm2.org_1   | 2023-01-06 03:50:57,615 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2023-01-06 03:50:57,621 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm2.org_1   | 2023-01-06 03:50:58,050 [Listener at 0.0.0.0/9860] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm2.org_1   | 2023-01-06 03:50:58,053 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2023-01-06 03:50:58,065 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm2.org_1   | 2023-01-06 03:50:58,066 [Listener at 0.0.0.0/9860] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
scm2.org_1   | 2023-01-06 03:50:58,256 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.client.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.client.port appended with serviceId and nodeId
scm2.org_1   | 2023-01-06 03:50:58,261 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.block.client.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.block.client.port appended with serviceId and nodeId
scm2.org_1   | 2023-01-06 03:50:58,261 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.datanode.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.datanode.port appended with serviceId and nodeId
scm2.org_1   | 2023-01-06 03:50:58,778 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: Persist certificate serialId 1 on Scm Bootstrap Node aea2b8cb-487c-4369-b11b-87f4d7694103
scm2.org_1   | 2023-01-06 03:50:58,817 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: Persist certificate serialId 1338275743915 on Scm Bootstrap Node aea2b8cb-487c-4369-b11b-87f4d7694103
scm1.org_1   | 2023-01-06 03:50:19,550 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1.org_1   | 2023-01-06 03:50:19,557 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1.org_1   | 2023-01-06 03:50:19,557 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1.org_1   | 2023-01-06 03:50:19,601 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2023-01-06 03:50:19,602 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1.org_1   | 2023-01-06 03:50:19,604 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1.org_1   | 2023-01-06 03:50:19,604 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1.org_1   | 2023-01-06 03:50:19,605 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm1.org_1   | 2023-01-06 03:50:19,607 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm1.org_1   | 2023-01-06 03:50:19,608 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm1.org_1   | 2023-01-06 03:50:19,608 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm1.org_1   | 2023-01-06 03:50:19,782 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
scm1.org_1   | 2023-01-06 03:50:19,986 [main] INFO reflections.Reflections: Reflections took 137 ms to scan 3 urls, producing 121 keys and 272 values 
scm1.org_1   | 2023-01-06 03:50:20,100 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm1.org_1   | 2023-01-06 03:50:20,101 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm1.org_1   | 2023-01-06 03:50:20,110 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm1.org_1   | 2023-01-06 03:50:20,133 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm1.org_1   | 2023-01-06 03:50:20,207 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm1.org_1   | 2023-01-06 03:50:20,233 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm1.org_1   | 2023-01-06 03:50:20,235 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1.org_1   | 2023-01-06 03:50:20,246 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm1.org_1   | 2023-01-06 03:50:20,334 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm1.org_1   | 2023-01-06 03:50:20,334 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1.org_1   | 2023-01-06 03:50:20,344 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm1.org_1   | 2023-01-06 03:50:20,344 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm1.org_1   | 2023-01-06 03:50:20,347 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm1.org_1   | 2023-01-06 03:50:20,350 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm1.org_1   | 2023-01-06 03:50:20,358 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm1.org_1   | 2023-01-06 03:50:20,359 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm1.org_1   | 2023-01-06 03:50:20,419 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm1.org_1   | 2023-01-06 03:50:20,441 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm1.org_1   | 2023-01-06 03:50:20,556 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm1.org_1   | 2023-01-06 03:50:20,582 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm1.org_1   | 2023-01-06 03:50:20,586 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm1.org_1   | 2023-01-06 03:50:20,603 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm1.org_1   | 2023-01-06 03:50:20,610 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-06 03:50:20,612 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm1.org_1   | 2023-01-06 03:50:20,676 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm1.org_1   | 2023-01-06 03:50:20,683 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm1.org_1   | 2023-01-06 03:50:20,684 [main] INFO server.StorageContainerManager: Storing sub-ca certificate serialId 1338275743915 on primary SCM
scm1.org_1   | 2023-01-06 03:50:20,694 [main] INFO server.StorageContainerManager: Storing root certificate serialId 1
scm1.org_1   | 2023-01-06 03:50:20,734 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2023-01-06 03:50:20,777 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm1.org_1   | 2023-01-06 03:50:21,897 [Listener at 0.0.0.0/9961] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1.org_1   | 2023-01-06 03:50:21,942 [Listener at 0.0.0.0/9961] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2023-01-06 03:50:21,949 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm1.org_1   | 2023-01-06 03:50:22,000 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1.org_1   | 2023-01-06 03:50:22,007 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2023-01-06 03:50:22,008 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm1.org_1   | 2023-01-06 03:50:22,052 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1.org_1   | 2023-01-06 03:50:22,075 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2023-01-06 03:50:22,076 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm1.org_1   | 2023-01-06 03:50:22,281 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm1.org_1   | 2023-01-06 03:50:22,284 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm1.org_1   | Container Balancer status:
scm1.org_1   | Key                            Value
scm1.org_1   | Running                        true
scm1.org_1   | Container Balancer Configuration values:
scm3.org_1   | 2023-01-06 03:52:01,593 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-06 03:52:02,606 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-06 03:52:12,988 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-06 03:52:13,161 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-06 03:52:14,763 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-06 03:52:38,757 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:34018
scm3.org_1   | 2023-01-06 03:52:38,796 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:44870
scm3.org_1   | 2023-01-06 03:52:38,810 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-01-06 03:52:38,876 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-01-06 03:52:39,271 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:48218
scm3.org_1   | 2023-01-06 03:52:39,281 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-01-06 03:52:43,245 [IPC Server handler 2 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f7a31870-78ff-406d-a144-7fe17a627699
scm3.org_1   | 2023-01-06 03:52:43,321 [IPC Server handler 2 on default port 9861] INFO node.SCMNodeManager: Registered Data node : f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1452870296728, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2023-01-06 03:52:43,421 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2023-01-06 03:52:43,450 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm3.org_1   | 2023-01-06 03:52:43,933 [IPC Server handler 2 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86
scm3.org_1   | 2023-01-06 03:52:43,934 [IPC Server handler 2 on default port 9861] INFO node.SCMNodeManager: Registered Data node : d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1454579200080, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2023-01-06 03:52:43,934 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2023-01-06 03:52:43,967 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm3.org_1   | 2023-01-06 03:52:43,994 [IPC Server handler 8 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/678ebbfa-a167-4ea6-8846-1b1f5c050dad
scm3.org_1   | 2023-01-06 03:52:43,998 [IPC Server handler 8 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1453571467283, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2023-01-06 03:52:44,003 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2023-01-06 03:52:44,085 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm3.org_1   | 2023-01-06 03:52:44,085 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm3.org_1   | 2023-01-06 03:52:44,085 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm3.org_1   | 2023-01-06 03:52:44,085 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm3.org_1   | 2023-01-06 03:52:44,130 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2023-01-06 03:52:45,258 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-962229bc-cc46-47e6-82cc-1ac6b63780ff: Detected pause in JVM or host machine (eg GC): pause of approximately 310234879ns.
scm3.org_1   | GC pool 'ParNew' had collection(s): count=1 time=192ms
scm3.org_1   | 2023-01-06 03:52:45,278 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm3.org_1   | 2023-01-06 03:52:45,642 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 5af4a3fc-47d6-497e-93af-69810f3160d1, Nodes: f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-06T03:52:43.589Z[UTC]].
scm3.org_1   | 2023-01-06 03:52:45,643 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
recon_1      | 2023-01-06 03:52:59,810 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 162 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:59,811 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 163 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:52:59,812 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 164 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:53:01,814 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 165 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:53:01,815 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 166 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:53:01,816 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 167 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:53:03,817 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 168 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:53:03,819 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 169 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:53:03,820 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 170 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:53:05,822 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 171 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:53:05,823 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 172 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:53:05,825 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 173 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:53:07,826 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 174 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:53:07,829 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 175 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:53:07,830 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 176 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:53:14,692 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:47330
scm2.org_1   | 2023-01-06 03:50:58,879 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7aafaf44] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm2.org_1   | 2023-01-06 03:50:58,941 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm2.org_1   | 2023-01-06 03:50:58,941 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
scm2.org_1   | 2023-01-06 03:50:58,947 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
scm2.org_1   | 2023-01-06 03:50:59,110 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @26722ms to org.eclipse.jetty.util.log.Slf4jLog
scm2.org_1   | 2023-01-06 03:50:59,722 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm2.org_1   | 2023-01-06 03:50:59,753 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm2.org_1   | 2023-01-06 03:50:59,759 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
scm2.org_1   | 2023-01-06 03:50:59,762 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm2.org_1   | 2023-01-06 03:50:59,762 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
scm2.org_1   | 2023-01-06 03:50:59,774 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
scm1.org_1   | Key                                                Value
scm1.org_1   | Threshold                                          10
scm1.org_1   | Max Datanodes to Involve per Iteration(percent)    20
scm1.org_1   | Max Size to Move per Iteration                     500GB
scm1.org_1   | Max Size Entering Target per Iteration             26GB
scm1.org_1   | Max Size Leaving Source per Iteration              26GB
scm1.org_1   | 
scm1.org_1   | 2023-01-06 03:50:22,285 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm1.org_1   | 2023-01-06 03:50:22,290 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm1.org_1   | 2023-01-06 03:50:22,296 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm1.org_1   | 2023-01-06 03:50:22,302 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3/in_use.lock acquired by nodename 7@scm1.org
scm1.org_1   | 2023-01-06 03:50:22,317 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674} from /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3/current/raft-meta
scm1.org_1   | 2023-01-06 03:50:22,360 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3: set configuration 0: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-01-06 03:50:22,365 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1.org_1   | 2023-01-06 03:50:22,373 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1.org_1   | 2023-01-06 03:50:22,373 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-01-06 03:50:22,375 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1.org_1   | 2023-01-06 03:50:22,376 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1.org_1   | 2023-01-06 03:50:22,378 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2023-01-06 03:50:22,384 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1.org_1   | 2023-01-06 03:50:22,385 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1.org_1   | 2023-01-06 03:50:22,391 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3
scm1.org_1   | 2023-01-06 03:50:22,392 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2023-01-06 03:50:22,393 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1.org_1   | 2023-01-06 03:50:22,394 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2023-01-06 03:50:22,395 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1.org_1   | 2023-01-06 03:50:22,395 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1.org_1   | 2023-01-06 03:50:22,396 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1.org_1   | 2023-01-06 03:50:22,396 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1.org_1   | 2023-01-06 03:50:22,397 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1.org_1   | 2023-01-06 03:50:22,411 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1.org_1   | 2023-01-06 03:50:22,412 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-01-06 03:50:22,425 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1.org_1   | 2023-01-06 03:50:22,426 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1.org_1   | 2023-01-06 03:50:22,427 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1.org_1   | 2023-01-06 03:50:22,457 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3: set configuration 0: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-01-06 03:50:22,463 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3/current/log_inprogress_0
scm1.org_1   | 2023-01-06 03:50:22,470 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 0
scm1.org_1   | 2023-01-06 03:50:22,471 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1.org_1   | 2023-01-06 03:50:22,550 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3: start as a follower, conf=0: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-01-06 03:50:22,551 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm1.org_1   | 2023-01-06 03:50:22,553 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO impl.RoleInfo: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: start e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-FollowerState
scm1.org_1   | 2023-01-06 03:50:22,563 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1.org_1   | 2023-01-06 03:50:22,565 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1.org_1   | 2023-01-06 03:50:22,571 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D75CE56C2BC3,id=e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674
scm1.org_1   | 2023-01-06 03:50:22,574 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1.org_1   | 2023-01-06 03:50:22,575 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1.org_1   | 2023-01-06 03:50:22,575 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1.org_1   | 2023-01-06 03:50:22,576 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1.org_1   | 2023-01-06 03:50:22,593 [Listener at 0.0.0.0/9860] INFO server.RaftServer: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: start RPC server
scm1.org_1   | 2023-01-06 03:50:22,614 [Listener at 0.0.0.0/9860] INFO server.GrpcService: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: GrpcService started, listening on 9894
scm1.org_1   | 2023-01-06 03:50:22,618 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: Started
scm1.org_1   | 2023-01-06 03:50:22,627 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm1.org_1   | 2023-01-06 03:50:22,628 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm1.org_1   | 2023-01-06 03:50:22,643 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: Starting token manager
scm1.org_1   | 2023-01-06 03:50:22,643 [Listener at 0.0.0.0/9860] INFO token.ContainerTokenSecretManager: Updating the current master key for generating tokens
scm1.org_1   | 2023-01-06 03:50:22,735 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm1.org_1   | 2023-01-06 03:50:22,747 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm1.org_1   | 2023-01-06 03:50:22,747 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm1.org_1   | 2023-01-06 03:50:23,084 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm1.org_1   | 2023-01-06 03:50:23,084 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2023-01-06 03:50:23,086 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm1.org_1   | 2023-01-06 03:50:23,152 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm1.org_1   | 2023-01-06 03:50:23,153 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm1.org_1   | 2023-01-06 03:50:23,158 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2023-01-06 03:50:23,176 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm1.org_1   | 2023-01-06 03:50:23,188 [Listener at 0.0.0.0/9860] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm1.org_1   | 2023-01-06 03:50:23,192 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2023-01-06 03:50:23,192 [Listener at 0.0.0.0/9860] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
recon_1      | 2023-01-06 03:53:14,712 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-01-06 03:53:14,715 [IPC Server handler 37 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net
scm2.org_1   | 2023-01-06 03:50:59,937 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm3.org_1   | 2023-01-06 03:52:45,808 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: c742cece-fb27-47b9-b231-5839005b1100, Nodes: f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-06T03:52:45.036Z[UTC]].
scm3.org_1   | 2023-01-06 03:52:45,812 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-06 03:52:45,911 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d88fe7be-cf18-4d52-90b0-fc9dcbe94408, Nodes: 678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-06T03:52:45.363Z[UTC]].
scm1.org_1   | 2023-01-06 03:50:23,192 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm3.org_1   | 2023-01-06 03:52:45,924 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-06 03:52:45,983 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm2.org_1   | 2023-01-06 03:50:59,941 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm2.org_1   | 2023-01-06 03:51:00,059 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm1.org_1   | 2023-01-06 03:50:23,409 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1f315f3d] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm2.org_1   | 2023-01-06 03:51:00,061 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm2.org_1   | 2023-01-06 03:51:00,065 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm2.org_1   | 2023-01-06 03:51:00,141 [Listener at 0.0.0.0/9860] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm2.org_1   | 2023-01-06 03:51:00,161 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5837b801{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm2.org_1   | 2023-01-06 03:51:00,164 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7959747d{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm2.org_1   | 2023-01-06 03:51:00,632 [Listener at 0.0.0.0/9860] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm2.org_1   | 2023-01-06 03:51:00,718 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1c5123ca{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-2800297313866555082/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm2.org_1   | 2023-01-06 03:51:00,794 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@31dd8a0b{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm2.org_1   | 2023-01-06 03:51:00,794 [Listener at 0.0.0.0/9860] INFO server.Server: Started @28408ms
scm2.org_1   | 2023-01-06 03:51:00,806 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm2.org_1   | 2023-01-06 03:51:00,806 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm1.org_1   | 2023-01-06 03:50:23,518 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
recon_1      | 2023-01-06 03:53:16,057 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:57890
scm2.org_1   | 2023-01-06 03:51:00,817 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm2.org_1   | 2023-01-06 03:51:04,104 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
recon_1      | 2023-01-06 03:53:16,063 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:52648
scm2.org_1   | 2023-01-06 03:51:30,142 [aea2b8cb-487c-4369-b11b-87f4d7694103-server-thread2] INFO server.RaftServer$Division: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3: set configuration 13: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 962229bc-cc46-47e6-82cc-1ac6b63780ff|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1.org_1   | 2023-01-06 03:50:23,518 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
scm3.org_1   | 2023-01-06 03:52:46,049 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm3.org_1   | 2023-01-06 03:52:46,061 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 7faae3af-cb79-4c3a-b244-507410c3658e, Nodes: f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-06T03:52:45.566Z[UTC]].
scm3.org_1   | 2023-01-06 03:52:46,070 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-06 03:52:46,073 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 88086403-84d0-4755-ada1-bf9cf73be954, Nodes: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-06T03:52:45.799Z[UTC]].
scm3.org_1   | 2023-01-06 03:52:46,079 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-06 03:51:30,280 [aea2b8cb-487c-4369-b11b-87f4d7694103-server-thread2] INFO server.RaftServer$Division: aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3: set configuration 15: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 962229bc-cc46-47e6-82cc-1ac6b63780ff|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-01-06 03:50:23,521 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
recon_1      | 2023-01-06 03:53:16,073 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-01-06 03:53:16,074 [IPC Server handler 17 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net
scm2.org_1   | 2023-01-06 03:52:01,031 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
recon_1      | 2023-01-06 03:53:16,157 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
scm3.org_1   | 2023-01-06 03:53:14,890 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:33654
scm3.org_1   | 2023-01-06 03:53:14,906 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-01-06 03:53:14,915 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm2.org_1   | 2023-01-06 03:52:01,578 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
recon_1      | 2023-01-06 03:53:16,159 [IPC Server handler 21 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net
recon_1      | 2023-01-06 03:53:17,134 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=5af4a3fc-47d6-497e-93af-69810f3160d1. Trying to get from SCM.
scm2.org_1   | 2023-01-06 03:52:02,566 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-06 03:50:23,602 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:43846
scm3.org_1   | 2023-01-06 03:53:16,179 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:55954
recon_1      | 2023-01-06 03:53:17,803 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 5af4a3fc-47d6-497e-93af-69810f3160d1, Nodes: f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:f7a31870-78ff-406d-a144-7fe17a627699, CreationTimestamp2023-01-06T03:52:43.589Z[UTC]] to Recon pipeline metadata.
scm2.org_1   | 2023-01-06 03:52:12,970 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-06 03:52:13,151 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-06 03:53:16,182 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
recon_1      | 2023-01-06 03:53:18,072 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 5af4a3fc-47d6-497e-93af-69810f3160d1, Nodes: f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:f7a31870-78ff-406d-a144-7fe17a627699, CreationTimestamp2023-01-06T03:52:43.589Z[UTC]].
scm1.org_1   | 2023-01-06 03:50:23,604 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:35767
scm2.org_1   | 2023-01-06 03:52:14,753 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-06 03:53:16,195 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm3.org_1   | 2023-01-06 03:53:16,203 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:36972
scm1.org_1   | 2023-01-06 03:50:23,648 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm2.org_1   | 2023-01-06 03:52:38,883 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:52262
recon_1      | 2023-01-06 03:53:19,036 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=c742cece-fb27-47b9-b231-5839005b1100. Trying to get from SCM.
scm3.org_1   | 2023-01-06 03:53:16,262 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-01-06 03:53:16,271 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 2023-01-06 03:50:23,652 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
recon_1      | 2023-01-06 03:53:19,042 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: c742cece-fb27-47b9-b231-5839005b1100, Nodes: f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-06T03:52:45.036Z[UTC]] to Recon pipeline metadata.
scm2.org_1   | 2023-01-06 03:52:38,916 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-01-06 03:52:39,298 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:40142
scm1.org_1   | 2023-01-06 03:50:23,671 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @8422ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1      | 2023-01-06 03:53:19,051 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: c742cece-fb27-47b9-b231-5839005b1100, Nodes: f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-06T03:52:45.036Z[UTC]].
recon_1      | 2023-01-06 03:53:19,061 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c742cece-fb27-47b9-b231-5839005b1100 reported by 678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1453571467283, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2023-01-06 03:53:17,208 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm2.org_1   | 2023-01-06 03:52:39,313 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-01-06 03:52:39,366 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:53358
scm2.org_1   | 2023-01-06 03:52:39,402 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-01-06 03:53:17,218 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 5af4a3fc-47d6-497e-93af-69810f3160d1, Nodes: f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:f7a31870-78ff-406d-a144-7fe17a627699, CreationTimestamp2023-01-06T03:52:43.589Z[UTC]] moved to OPEN state
scm3.org_1   | 2023-01-06 03:53:17,457 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
recon_1      | 2023-01-06 03:53:19,138 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c742cece-fb27-47b9-b231-5839005b1100 reported by d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1454579200080, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2023-01-06 03:53:19,066 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm3.org_1   | 2023-01-06 03:53:19,159 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm3.org_1   | 2023-01-06 03:53:19,850 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
recon_1      | 2023-01-06 03:53:19,857 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c742cece-fb27-47b9-b231-5839005b1100 reported by f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1452870296728, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-01-06 03:53:23,665 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c742cece-fb27-47b9-b231-5839005b1100 reported by f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1452870296728, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-01-06 03:53:27,170 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om1 is not the leader. Could not determine the leader node.
scm2.org_1   | 2023-01-06 03:52:43,310 [IPC Server handler 1 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f7a31870-78ff-406d-a144-7fe17a627699
scm2.org_1   | 2023-01-06 03:52:43,409 [IPC Server handler 1 on default port 9861] INFO node.SCMNodeManager: Registered Data node : f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1452870296728, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2.org_1   | 2023-01-06 03:52:43,496 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1.org_1   | 2023-01-06 03:50:23,948 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm2.org_1   | 2023-01-06 03:52:43,699 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:248)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:235)
scm2.org_1   | 2023-01-06 03:52:43,986 [IPC Server handler 59 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/678ebbfa-a167-4ea6-8846-1b1f5c050dad
scm3.org_1   | 2023-01-06 03:53:19,856 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-01-06 03:50:23,963 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:228)
scm3.org_1   | 2023-01-06 03:53:23,726 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm3.org_1   | 2023-01-06 03:53:23,727 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-01-06 03:50:23,967 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:175)
scm2.org_1   | 2023-01-06 03:52:43,998 [IPC Server handler 59 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1453571467283, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2.org_1   | 2023-01-06 03:52:44,004 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1.org_1   | 2023-01-06 03:50:23,967 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
recon_1      | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
scm2.org_1   | 2023-01-06 03:52:44,006 [IPC Server handler 81 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86
scm2.org_1   | 2023-01-06 03:52:44,012 [IPC Server handler 81 on default port 9861] INFO node.SCMNodeManager: Registered Data node : d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1454579200080, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2023-01-06 03:50:23,968 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm1.org_1   | 2023-01-06 03:50:23,973 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
scm3.org_1   | 2023-01-06 03:53:31,103 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:34908
scm1.org_1   | 2023-01-06 03:50:24,017 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#9 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from 172.25.0.115:35767
scm3.org_1   | 2023-01-06 03:53:31,306 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm3.org_1   | 2023-01-06 03:53:31,306 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
scm2.org_1   | 2023-01-06 03:52:44,020 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm2.org_1   | 2023-01-06 03:52:44,021 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2023-01-06 03:52:44,026 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm1.org_1   | org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674 is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
scm3.org_1   | 2023-01-06 03:53:31,315 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-01-06 03:53:31,317 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
scm2.org_1   | 2023-01-06 03:52:44,034 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm2.org_1   | 2023-01-06 03:52:44,034 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
scm3.org_1   | 2023-01-06 03:53:31,319 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d88fe7be-cf18-4d52-90b0-fc9dcbe94408, Nodes: 678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:678ebbfa-a167-4ea6-8846-1b1f5c050dad, CreationTimestamp2023-01-06T03:52:45.363Z[UTC]] moved to OPEN state
scm2.org_1   | 2023-01-06 03:52:44,036 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm3.org_1   | 2023-01-06 03:53:31,373 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
scm2.org_1   | 2023-01-06 03:52:44,037 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1.org_1   | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm3.org_1   | 2023-01-06 03:53:31,374 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d88fe7be-cf18-4d52-90b0-fc9dcbe94408, Nodes: 678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:678ebbfa-a167-4ea6-8846-1b1f5c050dad, CreationTimestamp2023-01-06T03:52:45.363Z[UTC]] moved to OPEN state
scm2.org_1   | 2023-01-06 03:52:44,963 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm2.org_1   | 2023-01-06 03:52:45,022 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 5af4a3fc-47d6-497e-93af-69810f3160d1, Nodes: f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-06T03:52:43.589Z[UTC]].
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3.org_1   | 2023-01-06 03:53:31,483 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-06 03:52:45,025 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
scm3.org_1   | 2023-01-06 03:53:31,838 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:46166
scm2.org_1   | 2023-01-06 03:52:45,284 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: c742cece-fb27-47b9-b231-5839005b1100, Nodes: f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-06T03:52:45.036Z[UTC]].
scm2.org_1   | 2023-01-06 03:52:45,299 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-06 03:52:45,568 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d88fe7be-cf18-4d52-90b0-fc9dcbe94408, Nodes: 678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-06T03:52:45.363Z[UTC]].
scm2.org_1   | 2023-01-06 03:52:45,570 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-06 03:53:31,909 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-01-06 03:52:45,716 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 7faae3af-cb79-4c3a-b244-507410c3658e, Nodes: f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-06T03:52:45.566Z[UTC]].
scm2.org_1   | 2023-01-06 03:52:45,716 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:93)
scm1.org_1   | 	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:16080)
scm3.org_1   | 2023-01-06 03:53:31,910 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1      | , while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 177 failover attempts. Trying to failover immediately.
scm3.org_1   | 2023-01-06 03:53:33,100 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
scm2.org_1   | 2023-01-06 03:52:45,922 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 88086403-84d0-4755-ada1-bf9cf73be954, Nodes: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-06T03:52:45.799Z[UTC]].
recon_1      | 2023-01-06 03:53:29,625 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om2 is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:248)
scm3.org_1   | 2023-01-06 03:53:33,100 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 88086403-84d0-4755-ada1-bf9cf73be954, Nodes: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86, CreationTimestamp2023-01-06T03:52:45.799Z[UTC]] moved to OPEN state
scm3.org_1   | 2023-01-06 03:53:33,150 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-06 03:53:36,078 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm3.org_1   | 2023-01-06 03:53:36,079 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-01-06 03:53:38,460 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-01-06 03:52:45,948 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:235)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:228)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:175)
recon_1      | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
scm2.org_1   | 2023-01-06 03:52:45,968 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
recon_1      | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
scm2.org_1   | 2023-01-06 03:52:46,048 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2.org_1   | 2023-01-06 03:53:14,919 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:59964
scm2.org_1   | 2023-01-06 03:53:14,954 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-01-06 03:53:38,460 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm3.org_1   | 2023-01-06 03:53:41,722 [962229bc-cc46-47e6-82cc-1ac6b63780ff@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-06 03:53:41,731 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm2.org_1   | 2023-01-06 03:53:14,966 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm2.org_1   | 2023-01-06 03:53:16,448 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:52082
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
scm1.org_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
scm2.org_1   | 2023-01-06 03:53:16,471 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-01-06 03:53:16,487 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1.org_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2.org_1   | 2023-01-06 03:53:16,511 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:37756
scm2.org_1   | 2023-01-06 03:53:16,527 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1.org_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2.org_1   | 2023-01-06 03:53:16,547 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 2023-01-06 03:50:24,118 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm3.org_1   | 2023-01-06 03:53:41,733 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm3.org_1   | 2023-01-06 03:53:41,733 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm3.org_1   | 2023-01-06 03:53:41,733 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1      | , while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 178 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:53:30,747 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:39658
recon_1      | 2023-01-06 03:53:30,781 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
scm1.org_1   | 2023-01-06 03:50:24,121 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm1.org_1   | 2023-01-06 03:50:24,164 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm1.org_1   | 2023-01-06 03:50:24,164 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm1.org_1   | 2023-01-06 03:50:24,169 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
scm1.org_1   | 2023-01-06 03:50:24,198 [Listener at 0.0.0.0/9860] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm1.org_1   | 2023-01-06 03:50:24,202 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@655e024{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm1.org_1   | 2023-01-06 03:50:24,203 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@12515d64{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm1.org_1   | 2023-01-06 03:50:24,300 [Listener at 0.0.0.0/9860] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm3.org_1   | 2023-01-06 03:53:41,734 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm3.org_1   | 2023-01-06 03:53:41,734 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm2.org_1   | 2023-01-06 03:53:17,176 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
recon_1      | 2023-01-06 03:53:30,787 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c742cece-fb27-47b9-b231-5839005b1100 reported by 678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1453571467283, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-01-06 03:53:30,789 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=d88fe7be-cf18-4d52-90b0-fc9dcbe94408. Trying to get from SCM.
recon_1      | 2023-01-06 03:53:30,975 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: d88fe7be-cf18-4d52-90b0-fc9dcbe94408, Nodes: 678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-06T03:52:45.363Z[UTC]] to Recon pipeline metadata.
recon_1      | 2023-01-06 03:53:30,976 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d88fe7be-cf18-4d52-90b0-fc9dcbe94408, Nodes: 678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-06T03:52:45.363Z[UTC]].
recon_1      | 2023-01-06 03:53:30,984 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=d88fe7be-cf18-4d52-90b0-fc9dcbe94408 reported by 678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1453571467283, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2.org_1   | 2023-01-06 03:53:17,178 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 5af4a3fc-47d6-497e-93af-69810f3160d1, Nodes: f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:f7a31870-78ff-406d-a144-7fe17a627699, CreationTimestamp2023-01-06T03:52:43.589Z[UTC]] moved to OPEN state
recon_1      | 2023-01-06 03:53:30,985 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d88fe7be-cf18-4d52-90b0-fc9dcbe94408, Nodes: 678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:678ebbfa-a167-4ea6-8846-1b1f5c050dad, CreationTimestamp2023-01-06T03:52:45.363Z[UTC]] moved to OPEN state
scm2.org_1   | 2023-01-06 03:53:17,411 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-06 03:53:19,029 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm2.org_1   | 2023-01-06 03:53:19,140 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm2.org_1   | 2023-01-06 03:53:19,849 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm2.org_1   | 2023-01-06 03:53:19,852 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-01-06 03:53:23,691 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm2.org_1   | 2023-01-06 03:53:23,691 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-01-06 03:53:31,130 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:51210
scm1.org_1   | 2023-01-06 03:50:24,313 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1e0d70db{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-1791038529757445350/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm1.org_1   | 2023-01-06 03:50:24,323 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@58453967{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm1.org_1   | 2023-01-06 03:50:24,324 [Listener at 0.0.0.0/9860] INFO server.Server: Started @9074ms
scm1.org_1   | 2023-01-06 03:50:24,326 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm1.org_1   | 2023-01-06 03:50:24,326 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm1.org_1   | 2023-01-06 03:50:24,328 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm1.org_1   | 2023-01-06 03:50:25,063 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:55924
scm1.org_1   | 2023-01-06 03:50:25,097 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-01-06 03:50:27,642 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-FollowerState] INFO impl.FollowerState: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5089773643ns, electionTimeout:5075ms
scm1.org_1   | 2023-01-06 03:50:27,644 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-FollowerState] INFO impl.RoleInfo: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: shutdown e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-FollowerState
scm1.org_1   | 2023-01-06 03:50:27,645 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-FollowerState] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm1.org_1   | 2023-01-06 03:50:27,648 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm1.org_1   | 2023-01-06 03:50:27,648 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-FollowerState] INFO impl.RoleInfo: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: start e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1
scm1.org_1   | 2023-01-06 03:50:27,676 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO impl.LeaderElection: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1      | 2023-01-06 03:53:31,298 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c742cece-fb27-47b9-b231-5839005b1100 reported by f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1452870296728, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-01-06 03:53:31,298 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=7faae3af-cb79-4c3a-b244-507410c3658e. Trying to get from SCM.
recon_1      | 2023-01-06 03:53:31,391 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 7faae3af-cb79-4c3a-b244-507410c3658e, Nodes: f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-06T03:52:45.566Z[UTC]] to Recon pipeline metadata.
recon_1      | 2023-01-06 03:53:31,394 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 7faae3af-cb79-4c3a-b244-507410c3658e, Nodes: f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-06T03:52:45.566Z[UTC]].
recon_1      | 2023-01-06 03:53:31,394 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=7faae3af-cb79-4c3a-b244-507410c3658e reported by f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1452870296728, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-01-06 03:53:31,560 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c742cece-fb27-47b9-b231-5839005b1100 reported by 678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1453571467283, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-01-06 03:53:31,560 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=7faae3af-cb79-4c3a-b244-507410c3658e reported by 678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1453571467283, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-01-06 03:53:31,652 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:58612
recon_1      | 2023-01-06 03:53:31,678 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-01-06 03:53:31,683 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c742cece-fb27-47b9-b231-5839005b1100 reported by d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1454579200080, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-01-06 03:53:31,691 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=7faae3af-cb79-4c3a-b244-507410c3658e reported by d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1454579200080, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-01-06 03:53:32,328 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om3 is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:248)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:235)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:228)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:175)
recon_1      | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
scm2.org_1   | 2023-01-06 03:53:31,272 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm2.org_1   | 2023-01-06 03:53:31,274 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-01-06 03:53:31,345 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-01-06 03:53:31,351 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d88fe7be-cf18-4d52-90b0-fc9dcbe94408, Nodes: 678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:678ebbfa-a167-4ea6-8846-1b1f5c050dad, CreationTimestamp2023-01-06T03:52:45.363Z[UTC]] moved to OPEN state
scm2.org_1   | 2023-01-06 03:53:31,356 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm2.org_1   | 2023-01-06 03:53:31,482 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-06 03:53:31,549 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm2.org_1   | 2023-01-06 03:53:31,551 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-01-06 03:53:31,846 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:50024
scm2.org_1   | 2023-01-06 03:53:31,913 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-01-06 03:50:27,678 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO impl.LeaderElection: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm2.org_1   | 2023-01-06 03:53:31,914 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm2.org_1   | 2023-01-06 03:53:33,143 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-06 03:53:33,215 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 2023-01-06 03:50:27,678 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO impl.RoleInfo: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: shutdown e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1
scm2.org_1   | 2023-01-06 03:53:33,216 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-01-06 03:53:36,071 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm2.org_1   | 2023-01-06 03:53:36,073 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-01-06 03:50:27,679 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm2.org_1   | 2023-01-06 03:53:38,465 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm2.org_1   | 2023-01-06 03:53:38,465 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-01-06 03:53:41,708 [aea2b8cb-487c-4369-b11b-87f4d7694103@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-06 03:53:41,735 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 2023-01-06 03:50:27,679 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm2.org_1   | 2023-01-06 03:53:41,736 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm1.org_1   | 2023-01-06 03:50:27,680 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm2.org_1   | 2023-01-06 03:53:41,737 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm2.org_1   | 2023-01-06 03:53:41,737 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
recon_1      | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
scm1.org_1   | 2023-01-06 03:50:27,682 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3: change Leader from null to e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674 at term 2 for becomeLeader, leader elected after 8129ms
scm1.org_1   | 2023-01-06 03:50:27,691 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1.org_1   | 2023-01-06 03:50:27,696 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2023-01-06 03:50:27,697 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm2.org_1   | 2023-01-06 03:53:41,738 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm2.org_1   | 2023-01-06 03:53:41,739 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm1.org_1   | 2023-01-06 03:50:27,708 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1.org_1   | 2023-01-06 03:50:27,708 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1      | , while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 179 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:53:33,104 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c742cece-fb27-47b9-b231-5839005b1100 reported by d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1454579200080, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-01-06 03:53:33,105 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=7faae3af-cb79-4c3a-b244-507410c3658e reported by d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1454579200080, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2023-01-06 03:50:27,709 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
recon_1      | 2023-01-06 03:53:33,105 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=88086403-84d0-4755-ada1-bf9cf73be954. Trying to get from SCM.
scm1.org_1   | 2023-01-06 03:50:27,714 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2023-01-06 03:50:27,717 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm1.org_1   | 2023-01-06 03:50:27,719 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO impl.RoleInfo: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: start e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderStateImpl
recon_1      | 2023-01-06 03:53:33,136 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 88086403-84d0-4755-ada1-bf9cf73be954, Nodes: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86, CreationTimestamp2023-01-06T03:52:45.799Z[UTC]] to Recon pipeline metadata.
scm1.org_1   | 2023-01-06 03:50:27,731 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm1.org_1   | 2023-01-06 03:50:27,736 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3/current/log_inprogress_0 to /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3/current/log_0-0
recon_1      | 2023-01-06 03:53:33,137 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 88086403-84d0-4755-ada1-bf9cf73be954, Nodes: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86, CreationTimestamp2023-01-06T03:52:45.799Z[UTC]].
scm1.org_1   | 2023-01-06 03:50:27,761 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderElection1] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3: set configuration 1: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-01-06 03:50:27,763 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/2e7972fd-840e-47e3-acc1-d75ce56c2bc3/current/log_inprogress_1
recon_1      | 2023-01-06 03:53:33,137 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=88086403-84d0-4755-ada1-bf9cf73be954 reported by d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1454579200080, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2023-01-06 03:50:27,769 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm1.org_1   | 2023-01-06 03:50:27,771 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
recon_1      | 2023-01-06 03:53:33,138 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 88086403-84d0-4755-ada1-bf9cf73be954, Nodes: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86, CreationTimestamp2023-01-06T03:52:45.799Z[UTC]] moved to OPEN state
scm1.org_1   | 2023-01-06 03:50:27,774 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
recon_1      | 2023-01-06 03:53:34,334 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om1 is not the leader. Could not determine the leader node.
scm1.org_1   | 2023-01-06 03:50:27,775 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:248)
scm1.org_1   | 2023-01-06 03:50:27,775 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm1.org_1   | 2023-01-06 03:50:27,776 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:235)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:228)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:175)
recon_1      | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
recon_1      | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1.org_1   | 2023-01-06 03:50:27,779 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2023-01-06 03:50:27,781 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm1.org_1   | 2023-01-06 03:50:29,645 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:40082
scm1.org_1   | 2023-01-06 03:50:29,658 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-06 03:50:29,732 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for scm scm2.org, nodeId: aea2b8cb-487c-4369-b11b-87f4d7694103
scm1.org_1   | 2023-01-06 03:50:30,006 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm1.org_1   | 2023-01-06 03:50:30,045 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for RECON recon, UUID: 9936d45b-c86f-4c18-a405-d45b4e3471d2
scm1.org_1   | 2023-01-06 03:50:30,353 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-06 03:50:30,353 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm1.org_1   | 2023-01-06 03:50:30,353 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm1.org_1   | 2023-01-06 03:50:31,437 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-06 03:50:49,234 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:58792
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1      | , while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 180 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:53:34,348 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om2 is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:248)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:235)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:228)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:175)
recon_1      | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
recon_1      | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
scm1.org_1   | 2023-01-06 03:50:49,284 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1      | , while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 181 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-06 03:53:34,354 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om3 is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:248)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:235)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:228)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:175)
recon_1      | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
recon_1      | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1.org_1   | 2023-01-06 03:50:51,033 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:41375
scm1.org_1   | 2023-01-06 03:50:51,046 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-01-06 03:50:52,505 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:58616
scm1.org_1   | 2023-01-06 03:50:52,636 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-01-06 03:50:52,643 [IPC Server handler 0 on default port 9863] INFO ha.SCMRatisServerImpl: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: Submitting SetConfiguration request to Ratis server with new SCM peers list: [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER]
scm1.org_1   | 2023-01-06 03:50:52,680 [IPC Server handler 0 on default port 9863] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3: receive setConfiguration SetConfigurationRequest:client-E5BD863F405B->e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3, cid=1, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1.org_1   | 2023-01-06 03:50:52,680 [IPC Server handler 0 on default port 9863] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-E5BD863F405B->e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3, cid=1, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1.org_1   | 2023-01-06 03:50:52,765 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1.org_1   | 2023-01-06 03:50:52,765 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-01-06 03:50:52,765 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm1.org_1   | 2023-01-06 03:50:52,839 [IPC Server handler 0 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1.org_1   | 2023-01-06 03:50:52,858 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-01-06 03:50:52,858 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2023-01-06 03:50:52,858 [IPC Server handler 0 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1.org_1   | 2023-01-06 03:50:52,858 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm1.org_1   | 2023-01-06 03:50:52,952 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->aea2b8cb-487c-4369-b11b-87f4d7694103-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->aea2b8cb-487c-4369-b11b-87f4d7694103-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:1, i:0)
scm1.org_1   | 2023-01-06 03:50:53,016 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->aea2b8cb-487c-4369-b11b-87f4d7694103-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->aea2b8cb-487c-4369-b11b-87f4d7694103-GrpcLogAppender: send e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674->aea2b8cb-487c-4369-b11b-87f4d7694103#0-t2,notify:(t:1, i:0)
scm1.org_1   | 2023-01-06 03:50:53,025 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->aea2b8cb-487c-4369-b11b-87f4d7694103-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcServerProtocolClient: Build channel for aea2b8cb-487c-4369-b11b-87f4d7694103
scm1.org_1   | 2023-01-06 03:50:54,603 [grpc-default-executor-0] INFO server.GrpcLogAppender: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->aea2b8cb-487c-4369-b11b-87f4d7694103-InstallSnapshotResponseHandler: received the first reply e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674<-aea2b8cb-487c-4369-b11b-87f4d7694103#0:OK-t0,ALREADY_INSTALLED
scm1.org_1   | 2023-01-06 03:50:54,652 [grpc-default-executor-0] INFO server.GrpcLogAppender: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->aea2b8cb-487c-4369-b11b-87f4d7694103-InstallSnapshotResponseHandler: Follower snapshot is already at index 0.
scm1.org_1   | 2023-01-06 03:50:54,654 [grpc-default-executor-0] INFO leader.FollowerInfo: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->aea2b8cb-487c-4369-b11b-87f4d7694103: snapshotIndex: setUnconditionally 0 -> 0
scm1.org_1   | 2023-01-06 03:50:54,654 [grpc-default-executor-0] INFO leader.FollowerInfo: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->aea2b8cb-487c-4369-b11b-87f4d7694103: matchIndex: setUnconditionally 0 -> 0
scm1.org_1   | 2023-01-06 03:50:54,657 [grpc-default-executor-0] INFO leader.FollowerInfo: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->aea2b8cb-487c-4369-b11b-87f4d7694103: nextIndex: setUnconditionally 0 -> 1
scm1.org_1   | 2023-01-06 03:50:54,657 [grpc-default-executor-0] INFO leader.FollowerInfo: Follower e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->aea2b8cb-487c-4369-b11b-87f4d7694103 acknowledged installing snapshot
scm1.org_1   | 2023-01-06 03:50:54,657 [grpc-default-executor-0] INFO leader.FollowerInfo: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->aea2b8cb-487c-4369-b11b-87f4d7694103: nextIndex: updateToMax old=1, new=1, updated? false
scm1.org_1   | 2023-01-06 03:50:54,890 [grpc-default-executor-2] INFO leader.FollowerInfo: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->aea2b8cb-487c-4369-b11b-87f4d7694103: nextIndex: updateUnconditionally 7 -> 0
scm1.org_1   | 2023-01-06 03:50:54,920 [grpc-default-executor-0] INFO leader.FollowerInfo: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->aea2b8cb-487c-4369-b11b-87f4d7694103: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2023-01-06 03:50:55,551 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderStateImpl] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3: set configuration 7: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1.org_1   | 2023-01-06 03:50:55,646 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderStateImpl] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3: set configuration 9: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-01-06 03:50:55,763 [IPC Server handler 0 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: aea2b8cb-487c-4369-b11b-87f4d7694103.
scm1.org_1   | 2023-01-06 03:50:58,603 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:58466
scm1.org_1   | 2023-01-06 03:50:58,626 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-06 03:50:59,682 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.118:38008
scm1.org_1   | 2023-01-06 03:50:59,738 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-01-06 03:51:03,784 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.118:59562
scm1.org_1   | 2023-01-06 03:51:03,794 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-06 03:51:03,794 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for scm scm3.org, nodeId: 962229bc-cc46-47e6-82cc-1ac6b63780ff
scm1.org_1   | 2023-01-06 03:51:03,835 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm1.org_1   | 2023-01-06 03:51:04,093 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-06 03:51:05,482 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:38018
scm1.org_1   | 2023-01-06 03:51:05,557 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1      | , while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 182 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-06 03:53:36,119 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c742cece-fb27-47b9-b231-5839005b1100 reported by 678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1453571467283, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-01-06 03:53:36,136 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=7faae3af-cb79-4c3a-b244-507410c3658e reported by 678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1453571467283, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-01-06 03:53:38,486 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c742cece-fb27-47b9-b231-5839005b1100 reported by d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1454579200080, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-01-06 03:53:38,486 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=7faae3af-cb79-4c3a-b244-507410c3658e reported by d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1454579200080, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-01-06 03:53:38,496 [pool-30-thread-1] ERROR impl.OzoneManagerServiceProviderImpl: Unable to update Recon's metadata with new OM DB. 
recon_1      | java.lang.reflect.UndeclaredThrowableException
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1894)
recon_1      | 	at org.apache.hadoop.security.SecurityUtil.doAsUser(SecurityUtil.java:536)
recon_1      | 	at org.apache.hadoop.security.SecurityUtil.doAsLoginUser(SecurityUtil.java:517)
recon_1      | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getOzoneManagerDBSnapshot(OzoneManagerServiceProviderImpl.java:346)
scm1.org_1   | 2023-01-06 03:51:21,649 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:33542
recon_1      | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.updateReconOmDBWithNewSnapshot(OzoneManagerServiceProviderImpl.java:378)
recon_1      | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:519)
recon_1      | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1      | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1      | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1      | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1      | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1      | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1      | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1      | Caused by: org.apache.hadoop.security.authentication.client.AuthenticationException: Error while authenticating with endpoint: http://om1:9874/dbCheckpoint
recon_1      | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
recon_1      | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
recon_1      | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
recon_1      | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
recon_1      | 	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.wrapExceptionWithMessage(KerberosAuthenticator.java:232)
recon_1      | 	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.authenticate(KerberosAuthenticator.java:219)
recon_1      | 	at org.apache.hadoop.security.authentication.client.AuthenticatedURL.openConnection(AuthenticatedURL.java:350)
recon_1      | 	at org.apache.hadoop.hdfs.web.URLConnectionFactory.openConnection(URLConnectionFactory.java:186)
recon_1      | 	at org.apache.hadoop.ozone.recon.ReconUtils.makeHttpCall(ReconUtils.java:244)
recon_1      | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$getOzoneManagerDBSnapshot$1(OzoneManagerServiceProviderImpl.java:347)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	... 12 more
recon_1      | Caused by: org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - LOOKING_UP_SERVER)
recon_1      | 	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.doSpnegoSequence(KerberosAuthenticator.java:360)
recon_1      | 	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.authenticate(KerberosAuthenticator.java:204)
recon_1      | 	... 19 more
recon_1      | Caused by: GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - LOOKING_UP_SERVER)
recon_1      | 	at java.security.jgss/sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:773)
recon_1      | 	at java.security.jgss/sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:266)
recon_1      | 	at java.security.jgss/sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:196)
recon_1      | 	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator$1.run(KerberosAuthenticator.java:336)
recon_1      | 	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator$1.run(KerberosAuthenticator.java:310)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.doSpnegoSequence(KerberosAuthenticator.java:310)
recon_1      | 	... 20 more
recon_1      | Caused by: KrbException: Server not found in Kerberos database (7) - LOOKING_UP_SERVER
recon_1      | 	at java.security.jgss/sun.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:73)
recon_1      | 	at java.security.jgss/sun.security.krb5.KrbTgsReq.getReply(KrbTgsReq.java:226)
recon_1      | 	at java.security.jgss/sun.security.krb5.KrbTgsReq.sendAndGetCreds(KrbTgsReq.java:237)
recon_1      | 	at java.security.jgss/sun.security.krb5.internal.CredentialsUtil.serviceCredsSingle(CredentialsUtil.java:477)
recon_1      | 	at java.security.jgss/sun.security.krb5.internal.CredentialsUtil.serviceCreds(CredentialsUtil.java:340)
recon_1      | 	at java.security.jgss/sun.security.krb5.internal.CredentialsUtil.serviceCreds(CredentialsUtil.java:314)
recon_1      | 	at java.security.jgss/sun.security.krb5.internal.CredentialsUtil.acquireServiceCreds(CredentialsUtil.java:169)
recon_1      | 	at java.security.jgss/sun.security.krb5.Credentials.acquireServiceCreds(Credentials.java:490)
recon_1      | 	at java.security.jgss/sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:697)
recon_1      | 	... 27 more
recon_1      | Caused by: KrbException: Identifier doesn't match expected value (906)
recon_1      | 	at java.security.jgss/sun.security.krb5.internal.KDCRep.init(KDCRep.java:140)
recon_1      | 	at java.security.jgss/sun.security.krb5.internal.TGSRep.init(TGSRep.java:65)
recon_1      | 	at java.security.jgss/sun.security.krb5.internal.TGSRep.<init>(TGSRep.java:60)
recon_1      | 	at java.security.jgss/sun.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:55)
scm1.org_1   | 2023-01-06 03:51:21,926 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.118:44240
scm1.org_1   | 2023-01-06 03:51:22,006 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-01-06 03:51:22,223 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-01-06 03:51:22,228 [IPC Server handler 95 on default port 9863] INFO ha.SCMRatisServerImpl: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674: Submitting SetConfiguration request to Ratis server with new SCM peers list: [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, 962229bc-cc46-47e6-82cc-1ac6b63780ff|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER]
scm1.org_1   | 2023-01-06 03:51:22,231 [IPC Server handler 95 on default port 9863] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3: receive setConfiguration SetConfigurationRequest:client-E5BD863F405B->e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3, cid=2, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, 962229bc-cc46-47e6-82cc-1ac6b63780ff|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1.org_1   | 2023-01-06 03:51:22,233 [IPC Server handler 95 on default port 9863] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-E5BD863F405B->e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3, cid=2, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, 962229bc-cc46-47e6-82cc-1ac6b63780ff|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1.org_1   | 2023-01-06 03:51:22,239 [IPC Server handler 95 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1.org_1   | 2023-01-06 03:51:22,257 [IPC Server handler 95 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-01-06 03:51:22,259 [IPC Server handler 95 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm1.org_1   | 2023-01-06 03:51:22,265 [IPC Server handler 95 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1.org_1   | 2023-01-06 03:51:22,269 [IPC Server handler 95 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-01-06 03:51:22,269 [IPC Server handler 95 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2023-01-06 03:51:22,270 [IPC Server handler 95 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1.org_1   | 2023-01-06 03:51:22,270 [IPC Server handler 95 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm1.org_1   | 2023-01-06 03:51:22,298 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->962229bc-cc46-47e6-82cc-1ac6b63780ff-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->962229bc-cc46-47e6-82cc-1ac6b63780ff-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:1, i:0)
scm1.org_1   | 2023-01-06 03:51:22,312 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->962229bc-cc46-47e6-82cc-1ac6b63780ff-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->962229bc-cc46-47e6-82cc-1ac6b63780ff-GrpcLogAppender: send e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674->962229bc-cc46-47e6-82cc-1ac6b63780ff#0-t2,notify:(t:1, i:0)
scm1.org_1   | 2023-01-06 03:51:22,313 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->962229bc-cc46-47e6-82cc-1ac6b63780ff-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcServerProtocolClient: Build channel for 962229bc-cc46-47e6-82cc-1ac6b63780ff
recon_1      | 	... 35 more
scm1.org_1   | 2023-01-06 03:51:27,205 [grpc-default-executor-2] INFO server.GrpcLogAppender: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->962229bc-cc46-47e6-82cc-1ac6b63780ff-InstallSnapshotResponseHandler: received the first reply e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674<-962229bc-cc46-47e6-82cc-1ac6b63780ff#0:OK-t0,ALREADY_INSTALLED
scm1.org_1   | 2023-01-06 03:51:27,206 [grpc-default-executor-2] INFO server.GrpcLogAppender: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->962229bc-cc46-47e6-82cc-1ac6b63780ff-InstallSnapshotResponseHandler: Follower snapshot is already at index 0.
scm1.org_1   | 2023-01-06 03:51:27,206 [grpc-default-executor-2] INFO leader.FollowerInfo: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->962229bc-cc46-47e6-82cc-1ac6b63780ff: snapshotIndex: setUnconditionally 0 -> 0
scm1.org_1   | 2023-01-06 03:51:27,206 [grpc-default-executor-2] INFO leader.FollowerInfo: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->962229bc-cc46-47e6-82cc-1ac6b63780ff: matchIndex: setUnconditionally 0 -> 0
scm1.org_1   | 2023-01-06 03:51:27,206 [grpc-default-executor-2] INFO leader.FollowerInfo: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->962229bc-cc46-47e6-82cc-1ac6b63780ff: nextIndex: setUnconditionally 0 -> 1
scm1.org_1   | 2023-01-06 03:51:27,207 [grpc-default-executor-2] INFO leader.FollowerInfo: Follower e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->962229bc-cc46-47e6-82cc-1ac6b63780ff acknowledged installing snapshot
scm1.org_1   | 2023-01-06 03:51:27,207 [grpc-default-executor-2] INFO leader.FollowerInfo: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->962229bc-cc46-47e6-82cc-1ac6b63780ff: nextIndex: updateToMax old=1, new=1, updated? false
scm1.org_1   | 2023-01-06 03:51:27,632 [grpc-default-executor-2] INFO leader.FollowerInfo: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->962229bc-cc46-47e6-82cc-1ac6b63780ff: nextIndex: updateUnconditionally 13 -> 0
scm1.org_1   | 2023-01-06 03:51:27,656 [grpc-default-executor-2] INFO leader.FollowerInfo: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3->962229bc-cc46-47e6-82cc-1ac6b63780ff: nextIndex: updateUnconditionally 13 -> 0
scm1.org_1   | 2023-01-06 03:51:30,134 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderStateImpl] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3: set configuration 13: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, 962229bc-cc46-47e6-82cc-1ac6b63780ff|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1.org_1   | 2023-01-06 03:51:30,235 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-LeaderStateImpl] INFO server.RaftServer$Division: e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3: set configuration 15: peers:[e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, aea2b8cb-487c-4369-b11b-87f4d7694103|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, 962229bc-cc46-47e6-82cc-1ac6b63780ff|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-01-06 03:51:30,365 [IPC Server handler 95 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: 962229bc-cc46-47e6-82cc-1ac6b63780ff.
scm1.org_1   | 2023-01-06 03:51:42,173 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.118:49346
scm1.org_1   | 2023-01-06 03:51:42,279 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-06 03:51:51,753 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:33515
scm1.org_1   | 2023-01-06 03:51:51,828 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-01-06 03:51:55,491 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:35832
scm1.org_1   | 2023-01-06 03:51:56,005 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-01-06 03:51:56,133 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.113:50658
scm1.org_1   | 2023-01-06 03:51:56,352 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
recon_1      | 2023-01-06 03:53:41,673 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=c742cece-fb27-47b9-b231-5839005b1100 reported by d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1454579200080, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-01-06 03:53:41,674 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=7faae3af-cb79-4c3a-b244-507410c3658e reported by d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1454579200080, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-01-06 03:53:41,674 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 7faae3af-cb79-4c3a-b244-507410c3658e, Nodes: f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86, CreationTimestamp2023-01-06T03:52:45.566Z[UTC]] moved to OPEN state
scm1.org_1   | 2023-01-06 03:51:56,805 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.112:39370
scm1.org_1   | 2023-01-06 03:51:56,950 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-01-06 03:52:00,319 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:59608
scm1.org_1   | 2023-01-06 03:52:00,495 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-06 03:52:00,500 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn 47737eac8b7e, UUID: f7a31870-78ff-406d-a144-7fe17a627699
scm1.org_1   | 2023-01-06 03:52:00,763 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:55258
scm1.org_1   | 2023-01-06 03:52:01,037 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-06 03:52:01,051 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn 03b09732740a, UUID: 678ebbfa-a167-4ea6-8846-1b1f5c050dad
scm1.org_1   | 2023-01-06 03:52:01,092 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-06 03:52:01,624 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-06 03:52:02,043 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:39562
scm1.org_1   | 2023-01-06 03:52:02,172 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-06 03:52:02,173 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn 98e33c1634d3, UUID: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86
scm1.org_1   | 2023-01-06 03:52:02,552 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-06 03:52:12,006 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.113:47080
scm1.org_1   | 2023-01-06 03:52:12,033 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-06 03:52:12,208 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om3, UUID: ed8d858c-7e16-40de-b452-8c343ff0ff32
scm1.org_1   | 2023-01-06 03:52:12,743 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:43812
scm1.org_1   | 2023-01-06 03:52:12,773 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-06 03:52:12,776 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om1, UUID: c83f252d-0e9d-48cc-a3ae-8c0c609f17ba
scm1.org_1   | 2023-01-06 03:52:12,913 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-06 03:52:13,112 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-06 03:52:14,174 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.112:57548
scm1.org_1   | 2023-01-06 03:52:14,261 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-06 03:52:14,269 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om2, UUID: 3ab37fc4-82e7-42c8-a3d5-85d84e53b43d
scm1.org_1   | 2023-01-06 03:52:14,708 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-06 03:52:16,520 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:35422
scm1.org_1   | 2023-01-06 03:52:16,589 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-06 03:52:17,595 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:35142
scm1.org_1   | 2023-01-06 03:52:17,633 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-06 03:52:18,646 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:40422
scm1.org_1   | 2023-01-06 03:52:18,689 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-06 03:52:27,175 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:41346
scm1.org_1   | 2023-01-06 03:52:27,450 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-01-06 03:52:38,404 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:54228
scm1.org_1   | 2023-01-06 03:52:38,456 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-01-06 03:52:38,505 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:56700
scm1.org_1   | 2023-01-06 03:52:38,598 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-01-06 03:52:39,087 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:46722
scm1.org_1   | 2023-01-06 03:52:39,142 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-01-06 03:52:43,371 [IPC Server handler 77 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f7a31870-78ff-406d-a144-7fe17a627699
scm1.org_1   | 2023-01-06 03:52:43,381 [IPC Server handler 77 on default port 9861] INFO node.SCMNodeManager: Registered Data node : f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1452870296728, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2023-01-06 03:52:43,514 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2023-01-06 03:52:43,589 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm1.org_1   | 2023-01-06 03:52:43,626 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5af4a3fc-47d6-497e-93af-69810f3160d1 to datanode:f7a31870-78ff-406d-a144-7fe17a627699
scm1.org_1   | 2023-01-06 03:52:43,925 [IPC Server handler 17 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86
scm1.org_1   | 2023-01-06 03:52:43,991 [IPC Server handler 80 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/678ebbfa-a167-4ea6-8846-1b1f5c050dad
scm1.org_1   | 2023-01-06 03:52:44,148 [IPC Server handler 17 on default port 9861] INFO node.SCMNodeManager: Registered Data node : d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1454579200080, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2023-01-06 03:52:44,230 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2023-01-06 03:52:44,172 [IPC Server handler 80 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1453571467283, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2023-01-06 03:52:44,230 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2023-01-06 03:52:44,233 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm1.org_1   | 2023-01-06 03:52:44,237 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm1.org_1   | 2023-01-06 03:52:44,237 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm1.org_1   | 2023-01-06 03:52:44,239 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm1.org_1   | 2023-01-06 03:52:44,252 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm1.org_1   | 2023-01-06 03:52:44,252 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2023-01-06 03:52:44,611 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 5af4a3fc-47d6-497e-93af-69810f3160d1, Nodes: f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-06T03:52:43.589Z[UTC]].
scm1.org_1   | 2023-01-06 03:52:44,635 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-06 03:52:44,906 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 2023-01-06 03:52:45,036 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=c742cece-fb27-47b9-b231-5839005b1100 to datanode:f7a31870-78ff-406d-a144-7fe17a627699
scm1.org_1   | 2023-01-06 03:52:45,082 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=c742cece-fb27-47b9-b231-5839005b1100 to datanode:d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86
scm1.org_1   | 2023-01-06 03:52:45,090 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=c742cece-fb27-47b9-b231-5839005b1100 to datanode:678ebbfa-a167-4ea6-8846-1b1f5c050dad
scm1.org_1   | 2023-01-06 03:52:45,213 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: c742cece-fb27-47b9-b231-5839005b1100, Nodes: f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-06T03:52:45.036Z[UTC]].
scm1.org_1   | 2023-01-06 03:52:45,215 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-06 03:52:45,363 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d88fe7be-cf18-4d52-90b0-fc9dcbe94408 to datanode:678ebbfa-a167-4ea6-8846-1b1f5c050dad
scm1.org_1   | 2023-01-06 03:52:45,488 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d88fe7be-cf18-4d52-90b0-fc9dcbe94408, Nodes: 678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-06T03:52:45.363Z[UTC]].
scm1.org_1   | 2023-01-06 03:52:45,555 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-06 03:52:45,566 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=7faae3af-cb79-4c3a-b244-507410c3658e to datanode:f7a31870-78ff-406d-a144-7fe17a627699
scm1.org_1   | 2023-01-06 03:52:45,631 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=7faae3af-cb79-4c3a-b244-507410c3658e to datanode:678ebbfa-a167-4ea6-8846-1b1f5c050dad
scm1.org_1   | 2023-01-06 03:52:45,631 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=7faae3af-cb79-4c3a-b244-507410c3658e to datanode:d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86
scm1.org_1   | 2023-01-06 03:52:45,707 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 7faae3af-cb79-4c3a-b244-507410c3658e, Nodes: f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-06T03:52:45.566Z[UTC]].
scm1.org_1   | 2023-01-06 03:52:45,710 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-06 03:52:45,736 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=7faae3af-cb79-4c3a-b244-507410c3658e contains same datanodes as previous pipelines: PipelineID=c742cece-fb27-47b9-b231-5839005b1100 nodeIds: f7a31870-78ff-406d-a144-7fe17a627699, 678ebbfa-a167-4ea6-8846-1b1f5c050dad, d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86
scm1.org_1   | 2023-01-06 03:52:45,799 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=88086403-84d0-4755-ada1-bf9cf73be954 to datanode:d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86
scm1.org_1   | 2023-01-06 03:52:45,894 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 88086403-84d0-4755-ada1-bf9cf73be954, Nodes: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-06T03:52:45.799Z[UTC]].
scm1.org_1   | 2023-01-06 03:52:45,895 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-06 03:52:45,896 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0.
scm1.org_1   | 2023-01-06 03:52:45,981 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 2023-01-06 03:52:46,013 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0.
scm1.org_1   | 2023-01-06 03:52:46,035 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 2023-01-06 03:52:49,624 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.113:37152
scm1.org_1   | 2023-01-06 03:52:49,634 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:34358
scm1.org_1   | 2023-01-06 03:52:49,682 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-01-06 03:52:49,731 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-01-06 03:52:50,188 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.112:41050
scm1.org_1   | 2023-01-06 03:52:50,224 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-01-06 03:53:00,090 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:47280
scm1.org_1   | 2023-01-06 03:53:00,096 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-06 03:53:00,814 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.113:55218
scm1.org_1   | 2023-01-06 03:53:00,834 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-06 03:53:01,431 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.112:34188
scm1.org_1   | 2023-01-06 03:53:01,457 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-06 03:53:08,573 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:60102
scm1.org_1   | 2023-01-06 03:53:08,704 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-01-06 03:53:14,939 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:34974
scm1.org_1   | 2023-01-06 03:53:14,986 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-01-06 03:53:14,991 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 2023-01-06 03:53:16,062 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0.
scm1.org_1   | 2023-01-06 03:53:16,294 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:50354
scm1.org_1   | 2023-01-06 03:53:16,304 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-01-06 03:53:16,313 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 2023-01-06 03:53:16,340 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:57926
scm1.org_1   | 2023-01-06 03:53:16,391 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-01-06 03:53:16,399 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 2023-01-06 03:53:17,197 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 2023-01-06 03:53:17,200 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 5af4a3fc-47d6-497e-93af-69810f3160d1, Nodes: f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:f7a31870-78ff-406d-a144-7fe17a627699, CreationTimestamp2023-01-06T03:52:43.589Z[UTC]] moved to OPEN state
scm1.org_1   | 2023-01-06 03:53:17,319 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-06 03:53:17,403 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:38703
scm1.org_1   | 2023-01-06 03:53:17,599 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-01-06 03:53:17,682 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-01-06 03:53:18,419 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-01-06 03:53:18,426 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 2023-01-06 03:53:19,086 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 2023-01-06 03:53:19,187 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 2023-01-06 03:53:23,748 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 2023-01-06 03:53:23,750 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-01-06 03:53:30,927 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:34535
scm1.org_1   | 2023-01-06 03:53:30,938 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-01-06 03:53:31,046 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:45032
scm1.org_1   | 2023-01-06 03:53:31,288 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 2023-01-06 03:53:31,288 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-01-06 03:53:31,319 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-01-06 03:53:31,327 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 2023-01-06 03:53:31,327 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d88fe7be-cf18-4d52-90b0-fc9dcbe94408, Nodes: 678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:678ebbfa-a167-4ea6-8846-1b1f5c050dad, CreationTimestamp2023-01-06T03:52:45.363Z[UTC]] moved to OPEN state
scm1.org_1   | 2023-01-06 03:53:31,388 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 2023-01-06 03:53:31,441 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-06 03:53:31,471 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-01-06 03:53:31,471 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-01-06 03:53:31,889 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:34276
scm1.org_1   | 2023-01-06 03:53:31,946 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-01-06 03:53:31,965 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 2023-01-06 03:53:33,115 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 2023-01-06 03:53:33,119 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 88086403-84d0-4755-ada1-bf9cf73be954, Nodes: d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86, CreationTimestamp2023-01-06T03:52:45.799Z[UTC]] moved to OPEN state
scm1.org_1   | 2023-01-06 03:53:33,151 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-06 03:53:33,152 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-01-06 03:53:36,103 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 2023-01-06 03:53:36,104 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-01-06 03:53:38,468 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-01-06 03:53:38,469 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 2023-01-06 03:53:40,114 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:60902
scm1.org_1   | 2023-01-06 03:53:40,156 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-01-06 03:53:41,671 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm1.org_1   | 2023-01-06 03:53:41,672 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 7faae3af-cb79-4c3a-b244-507410c3658e, Nodes: f7a31870-78ff-406d-a144-7fe17a627699{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}678ebbfa-a167-4ea6-8846-1b1f5c050dad{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:d9359eb9-8c50-4fc9-9d1f-9a96e5f67e86, CreationTimestamp2023-01-06T03:52:45.566Z[UTC]] moved to OPEN state
scm1.org_1   | 2023-01-06 03:53:41,694 [e6cdf19c-ac0d-4a3c-9fde-e0dcf35b6674@group-D75CE56C2BC3-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-06 03:53:41,698 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm1.org_1   | 2023-01-06 03:53:41,699 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm1.org_1   | 2023-01-06 03:53:41,699 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm1.org_1   | 2023-01-06 03:53:41,699 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm1.org_1   | 2023-01-06 03:53:41,700 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm1.org_1   | 2023-01-06 03:53:41,700 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm1.org_1   | 2023-01-06 03:53:41,700 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm1.org_1   | 2023-01-06 03:53:41,700 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm1.org_1   | 2023-01-06 03:53:41,749 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
