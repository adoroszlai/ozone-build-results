Attaching to ha_om2_1, ha_om1_1, ha_dn4_1, ha_s3g_1, ha_dn1_1, ha_dn2_1, ha_recon_1, ha_om3_1, ha_scm_1, ha_dn5_1, ha_dn3_1
dn1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn1_1    | 2023-03-18 09:21:27,280 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn1_1    | /************************************************************
dn1_1    | STARTUP_MSG: Starting HddsDatanodeService
dn1_1    | STARTUP_MSG:   host = 8698bf9a31d9/10.9.0.15
dn1_1    | STARTUP_MSG:   args = []
dn1_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/1236414c1c46c67c92f37499b0b61921cdfab39a ; compiled by 'runner' on 2023-03-18T08:55Z
dn1_1    | STARTUP_MSG:   java = 11.0.14.1
dn1_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=1440, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4))$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn1_1    | ************************************************************/
dn1_1    | 2023-03-18 09:21:27,346 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn1_1    | 2023-03-18 09:21:27,848 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn1_1    | 2023-03-18 09:21:28,485 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn1_1    | 2023-03-18 09:21:29,784 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn1_1    | 2023-03-18 09:21:29,784 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn1_1    | 2023-03-18 09:21:30,868 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:8698bf9a31d9 ip:10.9.0.15
dn1_1    | 2023-03-18 09:21:33,148 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn1_1    | 2023-03-18 09:21:34,656 [main] INFO reflections.Reflections: Reflections took 1226 ms to scan 2 urls, producing 103 keys and 227 values 
dn1_1    | 2023-03-18 09:21:34,988 [main] INFO upgrade.HDDSLayoutVersionManager: Registering Upgrade Action : DatanodeSchemaV3FinalizeAction
dn1_1    | 2023-03-18 09:21:35,181 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
dn1_1    | 2023-03-18 09:21:36,490 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 8192 at 2023-03-18T09:21:04.594Z
dn1_1    | 2023-03-18 09:21:36,611 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn1_1    | 2023-03-18 09:21:36,634 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn1_1    | 2023-03-18 09:21:36,648 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn1_1    | 2023-03-18 09:21:36,819 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn1_1    | 2023-03-18 09:21:36,899 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-03-18 09:21:36,939 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-03-18T09:21:04.593Z
dn1_1    | 2023-03-18 09:21:36,956 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn1_1    | 2023-03-18 09:21:36,962 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn1_1    | 2023-03-18 09:21:36,993 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn1_1    | 2023-03-18 09:21:37,107 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn1_1    | 2023-03-18 09:21:37,107 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn1_1    | 2023-03-18 09:21:37,114 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn1_1    | 2023-03-18 09:21:48,346 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn1_1    | 2023-03-18 09:21:48,790 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-03-18 09:21:49,170 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn1_1    | 2023-03-18 09:21:50,343 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-03-18 09:21:50,393 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn1_1    | 2023-03-18 09:21:50,416 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-03-18 09:21:50,417 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn1_1    | 2023-03-18 09:21:50,417 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn1_1    | 2023-03-18 09:21:50,417 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn1_1    | 2023-03-18 09:21:50,417 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn1_1    | 2023-03-18 09:21:50,418 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-03-18 09:21:50,418 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn1_1    | 2023-03-18 09:21:50,444 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-03-18 09:21:50,535 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 2023-03-18 09:21:50,590 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn1_1    | 2023-03-18 09:21:50,608 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn1_1    | 2023-03-18 09:21:52,152 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn1_1    | 2023-03-18 09:21:52,173 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn1_1    | 2023-03-18 09:21:52,196 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn1_1    | 2023-03-18 09:21:52,202 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-03-18 09:21:52,202 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-03-18 09:21:52,204 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-03-18 09:21:52,227 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServer: 4ba198f1-a013-49ce-b059-f73788d69001: found a subdirectory /data/metadata/ratis/67747242-de40-4a6c-83d8-e43a5a79ab67
dn1_1    | 2023-03-18 09:21:52,283 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServer: 4ba198f1-a013-49ce-b059-f73788d69001: addNew group-E43A5A79AB67:[] returns group-E43A5A79AB67:java.util.concurrent.CompletableFuture@28d660f4[Not completed]
dn1_1    | 2023-03-18 09:21:52,443 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn1_1    | 2023-03-18 09:21:52,926 [pool-22-thread-1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001: new RaftServerImpl for group-E43A5A79AB67:[] with ContainerStateMachine:uninitialized
dn1_1    | 2023-03-18 09:21:53,011 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-03-18 09:21:53,057 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-03-18 09:21:53,057 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-03-18 09:21:53,061 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-03-18 09:21:53,061 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-03-18 09:21:53,061 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-03-18 09:21:53,163 [pool-22-thread-1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-03-18 09:21:53,200 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-03-18 09:21:53,300 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-03-18 09:21:53,304 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-03-18 09:21:53,623 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-03-18 09:21:53,661 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-03-18 09:21:53,695 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-03-18 09:21:54,256 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn1_1    | 2023-03-18 09:21:54,371 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-03-18 09:21:54,388 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-03-18 09:21:54,401 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-03-18 09:21:54,412 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-03-18 09:21:54,416 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn1_1    | 2023-03-18 09:21:54,424 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-03-18 09:21:54,605 [main] INFO util.log: Logging initialized @37323ms to org.eclipse.jetty.util.log.Slf4jLog
dn1_1    | 2023-03-18 09:21:55,512 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn1_1    | 2023-03-18 09:21:55,520 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn1_1    | 2023-03-18 09:21:55,564 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn1_1    | 2023-03-18 09:21:55,607 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn1_1    | 2023-03-18 09:21:55,607 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn1_1    | 2023-03-18 09:21:55,610 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn1_1    | 2023-03-18 09:21:55,820 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn1_1    | 2023-03-18 09:21:55,844 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn1_1    | 2023-03-18 09:21:55,865 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn1_1    | 2023-03-18 09:21:56,141 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn1_1    | 2023-03-18 09:21:56,141 [main] INFO server.session: No SessionScavenger set, using defaults
dn1_1    | 2023-03-18 09:21:56,155 [main] INFO server.session: node0 Scavenging every 660000ms
dn1_1    | 2023-03-18 09:21:56,284 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6c075e9d{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn1_1    | 2023-03-18 09:21:56,293 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@361cd35c{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn1_1    | 2023-03-18 09:21:57,192 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@a91119b{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-275692958142734805/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn1_1    | 2023-03-18 09:21:57,250 [main] INFO server.AbstractConnector: Started ServerConnector@284bdeed{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn1_1    | 2023-03-18 09:21:57,256 [main] INFO server.Server: Started @39968ms
dn1_1    | 2023-03-18 09:21:57,276 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn1_1    | 2023-03-18 09:21:57,276 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn1_1    | 2023-03-18 09:21:57,277 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn1_1    | 2023-03-18 09:21:57,279 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn1_1    | 2023-03-18 09:21:57,567 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn2_1    | 2023-03-18 09:21:27,751 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn2_1    | /************************************************************
dn2_1    | STARTUP_MSG: Starting HddsDatanodeService
dn2_1    | STARTUP_MSG:   host = b27b4bae5a17/10.9.0.16
dn2_1    | STARTUP_MSG:   args = []
dn2_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/1236414c1c46c67c92f37499b0b61921cdfab39a ; compiled by 'runner' on 2023-03-18T08:55Z
dn2_1    | STARTUP_MSG:   java = 11.0.14.1
dn1_1    | 2023-03-18 09:21:57,568 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn1_1    | 2023-03-18 09:21:57,620 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@80e4fc7] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn1_1    | 2023-03-18 09:21:58,213 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.20:9891
dn1_1    | 2023-03-18 09:21:58,298 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn1_1    | 2023-03-18 09:22:00,816 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-03-18 09:22:01,817 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-03-18 09:22:04,062 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn1_1    | 2023-03-18 09:22:04,096 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn1_1    | 2023-03-18 09:22:04,522 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn1_1    | 2023-03-18 09:22:04,536 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 4ba198f1-a013-49ce-b059-f73788d69001
dn1_1    | 2023-03-18 09:22:04,676 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/67747242-de40-4a6c-83d8-e43a5a79ab67/in_use.lock acquired by nodename 7@8698bf9a31d9
dn1_1    | 2023-03-18 09:22:04,717 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=4ba198f1-a013-49ce-b059-f73788d69001} from /data/metadata/ratis/67747242-de40-4a6c-83d8-e43a5a79ab67/current/raft-meta
dn1_1    | 2023-03-18 09:22:04,927 [EndpointStateMachine task thread for recon/10.9.0.20:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From 8698bf9a31d9/10.9.0.15 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.15:55264 remote=recon/10.9.0.20:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn1_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.15:55264 remote=recon/10.9.0.20:9891]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn1_1    | 2023-03-18 09:22:05,048 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67: set configuration 3: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-18 09:22:05,140 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO ratis.ContainerStateMachine: group-E43A5A79AB67: Setting the last applied index to (t:3, i:4)
dn1_1    | 2023-03-18 09:22:05,603 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-03-18 09:22:05,668 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-03-18 09:22:05,669 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-03-18 09:22:05,686 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-03-18 09:22:05,689 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=1440, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4))$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn2_1    | ************************************************************/
dn2_1    | 2023-03-18 09:21:27,822 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn2_1    | 2023-03-18 09:21:28,233 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn2_1    | 2023-03-18 09:21:28,947 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn2_1    | 2023-03-18 09:21:30,425 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn2_1    | 2023-03-18 09:21:30,425 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn2_1    | 2023-03-18 09:21:31,369 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:b27b4bae5a17 ip:10.9.0.16
dn2_1    | 2023-03-18 09:21:33,471 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn2_1    | 2023-03-18 09:21:34,915 [main] INFO reflections.Reflections: Reflections took 1287 ms to scan 2 urls, producing 103 keys and 227 values 
dn2_1    | 2023-03-18 09:21:35,274 [main] INFO upgrade.HDDSLayoutVersionManager: Registering Upgrade Action : DatanodeSchemaV3FinalizeAction
dn2_1    | 2023-03-18 09:21:35,527 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
dn2_1    | 2023-03-18 09:21:36,876 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 8192 at 2023-03-18T09:21:04.564Z
dn2_1    | 2023-03-18 09:21:36,992 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn2_1    | 2023-03-18 09:21:37,034 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn2_1    | 2023-03-18 09:21:37,035 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn2_1    | 2023-03-18 09:21:37,174 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn2_1    | 2023-03-18 09:21:37,358 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-03-18 09:21:37,371 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-03-18T09:21:04.563Z
dn2_1    | 2023-03-18 09:21:37,424 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn2_1    | 2023-03-18 09:21:37,424 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn2_1    | 2023-03-18 09:21:37,425 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn2_1    | 2023-03-18 09:21:37,656 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn2_1    | 2023-03-18 09:21:37,656 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn2_1    | 2023-03-18 09:21:37,668 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn2_1    | 2023-03-18 09:21:49,117 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn2_1    | 2023-03-18 09:21:49,683 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-03-18 09:21:50,245 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn2_1    | 2023-03-18 09:21:51,202 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-03-18 09:21:51,223 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn2_1    | 2023-03-18 09:21:51,228 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-03-18 09:21:51,232 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn2_1    | 2023-03-18 09:21:51,240 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn2_1    | 2023-03-18 09:21:51,240 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn2_1    | 2023-03-18 09:21:51,241 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn2_1    | 2023-03-18 09:21:51,244 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-03-18 09:21:51,245 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn2_1    | 2023-03-18 09:21:51,252 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-03-18 09:21:51,329 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-03-18 09:21:51,340 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn2_1    | 2023-03-18 09:21:51,364 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn2_1    | 2023-03-18 09:21:52,653 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn2_1    | 2023-03-18 09:21:52,674 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn2_1    | 2023-03-18 09:21:52,684 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn2_1    | 2023-03-18 09:21:52,688 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-03-18 09:21:52,694 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-03-18 09:21:52,703 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-03-18 09:21:52,722 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServer: 2b374f74-8596-4317-81ba-ce0a491ddd96: found a subdirectory /data/metadata/ratis/c7fe038c-d6a9-4840-a081-bf56057ff98f
dn2_1    | 2023-03-18 09:21:52,762 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServer: 2b374f74-8596-4317-81ba-ce0a491ddd96: addNew group-BF56057FF98F:[] returns group-BF56057FF98F:java.util.concurrent.CompletableFuture@5aa11267[Not completed]
dn2_1    | 2023-03-18 09:21:52,917 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn2_1    | 2023-03-18 09:21:53,117 [pool-22-thread-1] INFO server.RaftServer$Division: 2b374f74-8596-4317-81ba-ce0a491ddd96: new RaftServerImpl for group-BF56057FF98F:[] with ContainerStateMachine:uninitialized
dn2_1    | 2023-03-18 09:21:53,137 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-03-18 09:21:53,146 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-03-18 09:21:53,171 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-03-18 09:21:53,171 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-03-18 09:21:53,172 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-03-18 09:21:53,176 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-03-18 09:21:53,430 [pool-22-thread-1] INFO server.RaftServer$Division: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-03-18 09:21:53,449 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-03-18 09:21:53,649 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-03-18 09:21:53,651 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-03-18 09:21:53,884 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-03-18 09:21:53,930 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-03-18 09:21:54,024 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-03-18 09:21:54,786 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-03-18 09:21:54,805 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-03-18 09:21:54,815 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-03-18 09:21:54,816 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-03-18 09:21:54,872 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-03-18 09:21:54,902 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn2_1    | 2023-03-18 09:21:55,027 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn2_1    | 2023-03-18 09:21:55,245 [main] INFO util.log: Logging initialized @37639ms to org.eclipse.jetty.util.log.Slf4jLog
dn2_1    | 2023-03-18 09:21:56,115 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn2_1    | 2023-03-18 09:21:56,178 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn2_1    | 2023-03-18 09:21:56,203 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn2_1    | 2023-03-18 09:21:56,222 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn2_1    | 2023-03-18 09:21:56,223 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn2_1    | 2023-03-18 09:21:56,223 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn2_1    | 2023-03-18 09:21:56,466 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn2_1    | 2023-03-18 09:21:56,549 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn2_1    | 2023-03-18 09:21:56,554 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn2_1    | 2023-03-18 09:21:56,729 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn2_1    | 2023-03-18 09:21:56,730 [main] INFO server.session: No SessionScavenger set, using defaults
dn2_1    | 2023-03-18 09:21:56,738 [main] INFO server.session: node0 Scavenging every 660000ms
dn2_1    | 2023-03-18 09:21:56,861 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@13250132{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn2_1    | 2023-03-18 09:21:56,867 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3c28e5b6{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn2_1    | 2023-03-18 09:21:58,120 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@48e41b5d{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-1069709545112496168/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn2_1    | 2023-03-18 09:21:58,263 [main] INFO server.AbstractConnector: Started ServerConnector@560271a1{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn2_1    | 2023-03-18 09:21:58,269 [main] INFO server.Server: Started @40662ms
dn2_1    | 2023-03-18 09:21:58,300 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn2_1    | 2023-03-18 09:21:58,300 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn2_1    | 2023-03-18 09:21:58,304 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn2_1    | 2023-03-18 09:21:58,312 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn2_1    | 2023-03-18 09:21:58,408 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn1_1    | 2023-03-18 09:22:05,705 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-03-18 09:22:05,763 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-03-18 09:22:05,781 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-03-18 09:22:05,800 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/67747242-de40-4a6c-83d8-e43a5a79ab67
dn1_1    | 2023-03-18 09:22:05,807 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-03-18 09:22:05,808 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-03-18 09:22:05,809 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-03-18 09:22:05,809 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-03-18 09:22:05,818 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-03-18 09:22:05,819 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-03-18 09:22:05,821 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-03-18 09:22:05,822 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-03-18 09:22:05,854 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-03-18 09:22:05,858 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-03-18 09:22:05,878 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-03-18 09:22:05,916 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-03-18 09:22:05,956 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-03-18 09:22:06,171 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67: set configuration 0: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-18 09:22:06,201 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/67747242-de40-4a6c-83d8-e43a5a79ab67/current/log_0-0
dn1_1    | 2023-03-18 09:22:06,229 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67: set configuration 1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-18 09:22:06,283 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/67747242-de40-4a6c-83d8-e43a5a79ab67/current/log_1-2
dn1_1    | 2023-03-18 09:22:06,294 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67: set configuration 3: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-18 09:22:06,316 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/67747242-de40-4a6c-83d8-e43a5a79ab67/current/log_inprogress_3
dn1_1    | 2023-03-18 09:22:06,329 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn1_1    | 2023-03-18 09:22:06,342 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn1_1    | 2023-03-18 09:22:06,838 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO raftlog.RaftLog: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-SegmentedRaftLog: commitIndex: updateToMax old=4, new=3, updated? false
dn1_1    | 2023-03-18 09:22:06,839 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67: start as a follower, conf=3: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-18 09:22:06,840 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn1_1    | 2023-03-18 09:22:06,844 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO impl.RoleInfo: 4ba198f1-a013-49ce-b059-f73788d69001: start 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-FollowerState
dn1_1    | 2023-03-18 09:22:06,847 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-03-18 09:22:06,852 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-03-18 09:22:06,872 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-E43A5A79AB67,id=4ba198f1-a013-49ce-b059-f73788d69001
dn1_1    | 2023-03-18 09:22:06,874 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-03-18 09:22:06,875 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-03-18 09:22:06,875 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-03-18 09:22:06,876 [4ba198f1-a013-49ce-b059-f73788d69001-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-03-18 09:22:06,879 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 4ba198f1-a013-49ce-b059-f73788d69001: start RPC server
dn1_1    | 2023-03-18 09:22:06,902 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 4ba198f1-a013-49ce-b059-f73788d69001: GrpcService started, listening on 9858
dn1_1    | 2023-03-18 09:22:06,903 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 4ba198f1-a013-49ce-b059-f73788d69001: GrpcService started, listening on 9856
dn1_1    | 2023-03-18 09:22:06,906 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 4ba198f1-a013-49ce-b059-f73788d69001: GrpcService started, listening on 9857
dn1_1    | 2023-03-18 09:22:06,926 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 4ba198f1-a013-49ce-b059-f73788d69001 is started using port 9858 for RATIS
dn1_1    | 2023-03-18 09:22:06,927 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 4ba198f1-a013-49ce-b059-f73788d69001 is started using port 9857 for RATIS_ADMIN
dn1_1    | 2023-03-18 09:22:06,927 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 4ba198f1-a013-49ce-b059-f73788d69001 is started using port 9856 for RATIS_SERVER
dn1_1    | 2023-03-18 09:22:06,928 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-4ba198f1-a013-49ce-b059-f73788d69001: Started
dn1_1    | 2023-03-18 09:22:07,003 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-03-18 09:22:12,031 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-FollowerState] INFO impl.FollowerState: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5186717010ns, electionTimeout:5177ms
dn1_1    | 2023-03-18 09:22:12,032 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-FollowerState] INFO impl.RoleInfo: 4ba198f1-a013-49ce-b059-f73788d69001: shutdown 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-FollowerState
dn1_1    | 2023-03-18 09:22:12,032 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-FollowerState] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn1_1    | 2023-03-18 09:22:12,034 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-03-18 09:22:12,035 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-FollowerState] INFO impl.RoleInfo: 4ba198f1-a013-49ce-b059-f73788d69001: start 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1
dn1_1    | 2023-03-18 09:22:12,041 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1] INFO impl.LeaderElection: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-18 09:22:12,042 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1] INFO impl.LeaderElection: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1 PRE_VOTE round 0: result PASSED (term=3)
dn1_1    | 2023-03-18 09:22:12,085 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1] INFO impl.LeaderElection: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1 ELECTION round 0: submit vote requests at term 4 for 3: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-18 09:22:12,085 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1] INFO impl.LeaderElection: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1 ELECTION round 0: result PASSED (term=4)
dn1_1    | 2023-03-18 09:22:12,086 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1] INFO impl.RoleInfo: 4ba198f1-a013-49ce-b059-f73788d69001: shutdown 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1
dn1_1    | 2023-03-18 09:22:12,086 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn1_1    | 2023-03-18 09:22:12,088 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-E43A5A79AB67 with new leaderId: 4ba198f1-a013-49ce-b059-f73788d69001
dn1_1    | 2023-03-18 09:22:12,093 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67: change Leader from null to 4ba198f1-a013-49ce-b059-f73788d69001 at term 4 for becomeLeader, leader elected after 18465ms
dn1_1    | 2023-03-18 09:22:12,123 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn1_1    | 2023-03-18 09:22:12,145 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-03-18 09:22:12,148 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn1_1    | 2023-03-18 09:22:12,164 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn1_1    | 2023-03-18 09:22:12,167 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn1_1    | 2023-03-18 09:22:12,168 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn1_1    | 2023-03-18 09:22:12,195 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-03-18 09:22:12,202 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn1_1    | 2023-03-18 09:22:12,218 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1] INFO impl.RoleInfo: 4ba198f1-a013-49ce-b059-f73788d69001: start 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderStateImpl
dn1_1    | 2023-03-18 09:22:12,261 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn1_1    | 2023-03-18 09:22:12,276 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderElection1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67: set configuration 5: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn3_1    | 2023-03-18 09:21:24,180 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn3_1    | /************************************************************
dn3_1    | STARTUP_MSG: Starting HddsDatanodeService
dn3_1    | STARTUP_MSG:   host = ac7a47589fa9/10.9.0.17
dn3_1    | STARTUP_MSG:   args = []
dn3_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn4_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn4_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn4_1    | 2023-03-18 09:21:27,263 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn4_1    | /************************************************************
dn4_1    | STARTUP_MSG: Starting HddsDatanodeService
dn4_1    | STARTUP_MSG:   host = 23f39be4c8d3/10.9.0.18
dn4_1    | STARTUP_MSG:   args = []
dn4_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn4_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn4_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/1236414c1c46c67c92f37499b0b61921cdfab39a ; compiled by 'runner' on 2023-03-18T08:55Z
dn4_1    | STARTUP_MSG:   java = 11.0.14.1
dn2_1    | 2023-03-18 09:21:58,409 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn2_1    | 2023-03-18 09:21:58,445 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@70aa17a8] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn2_1    | 2023-03-18 09:21:58,826 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.20:9891
dn2_1    | 2023-03-18 09:21:58,893 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn2_1    | 2023-03-18 09:22:01,851 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-03-18 09:22:04,070 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn2_1    | 2023-03-18 09:22:04,100 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn2_1    | 2023-03-18 09:22:04,568 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn2_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:321)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	... 1 more
dn2_1    | 2023-03-18 09:22:04,576 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn2_1    | 2023-03-18 09:22:04,590 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 2b374f74-8596-4317-81ba-ce0a491ddd96
dn2_1    | 2023-03-18 09:22:04,648 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/c7fe038c-d6a9-4840-a081-bf56057ff98f/in_use.lock acquired by nodename 7@b27b4bae5a17
dn2_1    | 2023-03-18 09:22:04,679 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=2b374f74-8596-4317-81ba-ce0a491ddd96} from /data/metadata/ratis/c7fe038c-d6a9-4840-a081-bf56057ff98f/current/raft-meta
dn2_1    | 2023-03-18 09:22:04,906 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServer$Division: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F: set configuration 3: peers:[2b374f74-8596-4317-81ba-ce0a491ddd96|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-18 09:22:04,950 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO ratis.ContainerStateMachine: group-BF56057FF98F: Setting the last applied index to (t:3, i:4)
dn2_1    | 2023-03-18 09:22:05,903 [EndpointStateMachine task thread for recon/10.9.0.20:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From b27b4bae5a17/10.9.0.16 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.16:59448 remote=recon/10.9.0.20:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn2_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.16:59448 remote=recon/10.9.0.20:9891]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/1236414c1c46c67c92f37499b0b61921cdfab39a ; compiled by 'runner' on 2023-03-18T08:55Z
dn3_1    | STARTUP_MSG:   java = 11.0.14.1
dn1_1    | 2023-03-18 09:22:12,292 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/67747242-de40-4a6c-83d8-e43a5a79ab67/current/log_inprogress_3 to /data/metadata/ratis/67747242-de40-4a6c-83d8-e43a5a79ab67/current/log_3-4
dn1_1    | 2023-03-18 09:22:12,316 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/67747242-de40-4a6c-83d8-e43a5a79ab67/current/log_inprogress_5
dn1_1    | 2023-03-18 09:23:07,013 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-03-18 09:23:13,095 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn1_1    | 2023-03-18 09:23:13,097 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn1_1    | 2023-03-18 09:23:13,097 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn1_1    | 2023-03-18 09:23:13,098 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: ERASURE_CODED_STORAGE_SUPPORT.
dn1_1    | 2023-03-18 09:23:13,100 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature ERASURE_CODED_STORAGE_SUPPORT has been finalized.
dn1_1    | 2023-03-18 09:23:13,100 [Command processor thread] INFO upgrade.UpgradeFinalizer: Running finalization actions for layout feature: DATANODE_SCHEMA_V3
dn1_1    | 2023-03-18 09:23:13,100 [Command processor thread] INFO upgrade.DatanodeSchemaV3FinalizeAction: Upgrading Datanode volume layout for Schema V3 support.
dn1_1    | 2023-03-18 09:23:13,143 [Command processor thread] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-78ab005c-03ae-4832-95ae-7f7875f735a7/container.db to cache
dn1_1    | 2023-03-18 09:23:13,143 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-78ab005c-03ae-4832-95ae-7f7875f735a7/container.db for volume DS-78ab005c-03ae-4832-95ae-7f7875f735a7
dn1_1    | 2023-03-18 09:23:13,148 [Command processor thread] INFO utils.DatanodeStoreCache: Removed db /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-78ab005c-03ae-4832-95ae-7f7875f735a7/container.db from cache
dn1_1    | 2023-03-18 09:23:13,149 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is stopped at /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-78ab005c-03ae-4832-95ae-7f7875f735a7/container.db for volume DS-78ab005c-03ae-4832-95ae-7f7875f735a7
dn1_1    | 2023-03-18 09:23:13,201 [Command processor thread] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-78ab005c-03ae-4832-95ae-7f7875f735a7/container.db to cache
dn1_1    | 2023-03-18 09:23:13,201 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-78ab005c-03ae-4832-95ae-7f7875f735a7/container.db for volume DS-78ab005c-03ae-4832-95ae-7f7875f735a7
dn1_1    | 2023-03-18 09:23:13,202 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature DATANODE_SCHEMA_V3 has been finalized.
dn1_1    | 2023-03-18 09:23:13,203 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn1_1    | 2023-03-18 09:23:13,203 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn1_1    | 2023-03-18 09:23:13,213 [Command processor thread] INFO server.RaftServer: 4ba198f1-a013-49ce-b059-f73788d69001: remove    LEADER 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67:t4, leader=4ba198f1-a013-49ce-b059-f73788d69001, voted=4ba198f1-a013-49ce-b059-f73788d69001, raftlog=Memoized:4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-SegmentedRaftLog:OPENED:c6, conf=5: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn1_1    | 2023-03-18 09:23:13,215 [Command processor thread] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67: shutdown
dn1_1    | 2023-03-18 09:23:13,216 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-E43A5A79AB67,id=4ba198f1-a013-49ce-b059-f73788d69001
dn1_1    | 2023-03-18 09:23:13,216 [Command processor thread] INFO impl.RoleInfo: 4ba198f1-a013-49ce-b059-f73788d69001: shutdown 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-LeaderStateImpl
dn1_1    | 2023-03-18 09:23:13,221 [Command processor thread] INFO impl.PendingRequests: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-PendingRequests: sendNotLeaderResponses
dn1_1    | 2023-03-18 09:23:13,223 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-E43A5A79AB67: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/67747242-de40-4a6c-83d8-e43a5a79ab67/sm/snapshot.4_6
dn1_1    | 2023-03-18 09:23:13,224 [Command processor thread] INFO impl.StateMachineUpdater: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-StateMachineUpdater: set stopIndex = 6
dn1_1    | 2023-03-18 09:23:13,225 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-E43A5A79AB67: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/67747242-de40-4a6c-83d8-e43a5a79ab67/sm/snapshot.4_6 took: 1 ms
dn1_1    | 2023-03-18 09:23:13,226 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-StateMachineUpdater] INFO impl.StateMachineUpdater: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-StateMachineUpdater: Took a snapshot at index 6
dn1_1    | 2023-03-18 09:23:13,226 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-StateMachineUpdater] INFO impl.StateMachineUpdater: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
dn1_1    | 2023-03-18 09:23:13,228 [Command processor thread] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67: closes. applyIndex: 6
dn1_1    | 2023-03-18 09:23:13,230 [4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn1_1    | 2023-03-18 09:23:13,232 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67-SegmentedRaftLogWorker close()
dn1_1    | 2023-03-18 09:23:13,236 [Command processor thread] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-E43A5A79AB67: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/67747242-de40-4a6c-83d8-e43a5a79ab67
dn1_1    | 2023-03-18 09:23:13,238 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=67747242-de40-4a6c-83d8-e43a5a79ab67 command on datanode 4ba198f1-a013-49ce-b059-f73788d69001.
dn1_1    | 2023-03-18 09:23:43,095 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn1_1    | 2023-03-18 09:23:52,255 [pool-22-thread-1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001: new RaftServerImpl for group-6F7919707118:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn1_1    | 2023-03-18 09:23:52,255 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-03-18 09:23:52,255 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-03-18 09:23:52,255 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-03-18 09:23:52,256 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-03-18 09:23:52,256 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-03-18 09:23:52,256 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-03-18 09:23:52,256 [pool-22-thread-1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118: ConfigurationManager, init=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-03-18 09:23:52,256 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-03-18 09:23:52,258 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-03-18 09:23:52,258 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-03-18 09:23:52,258 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-03-18 09:23:52,258 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-03-18 09:23:52,258 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-03-18 09:23:52,259 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-03-18 09:23:52,259 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-03-18 09:23:52,259 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-03-18 09:23:52,260 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-03-18 09:23:52,260 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-03-18 09:23:52,260 [grpc-default-executor-1] INFO server.RaftServer: 4ba198f1-a013-49ce-b059-f73788d69001: addNew group-6F7919707118:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER] returns      null 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118:t0, leader=null, voted=null, raftlog=UNINITIALIZED, conf=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null NEW
dn1_1    | 2023-03-18 09:23:52,268 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/66de196e-0a1c-4e0b-be4e-6f7919707118 does not exist. Creating ...
dn1_1    | 2023-03-18 09:23:52,270 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/66de196e-0a1c-4e0b-be4e-6f7919707118/in_use.lock acquired by nodename 7@8698bf9a31d9
dn1_1    | 2023-03-18 09:23:52,271 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/66de196e-0a1c-4e0b-be4e-6f7919707118 has been successfully formatted.
dn1_1    | 2023-03-18 09:23:52,272 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-6F7919707118: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn1_1    | 2023-03-18 09:23:52,272 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-03-18 09:23:52,272 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-03-18 09:23:52,274 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-03-18 09:23:52,274 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-03-18 09:23:52,274 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-03-18 09:23:52,275 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-03-18 09:23:52,275 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-03-18 09:23:52,275 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-03-18 09:23:52,275 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/66de196e-0a1c-4e0b-be4e-6f7919707118
dn1_1    | 2023-03-18 09:23:52,275 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-03-18 09:23:52,275 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-03-18 09:23:52,276 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-03-18 09:23:52,276 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-03-18 09:23:52,276 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-03-18 09:23:52,276 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-03-18 09:23:52,276 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-03-18 09:23:52,276 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-03-18 09:23:52,277 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=1440, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4))$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn4_1    | ************************************************************/
dn4_1    | 2023-03-18 09:21:27,344 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn4_1    | 2023-03-18 09:21:27,706 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn4_1    | 2023-03-18 09:21:28,361 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn4_1    | 2023-03-18 09:21:29,529 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn4_1    | 2023-03-18 09:21:29,530 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn4_1    | 2023-03-18 09:21:30,579 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:23f39be4c8d3 ip:10.9.0.18
dn4_1    | 2023-03-18 09:21:32,604 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn4_1    | 2023-03-18 09:21:34,037 [main] INFO reflections.Reflections: Reflections took 1125 ms to scan 2 urls, producing 103 keys and 227 values 
dn4_1    | 2023-03-18 09:21:34,318 [main] INFO upgrade.HDDSLayoutVersionManager: Registering Upgrade Action : DatanodeSchemaV3FinalizeAction
dn4_1    | 2023-03-18 09:21:34,640 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
dn4_1    | 2023-03-18 09:21:35,892 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 8414 at 2023-03-18T09:21:04.603Z
dn4_1    | 2023-03-18 09:21:36,011 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn4_1    | 2023-03-18 09:21:36,013 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn4_1    | 2023-03-18 09:21:36,013 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn4_1    | 2023-03-18 09:21:36,170 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn4_1    | 2023-03-18 09:21:36,364 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn4_1    | 2023-03-18 09:21:36,401 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-03-18T09:21:04.604Z
dn4_1    | 2023-03-18 09:21:36,404 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn4_1    | 2023-03-18 09:21:36,404 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn4_1    | 2023-03-18 09:21:36,439 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn4_1    | 2023-03-18 09:21:36,581 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn4_1    | 2023-03-18 09:21:37,471 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn4_1    | 2023-03-18 09:21:37,492 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn4_1    | 2023-03-18 09:21:48,908 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn4_1    | 2023-03-18 09:21:49,404 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn4_1    | 2023-03-18 09:21:50,187 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn4_1    | 2023-03-18 09:21:50,784 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn4_1    | 2023-03-18 09:21:50,797 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn4_1    | 2023-03-18 09:21:50,800 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn4_1    | 2023-03-18 09:21:50,808 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn4_1    | 2023-03-18 09:21:50,812 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn4_1    | 2023-03-18 09:21:50,812 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn4_1    | 2023-03-18 09:21:50,813 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn4_1    | 2023-03-18 09:21:50,870 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-18 09:21:50,889 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn4_1    | 2023-03-18 09:21:50,912 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-03-18 09:21:51,000 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-03-18 09:21:51,025 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn4_1    | 2023-03-18 09:21:51,042 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn4_1    | 2023-03-18 09:21:52,830 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn4_1    | 2023-03-18 09:21:52,848 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn4_1    | 2023-03-18 09:21:52,865 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn4_1    | 2023-03-18 09:21:52,865 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-03-18 09:21:52,868 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-03-18 09:21:52,875 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-03-18 09:21:52,891 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServer: 7c9e7a7d-2593-458c-8a46-0313d77c3278: found a subdirectory /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c
dn4_1    | 2023-03-18 09:21:52,929 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServer: 7c9e7a7d-2593-458c-8a46-0313d77c3278: addNew group-80313208139C:[] returns group-80313208139C:java.util.concurrent.CompletableFuture@2caff2bd[Not completed]
dn4_1    | 2023-03-18 09:21:52,930 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServer: 7c9e7a7d-2593-458c-8a46-0313d77c3278: found a subdirectory /data/metadata/ratis/a2f44939-53c6-4b4c-afd2-40b9ce60bd6c
dn4_1    | 2023-03-18 09:21:52,934 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServer: 7c9e7a7d-2593-458c-8a46-0313d77c3278: addNew group-40B9CE60BD6C:[] returns group-40B9CE60BD6C:java.util.concurrent.CompletableFuture@4480ba5a[Not completed]
dn4_1    | 2023-03-18 09:21:52,934 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServer: 7c9e7a7d-2593-458c-8a46-0313d77c3278: found a subdirectory /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d
dn4_1    | 2023-03-18 09:21:52,934 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServer: 7c9e7a7d-2593-458c-8a46-0313d77c3278: addNew group-8191D6D2324D:[] returns group-8191D6D2324D:java.util.concurrent.CompletableFuture@11977747[Not completed]
dn4_1    | 2023-03-18 09:21:53,062 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn4_1    | 2023-03-18 09:21:53,352 [pool-38-thread-1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278: new RaftServerImpl for group-80313208139C:[] with ContainerStateMachine:uninitialized
dn4_1    | 2023-03-18 09:21:53,427 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-03-18 09:21:53,463 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-03-18 09:21:53,463 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-03-18 09:21:53,464 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-03-18 09:21:53,464 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-03-18 09:21:53,464 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-03-18 09:21:53,560 [pool-38-thread-1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-03-18 09:21:53,573 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-03-18 09:21:53,681 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-03-18 09:21:53,695 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-03-18 09:21:53,950 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-03-18 09:21:54,006 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-03-18 09:21:54,012 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-03-18 09:21:54,868 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-03-18 09:21:54,869 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn4_1    | 2023-03-18 09:21:54,884 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-03-18 09:21:54,921 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-03-18 09:21:54,921 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-03-18 09:21:54,922 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-03-18 09:21:54,936 [pool-38-thread-1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278: new RaftServerImpl for group-40B9CE60BD6C:[] with ContainerStateMachine:uninitialized
dn4_1    | 2023-03-18 09:21:54,937 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-03-18 09:21:54,937 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-03-18 09:21:54,937 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-03-18 09:21:54,937 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-03-18 09:21:54,937 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn2_1    | 2023-03-18 09:22:06,065 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-03-18 09:22:06,133 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-03-18 09:22:06,143 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-03-18 09:22:06,156 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-03-18 09:22:06,164 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-03-18 09:22:06,200 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-03-18 09:22:06,249 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-03-18 09:22:06,251 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-03-18 09:22:06,291 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/c7fe038c-d6a9-4840-a081-bf56057ff98f
dn2_1    | 2023-03-18 09:22:06,305 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-03-18 09:22:06,305 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-03-18 09:22:06,306 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-03-18 09:22:06,310 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-03-18 09:22:06,313 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-03-18 09:22:06,314 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-03-18 09:22:06,316 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-03-18 09:22:06,319 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-03-18 09:22:06,431 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-03-18 09:22:06,441 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-03-18 09:22:06,517 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-03-18 09:22:06,530 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-03-18 09:22:06,568 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-03-18 09:22:06,792 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServer$Division: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F: set configuration 0: peers:[2b374f74-8596-4317-81ba-ce0a491ddd96|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-18 09:22:06,800 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/c7fe038c-d6a9-4840-a081-bf56057ff98f/current/log_0-0
dn2_1    | 2023-03-18 09:22:06,825 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServer$Division: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F: set configuration 1: peers:[2b374f74-8596-4317-81ba-ce0a491ddd96|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-18 09:22:06,842 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/c7fe038c-d6a9-4840-a081-bf56057ff98f/current/log_1-2
dn2_1    | 2023-03-18 09:22:06,852 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServer$Division: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F: set configuration 3: peers:[2b374f74-8596-4317-81ba-ce0a491ddd96|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-18 09:22:06,857 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/c7fe038c-d6a9-4840-a081-bf56057ff98f/current/log_inprogress_3
dn2_1    | 2023-03-18 09:22:06,863 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn2_1    | 2023-03-18 09:22:06,864 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn2_1    | 2023-03-18 09:22:08,240 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO raftlog.RaftLog: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-SegmentedRaftLog: commitIndex: updateToMax old=4, new=3, updated? false
dn2_1    | 2023-03-18 09:22:08,244 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServer$Division: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F: start as a follower, conf=3: peers:[2b374f74-8596-4317-81ba-ce0a491ddd96|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-18 09:22:08,246 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServer$Division: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn3_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=1440, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4))$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn3_1    | ************************************************************/
dn3_1    | 2023-03-18 09:21:24,252 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn3_1    | 2023-03-18 09:21:24,589 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn3_1    | 2023-03-18 09:21:25,237 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn3_1    | 2023-03-18 09:21:27,196 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn3_1    | 2023-03-18 09:21:27,196 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn3_1    | 2023-03-18 09:21:28,257 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:ac7a47589fa9 ip:10.9.0.17
dn3_1    | 2023-03-18 09:21:30,349 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn3_1    | 2023-03-18 09:21:31,861 [main] INFO reflections.Reflections: Reflections took 1260 ms to scan 2 urls, producing 103 keys and 227 values 
dn3_1    | 2023-03-18 09:21:32,381 [main] INFO upgrade.HDDSLayoutVersionManager: Registering Upgrade Action : DatanodeSchemaV3FinalizeAction
dn3_1    | 2023-03-18 09:21:32,570 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
dn3_1    | 2023-03-18 09:21:33,705 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 8414 at 2023-03-18T09:21:04.672Z
dn3_1    | 2023-03-18 09:21:33,784 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn3_1    | 2023-03-18 09:21:33,789 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn3_1    | 2023-03-18 09:21:33,817 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn3_1    | 2023-03-18 09:21:34,023 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn3_1    | 2023-03-18 09:21:34,184 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-03-18 09:21:34,217 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-03-18T09:21:04.693Z
dn3_1    | 2023-03-18 09:21:34,264 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn3_1    | 2023-03-18 09:21:34,265 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn3_1    | 2023-03-18 09:21:34,277 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn3_1    | 2023-03-18 09:21:34,477 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn3_1    | 2023-03-18 09:21:35,728 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn3_1    | 2023-03-18 09:21:35,729 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 1s
dn3_1    | 2023-03-18 09:21:47,125 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn3_1    | 2023-03-18 09:21:47,682 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-03-18 09:21:48,685 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn3_1    | 2023-03-18 09:21:49,314 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-03-18 09:21:49,325 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn3_1    | 2023-03-18 09:21:49,352 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-03-18 09:21:49,352 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn3_1    | 2023-03-18 09:21:49,353 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn3_1    | 2023-03-18 09:21:49,353 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn3_1    | 2023-03-18 09:21:49,353 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn3_1    | 2023-03-18 09:21:49,396 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-03-18 09:21:49,400 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn3_1    | 2023-03-18 09:21:49,405 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-03-18 09:21:49,469 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-03-18 09:21:49,510 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn3_1    | 2023-03-18 09:21:49,512 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn3_1    | 2023-03-18 09:21:50,928 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn3_1    | 2023-03-18 09:21:50,930 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn3_1    | 2023-03-18 09:21:50,939 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn3_1    | 2023-03-18 09:21:50,940 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-03-18 09:21:50,940 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-03-18 09:21:50,943 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-03-18 09:21:50,972 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServer: bae0cd8d-e50f-4d23-8974-b816860f8fc3: found a subdirectory /data/metadata/ratis/d1279162-045f-45cf-95f2-d1111d92119b
dn3_1    | 2023-03-18 09:21:51,107 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServer: bae0cd8d-e50f-4d23-8974-b816860f8fc3: addNew group-D1111D92119B:[] returns group-D1111D92119B:java.util.concurrent.CompletableFuture@3b50fd98[Not completed]
dn3_1    | 2023-03-18 09:21:51,107 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServer: bae0cd8d-e50f-4d23-8974-b816860f8fc3: found a subdirectory /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c
dn3_1    | 2023-03-18 09:21:51,111 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServer: bae0cd8d-e50f-4d23-8974-b816860f8fc3: addNew group-80313208139C:[] returns group-80313208139C:java.util.concurrent.CompletableFuture@1918cda8[Not completed]
dn3_1    | 2023-03-18 09:21:51,111 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServer: bae0cd8d-e50f-4d23-8974-b816860f8fc3: found a subdirectory /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d
dn3_1    | 2023-03-18 09:21:51,111 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServer: bae0cd8d-e50f-4d23-8974-b816860f8fc3: addNew group-8191D6D2324D:[] returns group-8191D6D2324D:java.util.concurrent.CompletableFuture@333ff6ba[Not completed]
dn3_1    | 2023-03-18 09:21:51,376 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn3_1    | 2023-03-18 09:21:51,757 [pool-38-thread-1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3: new RaftServerImpl for group-D1111D92119B:[] with ContainerStateMachine:uninitialized
dn4_1    | 2023-03-18 09:21:54,937 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-03-18 09:21:54,938 [pool-38-thread-1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-03-18 09:21:54,938 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-03-18 09:21:54,946 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-03-18 09:21:54,946 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-03-18 09:21:54,947 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-03-18 09:21:54,947 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-03-18 09:21:54,947 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-03-18 09:21:55,016 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-03-18 09:21:55,016 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-03-18 09:21:55,052 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-03-18 09:21:55,056 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-03-18 09:21:55,056 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-03-18 09:21:55,059 [pool-38-thread-1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278: new RaftServerImpl for group-8191D6D2324D:[] with ContainerStateMachine:uninitialized
dn4_1    | 2023-03-18 09:21:55,065 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-03-18 09:21:55,065 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-03-18 09:21:55,067 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-03-18 09:21:55,067 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-03-18 09:21:55,068 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-03-18 09:21:55,068 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-03-18 09:21:55,068 [pool-38-thread-1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-03-18 09:21:55,071 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-03-18 09:21:55,078 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-03-18 09:21:55,079 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-03-18 09:21:55,080 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-03-18 09:21:55,081 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-03-18 09:21:55,081 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-03-18 09:21:55,096 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn4_1    | 2023-03-18 09:21:55,100 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-03-18 09:21:55,118 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-03-18 09:21:55,120 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-03-18 09:21:55,120 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-03-18 09:21:55,120 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-03-18 09:21:55,428 [main] INFO util.log: Logging initialized @38349ms to org.eclipse.jetty.util.log.Slf4jLog
dn4_1    | 2023-03-18 09:21:56,310 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn4_1    | 2023-03-18 09:21:56,380 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn4_1    | 2023-03-18 09:21:56,432 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn4_1    | 2023-03-18 09:21:56,468 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn4_1    | 2023-03-18 09:21:56,468 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn4_1    | 2023-03-18 09:21:56,469 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn4_1    | 2023-03-18 09:21:56,749 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn4_1    | 2023-03-18 09:21:56,786 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn4_1    | 2023-03-18 09:21:56,787 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn4_1    | 2023-03-18 09:21:57,209 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn4_1    | 2023-03-18 09:21:57,209 [main] INFO server.session: No SessionScavenger set, using defaults
dn4_1    | 2023-03-18 09:21:57,240 [main] INFO server.session: node0 Scavenging every 600000ms
dn4_1    | 2023-03-18 09:21:57,336 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4ed18798{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn4_1    | 2023-03-18 09:21:57,352 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5d71b500{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn4_1    | 2023-03-18 09:21:58,241 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@236c098{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-5796207737848137091/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn4_1    | 2023-03-18 09:21:58,321 [main] INFO server.AbstractConnector: Started ServerConnector@76cdafa3{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn4_1    | 2023-03-18 09:21:58,326 [main] INFO server.Server: Started @41247ms
dn4_1    | 2023-03-18 09:21:58,340 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn4_1    | 2023-03-18 09:21:58,342 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn4_1    | 2023-03-18 09:21:58,350 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn4_1    | 2023-03-18 09:21:58,387 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn4_1    | 2023-03-18 09:21:58,458 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn4_1    | 2023-03-18 09:21:58,459 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn4_1    | 2023-03-18 09:21:58,474 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1b18d4c4] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn4_1    | 2023-03-18 09:21:58,884 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.20:9891
dn4_1    | 2023-03-18 09:21:59,005 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn4_1    | 2023-03-18 09:22:01,722 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-03-18 09:22:03,945 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn4_1    | 2023-03-18 09:22:03,976 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn3_1    | 2023-03-18 09:21:51,802 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-03-18 09:21:51,802 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-03-18 09:21:51,808 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-03-18 09:21:51,809 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-03-18 09:21:51,809 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-03-18 09:21:51,812 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-03-18 09:21:51,945 [pool-38-thread-1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-03-18 09:21:51,957 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-03-18 09:21:52,059 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-03-18 09:21:52,068 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-03-18 09:21:52,199 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-03-18 09:21:52,258 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-03-18 09:21:52,292 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-03-18 09:21:53,052 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-03-18 09:21:53,070 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-03-18 09:21:53,071 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-03-18 09:21:53,079 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-03-18 09:21:53,083 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-03-18 09:21:53,110 [pool-38-thread-1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3: new RaftServerImpl for group-80313208139C:[] with ContainerStateMachine:uninitialized
dn3_1    | 2023-03-18 09:21:53,122 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-03-18 09:21:53,142 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-03-18 09:21:53,148 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-03-18 09:21:53,148 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-03-18 09:21:53,156 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-03-18 09:21:53,157 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-03-18 09:21:53,157 [pool-38-thread-1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-03-18 09:21:53,157 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-03-18 09:21:53,161 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-03-18 09:21:53,167 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-03-18 09:21:53,171 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-03-18 09:21:53,176 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-03-18 09:21:53,161 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn3_1    | 2023-03-18 09:21:53,191 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-03-18 09:21:53,194 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-03-18 09:21:53,221 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-03-18 09:21:53,221 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-03-18 09:21:53,232 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-03-18 09:21:53,232 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-03-18 09:21:53,256 [pool-38-thread-1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3: new RaftServerImpl for group-8191D6D2324D:[] with ContainerStateMachine:uninitialized
dn3_1    | 2023-03-18 09:21:53,274 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-03-18 09:21:53,275 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-03-18 09:21:53,283 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-03-18 09:21:53,290 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-03-18 09:21:53,292 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-03-18 09:23:52,277 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-03-18 09:23:52,311 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-03-18 09:23:52,311 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-03-18 09:23:52,312 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-03-18 09:23:52,312 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-03-18 09:23:52,313 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-03-18 09:23:52,346 [pool-22-thread-1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118: start as a follower, conf=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-18 09:23:52,347 [pool-22-thread-1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn1_1    | 2023-03-18 09:23:52,347 [pool-22-thread-1] INFO impl.RoleInfo: 4ba198f1-a013-49ce-b059-f73788d69001: start 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-FollowerState
dn1_1    | 2023-03-18 09:23:52,372 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6F7919707118,id=4ba198f1-a013-49ce-b059-f73788d69001
dn1_1    | 2023-03-18 09:23:52,373 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-03-18 09:23:52,373 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-03-18 09:23:52,373 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-03-18 09:23:52,373 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-03-18 09:23:52,423 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-03-18 09:23:52,425 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-03-18 09:23:52,981 [grpc-default-executor-0] INFO server.RaftServer: 4ba198f1-a013-49ce-b059-f73788d69001: addNew group-CCB5F87F8193:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] returns group-CCB5F87F8193:java.util.concurrent.CompletableFuture@77d8a8c7[Not completed]
dn1_1    | 2023-03-18 09:23:52,983 [pool-22-thread-1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001: new RaftServerImpl for group-CCB5F87F8193:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn1_1    | 2023-03-18 09:23:52,985 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-03-18 09:23:52,985 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-03-18 09:23:52,986 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-03-18 09:23:52,986 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-03-18 09:23:52,986 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-03-18 09:23:52,987 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-03-18 09:23:52,988 [pool-22-thread-1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193: ConfigurationManager, init=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-03-18 09:23:52,990 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-03-18 09:23:52,991 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-03-18 09:23:52,992 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-03-18 09:23:52,992 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-03-18 09:23:52,992 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-03-18 09:23:52,993 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-03-18 09:23:52,997 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-03-18 09:23:52,997 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-03-18 09:23:52,997 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-03-18 09:23:52,997 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-03-18 09:23:52,997 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-03-18 09:23:52,997 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/84a08f9d-e905-447e-be3d-ccb5f87f8193 does not exist. Creating ...
dn2_1    | 2023-03-18 09:22:08,309 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO impl.RoleInfo: 2b374f74-8596-4317-81ba-ce0a491ddd96: start 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-FollowerState
dn2_1    | 2023-03-18 09:22:08,459 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-03-18 09:22:08,459 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-03-18 09:22:08,462 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-BF56057FF98F,id=2b374f74-8596-4317-81ba-ce0a491ddd96
dn2_1    | 2023-03-18 09:22:08,464 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-03-18 09:22:08,479 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-03-18 09:22:08,479 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-03-18 09:22:08,482 [2b374f74-8596-4317-81ba-ce0a491ddd96-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-03-18 09:22:08,533 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 2b374f74-8596-4317-81ba-ce0a491ddd96: start RPC server
dn2_1    | 2023-03-18 09:22:08,627 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 2b374f74-8596-4317-81ba-ce0a491ddd96: GrpcService started, listening on 9858
dn2_1    | 2023-03-18 09:22:08,630 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 2b374f74-8596-4317-81ba-ce0a491ddd96: GrpcService started, listening on 9856
dn2_1    | 2023-03-18 09:22:08,636 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 2b374f74-8596-4317-81ba-ce0a491ddd96: GrpcService started, listening on 9857
dn2_1    | 2023-03-18 09:22:08,676 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-2b374f74-8596-4317-81ba-ce0a491ddd96: Started
dn2_1    | 2023-03-18 09:22:08,681 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 2b374f74-8596-4317-81ba-ce0a491ddd96 is started using port 9858 for RATIS
dn2_1    | 2023-03-18 09:22:08,684 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 2b374f74-8596-4317-81ba-ce0a491ddd96 is started using port 9857 for RATIS_ADMIN
dn2_1    | 2023-03-18 09:22:08,684 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 2b374f74-8596-4317-81ba-ce0a491ddd96 is started using port 9856 for RATIS_SERVER
dn2_1    | 2023-03-18 09:22:08,799 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn2_1    | 2023-03-18 09:22:13,520 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-FollowerState] INFO impl.FollowerState: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5211374808ns, electionTimeout:5056ms
dn2_1    | 2023-03-18 09:22:13,608 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-FollowerState] INFO impl.RoleInfo: 2b374f74-8596-4317-81ba-ce0a491ddd96: shutdown 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-FollowerState
dn2_1    | 2023-03-18 09:22:13,618 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-FollowerState] INFO server.RaftServer$Division: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn2_1    | 2023-03-18 09:22:13,638 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn2_1    | 2023-03-18 09:22:13,640 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-FollowerState] INFO impl.RoleInfo: 2b374f74-8596-4317-81ba-ce0a491ddd96: start 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1
dn2_1    | 2023-03-18 09:22:13,681 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1] INFO impl.LeaderElection: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[2b374f74-8596-4317-81ba-ce0a491ddd96|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-18 09:22:13,682 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1] INFO impl.LeaderElection: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1 PRE_VOTE round 0: result PASSED (term=3)
dn2_1    | 2023-03-18 09:22:13,774 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1] INFO impl.LeaderElection: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1 ELECTION round 0: submit vote requests at term 4 for 3: peers:[2b374f74-8596-4317-81ba-ce0a491ddd96|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-18 09:22:13,777 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1] INFO impl.LeaderElection: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1 ELECTION round 0: result PASSED (term=4)
dn2_1    | 2023-03-18 09:22:13,777 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1] INFO impl.RoleInfo: 2b374f74-8596-4317-81ba-ce0a491ddd96: shutdown 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1
dn2_1    | 2023-03-18 09:22:13,779 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1] INFO server.RaftServer$Division: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn2_1    | 2023-03-18 09:22:13,780 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-BF56057FF98F with new leaderId: 2b374f74-8596-4317-81ba-ce0a491ddd96
dn2_1    | 2023-03-18 09:22:13,787 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1] INFO server.RaftServer$Division: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F: change Leader from null to 2b374f74-8596-4317-81ba-ce0a491ddd96 at term 4 for becomeLeader, leader elected after 19897ms
dn2_1    | 2023-03-18 09:22:13,854 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn2_1    | 2023-03-18 09:22:13,874 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-03-18 09:22:13,880 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn2_1    | 2023-03-18 09:22:13,894 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn2_1    | 2023-03-18 09:22:13,894 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1    | 2023-03-18 09:21:27,860 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1    | /************************************************************
om2_1    | STARTUP_MSG: Starting OzoneManager
om2_1    | STARTUP_MSG:   host = 7546b4869b99/10.9.0.12
om2_1    | STARTUP_MSG:   args = [--upgrade]
om2_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn2_1    | 2023-03-18 09:22:13,895 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn2_1    | 2023-03-18 09:22:13,907 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-03-18 09:22:13,913 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn2_1    | 2023-03-18 09:22:13,918 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1] INFO impl.RoleInfo: 2b374f74-8596-4317-81ba-ce0a491ddd96: start 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderStateImpl
dn2_1    | 2023-03-18 09:22:13,934 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn2_1    | 2023-03-18 09:22:13,948 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/c7fe038c-d6a9-4840-a081-bf56057ff98f/current/log_inprogress_3 to /data/metadata/ratis/c7fe038c-d6a9-4840-a081-bf56057ff98f/current/log_3-4
dn2_1    | 2023-03-18 09:22:13,958 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/c7fe038c-d6a9-4840-a081-bf56057ff98f/current/log_inprogress_5
dn2_1    | 2023-03-18 09:22:13,972 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderElection1] INFO server.RaftServer$Division: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F: set configuration 5: peers:[2b374f74-8596-4317-81ba-ce0a491ddd96|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-18 09:23:08,799 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn2_1    | 2023-03-18 09:23:14,783 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | 2023-03-18 09:23:14,784 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn2_1    | 2023-03-18 09:23:14,785 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn2_1    | 2023-03-18 09:23:14,785 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: ERASURE_CODED_STORAGE_SUPPORT.
dn2_1    | 2023-03-18 09:23:14,787 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature ERASURE_CODED_STORAGE_SUPPORT has been finalized.
dn2_1    | 2023-03-18 09:23:14,787 [Command processor thread] INFO upgrade.UpgradeFinalizer: Running finalization actions for layout feature: DATANODE_SCHEMA_V3
dn2_1    | 2023-03-18 09:23:14,787 [Command processor thread] INFO upgrade.DatanodeSchemaV3FinalizeAction: Upgrading Datanode volume layout for Schema V3 support.
dn2_1    | 2023-03-18 09:23:14,835 [Command processor thread] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-8e777586-f9ad-4dd9-8071-fea175485054/container.db to cache
dn2_1    | 2023-03-18 09:23:14,835 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-8e777586-f9ad-4dd9-8071-fea175485054/container.db for volume DS-8e777586-f9ad-4dd9-8071-fea175485054
dn2_1    | 2023-03-18 09:23:14,839 [Command processor thread] INFO utils.DatanodeStoreCache: Removed db /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-8e777586-f9ad-4dd9-8071-fea175485054/container.db from cache
dn2_1    | 2023-03-18 09:23:14,839 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is stopped at /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-8e777586-f9ad-4dd9-8071-fea175485054/container.db for volume DS-8e777586-f9ad-4dd9-8071-fea175485054
dn2_1    | 2023-03-18 09:23:14,863 [Command processor thread] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-8e777586-f9ad-4dd9-8071-fea175485054/container.db to cache
dn2_1    | 2023-03-18 09:23:14,863 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-8e777586-f9ad-4dd9-8071-fea175485054/container.db for volume DS-8e777586-f9ad-4dd9-8071-fea175485054
dn2_1    | 2023-03-18 09:23:14,864 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature DATANODE_SCHEMA_V3 has been finalized.
dn2_1    | 2023-03-18 09:23:14,864 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn2_1    | 2023-03-18 09:23:14,864 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn2_1    | 2023-03-18 09:23:14,874 [Command processor thread] INFO server.RaftServer: 2b374f74-8596-4317-81ba-ce0a491ddd96: remove    LEADER 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F:t4, leader=2b374f74-8596-4317-81ba-ce0a491ddd96, voted=2b374f74-8596-4317-81ba-ce0a491ddd96, raftlog=Memoized:2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-SegmentedRaftLog:OPENED:c6, conf=5: peers:[2b374f74-8596-4317-81ba-ce0a491ddd96|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn2_1    | 2023-03-18 09:23:14,876 [Command processor thread] INFO server.RaftServer$Division: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F: shutdown
dn2_1    | 2023-03-18 09:23:14,876 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-BF56057FF98F,id=2b374f74-8596-4317-81ba-ce0a491ddd96
dn2_1    | 2023-03-18 09:23:14,877 [Command processor thread] INFO impl.RoleInfo: 2b374f74-8596-4317-81ba-ce0a491ddd96: shutdown 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-LeaderStateImpl
dn2_1    | 2023-03-18 09:23:14,881 [Command processor thread] INFO impl.PendingRequests: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-PendingRequests: sendNotLeaderResponses
dn2_1    | 2023-03-18 09:23:14,883 [Command processor thread] INFO impl.StateMachineUpdater: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-StateMachineUpdater: set stopIndex = 6
dn2_1    | 2023-03-18 09:23:14,883 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-BF56057FF98F: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/c7fe038c-d6a9-4840-a081-bf56057ff98f/sm/snapshot.4_6
dn2_1    | 2023-03-18 09:23:14,884 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-BF56057FF98F: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/c7fe038c-d6a9-4840-a081-bf56057ff98f/sm/snapshot.4_6 took: 1 ms
dn2_1    | 2023-03-18 09:23:14,885 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-StateMachineUpdater] INFO impl.StateMachineUpdater: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-StateMachineUpdater: Took a snapshot at index 6
dn2_1    | 2023-03-18 09:23:14,886 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-StateMachineUpdater] INFO impl.StateMachineUpdater: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
dn4_1    | 2023-03-18 09:22:04,464 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn4_1    | 2023-03-18 09:22:04,466 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 7c9e7a7d-2593-458c-8a46-0313d77c3278
dn4_1    | 2023-03-18 09:22:04,508 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn4_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:321)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.util.concurrent.TimeoutException
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	... 1 more
dn4_1    | 2023-03-18 09:22:04,612 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a2f44939-53c6-4b4c-afd2-40b9ce60bd6c/in_use.lock acquired by nodename 7@23f39be4c8d3
dn4_1    | 2023-03-18 09:22:04,662 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=7c9e7a7d-2593-458c-8a46-0313d77c3278} from /data/metadata/ratis/a2f44939-53c6-4b4c-afd2-40b9ce60bd6c/current/raft-meta
dn4_1    | 2023-03-18 09:22:04,659 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/in_use.lock acquired by nodename 7@23f39be4c8d3
dn4_1    | 2023-03-18 09:22:04,729 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/in_use.lock acquired by nodename 7@23f39be4c8d3
dn4_1    | 2023-03-18 09:22:04,756 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=6, votedFor=04b88c83-026f-4630-aada-108e37835bee} from /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/current/raft-meta
dn4_1    | 2023-03-18 09:22:04,730 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=5, votedFor=bae0cd8d-e50f-4d23-8974-b816860f8fc3} from /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/current/raft-meta
dn4_1    | 2023-03-18 09:22:04,989 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D: set configuration 25: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:05,010 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C: set configuration 14: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:05,038 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C: set configuration 3: peers:[7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:05,118 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO ratis.ContainerStateMachine: group-80313208139C: Setting the last applied index to (t:5, i:27)
dn4_1    | 2023-03-18 09:22:05,125 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO ratis.ContainerStateMachine: group-40B9CE60BD6C: Setting the last applied index to (t:3, i:4)
dn4_1    | 2023-03-18 09:22:05,207 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO ratis.ContainerStateMachine: group-8191D6D2324D: Setting the last applied index to (t:6, i:38)
dn4_1    | 2023-03-18 09:22:05,759 [EndpointStateMachine task thread for recon/10.9.0.20:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn4_1    | java.net.SocketTimeoutException: Call From 23f39be4c8d3/10.9.0.18 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:45152 remote=recon/10.9.0.20:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn4_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 2023-03-18 09:23:53,002 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/84a08f9d-e905-447e-be3d-ccb5f87f8193/in_use.lock acquired by nodename 7@8698bf9a31d9
dn1_1    | 2023-03-18 09:23:53,007 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/84a08f9d-e905-447e-be3d-ccb5f87f8193 has been successfully formatted.
dn1_1    | 2023-03-18 09:23:53,008 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-CCB5F87F8193: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn1_1    | 2023-03-18 09:23:53,008 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-03-18 09:23:53,008 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-03-18 09:23:53,009 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-03-18 09:23:53,009 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-03-18 09:23:53,009 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-03-18 09:23:53,013 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-03-18 09:23:53,014 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-03-18 09:23:53,018 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-03-18 09:23:53,018 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/84a08f9d-e905-447e-be3d-ccb5f87f8193
dn1_1    | 2023-03-18 09:23:53,018 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-03-18 09:23:53,019 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-03-18 09:23:53,019 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-03-18 09:23:53,020 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-03-18 09:23:53,020 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-03-18 09:23:53,020 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-03-18 09:23:53,021 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-03-18 09:23:53,021 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-03-18 09:23:53,025 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-03-18 09:23:53,027 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-03-18 09:23:53,097 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-03-18 09:23:53,097 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-03-18 09:23:53,097 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-03-18 09:23:53,097 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-03-18 09:23:53,098 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-03-18 09:23:53,104 [pool-22-thread-1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193: start as a follower, conf=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-18 09:23:53,104 [pool-22-thread-1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn1_1    | 2023-03-18 09:23:53,104 [pool-22-thread-1] INFO impl.RoleInfo: 4ba198f1-a013-49ce-b059-f73788d69001: start 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193-FollowerState
dn1_1    | 2023-03-18 09:23:53,129 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-CCB5F87F8193,id=4ba198f1-a013-49ce-b059-f73788d69001
dn1_1    | 2023-03-18 09:23:53,130 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-03-18 09:23:53,130 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-03-18 09:23:53,130 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-03-18 09:23:53,130 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-03-18 09:23:53,145 [4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-03-18 09:23:53,146 [4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-03-18 09:23:56,773 [grpc-default-executor-0] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118: receive requestVote(PRE_VOTE, 7c9e7a7d-2593-458c-8a46-0313d77c3278, group-6F7919707118, 0, (t:0, i:0))
dn1_1    | 2023-03-18 09:23:56,775 [grpc-default-executor-0] INFO impl.VoteContext: 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-FOLLOWER: reject PRE_VOTE from 7c9e7a7d-2593-458c-8a46-0313d77c3278: our priority 1 > candidate's priority 0
dn1_1    | 2023-03-18 09:23:56,778 [grpc-default-executor-0] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118 replies to PRE_VOTE vote request: 7c9e7a7d-2593-458c-8a46-0313d77c3278<-4ba198f1-a013-49ce-b059-f73788d69001#0:FAIL-t0. Peer's state: 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118:t0, leader=null, voted=, raftlog=Memoized:4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn5_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn5_1    | 2023-03-18 09:21:25,421 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn5_1    | /************************************************************
dn5_1    | STARTUP_MSG: Starting HddsDatanodeService
dn5_1    | STARTUP_MSG:   host = e51427cbfe02/10.9.0.19
dn5_1    | STARTUP_MSG:   args = []
dn5_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn5_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn5_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/1236414c1c46c67c92f37499b0b61921cdfab39a ; compiled by 'runner' on 2023-03-18T08:55Z
dn5_1    | STARTUP_MSG:   java = 11.0.14.1
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:45152 remote=recon/10.9.0.20:9891]
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn4_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn4_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn4_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn4_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn4_1    | 2023-03-18 09:22:06,156 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-03-18 09:22:06,192 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-03-18 09:22:06,214 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-03-18 09:22:06,231 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-18 09:22:06,212 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-03-18 09:22:06,243 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-03-18 09:22:06,243 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-18 09:22:06,254 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-03-18 09:22:06,292 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-03-18 09:22:06,281 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-03-18 09:22:06,298 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-18 09:22:06,301 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-03-18 09:22:06,302 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-03-18 09:22:06,263 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-03-18 09:22:06,327 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-03-18 09:22:06,334 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-03-18 09:22:06,346 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-03-18 09:22:06,383 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-03-18 09:22:06,388 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-03-18 09:22:06,401 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-03-18 09:22:06,400 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-03-18 09:22:06,408 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-03-18 09:22:06,393 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-03-18 09:22:06,423 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-03-18 09:22:06,431 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a2f44939-53c6-4b4c-afd2-40b9ce60bd6c
dn4_1    | 2023-03-18 09:22:06,433 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-03-18 09:22:06,441 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c
dn4_1    | 2023-03-18 09:22:06,443 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-03-18 09:22:06,448 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-03-18 09:22:06,451 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-03-18 09:22:06,457 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-03-18 09:22:06,458 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-03-18 09:21:53,292 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-03-18 09:21:53,292 [pool-38-thread-1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-03-18 09:21:53,294 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-03-18 09:21:53,298 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-03-18 09:21:53,303 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-03-18 09:21:53,303 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-03-18 09:21:53,304 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-03-18 09:21:53,307 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-03-18 09:21:53,338 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-03-18 09:21:53,338 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-03-18 09:21:53,345 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-03-18 09:21:53,346 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-03-18 09:21:53,349 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-03-18 09:21:53,363 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn3_1    | 2023-03-18 09:21:53,594 [main] INFO util.log: Logging initialized @39401ms to org.eclipse.jetty.util.log.Slf4jLog
dn3_1    | 2023-03-18 09:21:54,409 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn3_1    | 2023-03-18 09:21:54,465 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn3_1    | 2023-03-18 09:21:54,510 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn3_1    | 2023-03-18 09:21:54,541 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn3_1    | 2023-03-18 09:21:54,542 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn3_1    | 2023-03-18 09:21:54,544 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn3_1    | 2023-03-18 09:21:54,778 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn3_1    | 2023-03-18 09:21:54,825 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn3_1    | 2023-03-18 09:21:54,827 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn3_1    | 2023-03-18 09:21:55,337 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn3_1    | 2023-03-18 09:21:55,355 [main] INFO server.session: No SessionScavenger set, using defaults
dn3_1    | 2023-03-18 09:21:55,386 [main] INFO server.session: node0 Scavenging every 660000ms
dn3_1    | 2023-03-18 09:21:55,495 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5d71b500{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn3_1    | 2023-03-18 09:21:55,511 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@437c4b25{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn3_1    | 2023-03-18 09:21:56,389 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3f5dfe69{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-12728136979457565584/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn3_1    | 2023-03-18 09:21:56,437 [main] INFO server.AbstractConnector: Started ServerConnector@21f7e537{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn3_1    | 2023-03-18 09:21:56,438 [main] INFO server.Server: Started @42245ms
dn3_1    | 2023-03-18 09:21:56,476 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn3_1    | 2023-03-18 09:21:56,476 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn3_1    | 2023-03-18 09:21:56,478 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn3_1    | 2023-03-18 09:21:56,508 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn3_1    | 2023-03-18 09:21:56,672 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn3_1    | 2023-03-18 09:21:56,677 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn3_1    | 2023-03-18 09:21:56,763 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@4396315a] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn3_1    | 2023-03-18 09:21:57,473 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.20:9891
dn3_1    | 2023-03-18 09:21:57,638 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn3_1    | 2023-03-18 09:22:00,145 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-03-18 09:22:00,146 [EndpointStateMachine task thread for recon/10.9.0.20:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.20:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-03-18 09:22:00,801 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn2_1    | 2023-03-18 09:23:14,888 [Command processor thread] INFO server.RaftServer$Division: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F: closes. applyIndex: 6
dn2_1    | 2023-03-18 09:23:14,888 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn2_1    | 2023-03-18 09:23:14,889 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F-SegmentedRaftLogWorker close()
dn2_1    | 2023-03-18 09:23:14,893 [Command processor thread] INFO server.RaftServer$Division: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-BF56057FF98F: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/c7fe038c-d6a9-4840-a081-bf56057ff98f
dn2_1    | 2023-03-18 09:23:14,894 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=c7fe038c-d6a9-4840-a081-bf56057ff98f command on datanode 2b374f74-8596-4317-81ba-ce0a491ddd96.
dn2_1    | 2023-03-18 09:23:44,782 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | 2023-03-18 09:24:08,800 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn2_1    | 2023-03-18 09:24:14,820 [Command processor thread] INFO server.RaftServer: 2b374f74-8596-4317-81ba-ce0a491ddd96: addNew group-ED015D5B9B4C:[2b374f74-8596-4317-81ba-ce0a491ddd96|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:10.9.0.16:9858|priority:1|startupRole:FOLLOWER] returns group-ED015D5B9B4C:java.util.concurrent.CompletableFuture@3b083b88[Not completed]
dn2_1    | 2023-03-18 09:24:14,834 [pool-22-thread-1] INFO server.RaftServer$Division: 2b374f74-8596-4317-81ba-ce0a491ddd96: new RaftServerImpl for group-ED015D5B9B4C:[2b374f74-8596-4317-81ba-ce0a491ddd96|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:10.9.0.16:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn2_1    | 2023-03-18 09:24:14,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-03-18 09:24:14,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-03-18 09:24:14,835 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-03-18 09:24:14,835 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-03-18 09:24:14,836 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-03-18 09:24:14,836 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-03-18 09:24:14,836 [pool-22-thread-1] INFO server.RaftServer$Division: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C: ConfigurationManager, init=-1: peers:[2b374f74-8596-4317-81ba-ce0a491ddd96|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:10.9.0.16:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-03-18 09:24:14,837 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-03-18 09:24:14,840 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-03-18 09:24:14,841 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-03-18 09:24:14,842 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-03-18 09:24:14,842 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-03-18 09:24:14,842 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-03-18 09:24:14,845 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-03-18 09:24:14,846 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-03-18 09:24:14,846 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-03-18 09:24:14,847 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-03-18 09:24:14,847 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-03-18 09:24:14,848 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/747ad178-d87d-4bbd-985b-ed015d5b9b4c does not exist. Creating ...
dn2_1    | 2023-03-18 09:24:14,853 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/747ad178-d87d-4bbd-985b-ed015d5b9b4c/in_use.lock acquired by nodename 7@b27b4bae5a17
dn2_1    | 2023-03-18 09:24:14,855 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/747ad178-d87d-4bbd-985b-ed015d5b9b4c has been successfully formatted.
dn2_1    | 2023-03-18 09:24:14,858 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-ED015D5B9B4C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn2_1    | 2023-03-18 09:24:14,859 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-03-18 09:24:14,859 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-03-18 09:24:14,859 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-03-18 09:24:14,860 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-03-18 09:24:14,860 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-03-18 09:24:14,860 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-03-18 09:24:14,867 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-03-18 09:24:14,867 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-03-18 09:24:14,868 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/747ad178-d87d-4bbd-985b-ed015d5b9b4c
dn2_1    | 2023-03-18 09:24:14,868 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-03-18 09:24:14,868 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-03-18 09:24:14,869 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-03-18 09:24:14,869 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-03-18 09:24:14,869 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-03-18 09:24:14,871 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-03-18 09:23:57,499 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-FollowerState] INFO impl.FollowerState: 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5152001022ns, electionTimeout:5074ms
dn1_1    | 2023-03-18 09:23:57,500 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-FollowerState] INFO impl.RoleInfo: 4ba198f1-a013-49ce-b059-f73788d69001: shutdown 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-FollowerState
dn1_1    | 2023-03-18 09:23:57,500 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-FollowerState] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn1_1    | 2023-03-18 09:23:57,501 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-03-18 09:23:57,501 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-FollowerState] INFO impl.RoleInfo: 4ba198f1-a013-49ce-b059-f73788d69001: start 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2
dn1_1    | 2023-03-18 09:23:57,508 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO impl.LeaderElection: 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-18 09:23:57,525 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 7c9e7a7d-2593-458c-8a46-0313d77c3278
dn1_1    | 2023-03-18 09:23:57,528 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-03-18 09:23:57,529 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-03-18 09:23:57,528 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 04b88c83-026f-4630-aada-108e37835bee
dn1_1    | 2023-03-18 09:23:57,680 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO impl.LeaderElection: 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn1_1    | 2023-03-18 09:23:57,680 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO impl.LeaderElection:   Response 0: 4ba198f1-a013-49ce-b059-f73788d69001<-7c9e7a7d-2593-458c-8a46-0313d77c3278#0:OK-t0
dn1_1    | 2023-03-18 09:23:57,680 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO impl.LeaderElection: 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2 PRE_VOTE round 0: result PASSED
dn1_1    | 2023-03-18 09:23:57,684 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO impl.LeaderElection: 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-18 09:23:57,692 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-03-18 09:23:57,692 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-03-18 09:23:57,719 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO impl.LeaderElection: 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn1_1    | 2023-03-18 09:23:57,719 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO impl.LeaderElection:   Response 0: 4ba198f1-a013-49ce-b059-f73788d69001<-7c9e7a7d-2593-458c-8a46-0313d77c3278#0:OK-t1
dn1_1    | 2023-03-18 09:23:57,719 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO impl.LeaderElection: 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2 ELECTION round 0: result PASSED
dn1_1    | 2023-03-18 09:23:57,719 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO impl.RoleInfo: 4ba198f1-a013-49ce-b059-f73788d69001: shutdown 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2
dn1_1    | 2023-03-18 09:23:57,719 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn1_1    | 2023-03-18 09:23:57,719 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6F7919707118 with new leaderId: 4ba198f1-a013-49ce-b059-f73788d69001
dn1_1    | 2023-03-18 09:23:57,720 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118: change Leader from null to 4ba198f1-a013-49ce-b059-f73788d69001 at term 1 for becomeLeader, leader elected after 5461ms
dn1_1    | 2023-03-18 09:23:57,720 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn1_1    | 2023-03-18 09:23:57,720 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-03-18 09:23:57,725 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn1_1    | 2023-03-18 09:23:57,725 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn1_1    | 2023-03-18 09:23:57,726 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn1_1    | 2023-03-18 09:23:57,726 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn1_1    | 2023-03-18 09:23:57,728 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:321)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	... 1 more
dn3_1    | 2023-03-18 09:22:01,146 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-03-18 09:22:02,147 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-03-18 09:22:04,012 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn3_1    | 2023-03-18 09:22:04,029 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn3_1    | 2023-03-18 09:22:04,476 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn3_1    | 2023-03-18 09:22:04,498 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis bae0cd8d-e50f-4d23-8974-b816860f8fc3
dn3_1    | 2023-03-18 09:22:04,648 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/in_use.lock acquired by nodename 7@ac7a47589fa9
dn3_1    | 2023-03-18 09:22:04,670 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/in_use.lock acquired by nodename 7@ac7a47589fa9
dn3_1    | 2023-03-18 09:22:04,680 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d1279162-045f-45cf-95f2-d1111d92119b/in_use.lock acquired by nodename 7@ac7a47589fa9
dn3_1    | 2023-03-18 09:22:04,733 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=6, votedFor=04b88c83-026f-4630-aada-108e37835bee} from /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/current/raft-meta
dn3_1    | 2023-03-18 09:22:04,766 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=5, votedFor=bae0cd8d-e50f-4d23-8974-b816860f8fc3} from /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/current/raft-meta
dn3_1    | 2023-03-18 09:22:04,766 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=bae0cd8d-e50f-4d23-8974-b816860f8fc3} from /data/metadata/ratis/d1279162-045f-45cf-95f2-d1111d92119b/current/raft-meta
dn3_1    | 2023-03-18 09:22:04,964 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D: set configuration 25: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:04,996 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C: set configuration 14: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:05,016 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B: set configuration 3: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:05,064 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO ratis.ContainerStateMachine: group-80313208139C: Setting the last applied index to (t:5, i:27)
dn3_1    | 2023-03-18 09:22:05,099 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO ratis.ContainerStateMachine: group-D1111D92119B: Setting the last applied index to (t:3, i:4)
dn3_1    | 2023-03-18 09:22:05,099 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO ratis.ContainerStateMachine: group-8191D6D2324D: Setting the last applied index to (t:6, i:38)
dn3_1    | 2023-03-18 09:22:05,185 [EndpointStateMachine task thread for recon/10.9.0.20:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From ac7a47589fa9/10.9.0.17 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:50534 remote=recon/10.9.0.20:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn3_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:50534 remote=recon/10.9.0.20:9891]
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn3_1    | 2023-03-18 09:22:06,140 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-03-18 09:22:06,175 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-03-18 09:22:06,176 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-03-18 09:23:57,729 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn1_1    | 2023-03-18 09:23:57,766 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn1_1    | 2023-03-18 09:23:57,766 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-03-18 09:23:57,767 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn1_1    | 2023-03-18 09:23:57,769 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn1_1    | 2023-03-18 09:23:57,769 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-03-18 09:23:57,769 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-03-18 09:23:57,770 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 2023-03-18 09:23:57,770 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn1_1    | 2023-03-18 09:23:57,778 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn1_1    | 2023-03-18 09:23:57,778 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-03-18 09:23:57,778 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn1_1    | 2023-03-18 09:23:57,778 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn1_1    | 2023-03-18 09:23:57,779 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-03-18 09:23:57,779 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-03-18 09:23:57,779 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 2023-03-18 09:23:57,780 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn1_1    | 2023-03-18 09:23:57,784 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO impl.RoleInfo: 4ba198f1-a013-49ce-b059-f73788d69001: start 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderStateImpl
dn1_1    | 2023-03-18 09:23:57,788 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-SegmentedRaftLogWorker: Starting segment from index:0
dn1_1    | 2023-03-18 09:23:57,789 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/66de196e-0a1c-4e0b-be4e-6f7919707118/current/log_inprogress_0
dn1_1    | 2023-03-18 09:23:57,797 [4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118-LeaderElection2] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-6F7919707118: set configuration 0: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-18 09:23:57,881 [grpc-default-executor-0] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193: receive requestVote(PRE_VOTE, 7c9e7a7d-2593-458c-8a46-0313d77c3278, group-CCB5F87F8193, 0, (t:0, i:0))
dn1_1    | 2023-03-18 09:23:57,882 [grpc-default-executor-0] INFO impl.VoteContext: 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193-FOLLOWER: accept PRE_VOTE from 7c9e7a7d-2593-458c-8a46-0313d77c3278: our priority 0 <= candidate's priority 0
dn1_1    | 2023-03-18 09:23:57,890 [grpc-default-executor-0] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193 replies to PRE_VOTE vote request: 7c9e7a7d-2593-458c-8a46-0313d77c3278<-4ba198f1-a013-49ce-b059-f73788d69001#0:OK-t0. Peer's state: 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193:t0, leader=null, voted=, raftlog=Memoized:4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-18 09:23:58,148 [grpc-default-executor-2] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193: receive requestVote(PRE_VOTE, 04b88c83-026f-4630-aada-108e37835bee, group-CCB5F87F8193, 0, (t:0, i:0))
dn1_1    | 2023-03-18 09:23:58,148 [grpc-default-executor-2] INFO impl.VoteContext: 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193-FOLLOWER: accept PRE_VOTE from 04b88c83-026f-4630-aada-108e37835bee: our priority 0 <= candidate's priority 1
dn1_1    | 2023-03-18 09:23:58,149 [grpc-default-executor-2] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193 replies to PRE_VOTE vote request: 04b88c83-026f-4630-aada-108e37835bee<-4ba198f1-a013-49ce-b059-f73788d69001#0:OK-t0. Peer's state: 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193:t0, leader=null, voted=, raftlog=Memoized:4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/1236414c1c46c67c92f37499b0b61921cdfab39a ; compiled by 'runner' on 2023-03-18T08:55Z
om2_1    | STARTUP_MSG:   java = 11.0.14.1
dn4_1    | 2023-03-18 09:22:06,469 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-03-18 09:22:06,436 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d
dn4_1    | 2023-03-18 09:22:06,471 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-03-18 09:22:06,472 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-03-18 09:22:06,472 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-03-18 09:22:06,472 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-03-18 09:22:06,473 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-03-18 09:22:06,474 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-03-18 09:22:06,476 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-03-18 09:22:06,477 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-03-18 09:22:06,479 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-03-18 09:22:06,480 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-03-18 09:22:06,480 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-03-18 09:22:06,480 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-03-18 09:22:06,480 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-03-18 09:22:06,480 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-03-18 09:22:06,482 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-03-18 09:22:06,496 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-03-18 09:22:06,534 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-03-18 09:22:06,597 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-03-18 09:22:06,633 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-18 09:22:06,626 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-03-18 09:22:06,605 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-03-18 09:22:06,647 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-18 09:22:07,563 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-03-18 09:22:07,563 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-03-18 09:22:07,564 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-03-18 09:22:07,564 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-18 09:22:07,617 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-03-18 09:22:07,620 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-03-18 09:22:07,620 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-03-18 09:22:07,690 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-03-18 09:22:07,696 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-03-18 09:22:07,696 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-03-18 09:22:07,854 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D: set configuration 0: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:07,854 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C: set configuration 0: peers:[7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:07,878 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C: set configuration 0: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-18 09:24:14,871 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-03-18 09:24:14,888 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-03-18 09:24:14,890 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-03-18 09:24:14,891 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-03-18 09:24:14,908 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-03-18 09:24:14,909 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-03-18 09:24:14,909 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-03-18 09:24:14,910 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-03-18 09:24:14,910 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-03-18 09:24:14,911 [pool-22-thread-1] INFO server.RaftServer$Division: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C: start as a follower, conf=-1: peers:[2b374f74-8596-4317-81ba-ce0a491ddd96|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:10.9.0.16:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-18 09:24:14,911 [pool-22-thread-1] INFO server.RaftServer$Division: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn2_1    | 2023-03-18 09:24:14,912 [pool-22-thread-1] INFO impl.RoleInfo: 2b374f74-8596-4317-81ba-ce0a491ddd96: start 2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-FollowerState
dn2_1    | 2023-03-18 09:24:14,913 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-ED015D5B9B4C,id=2b374f74-8596-4317-81ba-ce0a491ddd96
dn2_1    | 2023-03-18 09:24:14,913 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-03-18 09:24:14,913 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-03-18 09:24:14,913 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-03-18 09:24:14,913 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-03-18 09:24:14,915 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-03-18 09:24:14,915 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-03-18 09:24:14,923 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=747ad178-d87d-4bbd-985b-ed015d5b9b4c
dn2_1    | 2023-03-18 09:24:14,924 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=747ad178-d87d-4bbd-985b-ed015d5b9b4c.
dn2_1    | 2023-03-18 09:24:19,940 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-FollowerState] INFO impl.FollowerState: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5028099890ns, electionTimeout:5024ms
dn2_1    | 2023-03-18 09:24:19,941 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-FollowerState] INFO impl.RoleInfo: 2b374f74-8596-4317-81ba-ce0a491ddd96: shutdown 2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-FollowerState
dn2_1    | 2023-03-18 09:24:19,941 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-FollowerState] INFO server.RaftServer$Division: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn2_1    | 2023-03-18 09:24:19,941 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn2_1    | 2023-03-18 09:24:19,941 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-FollowerState] INFO impl.RoleInfo: 2b374f74-8596-4317-81ba-ce0a491ddd96: start 2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2
dn2_1    | 2023-03-18 09:24:19,946 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2] INFO impl.LeaderElection: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[2b374f74-8596-4317-81ba-ce0a491ddd96|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:10.9.0.16:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-18 09:24:19,947 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2] INFO impl.LeaderElection: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
dn2_1    | 2023-03-18 09:24:19,950 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2] INFO impl.LeaderElection: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[2b374f74-8596-4317-81ba-ce0a491ddd96|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:10.9.0.16:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-18 09:24:19,950 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2] INFO impl.LeaderElection: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2 ELECTION round 0: result PASSED (term=1)
dn2_1    | 2023-03-18 09:24:19,950 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2] INFO impl.RoleInfo: 2b374f74-8596-4317-81ba-ce0a491ddd96: shutdown 2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2
dn2_1    | 2023-03-18 09:24:19,950 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2] INFO server.RaftServer$Division: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn2_1    | 2023-03-18 09:24:19,950 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-ED015D5B9B4C with new leaderId: 2b374f74-8596-4317-81ba-ce0a491ddd96
dn2_1    | 2023-03-18 09:24:19,950 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2] INFO server.RaftServer$Division: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C: change Leader from null to 2b374f74-8596-4317-81ba-ce0a491ddd96 at term 1 for becomeLeader, leader elected after 5108ms
dn2_1    | 2023-03-18 09:24:19,951 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn2_1    | 2023-03-18 09:24:19,951 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-03-18 09:22:06,222 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-03-18 09:22:06,196 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-03-18 09:22:06,232 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-03-18 09:22:06,233 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-03-18 09:22:06,234 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-03-18 09:22:06,242 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-03-18 09:22:06,264 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-03-18 09:22:06,235 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-03-18 09:22:06,272 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-03-18 09:22:06,272 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-03-18 09:22:06,272 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-03-18 09:22:06,264 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-03-18 09:22:06,302 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-03-18 09:22:06,323 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-03-18 09:22:06,324 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-03-18 09:22:06,352 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-03-18 09:22:06,354 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-03-18 09:22:06,356 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-03-18 09:22:06,357 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-03-18 09:22:06,407 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-03-18 09:22:06,424 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-03-18 09:22:06,427 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d
dn3_1    | 2023-03-18 09:22:06,428 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-03-18 09:22:06,430 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-03-18 09:22:06,436 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d1279162-045f-45cf-95f2-d1111d92119b
dn3_1    | 2023-03-18 09:22:06,436 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-03-18 09:22:06,431 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c
dn3_1    | 2023-03-18 09:22:06,436 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-03-18 09:22:06,436 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-03-18 09:22:06,436 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-03-18 09:22:06,438 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-03-18 09:22:06,453 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-03-18 09:22:06,453 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-03-18 09:22:06,456 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-03-18 09:22:06,457 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-03-18 09:22:06,459 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-03-18 09:22:06,461 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-03-18 09:22:06,481 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-03-18 09:22:06,482 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-03-18 09:22:06,482 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-03-18 09:22:06,482 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-03-18 09:22:06,485 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-03-18 09:22:06,492 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-03-18 09:22:06,492 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-03-18 09:22:06,492 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-03-18 09:22:06,492 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-03-18 09:22:06,501 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-03-18 09:22:06,520 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-03-18 09:22:06,595 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-03-18 09:22:06,613 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-03-18 09:22:06,619 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-03-18 09:22:06,632 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-03-18 09:22:06,633 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-03-18 09:22:07,508 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-03-18 09:22:07,506 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-03-18 09:22:07,510 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-03-18 09:22:07,511 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-03-18 09:22:07,531 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-03-18 09:22:07,533 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-03-18 09:22:07,534 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-03-18 09:22:07,689 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-03-18 09:22:07,689 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-03-18 09:22:07,689 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-03-18 09:22:08,724 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B: set configuration 0: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:08,727 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C: set configuration 0: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4))$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om2_1    | ************************************************************/
om2_1    | 2023-03-18 09:21:27,936 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1    | 2023-03-18 09:21:36,131 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1    | 2023-03-18 09:21:39,291 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om2_1    | 2023-03-18 09:21:39,493 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1    | 2023-03-18 09:21:39,496 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
om2_1    | 2023-03-18 09:21:39,513 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-03-18 09:21:39,703 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = INITIAL_VERSION (version = 0), software layout = MULTITENANCY_SCHEMA (version = 3)
om2_1    | 2023-03-18 09:21:42,576 [main] INFO reflections.Reflections: Reflections took 2354 ms to scan 1 urls, producing 126 keys and 369 values [using 2 cores]
om2_1    | 2023-03-18 09:21:42,639 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-03-18 09:21:42,775 [main] WARN om.OzoneManager: ozone.default.bucket.layout configured to non-legacy bucket layout FILE_SYSTEM_OPTIMIZED when Ozone Manager is pre-finalized for bucket layout support. Legacy buckets will be created by default until Ozone Manager is finalized.
om2_1    | 2023-03-18 09:21:44,785 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863]
om2_1    | 2023-03-18 09:21:45,089 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863]
om2_1    | 2023-03-18 09:21:47,782 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7546b4869b99/10.9.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-03-18 09:21:49,784 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7546b4869b99/10.9.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-03-18 09:21:51,785 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7546b4869b99/10.9.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-03-18 09:21:53,786 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7546b4869b99/10.9.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-03-18 09:21:55,788 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7546b4869b99/10.9.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-03-18 09:21:57,789 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7546b4869b99/10.9.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-03-18 09:21:59,790 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7546b4869b99/10.9.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-03-18 09:22:01,792 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7546b4869b99/10.9.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-03-18 09:22:08,021 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om2_1    | 2023-03-18 09:22:08,319 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-03-18 09:22:09,385 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om2_1    | 2023-03-18 09:22:09,395 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om2_1    | 2023-03-18 09:22:09,994 [Thread-15] INFO rocksdiff.RocksDBCheckpointDiffer: Skipped the compaction entry. Compaction input files: [/data/metadata/om.db/000077.sst, /data/metadata/om.db/000067.sst, /data/metadata/om.db/000049.sst, /data/metadata/om.db/000042.sst] and output files: [/data/metadata/om.db/000077.sst, /data/metadata/om.db/000067.sst, /data/metadata/om.db/000049.sst, /data/metadata/om.db/000042.sst] are same.
om2_1    | 2023-03-18 09:22:10,403 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om2_1    | 2023-03-18 09:22:10,557 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-03-18 09:22:10,895 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1    | 2023-03-18 09:22:10,896 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om2_1    | 2023-03-18 09:22:10,976 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om2_1    | 2023-03-18 09:22:12,010 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om2_1    | 2023-03-18 09:22:12,135 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1    | 2023-03-18 09:22:12,406 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
om2_1    | 2023-03-18 09:22:12,406 [Finalizer] WARN managed.ManagedRocksObjectUtils: ManagedColumnFamilyOptions is not closed properly
om2_1    | 2023-03-18 09:22:12,406 [Finalizer] WARN managed.ManagedRocksObjectUtils: ManagedColumnFamilyOptions is not closed properly
om2_1    | 2023-03-18 09:22:12,416 [Finalizer] WARN managed.ManagedRocksObjectUtils: ManagedColumnFamilyOptions is not closed properly
om2_1    | 2023-03-18 09:22:12,416 [Finalizer] WARN managed.ManagedRocksObjectUtils: ManagedColumnFamilyOptions is not closed properly
om2_1    | 2023-03-18 09:22:12,471 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om2:9872, om1:9872, om3:9872
om2_1    | 2023-03-18 09:22:12,518 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:7, i:104)
om2_1    | 2023-03-18 09:22:12,669 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om2_1    | 2023-03-18 09:22:12,959 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om2_1    | 2023-03-18 09:22:12,962 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om2_1    | 2023-03-18 09:22:12,965 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om2_1    | 2023-03-18 09:22:12,968 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om2_1    | 2023-03-18 09:22:12,970 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om2_1    | 2023-03-18 09:22:12,970 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om2_1    | 2023-03-18 09:22:12,972 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om2_1    | 2023-03-18 09:22:12,980 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-03-18 09:22:12,980 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om2_1    | 2023-03-18 09:22:12,983 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1    | 2023-03-18 09:22:13,007 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1    | 2023-03-18 09:22:13,014 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om2_1    | 2023-03-18 09:22:13,015 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om2_1    | 2023-03-18 09:22:13,550 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om2_1    | 2023-03-18 09:22:13,557 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om2_1    | 2023-03-18 09:22:13,560 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om2_1    | 2023-03-18 09:22:13,560 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1    | 2023-03-18 09:22:13,561 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | 2023-03-18 09:22:13,577 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1    | 2023-03-18 09:22:13,707 [om2-impl-thread1] INFO server.RaftServer: om2: found a subdirectory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om2_1    | 2023-03-18 09:22:13,771 [main] INFO server.RaftServer: om2: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@38029686[Not completed]
om2_1    | 2023-03-18 09:22:13,771 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om2_1    | 2023-03-18 09:22:13,782 [main] INFO om.OzoneManager: Creating RPC Server
om2_1    | 2023-03-18 09:22:13,918 [pool-29-thread-1] INFO server.RaftServer$Division: om2: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om2_1    | 2023-03-18 09:22:13,950 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om2_1    | 2023-03-18 09:22:13,966 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om2_1    | 2023-03-18 09:22:13,966 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om2_1    | 2023-03-18 09:22:13,967 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1    | 2023-03-18 09:22:13,968 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | 2023-03-18 09:22:13,969 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1    | 2023-03-18 09:22:14,061 [pool-29-thread-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om2_1    | 2023-03-18 09:22:14,066 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1    | 2023-03-18 09:22:14,094 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1    | 2023-03-18 09:22:14,099 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om2_1    | 2023-03-18 09:22:14,331 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om2_1    | 2023-03-18 09:22:14,355 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
dn1_1    | 2023-03-18 09:23:58,152 [grpc-default-executor-2] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193: receive requestVote(ELECTION, 04b88c83-026f-4630-aada-108e37835bee, group-CCB5F87F8193, 1, (t:0, i:0))
dn1_1    | 2023-03-18 09:23:58,152 [grpc-default-executor-2] INFO impl.VoteContext: 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193-FOLLOWER: accept ELECTION from 04b88c83-026f-4630-aada-108e37835bee: our priority 0 <= candidate's priority 1
dn1_1    | 2023-03-18 09:23:58,152 [grpc-default-executor-2] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:04b88c83-026f-4630-aada-108e37835bee
dn1_1    | 2023-03-18 09:23:58,152 [grpc-default-executor-2] INFO impl.RoleInfo: 4ba198f1-a013-49ce-b059-f73788d69001: shutdown 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193-FollowerState
dn1_1    | 2023-03-18 09:23:58,153 [4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193-FollowerState] INFO impl.FollowerState: 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193-FollowerState was interrupted
dn1_1    | 2023-03-18 09:23:58,153 [grpc-default-executor-2] INFO impl.RoleInfo: 4ba198f1-a013-49ce-b059-f73788d69001: start 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193-FollowerState
dn1_1    | 2023-03-18 09:23:58,156 [4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-03-18 09:23:58,156 [4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-03-18 09:23:58,156 [grpc-default-executor-2] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193 replies to ELECTION vote request: 04b88c83-026f-4630-aada-108e37835bee<-4ba198f1-a013-49ce-b059-f73788d69001#0:OK-t1. Peer's state: 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193:t1, leader=null, voted=04b88c83-026f-4630-aada-108e37835bee, raftlog=Memoized:4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-18 09:23:58,263 [4ba198f1-a013-49ce-b059-f73788d69001-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-CCB5F87F8193 with new leaderId: 04b88c83-026f-4630-aada-108e37835bee
dn1_1    | 2023-03-18 09:23:58,263 [4ba198f1-a013-49ce-b059-f73788d69001-server-thread1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193: change Leader from null to 04b88c83-026f-4630-aada-108e37835bee at term 1 for appendEntries, leader elected after 5270ms
dn1_1    | 2023-03-18 09:23:58,271 [4ba198f1-a013-49ce-b059-f73788d69001-server-thread1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193: set configuration 0: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-18 09:23:58,271 [4ba198f1-a013-49ce-b059-f73788d69001-server-thread1] INFO segmented.SegmentedRaftLogWorker: 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193-SegmentedRaftLogWorker: Starting segment from index:0
dn1_1    | 2023-03-18 09:23:58,282 [4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4ba198f1-a013-49ce-b059-f73788d69001@group-CCB5F87F8193-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/84a08f9d-e905-447e-be3d-ccb5f87f8193/current/log_inprogress_0
dn1_1    | 2023-03-18 09:24:07,013 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-03-18 09:24:13,145 [pool-22-thread-1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001: new RaftServerImpl for group-D4E599C27FE1:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn1_1    | 2023-03-18 09:24:13,145 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-03-18 09:24:13,145 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-03-18 09:24:13,145 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-03-18 09:24:13,145 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-03-18 09:24:13,146 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-03-18 09:24:13,146 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-03-18 09:24:13,146 [pool-22-thread-1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1: ConfigurationManager, init=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-03-18 09:24:13,146 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-03-18 09:24:13,146 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-03-18 09:24:13,147 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-03-18 09:24:13,147 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-03-18 09:24:13,147 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-03-18 09:24:13,147 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-03-18 09:24:13,148 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-03-18 09:24:13,148 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-03-18 09:24:13,149 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-03-18 09:24:13,149 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-03-18 09:24:13,149 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-03-18 09:22:07,914 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/a2f44939-53c6-4b4c-afd2-40b9ce60bd6c/current/log_0-0
dn4_1    | 2023-03-18 09:22:07,929 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C: set configuration 1: peers:[7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:07,967 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO segmented.LogSegment: Successfully read 5 entries from segment file /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/current/log_0-4
dn4_1    | 2023-03-18 09:22:07,983 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C: set configuration 5: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:07,987 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO segmented.LogSegment: Successfully read 9 entries from segment file /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/current/log_5-13
dn4_1    | 2023-03-18 09:22:07,996 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C: set configuration 14: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:07,998 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO segmented.LogSegment: Successfully read 14 entries from segment file /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/current/log_inprogress_14
dn4_1    | 2023-03-18 09:22:08,003 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/a2f44939-53c6-4b4c-afd2-40b9ce60bd6c/current/log_1-2
dn4_1    | 2023-03-18 09:22:08,043 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C: set configuration 3: peers:[7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:08,060 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/a2f44939-53c6-4b4c-afd2-40b9ce60bd6c/current/log_inprogress_3
dn4_1    | 2023-03-18 09:22:08,070 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn4_1    | 2023-03-18 09:22:08,071 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn4_1    | 2023-03-18 09:22:08,034 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO segmented.LogSegment: Successfully read 8 entries from segment file /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/current/log_0-7
dn4_1    | 2023-03-18 09:22:08,058 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 27
dn4_1    | 2023-03-18 09:22:08,092 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 13
dn4_1    | 2023-03-18 09:22:08,110 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D: set configuration 8: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:08,114 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO segmented.LogSegment: Successfully read 17 entries from segment file /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/current/log_8-24
dn4_1    | 2023-03-18 09:22:08,193 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D: set configuration 25: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:08,235 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO segmented.LogSegment: Successfully read 14 entries from segment file /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/current/log_inprogress_25
dn4_1    | 2023-03-18 09:22:08,239 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 38
dn4_1    | 2023-03-18 09:22:08,240 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 24
dn4_1    | 2023-03-18 09:22:08,756 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO raftlog.RaftLog: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-SegmentedRaftLog: commitIndex: updateToMax old=4, new=3, updated? false
dn4_1    | 2023-03-18 09:22:09,388 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C: start as a follower, conf=3: peers:[7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:09,388 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn4_1    | 2023-03-18 09:22:08,762 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO raftlog.RaftLog: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-SegmentedRaftLog: commitIndex: updateToMax old=38, new=36, updated? false
dn5_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=1440, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4))$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn5_1    | ************************************************************/
dn5_1    | 2023-03-18 09:21:25,485 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn5_1    | 2023-03-18 09:21:25,887 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn5_1    | 2023-03-18 09:21:26,634 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn5_1    | 2023-03-18 09:21:28,061 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn5_1    | 2023-03-18 09:21:28,072 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn5_1    | 2023-03-18 09:21:28,922 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:e51427cbfe02 ip:10.9.0.19
dn5_1    | 2023-03-18 09:21:31,004 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn5_1    | 2023-03-18 09:21:32,576 [main] INFO reflections.Reflections: Reflections took 1337 ms to scan 2 urls, producing 103 keys and 227 values 
dn5_1    | 2023-03-18 09:21:33,076 [main] INFO upgrade.HDDSLayoutVersionManager: Registering Upgrade Action : DatanodeSchemaV3FinalizeAction
dn5_1    | 2023-03-18 09:21:33,436 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
dn5_1    | 2023-03-18 09:21:34,648 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 8414 at 2023-03-18T09:21:04.676Z
dn5_1    | 2023-03-18 09:21:34,756 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn5_1    | 2023-03-18 09:21:34,785 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn5_1    | 2023-03-18 09:21:34,791 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn5_1    | 2023-03-18 09:21:34,914 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn5_1    | 2023-03-18 09:21:35,102 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 2023-03-18 09:21:35,166 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-03-18T09:21:04.649Z
dn5_1    | 2023-03-18 09:21:35,171 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn5_1    | 2023-03-18 09:21:35,180 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn5_1    | 2023-03-18 09:21:35,199 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn5_1    | 2023-03-18 09:21:35,409 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn5_1    | 2023-03-18 09:21:36,650 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn5_1    | 2023-03-18 09:21:36,653 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 1s
dn5_1    | 2023-03-18 09:21:48,565 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn5_1    | 2023-03-18 09:21:49,143 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 2023-03-18 09:21:49,557 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn5_1    | 2023-03-18 09:21:50,832 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-03-18 09:21:50,840 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn5_1    | 2023-03-18 09:21:50,858 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-03-18 09:21:50,864 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn5_1    | 2023-03-18 09:21:50,868 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn5_1    | 2023-03-18 09:21:50,868 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn5_1    | 2023-03-18 09:21:50,869 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn5_1    | 2023-03-18 09:21:50,892 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-18 09:21:50,894 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn5_1    | 2023-03-18 09:21:50,905 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-03-18 09:21:50,988 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-03-18 09:21:50,995 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn5_1    | 2023-03-18 09:21:51,006 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn5_1    | 2023-03-18 09:21:52,719 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn5_1    | 2023-03-18 09:21:52,748 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn5_1    | 2023-03-18 09:21:52,750 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn5_1    | 2023-03-18 09:21:52,756 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-03-18 09:21:52,757 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-03-18 09:21:52,785 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-03-18 09:21:52,801 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServer: 04b88c83-026f-4630-aada-108e37835bee: found a subdirectory /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c
dn5_1    | 2023-03-18 09:21:52,854 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServer: 04b88c83-026f-4630-aada-108e37835bee: addNew group-80313208139C:[] returns group-80313208139C:java.util.concurrent.CompletableFuture@121bfb1f[Not completed]
dn5_1    | 2023-03-18 09:21:52,854 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServer: 04b88c83-026f-4630-aada-108e37835bee: found a subdirectory /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d
dn5_1    | 2023-03-18 09:21:52,854 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServer: 04b88c83-026f-4630-aada-108e37835bee: addNew group-8191D6D2324D:[] returns group-8191D6D2324D:java.util.concurrent.CompletableFuture@2f165f16[Not completed]
dn5_1    | 2023-03-18 09:21:52,856 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServer: 04b88c83-026f-4630-aada-108e37835bee: found a subdirectory /data/metadata/ratis/b5ca130e-c58f-4d3d-8bda-3212c22edf58
dn5_1    | 2023-03-18 09:21:52,858 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServer: 04b88c83-026f-4630-aada-108e37835bee: addNew group-3212C22EDF58:[] returns group-3212C22EDF58:java.util.concurrent.CompletableFuture@5b60c663[Not completed]
dn5_1    | 2023-03-18 09:21:52,969 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn5_1    | 2023-03-18 09:21:53,303 [pool-38-thread-1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee: new RaftServerImpl for group-80313208139C:[] with ContainerStateMachine:uninitialized
dn5_1    | 2023-03-18 09:21:53,391 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-03-18 09:21:53,431 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-03-18 09:21:53,441 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-03-18 09:21:53,441 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-03-18 09:21:53,442 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-03-18 09:21:53,449 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-03-18 09:21:53,566 [pool-38-thread-1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-03-18 09:21:53,608 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-03-18 09:21:53,681 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-03-18 09:21:53,689 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-03-18 09:21:53,936 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-03-18 09:21:53,988 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-03-18 09:21:54,014 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-03-18 09:21:54,716 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-03-18 09:21:54,717 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-03-18 09:21:54,719 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-03-18 09:21:54,721 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-03-18 09:21:54,721 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn5_1    | 2023-03-18 09:21:54,759 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-03-18 09:21:54,845 [pool-38-thread-1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee: new RaftServerImpl for group-8191D6D2324D:[] with ContainerStateMachine:uninitialized
dn5_1    | 2023-03-18 09:21:54,859 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-03-18 09:21:54,860 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-03-18 09:21:54,860 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-03-18 09:21:54,870 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-03-18 09:21:54,885 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-03-18 09:21:54,888 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-03-18 09:21:54,891 [pool-38-thread-1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-03-18 09:21:54,892 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-03-18 09:24:19,951 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn2_1    | 2023-03-18 09:24:19,951 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn2_1    | 2023-03-18 09:24:19,952 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn2_1    | 2023-03-18 09:24:19,952 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn2_1    | 2023-03-18 09:24:19,952 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-03-18 09:24:19,953 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn2_1    | 2023-03-18 09:24:19,953 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2] INFO impl.RoleInfo: 2b374f74-8596-4317-81ba-ce0a491ddd96: start 2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderStateImpl
dn2_1    | 2023-03-18 09:24:19,953 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-SegmentedRaftLogWorker: Starting segment from index:0
dn2_1    | 2023-03-18 09:24:19,959 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-LeaderElection2] INFO server.RaftServer$Division: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C: set configuration 0: peers:[2b374f74-8596-4317-81ba-ce0a491ddd96|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:10.9.0.16:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-18 09:24:19,960 [2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2b374f74-8596-4317-81ba-ce0a491ddd96@group-ED015D5B9B4C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/747ad178-d87d-4bbd-985b-ed015d5b9b4c/current/log_inprogress_0
dn2_1    | 2023-03-18 09:25:08,800 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn2_1    | 2023-03-18 09:26:08,801 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn4_1    | 2023-03-18 09:22:09,396 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D: start as a follower, conf=25: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:09,397 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D: changes role from      null to FOLLOWER at term 6 for startAsFollower
dn4_1    | 2023-03-18 09:22:08,761 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO raftlog.RaftLog: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-SegmentedRaftLog: commitIndex: updateToMax old=27, new=26, updated? false
dn4_1    | 2023-03-18 09:22:09,417 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C: start as a follower, conf=14: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:09,417 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C: changes role from      null to FOLLOWER at term 5 for startAsFollower
dn4_1    | 2023-03-18 09:22:09,398 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: start 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-FollowerState
dn4_1    | 2023-03-18 09:22:09,417 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: start 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-FollowerState
dn4_1    | 2023-03-18 09:22:09,417 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: start 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-FollowerState
dn4_1    | 2023-03-18 09:22:09,443 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-40B9CE60BD6C,id=7c9e7a7d-2593-458c-8a46-0313d77c3278
dn4_1    | 2023-03-18 09:22:09,456 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-03-18 09:22:09,467 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-03-18 09:22:09,468 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-03-18 09:22:09,468 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-03-18 09:22:09,504 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-18 09:22:09,505 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-18 09:22:09,535 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-18 09:22:09,538 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-18 09:22:09,563 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-18 09:22:09,564 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-18 09:22:09,601 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8191D6D2324D,id=7c9e7a7d-2593-458c-8a46-0313d77c3278
dn4_1    | 2023-03-18 09:22:09,605 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-03-18 09:22:09,605 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-03-18 09:22:09,605 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-03-18 09:22:09,605 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-03-18 09:22:09,628 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-80313208139C,id=7c9e7a7d-2593-458c-8a46-0313d77c3278
dn4_1    | 2023-03-18 09:22:09,634 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-03-18 09:22:09,634 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-03-18 09:22:09,637 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-03-18 09:22:09,638 [7c9e7a7d-2593-458c-8a46-0313d77c3278-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-03-18 09:22:09,722 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 7c9e7a7d-2593-458c-8a46-0313d77c3278: start RPC server
dn4_1    | 2023-03-18 09:22:09,785 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 7c9e7a7d-2593-458c-8a46-0313d77c3278: GrpcService started, listening on 9858
dn4_1    | 2023-03-18 09:22:09,787 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 7c9e7a7d-2593-458c-8a46-0313d77c3278: GrpcService started, listening on 9856
dn4_1    | 2023-03-18 09:22:09,793 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 7c9e7a7d-2593-458c-8a46-0313d77c3278: GrpcService started, listening on 9857
dn4_1    | 2023-03-18 09:22:09,844 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 7c9e7a7d-2593-458c-8a46-0313d77c3278 is started using port 9858 for RATIS
dn4_1    | 2023-03-18 09:22:09,845 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 7c9e7a7d-2593-458c-8a46-0313d77c3278 is started using port 9857 for RATIS_ADMIN
dn4_1    | 2023-03-18 09:22:09,845 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 7c9e7a7d-2593-458c-8a46-0313d77c3278 is started using port 9856 for RATIS_SERVER
dn4_1    | 2023-03-18 09:22:09,845 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-7c9e7a7d-2593-458c-8a46-0313d77c3278: Started
dn4_1    | 2023-03-18 09:22:10,091 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn4_1    | 2023-03-18 09:22:14,523 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-FollowerState] INFO impl.FollowerState: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5124868099ns, electionTimeout:5013ms
dn4_1    | 2023-03-18 09:22:14,524 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-FollowerState] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: shutdown 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-FollowerState
dn4_1    | 2023-03-18 09:22:14,524 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-FollowerState] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn4_1    | 2023-03-18 09:22:14,527 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | 2023-03-18 09:22:14,527 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-FollowerState] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: start 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1
dn4_1    | 2023-03-18 09:22:14,560 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-FollowerState] INFO impl.FollowerState: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5143336400ns, electionTimeout:5020ms
dn4_1    | 2023-03-18 09:22:14,568 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-FollowerState] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: shutdown 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-FollowerState
dn4_1    | 2023-03-18 09:22:14,568 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-FollowerState] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D: changes role from  FOLLOWER to CANDIDATE at term 6 for changeToCandidate
dn4_1    | 2023-03-18 09:22:14,571 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | 2023-03-18 09:22:14,571 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-FollowerState] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: start 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-LeaderElection2
dn4_1    | 2023-03-18 09:22:14,574 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1] INFO impl.LeaderElection: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:14,587 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1] INFO impl.LeaderElection: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1 PRE_VOTE round 0: result PASSED (term=3)
dn4_1    | 2023-03-18 09:22:14,619 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-LeaderElection2] INFO impl.LeaderElection: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 6 for 25: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:14,675 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-FollowerState] INFO impl.FollowerState: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5257650009ns, electionTimeout:5049ms
dn4_1    | 2023-03-18 09:22:14,675 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-FollowerState] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: shutdown 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-FollowerState
dn4_1    | 2023-03-18 09:22:14,675 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-FollowerState] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C: changes role from  FOLLOWER to CANDIDATE at term 5 for changeToCandidate
dn4_1    | 2023-03-18 09:22:14,676 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | 2023-03-18 09:22:14,676 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-FollowerState] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: start 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-LeaderElection3
dn4_1    | 2023-03-18 09:22:14,685 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1] INFO impl.LeaderElection: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1 ELECTION round 0: submit vote requests at term 4 for 3: peers:[7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:14,701 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1] INFO impl.LeaderElection: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1 ELECTION round 0: result PASSED (term=4)
dn4_1    | 2023-03-18 09:22:14,701 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: shutdown 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1
dn4_1    | 2023-03-18 09:22:14,702 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn4_1    | 2023-03-18 09:22:14,708 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-LeaderElection3] INFO impl.LeaderElection: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 5 for 14: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:14,708 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-40B9CE60BD6C with new leaderId: 7c9e7a7d-2593-458c-8a46-0313d77c3278
dn5_1    | 2023-03-18 09:21:54,896 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-03-18 09:21:54,903 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-03-18 09:21:54,921 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-03-18 09:21:54,922 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-03-18 09:21:54,930 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-03-18 09:21:54,943 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-03-18 09:21:55,001 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-03-18 09:21:55,002 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-03-18 09:21:55,005 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om2_1    | 2023-03-18 09:22:14,359 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om2_1    | 2023-03-18 09:22:15,114 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1    | 2023-03-18 09:22:15,133 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om2_1    | 2023-03-18 09:22:15,146 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om2_1    | 2023-03-18 09:22:15,147 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om2_1    | 2023-03-18 09:22:15,148 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om2_1    | 2023-03-18 09:22:17,640 [main] INFO reflections.Reflections: Reflections took 3633 ms to scan 8 urls, producing 23 keys and 586 values [using 2 cores]
om2_1    | 2023-03-18 09:22:18,293 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om2_1    | 2023-03-18 09:22:18,365 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om2_1    | 2023-03-18 09:22:19,277 [Listener at om2/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om2_1    | 2023-03-18 09:22:19,370 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om2_1    | 2023-03-18 09:22:19,370 [Listener at om2/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om2_1    | 2023-03-18 09:22:19,648 [Listener at om2/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om2/10.9.0.12:9862
om2_1    | 2023-03-18 09:22:19,648 [Listener at om2/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om2 at port 9872
om2_1    | 2023-03-18 09:22:19,663 [om2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 6@7546b4869b99
om2_1    | 2023-03-18 09:22:19,681 [om2-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=7, votedFor=} from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/raft-meta
om2_1    | 2023-03-18 09:22:19,835 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-03-18 09:22:19,840 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om2_1    | 2023-03-18 09:22:19,876 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om2_1    | 2023-03-18 09:22:19,877 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-03-18 09:22:19,883 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om2_1    | 2023-03-18 09:22:19,889 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om2_1    | 2023-03-18 09:22:19,902 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1    | 2023-03-18 09:22:19,928 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om2_1    | 2023-03-18 09:22:19,929 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om2_1    | 2023-03-18 09:22:19,944 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om2@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om2_1    | 2023-03-18 09:22:19,946 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om2_1    | 2023-03-18 09:22:19,947 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om2_1    | 2023-03-18 09:22:19,949 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1    | 2023-03-18 09:22:19,952 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om2_1    | 2023-03-18 09:22:19,955 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om2_1    | 2023-03-18 09:22:19,964 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om2_1    | 2023-03-18 09:22:19,964 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om2_1    | 2023-03-18 09:22:19,965 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om2_1    | 2023-03-18 09:22:20,005 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om2_1    | 2023-03-18 09:22:20,011 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-03-18 09:22:20,105 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om2_1    | 2023-03-18 09:22:20,109 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om2_1    | 2023-03-18 09:22:20,114 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om2_1    | 2023-03-18 09:22:20,367 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-03-18 09:22:20,401 [om2-impl-thread1] INFO segmented.LogSegment: Successfully read 21 entries from segment file /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_0-20
om2_1    | 2023-03-18 09:22:20,422 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 21: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-03-18 09:22:20,458 [om2-impl-thread1] INFO segmented.LogSegment: Successfully read 48 entries from segment file /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_21-68
om2_1    | 2023-03-18 09:22:20,463 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-03-18 09:22:20,479 [om2-impl-thread1] INFO segmented.LogSegment: Successfully read 36 entries from segment file /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_69
om2_1    | 2023-03-18 09:22:20,481 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 104
om2_1    | 2023-03-18 09:22:20,482 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 68
om2_1    | 2023-03-18 09:22:20,630 [om2-impl-thread1] INFO raftlog.RaftLog: om2@group-D66704EFC61C-SegmentedRaftLog: commitIndex: updateToMax old=104, new=103, updated? false
om2_1    | 2023-03-18 09:22:20,631 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: start as a follower, conf=69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-03-18 09:22:20,633 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from      null to FOLLOWER at term 7 for startAsFollower
om2_1    | 2023-03-18 09:22:20,646 [om2-impl-thread1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
om2_1    | 2023-03-18 09:22:20,684 [om2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om2
om2_1    | 2023-03-18 09:22:20,687 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-03-18 09:22:20,687 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-03-18 09:22:20,694 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om2_1    | 2023-03-18 09:22:20,697 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om2_1    | 2023-03-18 09:22:20,699 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om2_1    | 2023-03-18 09:22:20,700 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om2_1    | 2023-03-18 09:22:20,714 [Listener at om2/9862] INFO server.RaftServer: om2: start RPC server
om2_1    | 2023-03-18 09:22:21,030 [Listener at om2/9862] INFO server.GrpcService: om2: GrpcService started, listening on 9872
om2_1    | 2023-03-18 09:22:21,043 [Listener at om2/9862] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
om2_1    | 2023-03-18 09:22:21,048 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om2: Started
om2_1    | 2023-03-18 09:22:21,050 [Listener at om2/9862] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
om2_1    | 2023-03-18 09:22:21,052 [Listener at om2/9862] INFO om.OzoneManager: Version File has different layout version (0) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om2_1    | 2023-03-18 09:22:21,318 [Listener at om2/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om2_1    | 2023-03-18 09:22:21,321 [Listener at om2/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om2_1    | 2023-03-18 09:22:21,406 [Listener at om2/9862] INFO util.log: Logging initialized @64133ms to org.eclipse.jetty.util.log.Slf4jLog
om2_1    | 2023-03-18 09:22:21,914 [Listener at om2/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn1_1    | 2023-03-18 09:24:13,151 [Command processor thread] INFO server.RaftServer: 4ba198f1-a013-49ce-b059-f73788d69001: addNew group-D4E599C27FE1:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER] returns      null 4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1:t0, leader=null, voted=null, raftlog=UNINITIALIZED, conf=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null NEW
dn1_1    | 2023-03-18 09:24:13,152 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d5743016-6c44-4b2d-ae26-d4e599c27fe1 does not exist. Creating ...
dn1_1    | 2023-03-18 09:24:13,154 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d5743016-6c44-4b2d-ae26-d4e599c27fe1/in_use.lock acquired by nodename 7@8698bf9a31d9
dn1_1    | 2023-03-18 09:24:13,156 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d5743016-6c44-4b2d-ae26-d4e599c27fe1 has been successfully formatted.
dn1_1    | 2023-03-18 09:24:13,156 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-D4E599C27FE1: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn1_1    | 2023-03-18 09:24:13,156 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-03-18 09:24:13,157 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-03-18 09:24:13,157 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-03-18 09:24:13,157 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-03-18 09:24:13,157 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-03-18 09:24:13,157 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-03-18 09:24:13,157 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-03-18 09:24:13,158 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-03-18 09:24:13,158 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d5743016-6c44-4b2d-ae26-d4e599c27fe1
dn1_1    | 2023-03-18 09:24:13,158 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-03-18 09:24:13,158 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-03-18 09:24:13,158 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-03-18 09:24:13,158 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-03-18 09:24:13,159 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-03-18 09:24:13,159 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-03-18 09:24:13,160 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-03-18 09:24:13,160 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-03-18 09:24:13,161 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-03-18 09:24:13,162 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-03-18 09:24:13,185 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-03-18 09:24:13,186 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-03-18 09:24:13,186 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-03-18 09:24:13,186 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-03-18 09:24:13,186 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-03-18 09:24:13,186 [pool-22-thread-1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1: start as a follower, conf=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-18 09:24:13,186 [pool-22-thread-1] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn1_1    | 2023-03-18 09:24:13,187 [pool-22-thread-1] INFO impl.RoleInfo: 4ba198f1-a013-49ce-b059-f73788d69001: start 4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-FollowerState
dn1_1    | 2023-03-18 09:24:13,201 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D4E599C27FE1,id=4ba198f1-a013-49ce-b059-f73788d69001
dn1_1    | 2023-03-18 09:24:13,201 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-03-18 09:24:13,202 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-03-18 09:24:13,202 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-03-18 09:24:13,202 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-03-18 09:24:13,207 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-03-18 09:24:13,208 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=d5743016-6c44-4b2d-ae26-d4e599c27fe1
dn1_1    | 2023-03-18 09:24:13,214 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=d5743016-6c44-4b2d-ae26-d4e599c27fe1.
dn1_1    | 2023-03-18 09:24:13,217 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-03-18 09:24:18,388 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-FollowerState] INFO impl.FollowerState: 4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5200981473ns, electionTimeout:5170ms
dn1_1    | 2023-03-18 09:24:18,388 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-FollowerState] INFO impl.RoleInfo: 4ba198f1-a013-49ce-b059-f73788d69001: shutdown 4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-FollowerState
dn1_1    | 2023-03-18 09:24:18,388 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-FollowerState] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1    | 2023-03-18 09:21:28,497 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1    | /************************************************************
om1_1    | STARTUP_MSG: Starting OzoneManager
om1_1    | STARTUP_MSG:   host = 8260fa04b223/10.9.0.11
om1_1    | STARTUP_MSG:   args = [--upgrade]
om1_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/1236414c1c46c67c92f37499b0b61921cdfab39a ; compiled by 'runner' on 2023-03-18T08:55Z
om1_1    | STARTUP_MSG:   java = 11.0.14.1
om1_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4))$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om1_1    | ************************************************************/
om1_1    | 2023-03-18 09:21:28,578 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1    | 2023-03-18 09:21:37,455 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1    | 2023-03-18 09:21:40,497 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om1_1    | 2023-03-18 09:21:40,832 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1    | 2023-03-18 09:21:40,832 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
om1_1    | 2023-03-18 09:21:40,866 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-03-18 09:21:41,059 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = INITIAL_VERSION (version = 0), software layout = MULTITENANCY_SCHEMA (version = 3)
om1_1    | 2023-03-18 09:21:43,436 [main] INFO reflections.Reflections: Reflections took 1850 ms to scan 1 urls, producing 126 keys and 369 values [using 2 cores]
om1_1    | 2023-03-18 09:21:43,543 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-03-18 09:21:43,658 [main] WARN om.OzoneManager: ozone.default.bucket.layout configured to non-legacy bucket layout FILE_SYSTEM_OPTIMIZED when Ozone Manager is pre-finalized for bucket layout support. Legacy buckets will be created by default until Ozone Manager is finalized.
om1_1    | 2023-03-18 09:21:46,097 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863]
om1_1    | 2023-03-18 09:21:46,408 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863]
om1_1    | 2023-03-18 09:21:49,029 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8260fa04b223/10.9.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-03-18 09:21:51,030 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8260fa04b223/10.9.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-03-18 09:21:53,032 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8260fa04b223/10.9.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-03-18 09:21:55,034 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8260fa04b223/10.9.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-03-18 09:21:57,035 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8260fa04b223/10.9.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-03-18 09:21:59,038 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8260fa04b223/10.9.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-03-18 09:22:01,040 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8260fa04b223/10.9.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-03-18 09:22:07,651 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om1_1    | 2023-03-18 09:22:07,840 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-03-18 09:22:09,316 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om1_1    | 2023-03-18 09:22:09,317 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om1_1    | 2023-03-18 09:22:10,305 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om1_1    | 2023-03-18 09:22:10,491 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn1_1    | 2023-03-18 09:24:18,389 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-03-18 09:24:18,389 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-FollowerState] INFO impl.RoleInfo: 4ba198f1-a013-49ce-b059-f73788d69001: start 4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3
dn1_1    | 2023-03-18 09:24:18,393 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3] INFO impl.LeaderElection: 4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-18 09:24:18,393 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3] INFO impl.LeaderElection: 4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3 PRE_VOTE round 0: result PASSED (term=0)
dn1_1    | 2023-03-18 09:24:18,396 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3] INFO impl.LeaderElection: 4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-18 09:24:18,396 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3] INFO impl.LeaderElection: 4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3 ELECTION round 0: result PASSED (term=1)
dn1_1    | 2023-03-18 09:24:18,396 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3] INFO impl.RoleInfo: 4ba198f1-a013-49ce-b059-f73788d69001: shutdown 4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3
dn1_1    | 2023-03-18 09:24:18,396 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn1_1    | 2023-03-18 09:24:18,397 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D4E599C27FE1 with new leaderId: 4ba198f1-a013-49ce-b059-f73788d69001
dn1_1    | 2023-03-18 09:24:18,399 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1: change Leader from null to 4ba198f1-a013-49ce-b059-f73788d69001 at term 1 for becomeLeader, leader elected after 5249ms
dn1_1    | 2023-03-18 09:24:18,405 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn1_1    | 2023-03-18 09:24:18,412 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-03-18 09:24:18,413 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn1_1    | 2023-03-18 09:24:18,413 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn1_1    | 2023-03-18 09:24:18,414 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn1_1    | 2023-03-18 09:24:18,414 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn1_1    | 2023-03-18 09:24:18,415 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-03-18 09:24:18,415 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn1_1    | 2023-03-18 09:24:18,416 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3] INFO impl.RoleInfo: 4ba198f1-a013-49ce-b059-f73788d69001: start 4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderStateImpl
dn1_1    | 2023-03-18 09:24:18,417 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-SegmentedRaftLogWorker: Starting segment from index:0
dn1_1    | 2023-03-18 09:24:18,419 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d5743016-6c44-4b2d-ae26-d4e599c27fe1/current/log_inprogress_0
dn1_1    | 2023-03-18 09:24:18,422 [4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1-LeaderElection3] INFO server.RaftServer$Division: 4ba198f1-a013-49ce-b059-f73788d69001@group-D4E599C27FE1: set configuration 0: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-18 09:25:07,014 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-03-18 09:26:07,015 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-03-18 09:22:08,805 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D: set configuration 0: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:08,810 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/d1279162-045f-45cf-95f2-d1111d92119b/current/log_0-0
dn3_1    | 2023-03-18 09:22:08,914 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO segmented.LogSegment: Successfully read 5 entries from segment file /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/current/log_0-4
dn3_1    | 2023-03-18 09:22:08,891 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO segmented.LogSegment: Successfully read 8 entries from segment file /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/current/log_0-7
dn3_1    | 2023-03-18 09:22:08,934 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D: set configuration 8: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:08,935 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO segmented.LogSegment: Successfully read 17 entries from segment file /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/current/log_8-24
dn3_1    | 2023-03-18 09:22:09,025 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D: set configuration 25: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:08,914 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B: set configuration 1: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:09,070 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/d1279162-045f-45cf-95f2-d1111d92119b/current/log_1-2
dn3_1    | 2023-03-18 09:22:09,072 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B: set configuration 3: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:08,934 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C: set configuration 5: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:09,098 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO segmented.LogSegment: Successfully read 9 entries from segment file /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/current/log_5-13
dn3_1    | 2023-03-18 09:22:09,105 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C: set configuration 14: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:09,107 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO segmented.LogSegment: Successfully read 14 entries from segment file /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/current/log_inprogress_14
dn3_1    | 2023-03-18 09:22:09,084 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO segmented.LogSegment: Successfully read 14 entries from segment file /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/current/log_inprogress_25
dn3_1    | 2023-03-18 09:22:09,072 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/d1279162-045f-45cf-95f2-d1111d92119b/current/log_inprogress_3
dn3_1    | 2023-03-18 09:22:09,204 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn3_1    | 2023-03-18 09:22:09,204 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn3_1    | 2023-03-18 09:22:09,200 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 27
dn3_1    | 2023-03-18 09:22:09,207 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 13
dn3_1    | 2023-03-18 09:22:09,232 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 38
dn3_1    | 2023-03-18 09:22:09,232 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 24
dn3_1    | 2023-03-18 09:22:10,346 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO raftlog.RaftLog: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-SegmentedRaftLog: commitIndex: updateToMax old=4, new=3, updated? false
dn3_1    | 2023-03-18 09:22:10,351 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B: start as a follower, conf=3: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:10,359 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn3_1    | 2023-03-18 09:22:10,347 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO raftlog.RaftLog: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-SegmentedRaftLog: commitIndex: updateToMax old=27, new=26, updated? false
dn3_1    | 2023-03-18 09:22:10,374 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO raftlog.RaftLog: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-SegmentedRaftLog: commitIndex: updateToMax old=38, new=36, updated? false
dn3_1    | 2023-03-18 09:22:10,408 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C: start as a follower, conf=14: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:10,408 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C: changes role from      null to FOLLOWER at term 5 for startAsFollower
dn3_1    | 2023-03-18 09:22:10,408 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D: start as a follower, conf=25: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:10,414 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D: changes role from      null to FOLLOWER at term 6 for startAsFollower
dn3_1    | 2023-03-18 09:22:10,415 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: start bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-FollowerState
dn3_1    | 2023-03-18 09:22:10,425 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: start bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-FollowerState
dn3_1    | 2023-03-18 09:22:10,416 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: start bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-FollowerState
dn3_1    | 2023-03-18 09:22:10,433 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8191D6D2324D,id=bae0cd8d-e50f-4d23-8974-b816860f8fc3
dn3_1    | 2023-03-18 09:22:10,443 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-03-18 09:22:10,447 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-03-18 09:22:10,447 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-03-18 09:22:10,448 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-03-18 09:22:10,461 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-03-18 09:22:10,469 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-18 09:22:10,516 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D1111D92119B,id=bae0cd8d-e50f-4d23-8974-b816860f8fc3
dn3_1    | 2023-03-18 09:22:10,518 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-03-18 09:22:10,519 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-03-18 09:22:10,519 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-03-18 09:22:10,520 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-03-18 09:22:10,534 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-03-18 09:22:10,538 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-18 09:22:10,544 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-03-18 09:22:10,551 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-18 09:22:10,541 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-80313208139C,id=bae0cd8d-e50f-4d23-8974-b816860f8fc3
dn3_1    | 2023-03-18 09:22:10,551 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-03-18 09:22:10,552 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-03-18 09:22:10,554 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-03-18 09:22:10,554 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-03-18 09:22:10,602 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.RaftServer: bae0cd8d-e50f-4d23-8974-b816860f8fc3: start RPC server
dn3_1    | 2023-03-18 09:22:10,614 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: bae0cd8d-e50f-4d23-8974-b816860f8fc3: GrpcService started, listening on 9858
om1_1    | 2023-03-18 09:22:11,142 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1    | 2023-03-18 09:22:11,146 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om1_1    | 2023-03-18 09:22:11,302 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om1_1    | 2023-03-18 09:22:12,042 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om1_1    | 2023-03-18 09:22:12,121 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1    | 2023-03-18 09:22:12,332 [Finalizer] WARN managed.ManagedRocksObjectUtils: ManagedColumnFamilyOptions is not closed properly
om1_1    | 2023-03-18 09:22:12,336 [Finalizer] WARN managed.ManagedRocksObjectUtils: ManagedColumnFamilyOptions is not closed properly
om1_1    | 2023-03-18 09:22:12,337 [Finalizer] WARN managed.ManagedRocksObjectUtils: ManagedColumnFamilyOptions is not closed properly
om1_1    | 2023-03-18 09:22:12,339 [Finalizer] WARN managed.ManagedRocksObjectUtils: ManagedColumnFamilyOptions is not closed properly
om1_1    | 2023-03-18 09:22:12,339 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
om1_1    | 2023-03-18 09:22:12,388 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om1:9872, om3:9872, om2:9872
om1_1    | 2023-03-18 09:22:12,436 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:7, i:104)
om1_1    | 2023-03-18 09:22:12,580 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om1_1    | 2023-03-18 09:22:12,818 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om1_1    | 2023-03-18 09:22:12,824 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om1_1    | 2023-03-18 09:22:12,826 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om1_1    | 2023-03-18 09:22:12,827 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om1_1    | 2023-03-18 09:22:12,828 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om1_1    | 2023-03-18 09:22:12,828 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om1_1    | 2023-03-18 09:22:12,828 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om1_1    | 2023-03-18 09:22:12,836 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-03-18 09:22:12,838 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om1_1    | 2023-03-18 09:22:12,840 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1    | 2023-03-18 09:22:12,888 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1    | 2023-03-18 09:22:12,901 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om1_1    | 2023-03-18 09:22:12,905 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om1_1    | 2023-03-18 09:22:13,834 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om1_1    | 2023-03-18 09:22:13,949 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om1_1    | 2023-03-18 09:22:13,950 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om1_1    | 2023-03-18 09:22:13,956 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1    | 2023-03-18 09:22:13,956 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | 2023-03-18 09:22:13,979 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1    | 2023-03-18 09:22:14,008 [om1-impl-thread1] INFO server.RaftServer: om1: found a subdirectory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om1_1    | 2023-03-18 09:22:14,015 [main] INFO server.RaftServer: om1: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@380ce5a3[Not completed]
om1_1    | 2023-03-18 09:22:14,016 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om1_1    | 2023-03-18 09:22:14,019 [main] INFO om.OzoneManager: Creating RPC Server
om1_1    | 2023-03-18 09:22:14,138 [pool-29-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om1_1    | 2023-03-18 09:22:14,154 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om1_1    | 2023-03-18 09:22:14,158 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om1_1    | 2023-03-18 09:22:14,158 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om1_1    | 2023-03-18 09:22:14,159 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1    | 2023-03-18 09:22:14,162 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | 2023-03-18 09:22:14,165 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om1_1    | 2023-03-18 09:22:14,254 [pool-29-thread-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om1_1    | 2023-03-18 09:22:14,254 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1    | 2023-03-18 09:22:14,309 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om1_1    | 2023-03-18 09:22:14,311 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om1_1    | 2023-03-18 09:22:14,499 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om1_1    | 2023-03-18 09:22:14,536 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om1_1    | 2023-03-18 09:22:14,543 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om1_1    | 2023-03-18 09:22:15,531 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1    | 2023-03-18 09:22:15,581 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om1_1    | 2023-03-18 09:22:15,582 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om1_1    | 2023-03-18 09:22:15,590 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om1_1    | 2023-03-18 09:22:15,629 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-03-18 09:22:10,643 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: bae0cd8d-e50f-4d23-8974-b816860f8fc3: GrpcService started, listening on 9856
dn3_1    | 2023-03-18 09:22:10,662 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: bae0cd8d-e50f-4d23-8974-b816860f8fc3: GrpcService started, listening on 9857
dn3_1    | 2023-03-18 09:22:10,686 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis bae0cd8d-e50f-4d23-8974-b816860f8fc3 is started using port 9858 for RATIS
dn3_1    | 2023-03-18 09:22:10,688 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis bae0cd8d-e50f-4d23-8974-b816860f8fc3 is started using port 9857 for RATIS_ADMIN
dn3_1    | 2023-03-18 09:22:10,689 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis bae0cd8d-e50f-4d23-8974-b816860f8fc3 is started using port 9856 for RATIS_SERVER
dn3_1    | 2023-03-18 09:22:10,696 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-bae0cd8d-e50f-4d23-8974-b816860f8fc3: Started
dn3_1    | 2023-03-18 09:22:10,900 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-03-18 09:22:15,488 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-FollowerState] INFO impl.FollowerState: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5071840992ns, electionTimeout:5017ms
dn3_1    | 2023-03-18 09:22:15,489 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-FollowerState] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: shutdown bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-FollowerState
dn3_1    | 2023-03-18 09:22:15,490 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-FollowerState] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn3_1    | 2023-03-18 09:22:15,497 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-03-18 09:22:15,498 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-FollowerState] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: start bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1
dn3_1    | 2023-03-18 09:22:15,508 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1] INFO impl.LeaderElection: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:15,516 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1] INFO impl.LeaderElection: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1 PRE_VOTE round 0: result PASSED (term=3)
dn3_1    | 2023-03-18 09:22:15,609 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1] INFO impl.LeaderElection: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1 ELECTION round 0: submit vote requests at term 4 for 3: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:15,613 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1] INFO impl.LeaderElection: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1 ELECTION round 0: result PASSED (term=4)
dn3_1    | 2023-03-18 09:22:15,613 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: shutdown bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1
dn3_1    | 2023-03-18 09:22:15,618 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn3_1    | 2023-03-18 09:22:15,618 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D1111D92119B with new leaderId: bae0cd8d-e50f-4d23-8974-b816860f8fc3
dn3_1    | 2023-03-18 09:22:15,626 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B: change Leader from null to bae0cd8d-e50f-4d23-8974-b816860f8fc3 at term 4 for becomeLeader, leader elected after 23419ms
dn3_1    | 2023-03-18 09:22:15,663 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-FollowerState] INFO impl.FollowerState: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5237347104ns, electionTimeout:5099ms
dn3_1    | 2023-03-18 09:22:15,669 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-FollowerState] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: shutdown bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-FollowerState
dn3_1    | 2023-03-18 09:22:15,670 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-FollowerState] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D: changes role from  FOLLOWER to CANDIDATE at term 6 for changeToCandidate
dn3_1    | 2023-03-18 09:22:15,670 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-03-18 09:22:15,670 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-FollowerState] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: start bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-LeaderElection2
dn3_1    | 2023-03-18 09:22:15,689 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-LeaderElection2] INFO impl.LeaderElection: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 6 for 25: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:15,707 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn3_1    | 2023-03-18 09:22:15,747 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-FollowerState] INFO impl.FollowerState: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5334992912ns, electionTimeout:5196ms
dn3_1    | 2023-03-18 09:22:15,752 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-FollowerState] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: shutdown bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-FollowerState
dn3_1    | 2023-03-18 09:22:15,753 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-FollowerState] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C: changes role from  FOLLOWER to CANDIDATE at term 5 for changeToCandidate
dn4_1    | 2023-03-18 09:22:14,731 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C: change Leader from null to 7c9e7a7d-2593-458c-8a46-0313d77c3278 at term 4 for becomeLeader, leader elected after 19761ms
dn4_1    | 2023-03-18 09:22:14,804 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn4_1    | 2023-03-18 09:22:14,816 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-LeaderElection3-1] INFO server.GrpcServerProtocolClient: Build channel for bae0cd8d-e50f-4d23-8974-b816860f8fc3
dn4_1    | 2023-03-18 09:22:14,825 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-18 09:22:14,833 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-18 09:22:14,819 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-LeaderElection3-2] INFO server.GrpcServerProtocolClient: Build channel for 04b88c83-026f-4630-aada-108e37835bee
dn4_1    | 2023-03-18 09:22:14,823 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-03-18 09:22:14,816 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-18 09:22:14,845 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-18 09:22:14,847 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn4_1    | 2023-03-18 09:22:14,937 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn4_1    | 2023-03-18 09:22:14,939 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn4_1    | 2023-03-18 09:22:14,940 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn4_1    | 2023-03-18 09:22:14,959 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-03-18 09:22:14,994 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn4_1    | 2023-03-18 09:22:15,019 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: start 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderStateImpl
dn4_1    | 2023-03-18 09:22:15,060 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn4_1    | 2023-03-18 09:22:15,101 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/a2f44939-53c6-4b4c-afd2-40b9ce60bd6c/current/log_inprogress_3 to /data/metadata/ratis/a2f44939-53c6-4b4c-afd2-40b9ce60bd6c/current/log_3-4
dn4_1    | 2023-03-18 09:22:15,102 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderElection1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C: set configuration 5: peers:[7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:15,165 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a2f44939-53c6-4b4c-afd2-40b9ce60bd6c/current/log_inprogress_5
dn4_1    | 2023-03-18 09:22:18,083 [grpc-default-executor-0] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C: receive requestVote(PRE_VOTE, 04b88c83-026f-4630-aada-108e37835bee, group-80313208139C, 5, (t:5, i:27))
dn4_1    | 2023-03-18 09:22:18,094 [grpc-default-executor-1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D: receive requestVote(PRE_VOTE, 04b88c83-026f-4630-aada-108e37835bee, group-8191D6D2324D, 6, (t:6, i:38))
dn4_1    | 2023-03-18 09:22:18,098 [grpc-default-executor-0] INFO impl.VoteContext: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-CANDIDATE: accept PRE_VOTE from 04b88c83-026f-4630-aada-108e37835bee: our priority 0 <= candidate's priority 0
dn4_1    | 2023-03-18 09:22:18,132 [grpc-default-executor-0] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C replies to PRE_VOTE vote request: 04b88c83-026f-4630-aada-108e37835bee<-7c9e7a7d-2593-458c-8a46-0313d77c3278#0:OK-t5. Peer's state: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C:t5, leader=null, voted=bae0cd8d-e50f-4d23-8974-b816860f8fc3, raftlog=Memoized:7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-SegmentedRaftLog:OPENED:c27, conf=14: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:18,100 [grpc-default-executor-1] INFO impl.VoteContext: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-CANDIDATE: accept PRE_VOTE from 04b88c83-026f-4630-aada-108e37835bee: our priority 0 <= candidate's priority 1
dn4_1    | 2023-03-18 09:22:18,203 [grpc-default-executor-1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D replies to PRE_VOTE vote request: 04b88c83-026f-4630-aada-108e37835bee<-7c9e7a7d-2593-458c-8a46-0313d77c3278#0:OK-t6. Peer's state: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D:t6, leader=null, voted=04b88c83-026f-4630-aada-108e37835bee, raftlog=Memoized:7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-SegmentedRaftLog:OPENED:c38, conf=25: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:18,286 [grpc-default-executor-2] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C: receive requestVote(PRE_VOTE, bae0cd8d-e50f-4d23-8974-b816860f8fc3, group-80313208139C, 5, (t:5, i:27))
dn4_1    | 2023-03-18 09:22:18,316 [grpc-default-executor-2] INFO impl.VoteContext: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-CANDIDATE: accept PRE_VOTE from bae0cd8d-e50f-4d23-8974-b816860f8fc3: our priority 0 <= candidate's priority 1
dn4_1    | 2023-03-18 09:22:18,316 [grpc-default-executor-2] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C replies to PRE_VOTE vote request: bae0cd8d-e50f-4d23-8974-b816860f8fc3<-7c9e7a7d-2593-458c-8a46-0313d77c3278#0:OK-t5. Peer's state: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C:t5, leader=null, voted=bae0cd8d-e50f-4d23-8974-b816860f8fc3, raftlog=Memoized:7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-SegmentedRaftLog:OPENED:c27, conf=14: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:18,297 [grpc-default-executor-3] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D: receive requestVote(PRE_VOTE, bae0cd8d-e50f-4d23-8974-b816860f8fc3, group-8191D6D2324D, 6, (t:6, i:38))
dn4_1    | 2023-03-18 09:22:18,289 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-LeaderElection3] INFO impl.LeaderElection: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-LeaderElection3: PRE_VOTE REJECTED received 1 response(s) and 0 exception(s):
dn4_1    | 2023-03-18 09:22:18,353 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-LeaderElection3] INFO impl.LeaderElection:   Response 0: 7c9e7a7d-2593-458c-8a46-0313d77c3278<-bae0cd8d-e50f-4d23-8974-b816860f8fc3#0:FAIL-t5
dn4_1    | 2023-03-18 09:22:18,354 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-LeaderElection3] INFO impl.LeaderElection: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-LeaderElection3 PRE_VOTE round 0: result REJECTED
dn4_1    | 2023-03-18 09:22:18,354 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-LeaderElection3] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C: changes role from CANDIDATE to FOLLOWER at term 5 for REJECTED
dn4_1    | 2023-03-18 09:22:18,354 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-LeaderElection3] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: shutdown 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-LeaderElection3
dn4_1    | 2023-03-18 09:22:18,355 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-LeaderElection3] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: start 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-FollowerState
dn4_1    | 2023-03-18 09:22:18,375 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-LeaderElection2] INFO impl.LeaderElection: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-LeaderElection2: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
dn4_1    | 2023-03-18 09:22:18,376 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-LeaderElection2] INFO impl.LeaderElection:   Response 0: 7c9e7a7d-2593-458c-8a46-0313d77c3278<-bae0cd8d-e50f-4d23-8974-b816860f8fc3#0:OK-t6
dn4_1    | 2023-03-18 09:22:18,376 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-LeaderElection2] INFO impl.LeaderElection:   Response 1: 7c9e7a7d-2593-458c-8a46-0313d77c3278<-04b88c83-026f-4630-aada-108e37835bee#0:FAIL-t6
dn4_1    | 2023-03-18 09:22:18,377 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-LeaderElection2] INFO impl.LeaderElection: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-LeaderElection2 PRE_VOTE round 0: result REJECTED
dn4_1    | 2023-03-18 09:22:18,377 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-LeaderElection2] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D: changes role from CANDIDATE to FOLLOWER at term 6 for REJECTED
dn4_1    | 2023-03-18 09:22:18,377 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-LeaderElection2] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: shutdown 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-LeaderElection2
dn4_1    | 2023-03-18 09:22:18,378 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-LeaderElection2] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: start 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-FollowerState
dn4_1    | 2023-03-18 09:22:18,383 [grpc-default-executor-3] INFO impl.VoteContext: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-FOLLOWER: accept PRE_VOTE from bae0cd8d-e50f-4d23-8974-b816860f8fc3: our priority 0 <= candidate's priority 0
dn4_1    | 2023-03-18 09:22:18,390 [grpc-default-executor-3] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D replies to PRE_VOTE vote request: bae0cd8d-e50f-4d23-8974-b816860f8fc3<-7c9e7a7d-2593-458c-8a46-0313d77c3278#0:OK-t6. Peer's state: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D:t6, leader=null, voted=04b88c83-026f-4630-aada-108e37835bee, raftlog=Memoized:7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-SegmentedRaftLog:OPENED:c38, conf=25: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:18,395 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-18 09:22:18,445 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-18 09:22:18,439 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-18 09:22:18,436 [grpc-default-executor-2] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D: receive requestVote(ELECTION, 04b88c83-026f-4630-aada-108e37835bee, group-8191D6D2324D, 7, (t:6, i:38))
dn4_1    | 2023-03-18 09:22:18,449 [grpc-default-executor-2] INFO impl.VoteContext: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-FOLLOWER: accept ELECTION from 04b88c83-026f-4630-aada-108e37835bee: our priority 0 <= candidate's priority 1
dn4_1    | 2023-03-18 09:22:18,449 [grpc-default-executor-2] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D: changes role from  FOLLOWER to FOLLOWER at term 7 for candidate:04b88c83-026f-4630-aada-108e37835bee
dn4_1    | 2023-03-18 09:22:18,450 [grpc-default-executor-2] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: shutdown 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-FollowerState
dn4_1    | 2023-03-18 09:22:18,450 [grpc-default-executor-2] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: start 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-FollowerState
dn4_1    | 2023-03-18 09:22:18,451 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-18 09:22:18,451 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-FollowerState] INFO impl.FollowerState: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-FollowerState was interrupted
dn3_1    | 2023-03-18 09:22:15,753 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-03-18 09:22:15,753 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-FollowerState] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: start bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3
dn3_1    | 2023-03-18 09:22:15,783 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO impl.LeaderElection: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 5 for 14: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:15,784 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-03-18 09:22:15,784 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-03-18 09:22:15,833 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-03-18 09:22:15,835 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-03-18 09:22:15,839 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn3_1    | 2023-03-18 09:22:15,853 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-03-18 09:22:15,866 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-18 09:22:15,880 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-03-18 09:22:15,881 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-18 09:22:15,881 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3-2] INFO server.GrpcServerProtocolClient: Build channel for 04b88c83-026f-4630-aada-108e37835bee
dn3_1    | 2023-03-18 09:22:15,922 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-03-18 09:22:15,890 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3-1] INFO server.GrpcServerProtocolClient: Build channel for 7c9e7a7d-2593-458c-8a46-0313d77c3278
dn3_1    | 2023-03-18 09:22:15,970 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn3_1    | 2023-03-18 09:22:16,047 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: start bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderStateImpl
dn3_1    | 2023-03-18 09:22:16,092 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn3_1    | 2023-03-18 09:22:16,104 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/d1279162-045f-45cf-95f2-d1111d92119b/current/log_inprogress_3 to /data/metadata/ratis/d1279162-045f-45cf-95f2-d1111d92119b/current/log_3-4
dn3_1    | 2023-03-18 09:22:16,136 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d1279162-045f-45cf-95f2-d1111d92119b/current/log_inprogress_5
dn3_1    | 2023-03-18 09:22:16,249 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderElection1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B: set configuration 5: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:18,023 [grpc-default-executor-0] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D: receive requestVote(PRE_VOTE, 04b88c83-026f-4630-aada-108e37835bee, group-8191D6D2324D, 6, (t:6, i:38))
dn3_1    | 2023-03-18 09:22:18,032 [grpc-default-executor-2] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C: receive requestVote(PRE_VOTE, 04b88c83-026f-4630-aada-108e37835bee, group-80313208139C, 5, (t:5, i:27))
dn3_1    | 2023-03-18 09:22:18,040 [grpc-default-executor-1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D: receive requestVote(PRE_VOTE, 7c9e7a7d-2593-458c-8a46-0313d77c3278, group-8191D6D2324D, 6, (t:6, i:38))
dn3_1    | 2023-03-18 09:22:18,044 [grpc-default-executor-0] INFO impl.VoteContext: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-CANDIDATE: accept PRE_VOTE from 04b88c83-026f-4630-aada-108e37835bee: our priority 0 <= candidate's priority 1
dn3_1    | 2023-03-18 09:22:18,085 [grpc-default-executor-0] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D replies to PRE_VOTE vote request: 04b88c83-026f-4630-aada-108e37835bee<-bae0cd8d-e50f-4d23-8974-b816860f8fc3#0:OK-t6. Peer's state: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D:t6, leader=null, voted=04b88c83-026f-4630-aada-108e37835bee, raftlog=Memoized:bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-SegmentedRaftLog:OPENED:c38, conf=25: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:18,116 [grpc-default-executor-3] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C: receive requestVote(PRE_VOTE, 7c9e7a7d-2593-458c-8a46-0313d77c3278, group-80313208139C, 5, (t:5, i:27))
dn3_1    | 2023-03-18 09:22:18,085 [grpc-default-executor-2] INFO impl.VoteContext: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-CANDIDATE: reject PRE_VOTE from 04b88c83-026f-4630-aada-108e37835bee: our priority 1 > candidate's priority 0
dn3_1    | 2023-03-18 09:22:18,137 [grpc-default-executor-2] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C replies to PRE_VOTE vote request: 04b88c83-026f-4630-aada-108e37835bee<-bae0cd8d-e50f-4d23-8974-b816860f8fc3#0:FAIL-t5. Peer's state: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C:t5, leader=null, voted=bae0cd8d-e50f-4d23-8974-b816860f8fc3, raftlog=Memoized:bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-SegmentedRaftLog:OPENED:c27, conf=14: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:18,138 [grpc-default-executor-3] INFO impl.VoteContext: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-CANDIDATE: reject PRE_VOTE from 7c9e7a7d-2593-458c-8a46-0313d77c3278: our priority 1 > candidate's priority 0
dn3_1    | 2023-03-18 09:22:18,138 [grpc-default-executor-3] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C replies to PRE_VOTE vote request: 7c9e7a7d-2593-458c-8a46-0313d77c3278<-bae0cd8d-e50f-4d23-8974-b816860f8fc3#0:FAIL-t5. Peer's state: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C:t5, leader=null, voted=bae0cd8d-e50f-4d23-8974-b816860f8fc3, raftlog=Memoized:bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-SegmentedRaftLog:OPENED:c27, conf=14: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:18,089 [grpc-default-executor-1] INFO impl.VoteContext: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-CANDIDATE: accept PRE_VOTE from 7c9e7a7d-2593-458c-8a46-0313d77c3278: our priority 0 <= candidate's priority 0
dn3_1    | 2023-03-18 09:22:18,168 [grpc-default-executor-1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D replies to PRE_VOTE vote request: 7c9e7a7d-2593-458c-8a46-0313d77c3278<-bae0cd8d-e50f-4d23-8974-b816860f8fc3#0:OK-t6. Peer's state: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D:t6, leader=null, voted=04b88c83-026f-4630-aada-108e37835bee, raftlog=Memoized:bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-SegmentedRaftLog:OPENED:c38, conf=25: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:18,327 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO impl.LeaderElection: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn3_1    | 2023-03-18 09:22:18,331 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO impl.LeaderElection:   Response 0: bae0cd8d-e50f-4d23-8974-b816860f8fc3<-7c9e7a7d-2593-458c-8a46-0313d77c3278#0:OK-t5
dn3_1    | 2023-03-18 09:22:18,331 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO impl.LeaderElection: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3 PRE_VOTE round 0: result PASSED
dn3_1    | 2023-03-18 09:22:18,336 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO impl.LeaderElection: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3 ELECTION round 0: submit vote requests at term 6 for 14: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:18,345 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-03-18 09:22:18,345 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-18 09:22:18,397 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-LeaderElection2] INFO impl.LeaderElection: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-LeaderElection2: PRE_VOTE REJECTED received 1 response(s) and 0 exception(s):
dn3_1    | 2023-03-18 09:22:18,398 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-LeaderElection2] INFO impl.LeaderElection:   Response 0: bae0cd8d-e50f-4d23-8974-b816860f8fc3<-04b88c83-026f-4630-aada-108e37835bee#0:FAIL-t6
dn3_1    | 2023-03-18 09:22:18,398 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-LeaderElection2] INFO impl.LeaderElection: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-LeaderElection2 PRE_VOTE round 0: result REJECTED
dn3_1    | 2023-03-18 09:22:18,399 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-LeaderElection2] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D: changes role from CANDIDATE to FOLLOWER at term 6 for REJECTED
dn3_1    | 2023-03-18 09:22:18,400 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-LeaderElection2] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: shutdown bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-LeaderElection2
dn3_1    | 2023-03-18 09:22:18,400 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-LeaderElection2] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: start bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-FollowerState
dn3_1    | 2023-03-18 09:22:18,423 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-03-18 09:22:18,438 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-18 09:22:18,463 [grpc-default-executor-1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D: receive requestVote(ELECTION, 04b88c83-026f-4630-aada-108e37835bee, group-8191D6D2324D, 7, (t:6, i:38))
dn3_1    | 2023-03-18 09:22:18,466 [grpc-default-executor-1] INFO impl.VoteContext: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-FOLLOWER: accept ELECTION from 04b88c83-026f-4630-aada-108e37835bee: our priority 0 <= candidate's priority 1
dn3_1    | 2023-03-18 09:22:18,466 [grpc-default-executor-1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D: changes role from  FOLLOWER to FOLLOWER at term 7 for candidate:04b88c83-026f-4630-aada-108e37835bee
om2_1    | 2023-03-18 09:22:21,949 [Listener at om2/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om2_1    | 2023-03-18 09:22:22,069 [Listener at om2/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om2_1    | 2023-03-18 09:22:22,079 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om2_1    | 2023-03-18 09:22:22,085 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om2_1    | 2023-03-18 09:22:22,086 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om2_1    | 2023-03-18 09:22:22,236 [Listener at om2/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om2_1    | 2023-03-18 09:22:22,244 [Listener at om2/9862] INFO http.HttpServer2: Jetty bound to port 9874
om2_1    | 2023-03-18 09:22:22,251 [Listener at om2/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om2_1    | 2023-03-18 09:22:22,381 [Listener at om2/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om2_1    | 2023-03-18 09:22:22,383 [Listener at om2/9862] INFO server.session: No SessionScavenger set, using defaults
om2_1    | 2023-03-18 09:22:22,386 [Listener at om2/9862] INFO server.session: node0 Scavenging every 600000ms
om2_1    | 2023-03-18 09:22:22,432 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5e37fb82{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om2_1    | 2023-03-18 09:22:22,438 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@634aa81e{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om2_1    | 2023-03-18 09:22:22,983 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6f0c45f4{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-11594467207459785838/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om2_1    | 2023-03-18 09:22:23,021 [Listener at om2/9862] INFO server.AbstractConnector: Started ServerConnector@710ae6a7{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om2_1    | 2023-03-18 09:22:23,022 [Listener at om2/9862] INFO server.Server: Started @65749ms
om2_1    | 2023-03-18 09:22:23,031 [Listener at om2/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om2_1    | 2023-03-18 09:22:23,031 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om2_1    | 2023-03-18 09:22:23,032 [Listener at om2/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om2_1    | 2023-03-18 09:22:23,038 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om2_1    | 2023-03-18 09:22:23,110 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om2_1    | 2023-03-18 09:22:23,410 [Listener at om2/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om2_1    | 2023-03-18 09:22:23,449 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2a2b7a35] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om2_1    | 2023-03-18 09:22:25,818 [om2@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om2@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5172907689ns, electionTimeout:5130ms
om2_1    | 2023-03-18 09:22:25,819 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-FollowerState
om2_1    | 2023-03-18 09:22:25,820 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 7 for changeToCandidate
om2_1    | 2023-03-18 09:22:25,822 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om2_1    | 2023-03-18 09:22:25,823 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-LeaderElection1
om2_1    | 2023-03-18 09:22:25,829 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 7 for 69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-03-18 09:22:25,859 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-03-18 09:22:25,859 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-03-18 09:22:25,875 [om2@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
om2_1    | 2023-03-18 09:22:25,876 [om2@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om3
om2_1    | 2023-03-18 09:22:27,003 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(PRE_VOTE, om3, group-D66704EFC61C, 7, (t:7, i:104))
om2_1    | 2023-03-18 09:22:27,017 [grpc-default-executor-0] INFO impl.VoteContext: om2@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om3: our priority 0 <= candidate's priority 0
om2_1    | 2023-03-18 09:22:27,047 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to PRE_VOTE vote request: om3<-om2#0:OK-t7. Peer's state: om2@group-D66704EFC61C:t7, leader=null, voted=, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c104, conf=69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-03-18 09:22:27,059 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
om2_1    | 2023-03-18 09:22:27,060 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om2<-om3#0:OK-t7
om2_1    | 2023-03-18 09:22:27,062 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result PASSED
om2_1    | 2023-03-18 09:22:27,070 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 ELECTION round 0: submit vote requests at term 8 for 69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-03-18 09:22:27,089 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-03-18 09:22:27,089 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-03-18 09:22:27,118 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(ELECTION, om3, group-D66704EFC61C, 8, (t:7, i:104))
om2_1    | 2023-03-18 09:22:27,119 [grpc-default-executor-0] INFO impl.VoteContext: om2@group-D66704EFC61C-CANDIDATE: reject ELECTION from om3: already has voted for om2 at current term 8
om2_1    | 2023-03-18 09:22:27,119 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to ELECTION vote request: om3<-om2#0:FAIL-t8. Peer's state: om2@group-D66704EFC61C:t8, leader=null, voted=om2, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c104, conf=69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-03-18 09:22:27,477 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1: ELECTION REJECTED received 2 response(s) and 0 exception(s):
om2_1    | 2023-03-18 09:22:27,477 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om2<-om1#0:FAIL-t8
om2_1    | 2023-03-18 09:22:27,478 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 1: om2<-om3#0:FAIL-t8
om2_1    | 2023-03-18 09:22:27,478 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 ELECTION round 0: result REJECTED
om2_1    | 2023-03-18 09:22:27,479 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from CANDIDATE to FOLLOWER at term 8 for REJECTED
om2_1    | 2023-03-18 09:22:27,479 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-LeaderElection1
om2_1    | 2023-03-18 09:22:27,479 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
om2_1    | 2023-03-18 09:22:27,503 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-03-18 09:22:27,503 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-03-18 09:22:27,507 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(PRE_VOTE, om1, group-D66704EFC61C, 7, (t:7, i:104))
om2_1    | 2023-03-18 09:22:27,507 [grpc-default-executor-0] INFO impl.VoteContext: om2@group-D66704EFC61C-FOLLOWER: accept PRE_VOTE from om1: our priority 0 <= candidate's priority 0
om2_1    | 2023-03-18 09:22:27,510 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to PRE_VOTE vote request: om1<-om2#0:OK-t8. Peer's state: om2@group-D66704EFC61C:t8, leader=null, voted=om2, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c104, conf=69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-03-18 09:22:27,693 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: change Leader from null to om3 at term 8 for appendEntries, leader elected after 13362ms
om2_1    | 2023-03-18 09:22:27,775 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 105: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-03-18 09:22:27,790 [om2-server-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: Rolling segment log-69_104 to index:104
om2_1    | 2023-03-18 09:22:27,798 [om2@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_69 to /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_69-104
om2_1    | 2023-03-18 09:22:27,849 [om2@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_105
om2_1    | 2023-03-18 09:22:28,366 [om2@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om2_1    | [id: "om1"
om2_1    | address: "om1:9872"
om2_1    | startupRole: FOLLOWER
om2_1    | , id: "om3"
om2_1    | address: "om3:9872"
om2_1    | startupRole: FOLLOWER
om2_1    | , id: "om2"
om2_1    | address: "om2:9872"
om2_1    | startupRole: FOLLOWER
om2_1    | ]
om2_1    | 2023-03-18 09:24:04,657 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization started.
om2_1    | 2023-03-18 09:24:04,658 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: ERASURE_CODED_STORAGE_SUPPORT.
om2_1    | 2023-03-18 09:24:04,661 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature ERASURE_CODED_STORAGE_SUPPORT has been finalized.
om2_1    | 2023-03-18 09:24:04,662 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: BUCKET_LAYOUT_SUPPORT.
om2_1    | 2023-03-18 09:24:04,662 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature BUCKET_LAYOUT_SUPPORT has been finalized.
om2_1    | 2023-03-18 09:24:04,662 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: MULTITENANCY_SCHEMA.
om2_1    | 2023-03-18 09:24:04,663 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature MULTITENANCY_SCHEMA has been finalized.
dn3_1    | 2023-03-18 09:22:18,466 [grpc-default-executor-1] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: shutdown bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-FollowerState
dn3_1    | 2023-03-18 09:22:18,466 [grpc-default-executor-1] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: start bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-FollowerState
dn3_1    | 2023-03-18 09:22:18,467 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-FollowerState] INFO impl.FollowerState: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-FollowerState was interrupted
dn3_1    | 2023-03-18 09:22:18,496 [grpc-default-executor-1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D replies to ELECTION vote request: 04b88c83-026f-4630-aada-108e37835bee<-bae0cd8d-e50f-4d23-8974-b816860f8fc3#0:OK-t7. Peer's state: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D:t7, leader=null, voted=04b88c83-026f-4630-aada-108e37835bee, raftlog=Memoized:bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-SegmentedRaftLog:OPENED:c38, conf=25: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:18,508 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-03-18 09:22:18,512 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO impl.LeaderElection: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn3_1    | 2023-03-18 09:22:18,523 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-18 09:22:18,523 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO impl.LeaderElection:   Response 0: bae0cd8d-e50f-4d23-8974-b816860f8fc3<-7c9e7a7d-2593-458c-8a46-0313d77c3278#0:OK-t6
dn3_1    | 2023-03-18 09:22:18,524 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO impl.LeaderElection: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3 ELECTION round 0: result PASSED
dn3_1    | 2023-03-18 09:22:18,524 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: shutdown bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3
dn3_1    | 2023-03-18 09:22:18,524 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C: changes role from CANDIDATE to LEADER at term 6 for changeToLeader
dn3_1    | 2023-03-18 09:22:18,524 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-80313208139C with new leaderId: bae0cd8d-e50f-4d23-8974-b816860f8fc3
dn3_1    | 2023-03-18 09:22:18,525 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C: change Leader from null to bae0cd8d-e50f-4d23-8974-b816860f8fc3 at term 6 for becomeLeader, leader elected after 25354ms
dn3_1    | 2023-03-18 09:22:18,526 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn3_1    | 2023-03-18 09:22:18,526 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-03-18 09:22:18,526 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-03-18 09:22:18,527 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-03-18 09:22:18,529 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-03-18 09:22:18,533 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn3_1    | 2023-03-18 09:22:18,535 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-03-18 09:22:18,539 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn3_1    | 2023-03-18 09:22:18,613 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn3_1    | 2023-03-18 09:22:18,616 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-03-18 09:22:18,617 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn3_1    | 2023-03-18 09:22:18,648 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn3_1    | 2023-03-18 09:22:18,651 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-03-18 09:22:18,652 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-03-18 09:22:18,652 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-03-18 09:22:18,652 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn3_1    | 2023-03-18 09:22:18,670 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn3_1    | 2023-03-18 09:22:18,670 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-03-18 09:22:18,674 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn3_1    | 2023-03-18 09:22:18,675 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn3_1    | 2023-03-18 09:22:18,676 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-03-18 09:22:18,682 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-03-18 09:22:18,445 [grpc-default-executor-3] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C: receive requestVote(ELECTION, bae0cd8d-e50f-4d23-8974-b816860f8fc3, group-80313208139C, 6, (t:5, i:27))
dn4_1    | 2023-03-18 09:22:18,452 [grpc-default-executor-3] INFO impl.VoteContext: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-FOLLOWER: accept ELECTION from bae0cd8d-e50f-4d23-8974-b816860f8fc3: our priority 0 <= candidate's priority 1
dn4_1    | 2023-03-18 09:22:18,452 [grpc-default-executor-3] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C: changes role from  FOLLOWER to FOLLOWER at term 6 for candidate:bae0cd8d-e50f-4d23-8974-b816860f8fc3
dn4_1    | 2023-03-18 09:22:18,453 [grpc-default-executor-3] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: shutdown 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-FollowerState
dn4_1    | 2023-03-18 09:22:18,455 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-FollowerState] INFO impl.FollowerState: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-FollowerState was interrupted
dn4_1    | 2023-03-18 09:22:18,455 [grpc-default-executor-3] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: start 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-FollowerState
dn4_1    | 2023-03-18 09:22:18,505 [grpc-default-executor-3] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C replies to ELECTION vote request: bae0cd8d-e50f-4d23-8974-b816860f8fc3<-7c9e7a7d-2593-458c-8a46-0313d77c3278#0:OK-t6. Peer's state: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C:t6, leader=null, voted=bae0cd8d-e50f-4d23-8974-b816860f8fc3, raftlog=Memoized:7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-SegmentedRaftLog:OPENED:c27, conf=14: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:18,518 [grpc-default-executor-2] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D replies to ELECTION vote request: 04b88c83-026f-4630-aada-108e37835bee<-7c9e7a7d-2593-458c-8a46-0313d77c3278#0:OK-t7. Peer's state: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D:t7, leader=null, voted=04b88c83-026f-4630-aada-108e37835bee, raftlog=Memoized:7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-SegmentedRaftLog:OPENED:c38, conf=25: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:18,542 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-18 09:22:18,549 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-18 09:22:18,546 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-18 09:22:18,550 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-18 09:22:18,918 [7c9e7a7d-2593-458c-8a46-0313d77c3278-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-80313208139C with new leaderId: bae0cd8d-e50f-4d23-8974-b816860f8fc3
dn4_1    | 2023-03-18 09:22:18,919 [7c9e7a7d-2593-458c-8a46-0313d77c3278-server-thread1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C: change Leader from null to bae0cd8d-e50f-4d23-8974-b816860f8fc3 at term 6 for appendEntries, leader elected after 24968ms
dn4_1    | 2023-03-18 09:22:18,976 [7c9e7a7d-2593-458c-8a46-0313d77c3278-server-thread1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C: set configuration 28: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:18,979 [7c9e7a7d-2593-458c-8a46-0313d77c3278-server-thread1] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-SegmentedRaftLogWorker: Rolling segment log-14_27 to index:27
dn4_1    | 2023-03-18 09:22:18,984 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/current/log_inprogress_14 to /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/current/log_14-27
dn4_1    | 2023-03-18 09:22:18,986 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/current/log_inprogress_28
dn4_1    | 2023-03-18 09:22:19,107 [7c9e7a7d-2593-458c-8a46-0313d77c3278-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8191D6D2324D with new leaderId: 04b88c83-026f-4630-aada-108e37835bee
dn4_1    | 2023-03-18 09:22:19,108 [7c9e7a7d-2593-458c-8a46-0313d77c3278-server-thread1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D: change Leader from null to 04b88c83-026f-4630-aada-108e37835bee at term 7 for appendEntries, leader elected after 24026ms
dn4_1    | 2023-03-18 09:22:19,216 [7c9e7a7d-2593-458c-8a46-0313d77c3278-server-thread2] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D: set configuration 39: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:22:19,216 [7c9e7a7d-2593-458c-8a46-0313d77c3278-server-thread2] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-SegmentedRaftLogWorker: Rolling segment log-25_38 to index:38
dn4_1    | 2023-03-18 09:22:19,232 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/current/log_inprogress_25 to /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/current/log_25-38
dn4_1    | 2023-03-18 09:22:19,234 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/current/log_inprogress_39
dn4_1    | 2023-03-18 09:22:45,820 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-03-18 09:22:45,828 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn4_1    | 2023-03-18 09:22:45,828 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn4_1    | 2023-03-18 09:22:45,829 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: CLOSING
dn4_1    | 2023-03-18 09:22:45,829 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn4_1    | 2023-03-18 09:22:45,829 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn4_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-03-18 09:22:45,830 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn4_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-03-18 09:22:45,830 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-03-18 09:22:45,830 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn4_1    | 2023-03-18 09:22:45,830 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn4_1    | 2023-03-18 09:22:45,830 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: CLOSING
dn4_1    | 2023-03-18 09:22:45,830 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn4_1    | 2023-03-18 09:22:45,832 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn4_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-03-18 09:22:45,832 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn4_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-03-18 09:22:45,832 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-03-18 09:22:18,682 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-03-18 09:22:18,682 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn3_1    | 2023-03-18 09:22:18,692 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: start bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderStateImpl
dn3_1    | 2023-03-18 09:22:18,695 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-SegmentedRaftLogWorker: Rolling segment log-14_27 to index:27
dn3_1    | 2023-03-18 09:22:18,745 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/current/log_inprogress_14 to /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/current/log_14-27
dn3_1    | 2023-03-18 09:22:18,746 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderElection3] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C: set configuration 28: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:18,746 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/current/log_inprogress_28
dn3_1    | 2023-03-18 09:22:19,159 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8191D6D2324D with new leaderId: 04b88c83-026f-4630-aada-108e37835bee
dn3_1    | 2023-03-18 09:22:19,168 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-server-thread1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D: change Leader from null to 04b88c83-026f-4630-aada-108e37835bee at term 7 for appendEntries, leader elected after 25855ms
dn3_1    | 2023-03-18 09:22:19,198 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-server-thread2] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D: set configuration 39: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:22:19,200 [bae0cd8d-e50f-4d23-8974-b816860f8fc3-server-thread2] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-SegmentedRaftLogWorker: Rolling segment log-25_38 to index:38
dn3_1    | 2023-03-18 09:22:19,237 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/current/log_inprogress_25 to /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/current/log_25-38
dn3_1    | 2023-03-18 09:22:19,259 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/current/log_inprogress_39
dn3_1    | 2023-03-18 09:22:49,925 [ContainerOp-97be11a2-1597-458e-9379-8191d6d2324d-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 6.
dn3_1    | 2023-03-18 09:22:49,925 [ContainerOp-97be11a2-1597-458e-9379-8191d6d2324d-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 6.
dn3_1    | 2023-03-18 09:22:49,930 [ContainerOp-97be11a2-1597-458e-9379-8191d6d2324d-0] INFO keyvalue.KeyValueContainer: Container 2 is closed with bcsId 6.
dn3_1    | 2023-03-18 09:22:50,125 [ContainerOp-e319c2ba-524b-42f7-8b03-80313208139c-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 2.
dn3_1    | 2023-03-18 09:22:50,125 [ContainerOp-e319c2ba-524b-42f7-8b03-80313208139c-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 2.
dn3_1    | 2023-03-18 09:22:50,159 [ContainerOp-e319c2ba-524b-42f7-8b03-80313208139c-0] INFO keyvalue.KeyValueContainer: Container 1 is closed with bcsId 2.
dn3_1    | 2023-03-18 09:22:50,268 [ContainerOp-e319c2ba-524b-42f7-8b03-80313208139c-1] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 26.
dn3_1    | 2023-03-18 09:22:50,268 [ContainerOp-e319c2ba-524b-42f7-8b03-80313208139c-1] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 26.
dn3_1    | 2023-03-18 09:22:50,283 [ContainerOp-e319c2ba-524b-42f7-8b03-80313208139c-1] INFO keyvalue.KeyValueContainer: Container 1002 is closed with bcsId 26.
dn3_1    | 2023-03-18 09:22:50,320 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-03-18 09:22:50,322 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn3_1    | 2023-03-18 09:22:50,322 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn3_1    | 2023-03-18 09:22:50,326 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1001 to close, current state is: CLOSING
dn3_1    | 2023-03-18 09:22:50,326 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn3_1    | 2023-03-18 09:22:50,327 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
recon_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1  | 2023-03-18 09:21:26,909 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1  | /************************************************************
recon_1  | STARTUP_MSG: Starting ReconServer
recon_1  | STARTUP_MSG:   host = ed84add0dd68/10.9.0.20
om1_1    | 2023-03-18 09:22:18,164 [main] INFO reflections.Reflections: Reflections took 3933 ms to scan 8 urls, producing 23 keys and 586 values [using 2 cores]
om1_1    | 2023-03-18 09:22:18,917 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om1_1    | 2023-03-18 09:22:19,006 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om1_1    | 2023-03-18 09:22:19,809 [Listener at om1/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om1_1    | 2023-03-18 09:22:19,873 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om1_1    | 2023-03-18 09:22:19,874 [Listener at om1/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om1_1    | 2023-03-18 09:22:20,269 [Listener at om1/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om1/10.9.0.11:9862
om1_1    | 2023-03-18 09:22:20,270 [Listener at om1/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om1_1    | 2023-03-18 09:22:20,289 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@8260fa04b223
om1_1    | 2023-03-18 09:22:20,311 [om1-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=7, votedFor=om3} from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/raft-meta
om1_1    | 2023-03-18 09:22:20,402 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-03-18 09:22:20,407 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om1_1    | 2023-03-18 09:22:20,477 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1    | 2023-03-18 09:22:20,483 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-03-18 09:22:20,490 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om1_1    | 2023-03-18 09:22:20,493 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om1_1    | 2023-03-18 09:22:20,504 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1    | 2023-03-18 09:22:20,520 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om1_1    | 2023-03-18 09:22:20,521 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1    | 2023-03-18 09:22:20,543 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om1_1    | 2023-03-18 09:22:20,546 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om1_1    | 2023-03-18 09:22:20,549 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om1_1    | 2023-03-18 09:22:20,555 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1    | 2023-03-18 09:22:20,557 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om1_1    | 2023-03-18 09:22:20,559 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1    | 2023-03-18 09:22:20,563 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om1_1    | 2023-03-18 09:22:20,563 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om1_1    | 2023-03-18 09:22:20,564 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om1_1    | 2023-03-18 09:22:20,605 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om1_1    | 2023-03-18 09:22:20,606 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-03-18 09:22:20,635 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om1_1    | 2023-03-18 09:22:20,640 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om1_1    | 2023-03-18 09:22:20,641 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om1_1    | 2023-03-18 09:22:20,935 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-03-18 09:22:21,015 [om1-impl-thread1] INFO segmented.LogSegment: Successfully read 21 entries from segment file /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_0-20
om1_1    | 2023-03-18 09:22:21,026 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 21: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-03-18 09:22:21,070 [om1-impl-thread1] INFO segmented.LogSegment: Successfully read 48 entries from segment file /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_21-68
om1_1    | 2023-03-18 09:22:21,079 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-03-18 09:22:21,096 [om1-impl-thread1] INFO segmented.LogSegment: Successfully read 36 entries from segment file /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_69
om1_1    | 2023-03-18 09:22:21,102 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 104
om1_1    | 2023-03-18 09:22:21,105 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 68
om1_1    | 2023-03-18 09:22:21,445 [om1-impl-thread1] INFO raftlog.RaftLog: om1@group-D66704EFC61C-SegmentedRaftLog: commitIndex: updateToMax old=104, new=103, updated? false
om1_1    | 2023-03-18 09:22:21,445 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: start as a follower, conf=69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:21:55,006 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-03-18 09:21:55,072 [pool-38-thread-1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee: new RaftServerImpl for group-3212C22EDF58:[] with ContainerStateMachine:uninitialized
dn5_1    | 2023-03-18 09:21:55,075 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-03-18 09:21:55,076 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-03-18 09:21:55,076 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-03-18 09:21:55,076 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-03-18 09:21:55,076 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-03-18 09:21:55,076 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-03-18 09:21:55,077 [pool-38-thread-1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-03-18 09:21:55,077 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-03-18 09:21:55,078 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-03-18 09:21:55,078 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-03-18 09:21:55,078 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-03-18 09:21:55,079 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn5_1    | 2023-03-18 09:21:55,080 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-03-18 09:21:55,080 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-03-18 09:21:55,081 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-03-18 09:21:55,097 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-03-18 09:21:55,100 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-03-18 09:21:55,115 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-03-18 09:21:55,116 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-03-18 09:21:55,232 [main] INFO util.log: Logging initialized @40520ms to org.eclipse.jetty.util.log.Slf4jLog
dn5_1    | 2023-03-18 09:21:55,831 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn5_1    | 2023-03-18 09:21:55,844 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn5_1    | 2023-03-18 09:21:55,883 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn5_1    | 2023-03-18 09:21:55,900 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn5_1    | 2023-03-18 09:21:55,904 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn5_1    | 2023-03-18 09:21:55,906 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn5_1    | 2023-03-18 09:21:56,222 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn5_1    | 2023-03-18 09:21:56,279 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn5_1    | 2023-03-18 09:21:56,329 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn5_1    | 2023-03-18 09:21:56,587 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn5_1    | 2023-03-18 09:21:56,632 [main] INFO server.session: No SessionScavenger set, using defaults
dn5_1    | 2023-03-18 09:21:56,647 [main] INFO server.session: node0 Scavenging every 660000ms
dn5_1    | 2023-03-18 09:21:56,991 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1e1e837d{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn5_1    | 2023-03-18 09:21:56,992 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@527a8665{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn5_1    | 2023-03-18 09:21:58,108 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6ad6443{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-15309716829024101856/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn5_1    | 2023-03-18 09:21:58,173 [main] INFO server.AbstractConnector: Started ServerConnector@a2b54e3{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn5_1    | 2023-03-18 09:21:58,174 [main] INFO server.Server: Started @43461ms
dn5_1    | 2023-03-18 09:21:58,191 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn5_1    | 2023-03-18 09:21:58,191 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn5_1    | 2023-03-18 09:21:58,196 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn5_1    | 2023-03-18 09:21:58,201 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn5_1    | 2023-03-18 09:21:58,303 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn5_1    | 2023-03-18 09:21:58,308 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn5_1    | 2023-03-18 09:21:58,322 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1b94b614] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn5_1    | 2023-03-18 09:21:58,760 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.20:9891
dn5_1    | 2023-03-18 09:21:58,897 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn5_1    | 2023-03-18 09:22:01,528 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-03-18 09:22:02,529 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-03-18 09:22:04,155 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
om2_1    | 2023-03-18 09:24:04,663 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
om2_1    | 2023-03-18 09:24:04,663 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization is done.
om2_1    | 2023-03-18 09:24:04,668 [OMDoubleBufferFlushThread] INFO upgrade.OMFinalizeUpgradeResponse: Layout version to persist to DB : 3
om2_1    | 2023-03-18 09:25:13,801 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:new2-volume for user:hadoop
om2_1    | 2023-03-18 09:25:16,424 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: new2-volume
om2_1    | 2023-03-18 09:25:24,647 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: s3v
om2_1    | 2023-03-18 09:25:32,176 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:new2-bucket in volume:s3v
om2_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om2_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:207)
om2_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:337)
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om2_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1    | 2023-03-18 09:25:51,360 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:ectest-new for user:hadoop
om2_1    | 2023-03-18 09:25:53,933 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ectest of layout FILE_SYSTEM_OPTIMIZED in volume: ectest-new
om2_1    | 2023-03-18 09:25:59,155 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: testpropchange of layout FILE_SYSTEM_OPTIMIZED in volume: ectest-new
om1_1    | 2023-03-18 09:22:21,448 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from      null to FOLLOWER at term 7 for startAsFollower
om1_1    | 2023-03-18 09:22:21,456 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
om1_1    | 2023-03-18 09:22:21,465 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1    | 2023-03-18 09:22:21,470 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 2023-03-18 09:22:21,482 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om1
om1_1    | 2023-03-18 09:22:21,487 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om1_1    | 2023-03-18 09:22:21,489 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om1_1    | 2023-03-18 09:22:21,490 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om1_1    | 2023-03-18 09:22:21,490 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om1_1    | 2023-03-18 09:22:21,508 [Listener at om1/9862] INFO server.RaftServer: om1: start RPC server
om1_1    | 2023-03-18 09:22:21,819 [Listener at om1/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om1_1    | 2023-03-18 09:22:21,834 [Listener at om1/9862] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
om1_1    | 2023-03-18 09:22:21,837 [Listener at om1/9862] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
om1_1    | 2023-03-18 09:22:21,837 [Listener at om1/9862] INFO om.OzoneManager: Version File has different layout version (0) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om1_1    | 2023-03-18 09:22:21,853 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om1_1    | 2023-03-18 09:22:22,144 [Listener at om1/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om1_1    | 2023-03-18 09:22:22,144 [Listener at om1/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om1_1    | 2023-03-18 09:22:22,259 [Listener at om1/9862] INFO util.log: Logging initialized @64786ms to org.eclipse.jetty.util.log.Slf4jLog
om1_1    | 2023-03-18 09:22:22,772 [Listener at om1/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om1_1    | 2023-03-18 09:22:22,779 [Listener at om1/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om1_1    | 2023-03-18 09:22:22,789 [Listener at om1/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om1_1    | 2023-03-18 09:22:22,790 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om1_1    | 2023-03-18 09:22:22,790 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om1_1    | 2023-03-18 09:22:22,790 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om1_1    | 2023-03-18 09:22:22,821 [Listener at om1/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om1_1    | 2023-03-18 09:22:22,822 [Listener at om1/9862] INFO http.HttpServer2: Jetty bound to port 9874
om1_1    | 2023-03-18 09:22:22,823 [Listener at om1/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om1_1    | 2023-03-18 09:22:23,084 [Listener at om1/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om1_1    | 2023-03-18 09:22:23,084 [Listener at om1/9862] INFO server.session: No SessionScavenger set, using defaults
om1_1    | 2023-03-18 09:22:23,087 [Listener at om1/9862] INFO server.session: node0 Scavenging every 660000ms
om1_1    | 2023-03-18 09:22:23,113 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5c5a91b4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om1_1    | 2023-03-18 09:22:23,113 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@70f4f89e{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om1_1    | 2023-03-18 09:22:23,485 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5df92089{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-9985597310358555492/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om1_1    | 2023-03-18 09:22:23,511 [Listener at om1/9862] INFO server.AbstractConnector: Started ServerConnector@11e834ad{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om1_1    | 2023-03-18 09:22:23,511 [Listener at om1/9862] INFO server.Server: Started @66038ms
om1_1    | 2023-03-18 09:22:23,516 [Listener at om1/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om1_1    | 2023-03-18 09:22:23,516 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om1_1    | 2023-03-18 09:22:23,521 [Listener at om1/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om1_1    | 2023-03-18 09:22:23,524 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1    | 2023-03-18 09:22:23,570 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om1_1    | 2023-03-18 09:22:23,599 [Listener at om1/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om1_1    | 2023-03-18 09:22:23,632 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@4bfa5eb9] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om1_1    | 2023-03-18 09:22:26,542 [om1@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om1@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5086435981ns, electionTimeout:5071ms
om1_1    | 2023-03-18 09:22:26,543 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-FollowerState
om1_1    | 2023-03-18 09:22:26,544 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 7 for changeToCandidate
om1_1    | 2023-03-18 09:22:26,547 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om1_1    | 2023-03-18 09:22:26,547 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-LeaderElection1
om1_1    | 2023-03-18 09:22:26,563 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 7 for 69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-03-18 09:22:26,699 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1    | 2023-03-18 09:22:26,699 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 2023-03-18 09:22:26,709 [om1@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om3
om1_1    | 2023-03-18 09:22:26,712 [om1@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
om1_1    | 2023-03-18 09:22:27,404 [grpc-default-executor-2] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(PRE_VOTE, om3, group-D66704EFC61C, 7, (t:7, i:104))
om1_1    | 2023-03-18 09:22:27,404 [grpc-default-executor-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(PRE_VOTE, om2, group-D66704EFC61C, 7, (t:7, i:104))
om1_1    | 2023-03-18 09:22:27,407 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(ELECTION, om3, group-D66704EFC61C, 8, (t:7, i:104))
om1_1    | 2023-03-18 09:22:27,410 [grpc-default-executor-1] INFO impl.VoteContext: om1@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om2: our priority 0 <= candidate's priority 0
om1_1    | 2023-03-18 09:22:27,424 [grpc-default-executor-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to PRE_VOTE vote request: om2<-om1#0:OK-t7. Peer's state: om1@group-D66704EFC61C:t7, leader=null, voted=om3, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c104, conf=69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-03-18 09:22:27,424 [grpc-default-executor-0] INFO impl.VoteContext: om1@group-D66704EFC61C-CANDIDATE: accept ELECTION from om3: our priority 0 <= candidate's priority 0
om1_1    | 2023-03-18 09:22:27,424 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from CANDIDATE to FOLLOWER at term 8 for candidate:om3
om1_1    | 2023-03-18 09:22:27,424 [grpc-default-executor-0] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-LeaderElection1
om1_1    | 2023-03-18 09:22:27,425 [grpc-default-executor-0] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
om1_1    | 2023-03-18 09:22:27,432 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to ELECTION vote request: om3<-om1#0:OK-t8. Peer's state: om1@group-D66704EFC61C:t8, leader=null, voted=om3, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c104, conf=69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-03-18 09:22:27,432 [grpc-default-executor-2] INFO impl.VoteContext: om1@group-D66704EFC61C-FOLLOWER: accept PRE_VOTE from om3: our priority 0 <= candidate's priority 0
om1_1    | 2023-03-18 09:22:27,438 [grpc-default-executor-2] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to PRE_VOTE vote request: om3<-om1#0:OK-t8. Peer's state: om1@group-D66704EFC61C:t8, leader=null, voted=om3, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c104, conf=69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-03-18 09:22:27,458 [grpc-default-executor-2] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(ELECTION, om2, group-D66704EFC61C, 8, (t:7, i:104))
om1_1    | 2023-03-18 09:22:27,458 [grpc-default-executor-2] INFO impl.VoteContext: om1@group-D66704EFC61C-FOLLOWER: reject ELECTION from om2: already has voted for om3 at current term 8
om1_1    | 2023-03-18 09:22:27,458 [grpc-default-executor-2] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to ELECTION vote request: om2<-om1#0:FAIL-t8. Peer's state: om1@group-D66704EFC61C:t8, leader=null, voted=om3, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c104, conf=69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-03-18 09:22:27,463 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1    | 2023-03-18 09:22:27,464 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 2023-03-18 09:22:27,543 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1: PRE_VOTE DISCOVERED_A_NEW_TERM (term=8) received 1 response(s) and 0 exception(s):
om1_1    | 2023-03-18 09:22:27,544 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om1<-om2#0:OK-t8
om1_1    | 2023-03-18 09:22:27,544 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result DISCOVERED_A_NEW_TERM (term=8)
om1_1    | 2023-03-18 09:22:27,754 [om1-server-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: change Leader from null to om3 at term 8 for appendEntries, leader elected after 13256ms
om1_1    | 2023-03-18 09:22:27,778 [om1-server-thread2] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 105: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-03-18 09:22:27,788 [om1-server-thread2] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: Rolling segment log-69_104 to index:104
om1_1    | 2023-03-18 09:22:27,802 [om1@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_69 to /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_69-104
om1_1    | 2023-03-18 09:22:27,876 [om1@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_105
om1_1    | 2023-03-18 09:22:28,046 [om1@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om1_1    | [id: "om1"
om1_1    | address: "om1:9872"
om1_1    | startupRole: FOLLOWER
dn5_1    | 2023-03-18 09:22:04,165 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn5_1    | 2023-03-18 09:22:04,372 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn5_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:321)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	... 1 more
dn5_1    | 2023-03-18 09:22:04,694 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn5_1    | 2023-03-18 09:22:04,739 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 04b88c83-026f-4630-aada-108e37835bee
dn5_1    | 2023-03-18 09:22:04,864 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/in_use.lock acquired by nodename 7@e51427cbfe02
dn5_1    | 2023-03-18 09:22:04,890 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/in_use.lock acquired by nodename 7@e51427cbfe02
dn5_1    | 2023-03-18 09:22:04,873 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/b5ca130e-c58f-4d3d-8bda-3212c22edf58/in_use.lock acquired by nodename 7@e51427cbfe02
dn5_1    | 2023-03-18 09:22:04,945 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=6, votedFor=04b88c83-026f-4630-aada-108e37835bee} from /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/current/raft-meta
dn5_1    | 2023-03-18 09:22:04,945 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=04b88c83-026f-4630-aada-108e37835bee} from /data/metadata/ratis/b5ca130e-c58f-4d3d-8bda-3212c22edf58/current/raft-meta
dn5_1    | 2023-03-18 09:22:04,945 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=5, votedFor=bae0cd8d-e50f-4d23-8974-b816860f8fc3} from /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/current/raft-meta
dn5_1    | 2023-03-18 09:22:05,138 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D: set configuration 25: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:05,160 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58: set configuration 3: peers:[04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:05,160 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C: set configuration 14: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:05,213 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO ratis.ContainerStateMachine: group-3212C22EDF58: Setting the last applied index to (t:3, i:4)
dn5_1    | 2023-03-18 09:22:05,677 [EndpointStateMachine task thread for recon/10.9.0.20:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn5_1    | java.net.SocketTimeoutException: Call From e51427cbfe02/10.9.0.19 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:42910 remote=recon/10.9.0.20:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn5_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:42910 remote=recon/10.9.0.20:9891]
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn5_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn5_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn5_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn5_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn5_1    | 2023-03-18 09:22:05,715 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO ratis.ContainerStateMachine: group-8191D6D2324D: Setting the last applied index to (t:6, i:38)
dn5_1    | 2023-03-18 09:22:05,715 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO ratis.ContainerStateMachine: group-80313208139C: Setting the last applied index to (t:5, i:27)
dn5_1    | 2023-03-18 09:22:06,393 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-03-18 09:22:06,443 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-03-18 09:22:06,445 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-03-18 09:22:06,452 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-03-18 09:22:06,460 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-18 09:22:06,464 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-03-18 09:22:06,469 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-03-18 09:22:06,471 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-03-18 09:22:06,473 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-18 09:22:06,480 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-03-18 09:22:06,483 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-03-18 09:22:06,491 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-03-18 09:22:06,492 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | , id: "om3"
om1_1    | address: "om3:9872"
om1_1    | startupRole: FOLLOWER
om1_1    | , id: "om2"
om1_1    | address: "om2:9872"
om1_1    | startupRole: FOLLOWER
om1_1    | ]
om1_1    | 2023-03-18 09:24:04,659 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization started.
om1_1    | 2023-03-18 09:24:04,659 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: ERASURE_CODED_STORAGE_SUPPORT.
dn4_1    | 2023-03-18 09:22:45,832 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn4_1    | 2023-03-18 09:22:45,832 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn4_1    | 2023-03-18 09:22:45,833 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: CLOSING
dn4_1    | 2023-03-18 09:22:45,833 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn4_1    | 2023-03-18 09:22:45,833 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn4_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-03-18 09:22:45,833 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn4_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-03-18 09:22:45,833 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-03-18 09:22:45,833 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn4_1    | 2023-03-18 09:22:45,833 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn4_1    | 2023-03-18 09:22:45,833 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: CLOSING
dn4_1    | 2023-03-18 09:22:45,834 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn4_1    | 2023-03-18 09:22:45,834 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn4_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-03-18 09:22:45,834 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn4_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-03-18 09:22:45,834 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-03-18 09:22:45,834 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn4_1    | 2023-03-18 09:22:45,834 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn4_1    | 2023-03-18 09:22:45,834 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: CLOSING
dn4_1    | 2023-03-18 09:22:45,834 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
recon_1  | STARTUP_MSG:   args = []
recon_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn5_1    | 2023-03-18 09:22:06,494 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-03-18 09:22:06,496 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-03-18 09:22:06,507 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-03-18 09:22:06,508 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-03-18 09:22:06,523 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-03-18 09:22:06,561 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-03-18 09:22:06,566 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-03-18 09:22:06,569 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-03-18 09:22:06,572 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-03-18 09:22:06,587 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-03-18 09:22:06,594 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-03-18 09:22:06,627 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c
dn5_1    | 2023-03-18 09:22:06,631 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-03-18 09:22:06,640 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-03-18 09:22:06,651 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-03-18 09:22:06,645 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/b5ca130e-c58f-4d3d-8bda-3212c22edf58
dn5_1    | 2023-03-18 09:22:06,651 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-03-18 09:22:50,330 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-03-18 09:22:50,332 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-03-18 09:22:50,334 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn3_1    | 2023-03-18 09:22:50,335 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn3_1    | 2023-03-18 09:22:50,335 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1001 to close, current state is: CLOSING
dn3_1    | 2023-03-18 09:22:50,335 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn3_1    | 2023-03-18 09:22:50,335 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-03-18 09:22:50,336 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-03-18 09:22:50,338 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-03-18 09:22:50,339 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn3_1    | 2023-03-18 09:22:50,339 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn3_1    | 2023-03-18 09:22:50,339 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1001 to close, current state is: CLOSING
dn3_1    | 2023-03-18 09:22:50,339 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn3_1    | 2023-03-18 09:22:50,339 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-03-18 09:22:50,339 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn5_1    | 2023-03-18 09:22:06,651 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-03-18 09:22:06,652 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-03-18 09:22:06,655 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-03-18 09:22:06,658 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-03-18 09:22:06,662 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-03-18 09:22:06,665 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-03-18 09:22:06,670 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-03-18 09:22:06,664 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-03-18 09:22:06,672 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-03-18 09:22:06,676 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-03-18 09:22:06,680 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-03-18 09:22:06,687 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-03-18 09:22:06,692 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d
dn5_1    | 2023-03-18 09:22:06,697 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-03-18 09:22:06,698 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-03-18 09:22:06,701 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-03-18 09:22:06,703 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-03-18 09:22:06,704 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-03-18 09:22:06,704 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-03-18 09:22:06,704 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-03-18 09:22:06,705 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-03-18 09:22:06,762 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-03-18 09:22:06,763 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-03-18 09:22:06,764 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-18 09:22:06,766 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-18 09:22:07,690 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-03-18 09:22:07,695 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-03-18 09:22:07,706 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-03-18 09:22:07,713 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-03-18 09:22:07,713 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-03-18 09:22:07,723 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-03-18 09:22:07,744 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-03-18 09:22:07,744 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-18 09:22:07,938 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-03-18 09:22:07,939 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-03-18 09:22:07,940 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-03-18 09:22:07,960 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C: set configuration 0: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:07,939 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58: set configuration 0: peers:[04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-03-18 09:24:04,661 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature ERASURE_CODED_STORAGE_SUPPORT has been finalized.
om1_1    | 2023-03-18 09:24:04,663 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: BUCKET_LAYOUT_SUPPORT.
om1_1    | 2023-03-18 09:24:04,664 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature BUCKET_LAYOUT_SUPPORT has been finalized.
om1_1    | 2023-03-18 09:24:04,665 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: MULTITENANCY_SCHEMA.
om1_1    | 2023-03-18 09:24:04,666 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature MULTITENANCY_SCHEMA has been finalized.
om1_1    | 2023-03-18 09:24:04,666 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
om1_1    | 2023-03-18 09:24:04,666 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization is done.
om1_1    | 2023-03-18 09:24:04,672 [OMDoubleBufferFlushThread] INFO upgrade.OMFinalizeUpgradeResponse: Layout version to persist to DB : 3
om1_1    | 2023-03-18 09:25:13,770 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:new2-volume for user:hadoop
om1_1    | 2023-03-18 09:25:16,415 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: new2-volume
om1_1    | 2023-03-18 09:25:24,658 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: s3v
om1_1    | 2023-03-18 09:25:32,171 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:new2-bucket in volume:s3v
om1_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om1_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:207)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:337)
om1_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om1_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om1_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | 2023-03-18 09:25:51,377 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:ectest-new for user:hadoop
om1_1    | 2023-03-18 09:25:53,936 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ectest of layout FILE_SYSTEM_OPTIMIZED in volume: ectest-new
om1_1    | 2023-03-18 09:25:59,171 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: testpropchange of layout FILE_SYSTEM_OPTIMIZED in volume: ectest-new
dn4_1    | 2023-03-18 09:22:45,834 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn4_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-03-18 09:22:45,836 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn4_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-03-18 09:22:49,890 [ContainerOp-97be11a2-1597-458e-9379-8191d6d2324d-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 6.
dn4_1    | 2023-03-18 09:22:49,891 [ContainerOp-97be11a2-1597-458e-9379-8191d6d2324d-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 6.
dn4_1    | 2023-03-18 09:22:49,968 [ContainerOp-97be11a2-1597-458e-9379-8191d6d2324d-0] INFO keyvalue.KeyValueContainer: Container 2 is closed with bcsId 6.
dn4_1    | 2023-03-18 09:22:50,111 [ContainerOp-e319c2ba-524b-42f7-8b03-80313208139c-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 2.
dn4_1    | 2023-03-18 09:22:50,114 [ContainerOp-e319c2ba-524b-42f7-8b03-80313208139c-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 2.
dn4_1    | 2023-03-18 09:22:50,126 [ContainerOp-e319c2ba-524b-42f7-8b03-80313208139c-0] INFO keyvalue.KeyValueContainer: Container 1 is closed with bcsId 2.
dn4_1    | 2023-03-18 09:22:50,274 [ContainerOp-e319c2ba-524b-42f7-8b03-80313208139c-1] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 26.
dn4_1    | 2023-03-18 09:22:50,281 [ContainerOp-e319c2ba-524b-42f7-8b03-80313208139c-1] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 26.
dn4_1    | 2023-03-18 09:22:50,319 [ContainerOp-e319c2ba-524b-42f7-8b03-80313208139c-1] INFO keyvalue.KeyValueContainer: Container 1002 is closed with bcsId 26.
dn4_1    | 2023-03-18 09:22:50,372 [ContainerOp-97be11a2-1597-458e-9379-8191d6d2324d-1] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 36.
dn4_1    | 2023-03-18 09:22:50,383 [ContainerOp-97be11a2-1597-458e-9379-8191d6d2324d-1] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 36.
dn4_1    | 2023-03-18 09:22:50,414 [ContainerOp-97be11a2-1597-458e-9379-8191d6d2324d-1] INFO keyvalue.KeyValueContainer: Container 1001 is closed with bcsId 36.
dn4_1    | 2023-03-18 09:23:10,113 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn4_1    | 2023-03-18 09:23:16,819 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-03-18 09:23:16,819 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn4_1    | 2023-03-18 09:23:16,819 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn4_1    | 2023-03-18 09:23:16,820 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: ERASURE_CODED_STORAGE_SUPPORT.
dn4_1    | 2023-03-18 09:23:16,822 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature ERASURE_CODED_STORAGE_SUPPORT has been finalized.
dn4_1    | 2023-03-18 09:23:16,823 [Command processor thread] INFO upgrade.UpgradeFinalizer: Running finalization actions for layout feature: DATANODE_SCHEMA_V3
dn4_1    | 2023-03-18 09:23:16,823 [Command processor thread] INFO upgrade.DatanodeSchemaV3FinalizeAction: Upgrading Datanode volume layout for Schema V3 support.
dn4_1    | 2023-03-18 09:23:16,866 [Command processor thread] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-41720527-e7ad-4532-ba09-1ed2a5ef47ff/container.db to cache
dn4_1    | 2023-03-18 09:23:16,866 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-41720527-e7ad-4532-ba09-1ed2a5ef47ff/container.db for volume DS-41720527-e7ad-4532-ba09-1ed2a5ef47ff
dn4_1    | 2023-03-18 09:23:16,867 [Command processor thread] INFO utils.DatanodeStoreCache: Removed db /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-41720527-e7ad-4532-ba09-1ed2a5ef47ff/container.db from cache
dn4_1    | 2023-03-18 09:23:16,867 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is stopped at /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-41720527-e7ad-4532-ba09-1ed2a5ef47ff/container.db for volume DS-41720527-e7ad-4532-ba09-1ed2a5ef47ff
dn4_1    | 2023-03-18 09:23:16,884 [Command processor thread] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-41720527-e7ad-4532-ba09-1ed2a5ef47ff/container.db to cache
dn4_1    | 2023-03-18 09:23:16,884 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-41720527-e7ad-4532-ba09-1ed2a5ef47ff/container.db for volume DS-41720527-e7ad-4532-ba09-1ed2a5ef47ff
dn4_1    | 2023-03-18 09:23:16,886 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature DATANODE_SCHEMA_V3 has been finalized.
dn4_1    | 2023-03-18 09:23:16,886 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn4_1    | 2023-03-18 09:23:16,886 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn4_1    | 2023-03-18 09:23:16,886 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-03-18 09:23:16,886 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-03-18 09:23:21,417 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-03-18 09:23:21,418 [Command processor thread] INFO server.RaftServer: 7c9e7a7d-2593-458c-8a46-0313d77c3278: remove    LEADER 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C:t4, leader=7c9e7a7d-2593-458c-8a46-0313d77c3278, voted=7c9e7a7d-2593-458c-8a46-0313d77c3278, raftlog=Memoized:7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-SegmentedRaftLog:OPENED:c6, conf=5: peers:[7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn4_1    | 2023-03-18 09:23:21,420 [Command processor thread] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C: shutdown
dn4_1    | 2023-03-18 09:23:21,420 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-40B9CE60BD6C,id=7c9e7a7d-2593-458c-8a46-0313d77c3278
dn4_1    | 2023-03-18 09:23:21,420 [Command processor thread] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: shutdown 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-LeaderStateImpl
dn4_1    | 2023-03-18 09:23:21,421 [Command processor thread] INFO impl.PendingRequests: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-PendingRequests: sendNotLeaderResponses
dn4_1    | 2023-03-18 09:23:21,424 [Command processor thread] INFO impl.StateMachineUpdater: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-StateMachineUpdater: set stopIndex = 6
dn4_1    | 2023-03-18 09:23:21,424 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-40B9CE60BD6C: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/a2f44939-53c6-4b4c-afd2-40b9ce60bd6c/sm/snapshot.4_6
dn4_1    | 2023-03-18 09:23:21,434 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: 7c9e7a7d-2593-458c-8a46-0313d77c3278: Completed APPEND_ENTRIES, lastRequest: bae0cd8d-e50f-4d23-8974-b816860f8fc3->7c9e7a7d-2593-458c-8a46-0313d77c3278#268-t6,previous=(t:6, i:32),leaderCommit=32,initializing? true,entries: size=1, first=(t:6, i:33), METADATAENTRY(c:32)
dn4_1    | 2023-03-18 09:23:21,444 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-40B9CE60BD6C: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/a2f44939-53c6-4b4c-afd2-40b9ce60bd6c/sm/snapshot.4_6 took: 19 ms
dn4_1    | 2023-03-18 09:23:21,435 [grpc-default-executor-4] INFO server.GrpcServerProtocolService: 7c9e7a7d-2593-458c-8a46-0313d77c3278: Completed APPEND_ENTRIES, lastRequest: null
dn4_1    | 2023-03-18 09:23:21,446 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-StateMachineUpdater] INFO impl.StateMachineUpdater: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-StateMachineUpdater: Took a snapshot at index 6
dn4_1    | 2023-03-18 09:23:21,447 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-StateMachineUpdater] INFO impl.StateMachineUpdater: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
dn4_1    | 2023-03-18 09:23:21,459 [Command processor thread] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C: closes. applyIndex: 6
dn4_1    | 2023-03-18 09:23:21,460 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn4_1    | 2023-03-18 09:23:21,462 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C-SegmentedRaftLogWorker close()
dn4_1    | 2023-03-18 09:23:21,472 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: 7c9e7a7d-2593-458c-8a46-0313d77c3278: Completed APPEND_ENTRIES, lastRequest: 04b88c83-026f-4630-aada-108e37835bee->7c9e7a7d-2593-458c-8a46-0313d77c3278#309-t7,previous=(t:7, i:43),leaderCommit=43,initializing? true,entries: size=1, first=(t:7, i:44), METADATAENTRY(c:43)
dn4_1    | 2023-03-18 09:23:21,473 [grpc-default-executor-4] INFO server.GrpcServerProtocolService: 7c9e7a7d-2593-458c-8a46-0313d77c3278: Completed APPEND_ENTRIES, lastRequest: null
dn4_1    | 2023-03-18 09:23:21,491 [Command processor thread] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-40B9CE60BD6C: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/a2f44939-53c6-4b4c-afd2-40b9ce60bd6c
dn4_1    | 2023-03-18 09:23:21,492 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=a2f44939-53c6-4b4c-afd2-40b9ce60bd6c command on datanode 7c9e7a7d-2593-458c-8a46-0313d77c3278.
dn4_1    | 2023-03-18 09:23:21,493 [Command processor thread] INFO server.RaftServer: 7c9e7a7d-2593-458c-8a46-0313d77c3278: remove  FOLLOWER 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D:t7, leader=04b88c83-026f-4630-aada-108e37835bee, voted=04b88c83-026f-4630-aada-108e37835bee, raftlog=Memoized:7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-SegmentedRaftLog:OPENED:c44, conf=39: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn4_1    | 2023-03-18 09:23:21,493 [Command processor thread] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D: shutdown
dn4_1    | 2023-03-18 09:23:21,498 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-8191D6D2324D,id=7c9e7a7d-2593-458c-8a46-0313d77c3278
dn4_1    | 2023-03-18 09:23:21,499 [Command processor thread] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: shutdown 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-FollowerState
dn4_1    | 2023-03-18 09:23:21,499 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-FollowerState] INFO impl.FollowerState: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-FollowerState was interrupted
dn4_1    | 2023-03-18 09:23:21,500 [Command processor thread] INFO impl.StateMachineUpdater: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-StateMachineUpdater: set stopIndex = 44
dn4_1    | 2023-03-18 09:23:21,501 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-8191D6D2324D: Taking a snapshot at:(t:7, i:44) file /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/sm/snapshot.7_44
dn4_1    | 2023-03-18 09:23:21,516 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-8191D6D2324D: Finished taking a snapshot at:(t:7, i:44) file:/data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/sm/snapshot.7_44 took: 16 ms
recon_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.23.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.3.23.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.23.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.23.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.23.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
recon_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/1236414c1c46c67c92f37499b0b61921cdfab39a ; compiled by 'runner' on 2023-03-18T08:55Z
recon_1  | STARTUP_MSG:   java = 11.0.14.1
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-03-18 09:22:50,339 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-03-18 09:22:50,340 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn3_1    | 2023-03-18 09:22:50,341 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn3_1    | 2023-03-18 09:22:50,341 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1001 to close, current state is: CLOSING
dn3_1    | 2023-03-18 09:22:50,341 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn3_1    | 2023-03-18 09:22:50,341 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-03-18 09:22:50,341 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-03-18 09:22:50,341 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-03-18 09:22:50,341 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn3_1    | 2023-03-18 09:22:50,342 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn3_1    | 2023-03-18 09:22:50,342 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1001 to close, current state is: CLOSING
dn3_1    | 2023-03-18 09:22:50,342 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn3_1    | 2023-03-18 09:22:50,342 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-03-18 09:22:50,343 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn5_1    | 2023-03-18 09:22:07,960 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D: set configuration 0: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:07,971 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/b5ca130e-c58f-4d3d-8bda-3212c22edf58/current/log_0-0
dn5_1    | 2023-03-18 09:22:08,244 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO segmented.LogSegment: Successfully read 8 entries from segment file /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/current/log_0-7
dn5_1    | 2023-03-18 09:22:08,294 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58: set configuration 1: peers:[04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:08,335 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/b5ca130e-c58f-4d3d-8bda-3212c22edf58/current/log_1-2
dn5_1    | 2023-03-18 09:22:08,336 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D: set configuration 8: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:08,347 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO segmented.LogSegment: Successfully read 17 entries from segment file /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/current/log_8-24
dn5_1    | 2023-03-18 09:22:08,272 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO segmented.LogSegment: Successfully read 5 entries from segment file /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/current/log_0-4
dn5_1    | 2023-03-18 09:22:08,377 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C: set configuration 5: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:09,311 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO segmented.LogSegment: Successfully read 9 entries from segment file /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/current/log_5-13
dn5_1    | 2023-03-18 09:22:09,374 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C: set configuration 14: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:09,385 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO segmented.LogSegment: Successfully read 14 entries from segment file /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/current/log_inprogress_14
dn5_1    | 2023-03-18 09:22:08,377 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D: set configuration 25: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:08,377 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58: set configuration 3: peers:[04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:09,511 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/b5ca130e-c58f-4d3d-8bda-3212c22edf58/current/log_inprogress_3
dn5_1    | 2023-03-18 09:22:09,512 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO segmented.LogSegment: Successfully read 14 entries from segment file /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/current/log_inprogress_25
dn5_1    | 2023-03-18 09:22:09,520 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn5_1    | 2023-03-18 09:22:09,520 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn5_1    | 2023-03-18 09:22:09,521 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 38
dn5_1    | 2023-03-18 09:22:09,530 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 24
dn5_1    | 2023-03-18 09:22:09,512 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 27
dn5_1    | 2023-03-18 09:22:09,530 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 13
dn5_1    | 2023-03-18 09:22:10,499 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO raftlog.RaftLog: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-SegmentedRaftLog: commitIndex: updateToMax old=27, new=26, updated? false
dn5_1    | 2023-03-18 09:22:10,501 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C: start as a follower, conf=14: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:10,508 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C: changes role from      null to FOLLOWER at term 5 for startAsFollower
dn5_1    | 2023-03-18 09:22:10,504 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO raftlog.RaftLog: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-SegmentedRaftLog: commitIndex: updateToMax old=4, new=3, updated? false
dn5_1    | 2023-03-18 09:22:10,520 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58: start as a follower, conf=3: peers:[04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:10,520 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn5_1    | 2023-03-18 09:22:10,517 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO raftlog.RaftLog: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-SegmentedRaftLog: commitIndex: updateToMax old=38, new=36, updated? false
dn5_1    | 2023-03-18 09:22:10,540 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D: start as a follower, conf=25: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:10,541 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D: changes role from      null to FOLLOWER at term 6 for startAsFollower
dn5_1    | 2023-03-18 09:22:10,545 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: start 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-FollowerState
dn5_1    | 2023-03-18 09:22:10,598 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-80313208139C,id=04b88c83-026f-4630-aada-108e37835bee
dn5_1    | 2023-03-18 09:22:10,601 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-18 09:22:10,602 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-18 09:22:10,601 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-03-18 09:22:10,556 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: start 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-FollowerState
dn5_1    | 2023-03-18 09:22:10,556 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: start 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-FollowerState
dn5_1    | 2023-03-18 09:22:10,696 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8191D6D2324D,id=04b88c83-026f-4630-aada-108e37835bee
dn5_1    | 2023-03-18 09:22:10,696 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-03-18 09:22:10,696 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-03-18 09:22:10,697 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-03-18 09:22:10,697 [04b88c83-026f-4630-aada-108e37835bee-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-03-18 09:22:10,737 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-03-18 09:22:10,737 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-03-18 09:22:10,737 [04b88c83-026f-4630-aada-108e37835bee-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-03-18 09:22:10,784 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-18 09:22:10,784 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-18 09:22:10,804 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3212C22EDF58,id=04b88c83-026f-4630-aada-108e37835bee
dn5_1    | 2023-03-18 09:22:10,804 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-03-18 09:22:10,804 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-03-18 09:22:10,804 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-03-18 09:22:10,804 [04b88c83-026f-4630-aada-108e37835bee-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-03-18 09:22:10,806 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-18 09:22:10,806 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-18 09:22:10,818 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 04b88c83-026f-4630-aada-108e37835bee: start RPC server
dn5_1    | 2023-03-18 09:22:10,856 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 04b88c83-026f-4630-aada-108e37835bee: GrpcService started, listening on 9858
dn5_1    | 2023-03-18 09:22:10,864 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 04b88c83-026f-4630-aada-108e37835bee: GrpcService started, listening on 9856
dn5_1    | 2023-03-18 09:22:10,865 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 04b88c83-026f-4630-aada-108e37835bee: GrpcService started, listening on 9857
recon_1  | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.timeout=30m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=1440, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.command.deadline.factor=0.9, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=true, hdds.scm.replication.event.timeout=30m, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.sql.db.auto.commit=true, ozone.recon.sql.db.conn.idle.max.age=3600s, ozone.recon.sql.db.conn.idle.test=SELECT 1, ozone.recon.sql.db.conn.idle.test.period=60s, ozone.recon.sql.db.conn.max.active=5, ozone.recon.sql.db.conn.max.age=1800s, ozone.recon.sql.db.conn.timeout=30000ms, ozone.recon.sql.db.driver=org.apache.derby.jdbc.EmbeddedDriver, ozone.recon.sql.db.jdbc.url=jdbc:derby:/data/metadata/recon/ozone_recon_derby.db, ozone.recon.sql.db.jooq.dialect=DERBY, ozone.recon.task.missingcontainer.interval=300s, ozone.recon.task.pipelinesync.interval=300s, ozone.recon.task.safemode.wait.threshold=300s, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4))$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
recon_1  | ************************************************************/
recon_1  | 2023-03-18 09:21:26,952 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1  | WARNING: An illegal reflective access operation has occurred
recon_1  | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1  | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1  | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1  | WARNING: All illegal access operations will be denied in a future release
recon_1  | 2023-03-18 09:21:31,944 [main] INFO reflections.Reflections: Reflections took 573 ms to scan 1 urls, producing 17 keys and 54 values 
recon_1  | 2023-03-18 09:21:35,185 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1  | 2023-03-18 09:21:35,206 [main] INFO impl.ReconDBProvider: Last known Recon DB : /data/metadata/recon/recon-container-key.db_1679130799894
recon_1  | 2023-03-18 09:21:37,146 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1  | 2023-03-18 09:21:41,551 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1  | 2023-03-18 09:21:44,505 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1  | 2023-03-18 09:21:44,541 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1  | 2023-03-18 09:21:44,542 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1  | 2023-03-18 09:21:50,027 [main] INFO codegen.SqlDbUtils: FILE_COUNT_BY_SIZE table already exists, skipping creation.
recon_1  | 2023-03-18 09:21:50,256 [main] INFO codegen.SqlDbUtils: CLUSTER_GROWTH_DAILY table already exists, skipping creation.
recon_1  | 2023-03-18 09:21:50,417 [main] INFO codegen.SqlDbUtils: RECON_TASK_STATUS table already exists, skipping creation.
recon_1  | 2023-03-18 09:21:50,507 [main] INFO codegen.SqlDbUtils: GLOBAL_STATS table already exists, skipping creation.
recon_1  | 2023-03-18 09:21:50,597 [main] INFO codegen.SqlDbUtils: UNHEALTHY_CONTAINERS table already exists, skipping creation.
recon_1  | 2023-03-18 09:21:50,939 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1  | 2023-03-18 09:21:51,022 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1  | 2023-03-18 09:21:51,125 [main] INFO util.log: Logging initialized @35189ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1  | 2023-03-18 09:21:51,661 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1  | 2023-03-18 09:21:51,731 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1  | 2023-03-18 09:21:51,764 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1  | 2023-03-18 09:21:51,801 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1  | 2023-03-18 09:21:51,803 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1  | 2023-03-18 09:21:51,804 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1  | 2023-03-18 09:21:52,206 [main] INFO http.BaseHttpServer: HTTP server of recon uses base directory /data/metadata/webserver
recon_1  | 2023-03-18 09:21:52,769 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1  | 2023-03-18 09:21:53,056 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1  | 2023-03-18 09:21:53,088 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1  | 2023-03-18 09:21:53,116 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1  | 2023-03-18 09:21:53,201 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1  | 2023-03-18 09:21:53,202 [main] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
recon_1  | 2023-03-18 09:21:55,917 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-03-18 09:21:56,672 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-03-18 09:21:57,088 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1  | 2023-03-18 09:21:57,112 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1  | 2023-03-18 09:21:57,588 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-03-18 09:21:57,884 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
recon_1  | 2023-03-18 09:21:58,553 [main] INFO reflections.Reflections: Reflections took 620 ms to scan 3 urls, producing 128 keys and 283 values 
recon_1  | 2023-03-18 09:21:58,825 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1  | 2023-03-18 09:21:58,987 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1  | 2023-03-18 09:21:59,131 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/04b88c83-026f-4630-aada-108e37835bee
recon_1  | 2023-03-18 09:21:59,146 [main] INFO node.SCMNodeManager: Registered Data node : 04b88c83-026f-4630-aada-108e37835bee{ip: 10.9.0.19, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-03-18 09:21:59,157 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/2b374f74-8596-4317-81ba-ce0a491ddd96
recon_1  | 2023-03-18 09:21:59,159 [main] INFO node.SCMNodeManager: Registered Data node : 2b374f74-8596-4317-81ba-ce0a491ddd96{ip: 10.9.0.16, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-03-18 09:21:59,168 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/4ba198f1-a013-49ce-b059-f73788d69001
recon_1  | 2023-03-18 09:21:59,170 [main] INFO node.SCMNodeManager: Registered Data node : 4ba198f1-a013-49ce-b059-f73788d69001{ip: 10.9.0.15, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-03-18 09:21:59,170 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/7c9e7a7d-2593-458c-8a46-0313d77c3278
recon_1  | 2023-03-18 09:21:59,178 [main] INFO node.SCMNodeManager: Registered Data node : 7c9e7a7d-2593-458c-8a46-0313d77c3278{ip: 10.9.0.18, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-03-18 09:21:59,180 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/bae0cd8d-e50f-4d23-8974-b816860f8fc3
recon_1  | 2023-03-18 09:21:59,185 [main] INFO node.SCMNodeManager: Registered Data node : bae0cd8d-e50f-4d23-8974-b816860f8fc3{ip: 10.9.0.17, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-03-18 09:21:59,185 [main] INFO scm.ReconNodeManager: Loaded 5 nodes from node DB.
recon_1  | 2023-03-18 09:21:59,202 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1  | 2023-03-18 09:21:59,345 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1  | 2023-03-18 09:21:59,400 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1  | 2023-03-18 09:21:59,616 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1  | 2023-03-18 09:21:59,922 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
recon_1  | 2023-03-18 09:21:59,926 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1  | 2023-03-18 09:22:00,051 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1  | 2023-03-18 09:22:00,070 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1  | 2023-03-18 09:22:00,071 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1  | 2023-03-18 09:22:00,508 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1  | 2023-03-18 09:22:00,516 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
recon_1  | 2023-03-18 09:22:00,685 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1  | 2023-03-18 09:22:00,685 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1  | 2023-03-18 09:22:00,686 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 600000ms
dn5_1    | 2023-03-18 09:22:10,890 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 04b88c83-026f-4630-aada-108e37835bee is started using port 9858 for RATIS
dn5_1    | 2023-03-18 09:22:10,896 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 04b88c83-026f-4630-aada-108e37835bee is started using port 9857 for RATIS_ADMIN
dn5_1    | 2023-03-18 09:22:10,896 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 04b88c83-026f-4630-aada-108e37835bee is started using port 9856 for RATIS_SERVER
dn5_1    | 2023-03-18 09:22:10,896 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-04b88c83-026f-4630-aada-108e37835bee: Started
dn5_1    | 2023-03-18 09:22:11,075 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 2023-03-18 09:22:15,649 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-FollowerState] INFO impl.FollowerState: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5103820994ns, electionTimeout:5046ms
dn5_1    | 2023-03-18 09:22:15,650 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-FollowerState] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: shutdown 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-FollowerState
dn5_1    | 2023-03-18 09:22:15,651 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-FollowerState] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C: changes role from  FOLLOWER to CANDIDATE at term 5 for changeToCandidate
dn5_1    | 2023-03-18 09:22:15,654 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-03-18 09:22:15,654 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-FollowerState] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: start 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-LeaderElection1
dn5_1    | 2023-03-18 09:22:15,666 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-LeaderElection1] INFO impl.LeaderElection: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 5 for 14: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:15,749 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for bae0cd8d-e50f-4d23-8974-b816860f8fc3
dn5_1    | 2023-03-18 09:22:15,756 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-18 09:22:15,767 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-18 09:22:15,756 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 7c9e7a7d-2593-458c-8a46-0313d77c3278
dn5_1    | 2023-03-18 09:22:15,811 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-FollowerState] INFO impl.FollowerState: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5254061505ns, electionTimeout:5023ms
dn5_1    | 2023-03-18 09:22:15,829 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-FollowerState] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: shutdown 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-FollowerState
dn5_1    | 2023-03-18 09:22:15,832 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-FollowerState] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D: changes role from  FOLLOWER to CANDIDATE at term 6 for changeToCandidate
dn5_1    | 2023-03-18 09:22:15,832 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-03-18 09:22:15,832 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-FollowerState] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: start 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2
dn5_1    | 2023-03-18 09:22:15,862 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO impl.LeaderElection: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 6 for 25: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:15,879 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-18 09:22:15,880 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-18 09:22:15,990 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-FollowerState] INFO impl.FollowerState: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5433922419ns, electionTimeout:5182ms
dn5_1    | 2023-03-18 09:22:15,994 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-FollowerState] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: shutdown 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-FollowerState
dn5_1    | 2023-03-18 09:22:15,994 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-FollowerState] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn5_1    | 2023-03-18 09:22:15,995 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-03-18 09:22:15,995 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-FollowerState] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: start 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3
dn5_1    | 2023-03-18 09:22:16,030 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3] INFO impl.LeaderElection: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:16,033 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3] INFO impl.LeaderElection: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3 PRE_VOTE round 0: result PASSED (term=3)
dn4_1    | 2023-03-18 09:23:21,522 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-StateMachineUpdater] INFO impl.StateMachineUpdater: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-StateMachineUpdater: Took a snapshot at index 44
dn4_1    | 2023-03-18 09:23:21,523 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-StateMachineUpdater] INFO impl.StateMachineUpdater: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-StateMachineUpdater: snapshotIndex: updateIncreasingly 38 -> 44
dn4_1    | 2023-03-18 09:23:21,524 [Command processor thread] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D: closes. applyIndex: 44
dn4_1    | 2023-03-18 09:23:21,525 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn4_1    | 2023-03-18 09:23:21,526 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D-SegmentedRaftLogWorker close()
dn4_1    | 2023-03-18 09:23:21,530 [Command processor thread] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-8191D6D2324D: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d
dn4_1    | 2023-03-18 09:23:21,534 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=97be11a2-1597-458e-9379-8191d6d2324d command on datanode 7c9e7a7d-2593-458c-8a46-0313d77c3278.
dn4_1    | 2023-03-18 09:23:21,535 [Command processor thread] INFO server.RaftServer: 7c9e7a7d-2593-458c-8a46-0313d77c3278: remove  FOLLOWER 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C:t6, leader=bae0cd8d-e50f-4d23-8974-b816860f8fc3, voted=bae0cd8d-e50f-4d23-8974-b816860f8fc3, raftlog=Memoized:7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-SegmentedRaftLog:OPENED:c33, conf=28: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn4_1    | 2023-03-18 09:23:21,535 [Command processor thread] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C: shutdown
dn4_1    | 2023-03-18 09:23:21,537 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-80313208139C,id=7c9e7a7d-2593-458c-8a46-0313d77c3278
dn4_1    | 2023-03-18 09:23:21,538 [Command processor thread] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: shutdown 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-FollowerState
dn4_1    | 2023-03-18 09:23:21,538 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-FollowerState] INFO impl.FollowerState: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-FollowerState was interrupted
dn4_1    | 2023-03-18 09:23:21,538 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-80313208139C: Taking a snapshot at:(t:6, i:33) file /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/sm/snapshot.6_33
dn4_1    | 2023-03-18 09:23:21,540 [Command processor thread] INFO impl.StateMachineUpdater: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-StateMachineUpdater: set stopIndex = 33
dn4_1    | 2023-03-18 09:23:21,540 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-80313208139C: Finished taking a snapshot at:(t:6, i:33) file:/data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/sm/snapshot.6_33 took: 2 ms
dn4_1    | 2023-03-18 09:23:21,541 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-StateMachineUpdater] INFO impl.StateMachineUpdater: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-StateMachineUpdater: Took a snapshot at index 33
dn4_1    | 2023-03-18 09:23:21,542 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-StateMachineUpdater] INFO impl.StateMachineUpdater: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-StateMachineUpdater: snapshotIndex: updateIncreasingly 27 -> 33
dn4_1    | 2023-03-18 09:23:21,543 [Command processor thread] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C: closes. applyIndex: 33
dn4_1    | 2023-03-18 09:23:21,546 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn4_1    | 2023-03-18 09:23:21,547 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C-SegmentedRaftLogWorker close()
dn4_1    | 2023-03-18 09:23:21,558 [Command processor thread] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-80313208139C: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c
dn4_1    | 2023-03-18 09:23:21,563 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=e319c2ba-524b-42f7-8b03-80313208139c command on datanode 7c9e7a7d-2593-458c-8a46-0313d77c3278.
dn4_1    | 2023-03-18 09:23:22,547 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@623e0631] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(0),2(0)], numOfContainers=2, numOfBlocks=3
dn4_1    | 2023-03-18 09:23:51,432 [pool-38-thread-1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278: new RaftServerImpl for group-BDE6DABFBCFE:[7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn4_1    | 2023-03-18 09:23:51,432 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-03-18 09:23:51,433 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-03-18 09:23:51,433 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-03-18 09:23:51,433 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-03-18 09:23:51,433 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-03-18 09:23:51,433 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-03-18 09:23:51,433 [pool-38-thread-1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE: ConfigurationManager, init=-1: peers:[7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-03-18 09:23:51,433 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-03-18 09:23:51,435 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-03-18 09:23:51,436 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-03-18 09:23:51,436 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
s3g_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1    | 2023-03-18 09:21:26,798 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1    | 2023-03-18 09:21:26,810 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1    | 2023-03-18 09:21:27,035 [main] INFO util.log: Logging initialized @10306ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1    | 2023-03-18 09:21:28,263 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1    | 2023-03-18 09:21:28,523 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1    | 2023-03-18 09:21:28,648 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1    | 2023-03-18 09:21:28,689 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1    | 2023-03-18 09:21:28,694 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1    | 2023-03-18 09:21:28,694 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1    | 2023-03-18 09:21:29,089 [main] INFO http.BaseHttpServer: HTTP server of s3gateway uses base directory /data/metadata/webserver
s3g_1    | 2023-03-18 09:21:30,411 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1    | /************************************************************
s3g_1    | STARTUP_MSG: Starting Gateway
s3g_1    | STARTUP_MSG:   host = 5673d6c55a2a/10.9.0.21
s3g_1    | STARTUP_MSG:   args = []
s3g_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
s3g_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-shaded-3.1.9.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/cdi-api-2.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
s3g_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/1236414c1c46c67c92f37499b0b61921cdfab39a ; compiled by 'runner' on 2023-03-18T08:55Z
s3g_1    | STARTUP_MSG:   java = 11.0.14.1
om3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1    | 2023-03-18 09:21:26,389 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1    | /************************************************************
om3_1    | STARTUP_MSG: Starting OzoneManager
om3_1    | STARTUP_MSG:   host = 9010d708b962/10.9.0.13
om3_1    | STARTUP_MSG:   args = [--upgrade]
om3_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn4_1    | 2023-03-18 09:23:51,436 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-03-18 09:23:51,436 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-03-18 09:23:51,438 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-03-18 09:23:51,439 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-03-18 09:23:51,439 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-03-18 09:23:51,439 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-03-18 09:23:51,439 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-03-18 09:23:51,440 [Command processor thread] INFO server.RaftServer: 7c9e7a7d-2593-458c-8a46-0313d77c3278: addNew group-BDE6DABFBCFE:[7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] returns      null 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE:t0, leader=null, voted=null, raftlog=UNINITIALIZED, conf=-1: peers:[7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null NEW
dn4_1    | 2023-03-18 09:23:51,441 [pool-38-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/3538aed8-d817-47ae-96b3-bde6dabfbcfe does not exist. Creating ...
dn4_1    | 2023-03-18 09:23:51,443 [pool-38-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/3538aed8-d817-47ae-96b3-bde6dabfbcfe/in_use.lock acquired by nodename 7@23f39be4c8d3
dn4_1    | 2023-03-18 09:23:51,444 [pool-38-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/3538aed8-d817-47ae-96b3-bde6dabfbcfe has been successfully formatted.
dn4_1    | 2023-03-18 09:23:51,452 [pool-38-thread-1] INFO ratis.ContainerStateMachine: group-BDE6DABFBCFE: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn4_1    | 2023-03-18 09:23:51,452 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-03-18 09:23:51,452 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-03-18 09:23:51,452 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-18 09:23:51,453 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-03-18 09:23:51,453 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-03-18 09:23:51,453 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-03-18 09:23:51,454 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-03-18 09:23:51,455 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-03-18 09:23:51,456 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: new 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/3538aed8-d817-47ae-96b3-bde6dabfbcfe
dn4_1    | 2023-03-18 09:23:51,456 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-03-18 09:23:51,460 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-03-18 09:23:51,460 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-03-18 09:23:51,460 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-03-18 09:23:51,460 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-03-18 09:23:51,460 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-03-18 09:23:51,460 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-03-18 09:23:51,460 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-03-18 09:23:51,462 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-03-18 09:23:51,464 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-18 09:23:51,472 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-03-18 09:23:51,472 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-03-18 09:23:51,472 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-03-18 09:23:51,473 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-03-18 09:23:51,473 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-03-18 09:23:51,474 [pool-38-thread-1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE: start as a follower, conf=-1: peers:[7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:23:51,475 [pool-38-thread-1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn4_1    | 2023-03-18 09:23:51,475 [pool-38-thread-1] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: start 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-FollowerState
dn4_1    | 2023-03-18 09:23:51,491 [pool-38-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-BDE6DABFBCFE,id=7c9e7a7d-2593-458c-8a46-0313d77c3278
dn4_1    | 2023-03-18 09:23:51,491 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-03-18 09:23:51,491 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-03-18 09:23:51,491 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-03-18 09:23:51,491 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-03-18 09:23:51,494 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-18 09:23:51,497 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-18 09:23:51,496 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=3538aed8-d817-47ae-96b3-bde6dabfbcfe
dn4_1    | 2023-03-18 09:23:51,497 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=3538aed8-d817-47ae-96b3-bde6dabfbcfe.
dn4_1    | 2023-03-18 09:23:51,498 [Command processor thread] INFO server.RaftServer: 7c9e7a7d-2593-458c-8a46-0313d77c3278: addNew group-6F7919707118:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER] returns group-6F7919707118:java.util.concurrent.CompletableFuture@64e0ef25[Not completed]
dn4_1    | 2023-03-18 09:23:51,499 [pool-38-thread-1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278: new RaftServerImpl for group-6F7919707118:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn4_1    | 2023-03-18 09:23:51,502 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-03-18 09:23:51,502 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-03-18 09:23:51,503 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-03-18 09:23:51,503 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-03-18 09:23:51,503 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-03-18 09:23:51,503 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-03-18 09:23:51,505 [pool-38-thread-1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118: ConfigurationManager, init=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-03-18 09:23:51,506 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-03-18 09:23:51,506 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-03-18 09:23:51,507 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-03-18 09:23:51,507 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-03-18 09:23:51,508 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-03-18 09:23:51,509 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-03-18 09:23:51,510 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-03-18 09:23:51,514 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-03-18 09:23:51,514 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-03-18 09:23:51,515 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-03-18 09:23:51,516 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-03-18 09:23:51,518 [pool-38-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/66de196e-0a1c-4e0b-be4e-6f7919707118 does not exist. Creating ...
dn4_1    | 2023-03-18 09:23:51,520 [pool-38-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/66de196e-0a1c-4e0b-be4e-6f7919707118/in_use.lock acquired by nodename 7@23f39be4c8d3
dn4_1    | 2023-03-18 09:23:51,525 [pool-38-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/66de196e-0a1c-4e0b-be4e-6f7919707118 has been successfully formatted.
dn4_1    | 2023-03-18 09:23:51,529 [pool-38-thread-1] INFO ratis.ContainerStateMachine: group-6F7919707118: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn4_1    | 2023-03-18 09:23:51,530 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-03-18 09:23:51,530 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-03-18 09:23:51,531 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-18 09:23:51,532 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-03-18 09:23:51,532 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-03-18 09:23:51,536 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-03-18 09:23:51,537 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/1236414c1c46c67c92f37499b0b61921cdfab39a ; compiled by 'runner' on 2023-03-18T08:55Z
om3_1    | STARTUP_MSG:   java = 11.0.14.1
s3g_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4))$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
s3g_1    | ************************************************************/
s3g_1    | 2023-03-18 09:21:30,468 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1    | 2023-03-18 09:21:30,644 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1    | 2023-03-18 09:21:31,364 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1    | 2023-03-18 09:21:32,453 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1    | 2023-03-18 09:21:32,453 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1    | 2023-03-18 09:21:32,882 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1    | 2023-03-18 09:21:32,885 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
s3g_1    | 2023-03-18 09:21:33,252 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1    | 2023-03-18 09:21:33,256 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1    | 2023-03-18 09:21:33,285 [main] INFO server.session: node0 Scavenging every 600000ms
s3g_1    | 2023-03-18 09:21:33,446 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6853425f{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1    | 2023-03-18 09:21:33,447 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@68d279ec{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1    | WARNING: An illegal reflective access operation has occurred
s3g_1    | WARNING: Illegal reflective access by com.sun.xml.bind.v2.runtime.reflect.opt.Injector (file:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int)
s3g_1    | WARNING: Please consider reporting this to the maintainers of com.sun.xml.bind.v2.runtime.reflect.opt.Injector
s3g_1    | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1    | WARNING: All illegal access operations will be denied in a future release
s3g_1    | Mar 18, 2023 9:21:58 AM org.glassfish.jersey.internal.Errors logErrors
s3g_1    | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1    | 
s3g_1    | 2023-03-18 09:21:58,617 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7757025d{s3gateway,/,file:///data/metadata/webserver/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-17264766045489723645/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1    | 2023-03-18 09:21:58,649 [main] INFO server.AbstractConnector: Started ServerConnector@327bcebd{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1    | 2023-03-18 09:21:58,650 [main] INFO server.Server: Started @41921ms
s3g_1    | 2023-03-18 09:21:58,676 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1    | 2023-03-18 09:21:58,676 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1    | 2023-03-18 09:21:58,685 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
s3g_1    | 2023-03-18 09:24:41,795 [qtp1683662486-21] INFO audit.AuditLogger: Refresh DebugCmdSet for S3GAudit to [].
s3g_1    | 2023-03-18 09:24:41,833 [qtp1683662486-21] INFO audit.AuditLogger: Refresh DebugCmdSet for S3GAudit to [].
s3g_1    | 2023-03-18 09:24:41,839 [qtp1683662486-21] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
s3g_1    | 2023-03-18 09:24:41,839 [qtp1683662486-21] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
s3g_1    | 2023-03-18 09:24:42,690 [qtp1683662486-21] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om2 is not the leader. Suggested leader is OM:om3[om3/10.9.0.13].
s3g_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:245)
s3g_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:232)
s3g_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:225)
s3g_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:177)
s3g_1    | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
s3g_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
s3g_1    | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
s3g_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
s3g_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
s3g_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
s3g_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
s3g_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
s3g_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
s3g_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
s3g_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
s3g_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
s3g_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
s3g_1    | , while invoking $Proxy120.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 1 failover attempts. Trying to failover immediately.
s3g_1    | 2023-03-18 09:24:43,013 [qtp1683662486-21] WARN impl.MetricsSystemImpl: S3Gateway metrics system already initialized!
s3g_1    | 2023-03-18 09:25:32,139 [qtp1683662486-23] INFO rpc.RpcClient: Creating Bucket: s3v/new2-bucket, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false, Replication Type set to server-side default replication type, Namespace Quota set to -1, Space Quota set to -1 
s3g_1    | 2023-03-18 09:25:32,962 [qtp1683662486-20] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
recon_1  | 2023-03-18 09:22:00,750 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7fd99443{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1  | 2023-03-18 09:22:00,751 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@68a305eb{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1  | 2023-03-18 09:22:05,558 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@12925d2{recon,/,file:///data/metadata/webserver/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-8225993744452548044/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
recon_1  | 2023-03-18 09:22:05,831 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@7fbf26fc{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1  | 2023-03-18 09:22:05,831 [Listener at 0.0.0.0/9891] INFO server.Server: Started @49895ms
recon_1  | 2023-03-18 09:22:05,873 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1  | 2023-03-18 09:22:05,873 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1  | 2023-03-18 09:22:05,899 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1  | 2023-03-18 09:22:05,900 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1  | 2023-03-18 09:22:05,946 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1  | 2023-03-18 09:22:05,946 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-03-18 09:22:05,997 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Last known snapshot for OM : /data/metadata/om.snapshot.db_1679131228312
recon_1  | 2023-03-18 09:22:06,005 [Listener at 0.0.0.0/9891] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-03-18 09:22:06,037 [Listener at 0.0.0.0/9891] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-03-18 09:22:06,323 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1679131228312.
recon_1  | 2023-03-18 09:22:06,537 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1  | 2023-03-18 09:22:06,537 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1  | 2023-03-18 09:22:07,679 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 7 pipelines from SCM.
recon_1  | 2023-03-18 09:22:07,679 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 7 pipelines in house.
recon_1  | 2023-03-18 09:22:07,795 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1  | 2023-03-18 09:22:07,801 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1  | 2023-03-18 09:22:07,809 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1  | 2023-03-18 09:22:07,810 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1  | 2023-03-18 09:22:09,381 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.15:55264: output error
recon_1  | 2023-03-18 09:22:09,443 [IPC Server handler 6 on default port 9891] WARN ipc.Server: IPC Server handler 6 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:50534: output error
recon_1  | 2023-03-18 09:22:09,443 [IPC Server handler 7 on default port 9891] WARN ipc.Server: IPC Server handler 7 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:45152: output error
recon_1  | 2023-03-18 09:22:09,443 [IPC Server handler 8 on default port 9891] WARN ipc.Server: IPC Server handler 8 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.16:59448: output error
recon_1  | 2023-03-18 09:22:09,445 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:42910: output error
recon_1  | 2023-03-18 09:22:09,447 [IPC Server handler 6 on default port 9891] INFO ipc.Server: IPC Server handler 6 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-03-18 09:22:09,447 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1    | 2023-03-18 09:21:36,257 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1    | /************************************************************
scm_1    | STARTUP_MSG: Starting StorageContainerManager
scm_1    | STARTUP_MSG:   host = b6dfe97b58f7/10.9.0.14
scm_1    | STARTUP_MSG:   args = []
scm_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/1236414c1c46c67c92f37499b0b61921cdfab39a ; compiled by 'runner' on 2023-03-18T08:55Z
scm_1    | STARTUP_MSG:   java = 11.0.14.1
scm_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.timeout=30m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=1440, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.command.deadline.factor=0.9, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=true, hdds.scm.replication.event.timeout=30m, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4))$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm_1    | ************************************************************/
scm_1    | 2023-03-18 09:21:36,401 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1    | 2023-03-18 09:21:37,320 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1    | 2023-03-18 09:21:37,798 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1    | 2023-03-18 09:21:38,300 [main] WARN ha.SCMHANodeDetails: Invalid config ozone.scm.ratis.enable. The config was not specified, but the default value true conflicts with the expected config value false. Falling back to the expected value. Current State of SCM: SCM is running in Non-HA without Ratis Ratis SCM -> Non Ratis SCM or Non HA SCM -> HA SCM is not supported
scm_1    | 2023-03-18 09:21:38,302 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1    | 2023-03-18 09:21:47,265 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1    | 2023-03-18 09:21:49,793 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1    | 2023-03-18 09:21:51,646 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm_1    | 2023-03-18 09:21:51,656 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1    | 2023-03-18 09:21:52,799 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1    | 2023-03-18 09:21:52,835 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = DATANODE_SCHEMA_V3 (version = 4)
scm_1    | 2023-03-18 09:21:57,033 [main] INFO reflections.Reflections: Reflections took 1987 ms to scan 3 urls, producing 128 keys and 283 values 
scm_1    | 2023-03-18 09:21:57,701 [main] INFO upgrade.HDDSLayoutVersionManager: Registering Upgrade Action : DatanodeSchemaV3FinalizeAction
scm_1    | 2023-03-18 09:21:57,857 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm_1    | 2023-03-18 09:21:58,584 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1    | 2023-03-18 09:21:58,695 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1    | 2023-03-18 09:21:58,749 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1    | 2023-03-18 09:21:59,385 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1    | 2023-03-18 09:21:59,396 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1    | 2023-03-18 09:21:59,427 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm_1    | 2023-03-18 09:21:59,431 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm_1    | 2023-03-18 09:21:59,463 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm_1    | 2023-03-18 09:21:59,471 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm_1    | 2023-03-18 09:21:59,493 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-03-18 09:22:09,447 [IPC Server handler 8 on default port 9891] INFO ipc.Server: IPC Server handler 8 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-03-18 09:22:09,447 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-03-18 09:22:09,460 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-03-18 09:22:09,968 [IPC Server handler 71 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ac7a47589fa9
recon_1  | 2023-03-18 09:22:10,209 [IPC Server handler 2 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for 23f39be4c8d3
recon_1  | 2023-03-18 09:22:11,005 [IPC Server handler 70 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn2_1.ha_net
recon_1  | 2023-03-18 09:22:11,317 [IPC Server handler 5 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for e51427cbfe02
recon_1  | 2023-03-18 09:22:11,584 [IPC Server handler 10 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:11,649 [IPC Server handler 11 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:11,797 [IPC Server handler 12 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:12,114 [IPC Server handler 71 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn1_1.ha_net
recon_1  | 2023-03-18 09:22:13,511 [IPC Server handler 10 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:13,571 [IPC Server handler 11 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:13,759 [IPC Server handler 12 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:13,801 [IPC Server handler 13 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn5_1    | 2023-03-18 09:22:16,198 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3] INFO impl.LeaderElection: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3 ELECTION round 0: submit vote requests at term 4 for 3: peers:[04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:16,199 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3] INFO impl.LeaderElection: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3 ELECTION round 0: result PASSED (term=4)
dn5_1    | 2023-03-18 09:22:16,200 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: shutdown 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3
dn5_1    | 2023-03-18 09:22:16,203 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn5_1    | 2023-03-18 09:22:16,210 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-3212C22EDF58 with new leaderId: 04b88c83-026f-4630-aada-108e37835bee
dn5_1    | 2023-03-18 09:22:16,225 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58: change Leader from null to 04b88c83-026f-4630-aada-108e37835bee at term 4 for becomeLeader, leader elected after 21131ms
dn5_1    | 2023-03-18 09:22:16,302 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn5_1    | 2023-03-18 09:22:16,389 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-03-18 09:22:16,390 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn5_1    | 2023-03-18 09:22:16,434 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn5_1    | 2023-03-18 09:22:16,445 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 2023-03-18 09:22:16,447 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn5_1    | 2023-03-18 09:22:16,602 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-03-18 09:22:16,654 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn5_1    | 2023-03-18 09:22:16,678 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: start 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderStateImpl
dn5_1    | 2023-03-18 09:22:16,734 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn5_1    | 2023-03-18 09:22:16,763 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/b5ca130e-c58f-4d3d-8bda-3212c22edf58/current/log_inprogress_3 to /data/metadata/ratis/b5ca130e-c58f-4d3d-8bda-3212c22edf58/current/log_3-4
dn5_1    | 2023-03-18 09:22:16,779 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/b5ca130e-c58f-4d3d-8bda-3212c22edf58/current/log_inprogress_5
dn5_1    | 2023-03-18 09:22:16,847 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderElection3] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58: set configuration 5: peers:[04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:18,080 [grpc-default-executor-0] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C: receive requestVote(PRE_VOTE, bae0cd8d-e50f-4d23-8974-b816860f8fc3, group-80313208139C, 5, (t:5, i:27))
dn5_1    | 2023-03-18 09:22:18,084 [grpc-default-executor-1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D: receive requestVote(PRE_VOTE, bae0cd8d-e50f-4d23-8974-b816860f8fc3, group-8191D6D2324D, 6, (t:6, i:38))
dn5_1    | 2023-03-18 09:22:18,089 [grpc-default-executor-0] INFO impl.VoteContext: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-CANDIDATE: accept PRE_VOTE from bae0cd8d-e50f-4d23-8974-b816860f8fc3: our priority 0 <= candidate's priority 1
dn5_1    | 2023-03-18 09:22:18,105 [grpc-default-executor-1] INFO impl.VoteContext: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-CANDIDATE: reject PRE_VOTE from bae0cd8d-e50f-4d23-8974-b816860f8fc3: our priority 1 > candidate's priority 0
dn5_1    | 2023-03-18 09:22:18,131 [grpc-default-executor-2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D: receive requestVote(PRE_VOTE, 7c9e7a7d-2593-458c-8a46-0313d77c3278, group-8191D6D2324D, 6, (t:6, i:38))
dn5_1    | 2023-03-18 09:22:18,134 [grpc-default-executor-1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D replies to PRE_VOTE vote request: bae0cd8d-e50f-4d23-8974-b816860f8fc3<-04b88c83-026f-4630-aada-108e37835bee#0:FAIL-t6. Peer's state: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D:t6, leader=null, voted=04b88c83-026f-4630-aada-108e37835bee, raftlog=Memoized:04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-SegmentedRaftLog:OPENED:c38, conf=25: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:18,215 [grpc-default-executor-2] INFO impl.VoteContext: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-CANDIDATE: reject PRE_VOTE from 7c9e7a7d-2593-458c-8a46-0313d77c3278: our priority 1 > candidate's priority 0
dn5_1    | 2023-03-18 09:22:18,216 [grpc-default-executor-2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D replies to PRE_VOTE vote request: 7c9e7a7d-2593-458c-8a46-0313d77c3278<-04b88c83-026f-4630-aada-108e37835bee#0:FAIL-t6. Peer's state: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D:t6, leader=null, voted=04b88c83-026f-4630-aada-108e37835bee, raftlog=Memoized:04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-SegmentedRaftLog:OPENED:c38, conf=25: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:18,229 [grpc-default-executor-0] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C replies to PRE_VOTE vote request: bae0cd8d-e50f-4d23-8974-b816860f8fc3<-04b88c83-026f-4630-aada-108e37835bee#0:OK-t5. Peer's state: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C:t5, leader=null, voted=bae0cd8d-e50f-4d23-8974-b816860f8fc3, raftlog=Memoized:04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-SegmentedRaftLog:OPENED:c27, conf=14: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:18,265 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO impl.LeaderElection: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn5_1    | 2023-03-18 09:22:18,268 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO impl.LeaderElection:   Response 0: 04b88c83-026f-4630-aada-108e37835bee<-bae0cd8d-e50f-4d23-8974-b816860f8fc3#0:OK-t6
dn5_1    | 2023-03-18 09:22:18,269 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO impl.LeaderElection: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2 PRE_VOTE round 0: result PASSED
dn5_1    | 2023-03-18 09:22:18,275 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO impl.LeaderElection: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2 ELECTION round 0: submit vote requests at term 7 for 25: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:18,321 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-18 09:22:18,322 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-18 09:22:18,332 [grpc-default-executor-2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C: receive requestVote(PRE_VOTE, 7c9e7a7d-2593-458c-8a46-0313d77c3278, group-80313208139C, 5, (t:5, i:27))
dn5_1    | 2023-03-18 09:22:18,342 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-LeaderElection1] INFO impl.LeaderElection: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-LeaderElection1: PRE_VOTE REJECTED received 1 response(s) and 0 exception(s):
dn5_1    | 2023-03-18 09:22:18,419 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-LeaderElection1] INFO impl.LeaderElection:   Response 0: 04b88c83-026f-4630-aada-108e37835bee<-bae0cd8d-e50f-4d23-8974-b816860f8fc3#0:FAIL-t5
dn5_1    | 2023-03-18 09:22:18,419 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-LeaderElection1] INFO impl.LeaderElection: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-LeaderElection1 PRE_VOTE round 0: result REJECTED
dn5_1    | 2023-03-18 09:22:18,419 [grpc-default-executor-2] INFO impl.VoteContext: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-CANDIDATE: accept PRE_VOTE from 7c9e7a7d-2593-458c-8a46-0313d77c3278: our priority 0 <= candidate's priority 0
dn5_1    | 2023-03-18 09:22:18,419 [grpc-default-executor-2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C replies to PRE_VOTE vote request: 7c9e7a7d-2593-458c-8a46-0313d77c3278<-04b88c83-026f-4630-aada-108e37835bee#0:OK-t5. Peer's state: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C:t5, leader=null, voted=bae0cd8d-e50f-4d23-8974-b816860f8fc3, raftlog=Memoized:04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-SegmentedRaftLog:OPENED:c27, conf=14: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:18,420 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-LeaderElection1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C: changes role from CANDIDATE to FOLLOWER at term 5 for REJECTED
dn5_1    | 2023-03-18 09:22:18,420 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-LeaderElection1] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: shutdown 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-LeaderElection1
dn5_1    | 2023-03-18 09:22:18,420 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-LeaderElection1] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: start 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-FollowerState
dn5_1    | 2023-03-18 09:22:18,488 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-18 09:22:18,540 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-18 09:22:18,522 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO impl.LeaderElection: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn5_1    | 2023-03-18 09:22:18,543 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO impl.LeaderElection:   Response 0: 04b88c83-026f-4630-aada-108e37835bee<-bae0cd8d-e50f-4d23-8974-b816860f8fc3#0:OK-t7
dn5_1    | 2023-03-18 09:22:18,544 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO impl.LeaderElection: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2 ELECTION round 0: result PASSED
dn5_1    | 2023-03-18 09:22:18,546 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: shutdown 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2
dn5_1    | 2023-03-18 09:22:18,546 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D: changes role from CANDIDATE to LEADER at term 7 for changeToLeader
dn5_1    | 2023-03-18 09:22:18,544 [grpc-default-executor-2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C: receive requestVote(ELECTION, bae0cd8d-e50f-4d23-8974-b816860f8fc3, group-80313208139C, 6, (t:5, i:27))
dn4_1    | 2023-03-18 09:23:51,540 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-03-18 09:23:51,540 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: new 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/66de196e-0a1c-4e0b-be4e-6f7919707118
dn4_1    | 2023-03-18 09:23:51,540 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-03-18 09:23:51,540 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-03-18 09:23:51,540 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-03-18 09:23:51,541 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-03-18 09:23:51,541 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-03-18 09:23:51,541 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
recon_1  | 2023-03-18 09:22:14,783 [IPC Server handler 13 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:15,671 [IPC Server handler 12 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:16,235 [IPC Server handler 2 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:18,533 [IPC Server handler 11 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:18,662 [IPC Server handler 12 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:38,077 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1  | 2023-03-18 09:22:38,081 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1  | 2023-03-18 09:22:38,115 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 7 pipelines in house.
recon_1  | 2023-03-18 09:22:38,128 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 40 milliseconds.
recon_1  | 2023-03-18 09:22:38,262 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 179 milliseconds to process 4 existing database records.
recon_1  | 2023-03-18 09:22:38,265 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 3 milliseconds for processing 4 containers.
recon_1  | 2023-03-18 09:22:42,109 [IPC Server handler 91 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:43,790 [IPC Server handler 14 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:44,741 [IPC Server handler 13 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:45,746 [IPC Server handler 14 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:45,749 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Container #2 has state OPEN, but given state is CLOSING.
recon_1  | 2023-03-18 09:22:45,796 [IPC Server handler 15 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:45,797 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Container #1001 has state OPEN, but given state is CLOSING.
recon_1  | 2023-03-18 09:22:45,812 [IPC Server handler 16 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:45,814 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Container #1 has state OPEN, but given state is CLOSING.
recon_1  | 2023-03-18 09:22:45,822 [IPC Server handler 17 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:45,825 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Container #1002 has state OPEN, but given state is CLOSING.
recon_1  | 2023-03-18 09:22:48,538 [IPC Server handler 11 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:48,597 [IPC Server handler 12 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:49,561 [IPC Server handler 12 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:49,597 [IPC Server handler 13 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:49,687 [IPC Server handler 14 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:49,740 [IPC Server handler 15 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:49,934 [IPC Server handler 72 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:49,935 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode bae0cd8d-e50f-4d23-8974-b816860f8fc3(ha_dn3_1.ha_net/10.9.0.17) reported CLOSED replica.
recon_1  | 2023-03-18 09:22:49,981 [IPC Server handler 70 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:49,995 [IPC Server handler 89 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:50,082 [IPC Server handler 91 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:50,151 [IPC Server handler 97 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:50,152 [FixedThreadPoolWithAffinityExecutor-8-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode 7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18) reported CLOSED replica.
recon_1  | 2023-03-18 09:22:50,186 [IPC Server handler 2 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
om3_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4))$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om3_1    | ************************************************************/
om3_1    | 2023-03-18 09:21:26,498 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1    | 2023-03-18 09:21:34,306 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1    | 2023-03-18 09:21:37,065 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om3_1    | 2023-03-18 09:21:37,361 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1    | 2023-03-18 09:21:37,363 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om3_1    | 2023-03-18 09:21:37,383 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-03-18 09:21:37,633 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = INITIAL_VERSION (version = 0), software layout = MULTITENANCY_SCHEMA (version = 3)
om3_1    | 2023-03-18 09:21:40,249 [main] INFO reflections.Reflections: Reflections took 2012 ms to scan 1 urls, producing 126 keys and 369 values [using 2 cores]
om3_1    | 2023-03-18 09:21:40,446 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-03-18 09:21:40,527 [main] WARN om.OzoneManager: ozone.default.bucket.layout configured to non-legacy bucket layout FILE_SYSTEM_OPTIMIZED when Ozone Manager is pre-finalized for bucket layout support. Legacy buckets will be created by default until Ozone Manager is finalized.
om3_1    | 2023-03-18 09:21:43,294 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863]
om3_1    | 2023-03-18 09:21:43,835 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863]
om3_1    | 2023-03-18 09:21:46,691 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9010d708b962/10.9.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-03-18 09:21:48,694 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9010d708b962/10.9.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-03-18 09:21:50,696 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9010d708b962/10.9.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-03-18 09:21:52,697 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9010d708b962/10.9.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-03-18 09:21:54,698 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9010d708b962/10.9.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-03-18 09:21:56,700 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9010d708b962/10.9.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-03-18 09:21:58,701 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9010d708b962/10.9.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-03-18 09:22:00,704 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9010d708b962/10.9.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-03-18 09:22:07,833 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om3_1    | 2023-03-18 09:22:07,968 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-03-18 09:22:09,620 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
dn5_1    | 2023-03-18 09:22:18,551 [grpc-default-executor-2] INFO impl.VoteContext: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-FOLLOWER: accept ELECTION from bae0cd8d-e50f-4d23-8974-b816860f8fc3: our priority 0 <= candidate's priority 1
dn5_1    | 2023-03-18 09:22:18,552 [grpc-default-executor-2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C: changes role from  FOLLOWER to FOLLOWER at term 6 for candidate:bae0cd8d-e50f-4d23-8974-b816860f8fc3
dn5_1    | 2023-03-18 09:22:18,552 [grpc-default-executor-2] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: shutdown 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-FollowerState
dn5_1    | 2023-03-18 09:22:18,552 [grpc-default-executor-2] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: start 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-FollowerState
dn5_1    | 2023-03-18 09:22:18,547 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8191D6D2324D with new leaderId: 04b88c83-026f-4630-aada-108e37835bee
dn5_1    | 2023-03-18 09:22:18,573 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D: change Leader from null to 04b88c83-026f-4630-aada-108e37835bee at term 7 for becomeLeader, leader elected after 23637ms
dn5_1    | 2023-03-18 09:22:18,572 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-FollowerState] INFO impl.FollowerState: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-FollowerState was interrupted
dn5_1    | 2023-03-18 09:22:18,594 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn5_1    | 2023-03-18 09:22:18,595 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-03-18 09:22:18,596 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn5_1    | 2023-03-18 09:22:18,597 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn5_1    | 2023-03-18 09:22:18,598 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 2023-03-18 09:22:18,598 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn5_1    | 2023-03-18 09:22:18,599 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-03-18 09:22:18,599 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn5_1    | 2023-03-18 09:22:18,625 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-18 09:22:18,626 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-18 09:22:18,630 [grpc-default-executor-2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C replies to ELECTION vote request: bae0cd8d-e50f-4d23-8974-b816860f8fc3<-04b88c83-026f-4630-aada-108e37835bee#0:OK-t6. Peer's state: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C:t6, leader=null, voted=bae0cd8d-e50f-4d23-8974-b816860f8fc3, raftlog=Memoized:04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-SegmentedRaftLog:OPENED:c27, conf=14: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:18,703 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn5_1    | 2023-03-18 09:22:18,703 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-18 09:22:18,706 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn5_1    | 2023-03-18 09:22:18,725 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn5_1    | 2023-03-18 09:22:18,729 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-03-18 09:22:18,730 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-03-18 09:22:18,731 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-03-18 09:22:18,731 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn5_1    | 2023-03-18 09:22:18,749 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn5_1    | 2023-03-18 09:22:18,749 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-18 09:22:18,749 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn5_1    | 2023-03-18 09:22:18,750 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn5_1    | 2023-03-18 09:22:18,756 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-03-18 09:22:18,757 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-03-18 09:22:18,757 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-03-18 09:22:18,758 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-03-18 09:22:50,344 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-03-18 09:22:50,344 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn3_1    | 2023-03-18 09:22:50,344 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn3_1    | 2023-03-18 09:22:50,344 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1001 to close, current state is: CLOSING
dn3_1    | 2023-03-18 09:22:50,344 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn3_1    | 2023-03-18 09:22:50,344 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-03-18 09:22:50,344 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-03-18 09:22:50,453 [ContainerOp-97be11a2-1597-458e-9379-8191d6d2324d-1] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 36.
dn3_1    | 2023-03-18 09:22:50,453 [ContainerOp-97be11a2-1597-458e-9379-8191d6d2324d-1] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 36.
dn3_1    | 2023-03-18 09:22:50,466 [ContainerOp-97be11a2-1597-458e-9379-8191d6d2324d-1] INFO keyvalue.KeyValueContainer: Container 1001 is closed with bcsId 36.
dn3_1    | 2023-03-18 09:23:10,915 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-03-18 09:23:21,284 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-03-18 09:23:21,284 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn3_1    | 2023-03-18 09:23:21,284 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn3_1    | 2023-03-18 09:23:21,284 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: ERASURE_CODED_STORAGE_SUPPORT.
dn3_1    | 2023-03-18 09:23:21,286 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature ERASURE_CODED_STORAGE_SUPPORT has been finalized.
dn3_1    | 2023-03-18 09:23:21,286 [Command processor thread] INFO upgrade.UpgradeFinalizer: Running finalization actions for layout feature: DATANODE_SCHEMA_V3
dn3_1    | 2023-03-18 09:23:21,286 [Command processor thread] INFO upgrade.DatanodeSchemaV3FinalizeAction: Upgrading Datanode volume layout for Schema V3 support.
dn3_1    | 2023-03-18 09:23:21,365 [Command processor thread] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-977c5c75-9b59-41eb-976d-229560b8bad8/container.db to cache
dn3_1    | 2023-03-18 09:23:21,365 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-977c5c75-9b59-41eb-976d-229560b8bad8/container.db for volume DS-977c5c75-9b59-41eb-976d-229560b8bad8
dn3_1    | 2023-03-18 09:23:21,367 [Command processor thread] INFO utils.DatanodeStoreCache: Removed db /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-977c5c75-9b59-41eb-976d-229560b8bad8/container.db from cache
dn3_1    | 2023-03-18 09:23:21,367 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is stopped at /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-977c5c75-9b59-41eb-976d-229560b8bad8/container.db for volume DS-977c5c75-9b59-41eb-976d-229560b8bad8
dn3_1    | 2023-03-18 09:23:21,393 [Command processor thread] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-977c5c75-9b59-41eb-976d-229560b8bad8/container.db to cache
dn3_1    | 2023-03-18 09:23:21,393 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-977c5c75-9b59-41eb-976d-229560b8bad8/container.db for volume DS-977c5c75-9b59-41eb-976d-229560b8bad8
dn3_1    | 2023-03-18 09:23:21,394 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature DATANODE_SCHEMA_V3 has been finalized.
dn3_1    | 2023-03-18 09:23:21,394 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn3_1    | 2023-03-18 09:23:21,394 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn3_1    | 2023-03-18 09:23:21,394 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-03-18 09:23:51,549 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-03-18 09:23:51,549 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-03-18 09:23:51,550 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-03-18 09:23:51,553 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-18 09:23:51,675 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-03-18 09:23:51,675 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-03-18 09:23:51,675 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-03-18 09:23:51,675 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-03-18 09:23:51,675 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-03-18 09:23:51,687 [pool-38-thread-1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118: start as a follower, conf=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:23:51,687 [pool-38-thread-1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn4_1    | 2023-03-18 09:23:51,688 [pool-38-thread-1] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: start 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-FollowerState
dn4_1    | 2023-03-18 09:23:51,693 [pool-38-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6F7919707118,id=7c9e7a7d-2593-458c-8a46-0313d77c3278
dn4_1    | 2023-03-18 09:23:51,694 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-03-18 09:23:51,694 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-03-18 09:23:51,694 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-03-18 09:23:51,694 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-03-18 09:23:51,695 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-18 09:23:51,701 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=66de196e-0a1c-4e0b-be4e-6f7919707118
dn4_1    | 2023-03-18 09:23:51,711 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-18 09:23:52,366 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=66de196e-0a1c-4e0b-be4e-6f7919707118.
dn4_1    | 2023-03-18 09:23:52,367 [Command processor thread] INFO server.RaftServer: 7c9e7a7d-2593-458c-8a46-0313d77c3278: addNew group-CCB5F87F8193:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] returns group-CCB5F87F8193:java.util.concurrent.CompletableFuture@4212b930[Not completed]
dn4_1    | 2023-03-18 09:23:52,369 [pool-38-thread-1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278: new RaftServerImpl for group-CCB5F87F8193:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn4_1    | 2023-03-18 09:23:52,370 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-03-18 09:23:52,370 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-03-18 09:23:52,370 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-03-18 09:23:52,371 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-03-18 09:23:52,371 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-03-18 09:23:52,371 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-03-18 09:23:52,371 [pool-38-thread-1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193: ConfigurationManager, init=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-03-18 09:23:52,371 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-03-18 09:23:52,372 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-03-18 09:23:52,372 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-03-18 09:23:52,372 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-03-18 09:23:52,375 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-03-18 09:23:52,376 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-03-18 09:23:52,376 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1    | 2023-03-18 09:21:59,504 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm_1    | 2023-03-18 09:21:59,620 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1    | 2023-03-18 09:21:59,663 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm_1    | 2023-03-18 09:21:59,918 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm_1    | 2023-03-18 09:21:59,957 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm_1    | 2023-03-18 09:21:59,967 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
scm_1    | WARNING: An illegal reflective access operation has occurred
scm_1    | WARNING: Illegal reflective access by org.apache.hadoop.hdds.utils.MetricsUtil (file:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar) to method java.lang.Class.annotationData()
scm_1    | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.hdds.utils.MetricsUtil
scm_1    | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
scm_1    | WARNING: All illegal access operations will be denied in a future release
scm_1    | 2023-03-18 09:22:00,025 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm_1    | 2023-03-18 09:22:00,034 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 2, healthy pipeline threshold count is 1
scm_1    | 2023-03-18 09:22:00,043 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
scm_1    | 2023-03-18 09:22:00,161 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
scm_1    | 2023-03-18 09:22:01,572 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1    | 2023-03-18 09:22:01,657 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1    | 2023-03-18 09:22:01,726 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1    | 2023-03-18 09:22:01,860 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1    | 2023-03-18 09:22:01,875 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1    | 2023-03-18 09:22:01,888 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1    | 2023-03-18 09:22:01,955 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1    | 2023-03-18 09:22:01,960 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1    | 2023-03-18 09:22:01,961 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1    | 2023-03-18 09:22:02,067 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm_1    | 2023-03-18 09:22:02,070 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm_1    | Container Balancer status:
scm_1    | Key                            Value
scm_1    | Running                        true
scm_1    | Container Balancer Configuration values:
scm_1    | Key                                                Value
scm_1    | Threshold                                          10
scm_1    | Max Datanodes to Involve per Iteration(percent)    20
scm_1    | Max Size to Move per Iteration                     500GB
scm_1    | Max Size Entering Target per Iteration             26GB
scm_1    | Max Size Leaving Source per Iteration              26GB
scm_1    | 
scm_1    | 2023-03-18 09:22:02,070 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm_1    | 2023-03-18 09:22:02,088 [Listener at 0.0.0.0/9860] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
scm_1    | 2023-03-18 09:22:02,094 [Listener at 0.0.0.0/9860] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
scm_1    | 2023-03-18 09:22:02,094 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1    | 2023-03-18 09:22:02,265 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1    | 2023-03-18 09:22:02,313 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1    | 2023-03-18 09:22:02,315 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1    | 2023-03-18 09:22:02,865 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1    | 2023-03-18 09:22:02,873 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1    | 2023-03-18 09:22:02,962 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1    | 2023-03-18 09:22:03,139 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1    | 2023-03-18 09:22:03,142 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1    | 2023-03-18 09:22:03,142 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1    | 2023-03-18 09:22:03,146 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1    | 2023-03-18 09:22:03,181 [Listener at 0.0.0.0/9860] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1    | 2023-03-18 09:22:03,229 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1    | 2023-03-18 09:22:03,229 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1    | 2023-03-18 09:22:03,311 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@4e0cc334] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm_1    | 2023-03-18 09:22:03,314 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1    | 2023-03-18 09:22:03,314 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1    | 2023-03-18 09:22:03,443 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @47986ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1    | 2023-03-18 09:22:04,939 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm_1    | 2023-03-18 09:22:05,002 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1    | 2023-03-18 09:22:05,125 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn5_1    | 2023-03-18 09:22:18,768 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: start 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderStateImpl
dn5_1    | 2023-03-18 09:22:18,772 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-SegmentedRaftLogWorker: Rolling segment log-25_38 to index:38
dn5_1    | 2023-03-18 09:22:18,809 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderElection2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D: set configuration 39: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:18,817 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/current/log_inprogress_25 to /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/current/log_25-38
dn5_1    | 2023-03-18 09:22:18,819 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/current/log_inprogress_39
dn5_1    | 2023-03-18 09:22:18,996 [04b88c83-026f-4630-aada-108e37835bee-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-80313208139C with new leaderId: bae0cd8d-e50f-4d23-8974-b816860f8fc3
dn5_1    | 2023-03-18 09:22:19,054 [04b88c83-026f-4630-aada-108e37835bee-server-thread1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C: change Leader from null to bae0cd8d-e50f-4d23-8974-b816860f8fc3 at term 6 for appendEntries, leader elected after 25065ms
dn5_1    | 2023-03-18 09:22:19,067 [04b88c83-026f-4630-aada-108e37835bee-server-thread1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C: set configuration 28: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:22:19,141 [04b88c83-026f-4630-aada-108e37835bee-server-thread1] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-SegmentedRaftLogWorker: Rolling segment log-14_27 to index:27
dn5_1    | 2023-03-18 09:22:19,178 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/current/log_inprogress_14 to /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/current/log_14-27
dn5_1    | 2023-03-18 09:22:19,186 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/current/log_inprogress_28
dn5_1    | 2023-03-18 09:22:49,923 [ContainerOp-97be11a2-1597-458e-9379-8191d6d2324d-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 6.
dn5_1    | 2023-03-18 09:22:49,925 [ContainerOp-97be11a2-1597-458e-9379-8191d6d2324d-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 6.
dn5_1    | 2023-03-18 09:22:49,958 [ContainerOp-97be11a2-1597-458e-9379-8191d6d2324d-0] INFO keyvalue.KeyValueContainer: Container 2 is closed with bcsId 6.
dn5_1    | 2023-03-18 09:22:50,258 [ContainerOp-e319c2ba-524b-42f7-8b03-80313208139c-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 2.
dn5_1    | 2023-03-18 09:22:50,269 [ContainerOp-e319c2ba-524b-42f7-8b03-80313208139c-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 2.
dn5_1    | 2023-03-18 09:22:50,314 [ContainerOp-e319c2ba-524b-42f7-8b03-80313208139c-0] INFO keyvalue.KeyValueContainer: Container 1 is closed with bcsId 2.
dn5_1    | 2023-03-18 09:22:50,374 [ContainerOp-e319c2ba-524b-42f7-8b03-80313208139c-1] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 26.
dn5_1    | 2023-03-18 09:22:50,375 [ContainerOp-e319c2ba-524b-42f7-8b03-80313208139c-1] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 26.
dn5_1    | 2023-03-18 09:22:50,385 [ContainerOp-e319c2ba-524b-42f7-8b03-80313208139c-1] INFO keyvalue.KeyValueContainer: Container 1002 is closed with bcsId 26.
dn5_1    | 2023-03-18 09:22:50,448 [ContainerOp-97be11a2-1597-458e-9379-8191d6d2324d-1] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 36.
dn5_1    | 2023-03-18 09:22:50,450 [ContainerOp-97be11a2-1597-458e-9379-8191d6d2324d-1] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 36.
dn5_1    | 2023-03-18 09:22:50,456 [ContainerOp-97be11a2-1597-458e-9379-8191d6d2324d-1] INFO keyvalue.KeyValueContainer: Container 1001 is closed with bcsId 36.
dn5_1    | 2023-03-18 09:22:50,467 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    | 2023-03-18 09:22:50,468 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn5_1    | 2023-03-18 09:22:50,469 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn5_1    | 2023-03-18 09:22:50,469 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: ERASURE_CODED_STORAGE_SUPPORT.
dn5_1    | 2023-03-18 09:22:50,471 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature ERASURE_CODED_STORAGE_SUPPORT has been finalized.
dn5_1    | 2023-03-18 09:22:50,471 [Command processor thread] INFO upgrade.UpgradeFinalizer: Running finalization actions for layout feature: DATANODE_SCHEMA_V3
dn5_1    | 2023-03-18 09:22:50,471 [Command processor thread] INFO upgrade.DatanodeSchemaV3FinalizeAction: Upgrading Datanode volume layout for Schema V3 support.
dn5_1    | 2023-03-18 09:22:50,573 [Command processor thread] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-65290497-0571-43ad-9820-2f27fd316df2/container.db to cache
dn5_1    | 2023-03-18 09:22:50,574 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-65290497-0571-43ad-9820-2f27fd316df2/container.db for volume DS-65290497-0571-43ad-9820-2f27fd316df2
dn5_1    | 2023-03-18 09:22:50,578 [Command processor thread] INFO utils.DatanodeStoreCache: Removed db /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-65290497-0571-43ad-9820-2f27fd316df2/container.db from cache
dn5_1    | 2023-03-18 09:22:50,578 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is stopped at /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-65290497-0571-43ad-9820-2f27fd316df2/container.db for volume DS-65290497-0571-43ad-9820-2f27fd316df2
om3_1    | 2023-03-18 09:22:09,621 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om3_1    | 2023-03-18 09:22:10,555 [Thread-15] INFO rocksdiff.RocksDBCheckpointDiffer: Skipped the compaction entry. Compaction input files: [/data/metadata/om.db/000081.sst, /data/metadata/om.db/000074.sst, /data/metadata/om.db/000063.sst, /data/metadata/om.db/000041.sst] and output files: [/data/metadata/om.db/000081.sst, /data/metadata/om.db/000074.sst, /data/metadata/om.db/000063.sst, /data/metadata/om.db/000041.sst] are same.
om3_1    | 2023-03-18 09:22:11,335 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om3_1    | 2023-03-18 09:22:11,508 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-03-18 09:22:11,856 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1    | 2023-03-18 09:22:11,859 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om3_1    | 2023-03-18 09:22:11,887 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om3_1    | 2023-03-18 09:22:12,142 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
om3_1    | 2023-03-18 09:22:12,156 [Finalizer] WARN managed.ManagedRocksObjectUtils: ManagedColumnFamilyOptions is not closed properly
om3_1    | 2023-03-18 09:22:12,157 [Finalizer] WARN managed.ManagedRocksObjectUtils: ManagedColumnFamilyOptions is not closed properly
om3_1    | 2023-03-18 09:22:12,157 [Finalizer] WARN managed.ManagedRocksObjectUtils: ManagedColumnFamilyOptions is not closed properly
om3_1    | 2023-03-18 09:22:12,157 [Finalizer] WARN managed.ManagedRocksObjectUtils: ManagedColumnFamilyOptions is not closed properly
om3_1    | 2023-03-18 09:22:12,703 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om3_1    | 2023-03-18 09:22:12,750 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1    | 2023-03-18 09:22:12,926 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om3:9872, om1:9872, om2:9872
om3_1    | 2023-03-18 09:22:12,970 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:7, i:104)
om3_1    | 2023-03-18 09:22:13,167 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om3_1    | 2023-03-18 09:22:13,417 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om3_1    | 2023-03-18 09:22:13,425 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om3_1    | 2023-03-18 09:22:13,432 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om3_1    | 2023-03-18 09:22:13,433 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om3_1    | 2023-03-18 09:22:13,433 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om3_1    | 2023-03-18 09:22:13,433 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om3_1    | 2023-03-18 09:22:13,434 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om3_1    | 2023-03-18 09:22:13,435 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-03-18 09:22:13,436 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om3_1    | 2023-03-18 09:22:13,436 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1    | 2023-03-18 09:22:13,461 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om3_1    | 2023-03-18 09:22:13,467 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om3_1    | 2023-03-18 09:22:13,477 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om3_1    | 2023-03-18 09:22:14,253 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om3_1    | 2023-03-18 09:22:14,260 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om3_1    | 2023-03-18 09:22:14,263 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om3_1    | 2023-03-18 09:22:14,263 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1    | 2023-03-18 09:22:14,263 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1    | 2023-03-18 09:22:14,273 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1    | 2023-03-18 09:22:14,295 [om3-impl-thread1] INFO server.RaftServer: om3: found a subdirectory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om3_1    | 2023-03-18 09:22:14,316 [main] INFO server.RaftServer: om3: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@7d9c45ee[Not completed]
om3_1    | 2023-03-18 09:22:14,316 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om3_1    | 2023-03-18 09:22:14,319 [main] INFO om.OzoneManager: Creating RPC Server
om3_1    | 2023-03-18 09:22:14,404 [pool-29-thread-1] INFO server.RaftServer$Division: om3: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om3_1    | 2023-03-18 09:22:14,410 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om3_1    | 2023-03-18 09:22:14,415 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om3_1    | 2023-03-18 09:22:14,415 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om3_1    | 2023-03-18 09:22:14,415 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1    | 2023-03-18 09:22:14,415 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1    | 2023-03-18 09:22:14,415 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om3_1    | 2023-03-18 09:22:14,451 [pool-29-thread-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om3_1    | 2023-03-18 09:22:14,454 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1    | 2023-03-18 09:22:14,491 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om3_1    | 2023-03-18 09:22:14,494 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om3_1    | 2023-03-18 09:22:14,585 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om3_1    | 2023-03-18 09:22:14,623 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
recon_1  | 2023-03-18 09:22:50,248 [IPC Server handler 5 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:50,292 [IPC Server handler 1 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:50,293 [IPC Server handler 6 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:50,294 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1002 to CLOSED state, datanode bae0cd8d-e50f-4d23-8974-b816860f8fc3(ha_dn3_1.ha_net/10.9.0.17) reported CLOSED replica.
recon_1  | 2023-03-18 09:22:50,332 [IPC Server handler 9 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:50,356 [IPC Server handler 3 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:50,402 [IPC Server handler 7 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:50,425 [IPC Server handler 0 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:50,428 [FixedThreadPoolWithAffinityExecutor-8-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to CLOSED state, datanode 7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18) reported CLOSED replica.
recon_1  | 2023-03-18 09:22:50,476 [IPC Server handler 4 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:22:50,491 [IPC Server handler 10 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:23:06,537 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1  | 2023-03-18 09:23:06,540 [pool-27-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-03-18 09:23:06,540 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-03-18 09:23:06,540 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-03-18 09:23:06,540 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-03-18 09:23:06,540 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-03-18 09:23:06,540 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-03-18 09:23:06,541 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1  | 2023-03-18 09:23:06,541 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 110 
recon_1  | 2023-03-18 09:23:06,578 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om2 is not the leader. Suggested leader is OM:om3[om3/10.9.0.13].
recon_1  | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:245)
recon_1  | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:232)
recon_1  | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:225)
recon_1  | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:177)
recon_1  | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
recon_1  | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
recon_1  | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1  | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | , while invoking $Proxy42.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 1 failover attempts. Trying to failover immediately.
recon_1  | 2023-03-18 09:23:06,686 [pool-27-thread-1] WARN impl.OzoneManagerServiceProviderImpl: Unable to get and apply delta updates from OM.
recon_1  | INTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: Invalid transaction log iterator when getting updates since sequence number 110
recon_1  | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:709)
recon_1  | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getDBUpdates(OzoneManagerProtocolClientSideTranslatorPB.java:1962)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:443)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm_1    | 2023-03-18 09:22:05,168 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1    | 2023-03-18 09:22:05,168 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1    | 2023-03-18 09:22:05,181 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1    | 2023-03-18 09:22:05,758 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm_1    | 2023-03-18 09:22:05,772 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm_1    | 2023-03-18 09:22:05,783 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm_1    | 2023-03-18 09:22:06,441 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1    | 2023-03-18 09:22:06,441 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm_1    | 2023-03-18 09:22:06,497 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm_1    | 2023-03-18 09:22:06,760 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7742a45c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1    | 2023-03-18 09:22:06,768 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3b41e1bf{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm_1    | 2023-03-18 09:22:07,298 [IPC Server handler 99 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/4ba198f1-a013-49ce-b059-f73788d69001
scm_1    | 2023-03-18 09:22:07,393 [IPC Server handler 99 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 4ba198f1-a013-49ce-b059-f73788d69001{ip: 10.9.0.15, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm_1    | 2023-03-18 09:22:07,739 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1    | 2023-03-18 09:22:07,739 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1    | 2023-03-18 09:22:07,774 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm_1    | 2023-03-18 09:22:07,860 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1    | 2023-03-18 09:22:07,928 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-03-18 09:22:09,198 [IPC Server handler 62 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/2b374f74-8596-4317-81ba-ce0a491ddd96
scm_1    | 2023-03-18 09:22:09,234 [IPC Server handler 62 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 2b374f74-8596-4317-81ba-ce0a491ddd96{ip: 10.9.0.16, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm_1    | 2023-03-18 09:22:09,234 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1    | 2023-03-18 09:22:09,235 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1    | 2023-03-18 09:22:09,239 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-03-18 09:22:09,239 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm_1    | 2023-03-18 09:22:09,270 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@69a40b3c{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-11285403946347137574/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm_1    | 2023-03-18 09:22:09,340 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm_1    | 2023-03-18 09:22:09,340 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-03-18 09:22:09,432 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@5b733ef7{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1    | 2023-03-18 09:22:09,432 [Listener at 0.0.0.0/9860] INFO server.Server: Started @53975ms
scm_1    | 2023-03-18 09:22:09,446 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1    | 2023-03-18 09:22:09,446 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1    | 2023-03-18 09:22:09,468 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1    | 2023-03-18 09:22:11,260 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm_1    | 2023-03-18 09:22:11,260 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-03-18 09:22:11,534 [IPC Server handler 99 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/bae0cd8d-e50f-4d23-8974-b816860f8fc3
scm_1    | 2023-03-18 09:22:11,537 [IPC Server handler 99 on default port 9861] INFO node.SCMNodeManager: Registered Data node : bae0cd8d-e50f-4d23-8974-b816860f8fc3{ip: 10.9.0.17, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm_1    | 2023-03-18 09:22:11,547 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1    | 2023-03-18 09:22:11,602 [IPC Server handler 4 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/7c9e7a7d-2593-458c-8a46-0313d77c3278
dn5_1    | 2023-03-18 09:22:50,598 [Command processor thread] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-65290497-0571-43ad-9820-2f27fd316df2/container.db to cache
dn5_1    | 2023-03-18 09:22:50,598 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/DS-65290497-0571-43ad-9820-2f27fd316df2/container.db for volume DS-65290497-0571-43ad-9820-2f27fd316df2
dn5_1    | 2023-03-18 09:22:50,622 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature DATANODE_SCHEMA_V3 has been finalized.
dn5_1    | 2023-03-18 09:22:50,623 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn5_1    | 2023-03-18 09:22:50,623 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn5_1    | 2023-03-18 09:22:50,623 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    | 2023-03-18 09:22:50,623 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    | 2023-03-18 09:22:50,623 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    | 2023-03-18 09:22:50,623 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    | 2023-03-18 09:22:50,623 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    | 2023-03-18 09:22:50,623 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    | 2023-03-18 09:22:50,624 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    | 2023-03-18 09:22:50,624 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    | 2023-03-18 09:23:11,156 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 2023-03-18 09:23:21,453 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: 04b88c83-026f-4630-aada-108e37835bee: Completed APPEND_ENTRIES, lastRequest: bae0cd8d-e50f-4d23-8974-b816860f8fc3->04b88c83-026f-4630-aada-108e37835bee#42-t6,previous=(t:6, i:32),leaderCommit=32,initializing? true,entries: size=1, first=(t:6, i:33), METADATAENTRY(c:32)
dn5_1    | 2023-03-18 09:23:21,456 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: 04b88c83-026f-4630-aada-108e37835bee: Completed APPEND_ENTRIES, lastRequest: null
dn5_1    | 2023-03-18 09:23:21,465 [Command processor thread] INFO server.RaftServer: 04b88c83-026f-4630-aada-108e37835bee: remove    LEADER 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D:t7, leader=04b88c83-026f-4630-aada-108e37835bee, voted=04b88c83-026f-4630-aada-108e37835bee, raftlog=Memoized:04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-SegmentedRaftLog:OPENED:c44, conf=39: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn5_1    | 2023-03-18 09:23:21,467 [Command processor thread] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D: shutdown
dn5_1    | 2023-03-18 09:23:21,467 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-8191D6D2324D,id=04b88c83-026f-4630-aada-108e37835bee
dn5_1    | 2023-03-18 09:23:21,467 [Command processor thread] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: shutdown 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-LeaderStateImpl
dn5_1    | 2023-03-18 09:23:21,468 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D->bae0cd8d-e50f-4d23-8974-b816860f8fc3-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D->bae0cd8d-e50f-4d23-8974-b816860f8fc3-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
dn5_1    | 2023-03-18 09:23:21,468 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D->7c9e7a7d-2593-458c-8a46-0313d77c3278-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D->7c9e7a7d-2593-458c-8a46-0313d77c3278-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
dn5_1    | 2023-03-18 09:23:21,470 [Command processor thread] INFO impl.PendingRequests: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-PendingRequests: sendNotLeaderResponses
dn5_1    | 2023-03-18 09:23:21,476 [grpc-default-executor-2] INFO server.GrpcLogAppender: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D->7c9e7a7d-2593-458c-8a46-0313d77c3278-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn5_1    | 2023-03-18 09:23:21,479 [grpc-default-executor-2] INFO leader.FollowerInfo: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D->7c9e7a7d-2593-458c-8a46-0313d77c3278: nextIndex: updateUnconditionally 45 -> 44
dn5_1    | 2023-03-18 09:23:21,480 [grpc-default-executor-3] INFO server.GrpcLogAppender: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D->7c9e7a7d-2593-458c-8a46-0313d77c3278-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn5_1    | 2023-03-18 09:23:21,480 [grpc-default-executor-3] INFO leader.FollowerInfo: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D->7c9e7a7d-2593-458c-8a46-0313d77c3278: nextIndex: updateUnconditionally 44 -> 43
dn5_1    | 2023-03-18 09:23:21,483 [grpc-default-executor-3] INFO server.GrpcLogAppender: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D->bae0cd8d-e50f-4d23-8974-b816860f8fc3-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn5_1    | 2023-03-18 09:23:21,483 [grpc-default-executor-3] INFO leader.FollowerInfo: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D->bae0cd8d-e50f-4d23-8974-b816860f8fc3: nextIndex: updateUnconditionally 45 -> 44
dn5_1    | 2023-03-18 09:23:21,483 [Command processor thread] INFO impl.StateMachineUpdater: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-StateMachineUpdater: set stopIndex = 44
dn5_1    | 2023-03-18 09:23:21,486 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-8191D6D2324D: Taking a snapshot at:(t:7, i:44) file /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/sm/snapshot.7_44
dn5_1    | 2023-03-18 09:23:21,495 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-8191D6D2324D: Finished taking a snapshot at:(t:7, i:44) file:/data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/sm/snapshot.7_44 took: 11 ms
dn5_1    | 2023-03-18 09:23:21,499 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-StateMachineUpdater] INFO impl.StateMachineUpdater: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-StateMachineUpdater: Took a snapshot at index 44
dn5_1    | 2023-03-18 09:23:21,500 [grpc-default-executor-3] INFO server.GrpcLogAppender: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D->bae0cd8d-e50f-4d23-8974-b816860f8fc3-AppendLogResponseHandler: follower responses appendEntries COMPLETED
recon_1  | 2023-03-18 09:23:06,687 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1  | 2023-03-18 09:23:06,886 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1679131386687
recon_1  | 2023-03-18 09:23:06,887 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Cleaning up old OM snapshot db at /data/metadata/om.snapshot.db_1679131228312.
recon_1  | 2023-03-18 09:23:06,895 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-03-18 09:23:06,895 [pool-27-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-03-18 09:23:06,931 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1679131386687.
recon_1  | 2023-03-18 09:23:06,936 [pool-27-thread-1] ERROR impl.OzoneManagerServiceProviderImpl: Unable to update Recon's metadata with new OM DB. 
recon_1  | org.apache.hadoop.metrics2.MetricsException: Metrics source userTableCache already exists!
recon_1  | 	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:152)
recon_1  | 	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:125)
recon_1  | 	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:229)
recon_1  | 	at org.apache.hadoop.hdds.utils.TableCacheMetrics.create(TableCacheMetrics.java:67)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.createCacheMetrics(TypedTable.java:333)
recon_1  | 	at org.apache.hadoop.ozone.om.OmMetadataManagerImpl.checkTableStatus(OmMetadataManagerImpl.java:430)
recon_1  | 	at org.apache.hadoop.ozone.om.OmMetadataManagerImpl.initializeOmTables(OmMetadataManagerImpl.java:559)
recon_1  | 	at org.apache.hadoop.ozone.recon.recovery.ReconOmMetadataManagerImpl.initializeNewRdbStore(ReconOmMetadataManagerImpl.java:103)
recon_1  | 	at org.apache.hadoop.ozone.recon.recovery.ReconOmMetadataManagerImpl.updateOmDB(ReconOmMetadataManagerImpl.java:120)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.updateReconOmDBWithNewSnapshot(OzoneManagerServiceProviderImpl.java:384)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:519)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:23:07,814 [pool-50-thread-1] INFO scm.ReconStorageContainerManagerFacade: Got list of containers from SCM : 4
recon_1  | 2023-03-18 09:23:12,104 [IPC Server handler 93 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:23:13,796 [IPC Server handler 16 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:23:20,495 [IPC Server handler 11 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-18 09:23:51,448 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=3538aed8-d817-47ae-96b3-bde6dabfbcfe. Trying to get from SCM.
recon_1  | 2023-03-18 09:23:51,467 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 3538aed8-d817-47ae-96b3-bde6dabfbcfe, Nodes: 7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:7c9e7a7d-2593-458c-8a46-0313d77c3278, CreationTimestamp2023-03-18T09:23:22.610Z[UTC]] to Recon pipeline metadata.
recon_1  | 2023-03-18 09:23:51,470 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 3538aed8-d817-47ae-96b3-bde6dabfbcfe, Nodes: 7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:7c9e7a7d-2593-458c-8a46-0313d77c3278, CreationTimestamp2023-03-18T09:23:22.610Z[UTC]].
recon_1  | 2023-03-18 09:23:51,532 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=66de196e-0a1c-4e0b-be4e-6f7919707118. Trying to get from SCM.
recon_1  | 2023-03-18 09:23:51,539 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 66de196e-0a1c-4e0b-be4e-6f7919707118, Nodes: 7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18)04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19)4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-18T09:23:43.610Z[UTC]] to Recon pipeline metadata.
recon_1  | 2023-03-18 09:23:51,545 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 66de196e-0a1c-4e0b-be4e-6f7919707118, Nodes: 7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18)04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19)4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-18T09:23:43.610Z[UTC]].
recon_1  | 2023-03-18 09:23:51,549 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=66de196e-0a1c-4e0b-be4e-6f7919707118 reported by 7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18)
recon_1  | 2023-03-18 09:23:51,549 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=087cc0b6-0f7f-40d8-82ec-5da9883170e4. Trying to get from SCM.
recon_1  | 2023-03-18 09:23:51,555 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 087cc0b6-0f7f-40d8-82ec-5da9883170e4, Nodes: 04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:04b88c83-026f-4630-aada-108e37835bee, CreationTimestamp2023-03-18T09:23:22.616Z[UTC]] to Recon pipeline metadata.
recon_1  | 2023-03-18 09:23:51,556 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 087cc0b6-0f7f-40d8-82ec-5da9883170e4, Nodes: 04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:04b88c83-026f-4630-aada-108e37835bee, CreationTimestamp2023-03-18T09:23:22.616Z[UTC]].
recon_1  | 2023-03-18 09:23:51,611 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=66de196e-0a1c-4e0b-be4e-6f7919707118 reported by 04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19)
scm_1    | 2023-03-18 09:22:11,606 [IPC Server handler 4 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 7c9e7a7d-2593-458c-8a46-0313d77c3278{ip: 10.9.0.18, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm_1    | 2023-03-18 09:22:11,606 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1    | 2023-03-18 09:22:11,614 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-03-18 09:22:11,614 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1    | 2023-03-18 09:22:11,618 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 2, required at least one datanode reported per pipeline count is 2
scm_1    | 2023-03-18 09:22:11,633 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-03-18 09:22:11,647 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1    | 2023-03-18 09:22:11,647 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1    | 2023-03-18 09:22:11,647 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm_1    | 2023-03-18 09:22:11,667 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1    | 2023-03-18 09:22:11,667 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1    | 2023-03-18 09:22:11,673 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
scm_1    | 2023-03-18 09:22:11,774 [IPC Server handler 3 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/04b88c83-026f-4630-aada-108e37835bee
scm_1    | 2023-03-18 09:22:11,775 [IPC Server handler 3 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 04b88c83-026f-4630-aada-108e37835bee{ip: 10.9.0.19, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm_1    | 2023-03-18 09:22:11,779 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1    | 2023-03-18 09:22:11,781 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
scm_1    | 2023-03-18 09:22:11,784 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1    | 2023-03-18 09:22:11,784 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1    | 2023-03-18 09:22:11,787 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1    | 2023-03-18 09:22:11,789 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1    | 2023-03-18 09:22:11,789 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1    | 2023-03-18 09:22:11,789 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm_1    | 2023-03-18 09:22:11,789 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm_1    | 2023-03-18 09:22:11,794 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm_1    | 2023-03-18 09:22:11,804 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm_1    | 2023-03-18 09:22:11,815 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm_1    | 2023-03-18 09:22:11,815 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1    | 2023-03-18 09:22:11,821 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1    | 2023-03-18 09:22:11,822 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=false, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1    | 2023-03-18 09:22:11,823 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm_1    | 2023-03-18 09:22:36,302 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer: Finalization started.
scm_1    | 2023-03-18 09:22:36,304 [IPC Server handler 1 on default port 9860] INFO pipeline.BackgroundPipelineCreator: Stopping RatisPipelineUtilsThread.
scm_1    | 2023-03-18 09:22:36,304 [RatisPipelineUtilsThread - 0] WARN pipeline.BackgroundPipelineCreator: RatisPipelineUtilsThread is interrupted.
scm_1    | 2023-03-18 09:22:36,305 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer: SCM Finalization has crossed checkpoint FINALIZATION_STARTED
scm_1    | 2023-03-18 09:22:36,307 [IPC Server handler 1 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: a2f44939-53c6-4b4c-afd2-40b9ce60bd6c, Nodes: 7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:7c9e7a7d-2593-458c-8a46-0313d77c3278, CreationTimestamp2023-03-18T09:21:59.336504Z[UTC]] moved to CLOSED state
scm_1    | 2023-03-18 09:22:36,315 [IPC Server handler 1 on default port 9860] INFO pipeline.PipelineManagerImpl: Container #2 closed for pipeline=PipelineID=97be11a2-1597-458e-9379-8191d6d2324d
scm_1    | 2023-03-18 09:22:36,315 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #2, current state: CLOSING
scm_1    | 2023-03-18 09:22:36,316 [IPC Server handler 1 on default port 9860] INFO pipeline.PipelineManagerImpl: Container #1001 closed for pipeline=PipelineID=97be11a2-1597-458e-9379-8191d6d2324d
om3_1    | 2023-03-18 09:22:14,627 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om3_1    | 2023-03-18 09:22:15,521 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1    | 2023-03-18 09:22:15,633 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om3_1    | 2023-03-18 09:22:15,639 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om3_1    | 2023-03-18 09:22:15,660 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om3_1    | 2023-03-18 09:22:15,668 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om3_1    | 2023-03-18 09:22:17,697 [main] INFO reflections.Reflections: Reflections took 3261 ms to scan 8 urls, producing 23 keys and 586 values [using 2 cores]
om3_1    | 2023-03-18 09:22:18,575 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1    | 2023-03-18 09:22:18,657 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om3_1    | 2023-03-18 09:22:19,325 [Listener at om3/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om3_1    | 2023-03-18 09:22:19,418 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om3_1    | 2023-03-18 09:22:19,418 [Listener at om3/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om3_1    | 2023-03-18 09:22:19,645 [Listener at om3/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om3/10.9.0.13:9862
om3_1    | 2023-03-18 09:22:19,646 [Listener at om3/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om3 at port 9872
om3_1    | 2023-03-18 09:22:19,657 [om3-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 6@9010d708b962
om3_1    | 2023-03-18 09:22:19,662 [om3-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=7, votedFor=om3} from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/raft-meta
om3_1    | 2023-03-18 09:22:19,724 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-03-18 09:22:19,725 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om3_1    | 2023-03-18 09:22:19,798 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om3_1    | 2023-03-18 09:22:19,808 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-03-18 09:22:19,816 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om3_1    | 2023-03-18 09:22:19,818 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om3_1    | 2023-03-18 09:22:19,836 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1    | 2023-03-18 09:22:19,856 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om3_1    | 2023-03-18 09:22:19,860 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om3_1    | 2023-03-18 09:22:19,877 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om3@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om3_1    | 2023-03-18 09:22:19,884 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om3_1    | 2023-03-18 09:22:19,884 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om3_1    | 2023-03-18 09:22:19,885 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1    | 2023-03-18 09:22:19,887 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om3_1    | 2023-03-18 09:22:19,888 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om3_1    | 2023-03-18 09:22:19,891 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om3_1    | 2023-03-18 09:22:19,891 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om3_1    | 2023-03-18 09:22:19,892 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om3_1    | 2023-03-18 09:22:19,947 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om3_1    | 2023-03-18 09:22:19,947 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-03-18 09:22:19,989 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om3_1    | 2023-03-18 09:22:19,998 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om3_1    | 2023-03-18 09:22:19,999 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om3_1    | 2023-03-18 09:22:20,276 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-03-18 09:22:20,322 [om3-impl-thread1] INFO segmented.LogSegment: Successfully read 21 entries from segment file /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_0-20
om3_1    | 2023-03-18 09:22:20,338 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 21: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-03-18 09:22:20,379 [om3-impl-thread1] INFO segmented.LogSegment: Successfully read 48 entries from segment file /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_21-68
om3_1    | 2023-03-18 09:22:20,384 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-03-18 09:22:20,399 [om3-impl-thread1] INFO segmented.LogSegment: Successfully read 36 entries from segment file /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_69
recon_1  | 2023-03-18 09:23:52,308 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=66de196e-0a1c-4e0b-be4e-6f7919707118 reported by 4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15)
recon_1  | 2023-03-18 09:23:52,398 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=84a08f9d-e905-447e-be3d-ccb5f87f8193. Trying to get from SCM.
recon_1  | 2023-03-18 09:23:52,406 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 84a08f9d-e905-447e-be3d-ccb5f87f8193, Nodes: 04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19)7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18)4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-18T09:23:43.612Z[UTC]] to Recon pipeline metadata.
recon_1  | 2023-03-18 09:23:52,408 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 84a08f9d-e905-447e-be3d-ccb5f87f8193, Nodes: 04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19)7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18)4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-18T09:23:43.612Z[UTC]].
recon_1  | 2023-03-18 09:23:52,408 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=84a08f9d-e905-447e-be3d-ccb5f87f8193 reported by 7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18)
recon_1  | 2023-03-18 09:23:52,408 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=66de196e-0a1c-4e0b-be4e-6f7919707118 reported by 7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18)
recon_1  | 2023-03-18 09:23:52,895 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=84a08f9d-e905-447e-be3d-ccb5f87f8193 reported by 04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19)
recon_1  | 2023-03-18 09:23:52,895 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=66de196e-0a1c-4e0b-be4e-6f7919707118 reported by 04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19)
recon_1  | 2023-03-18 09:23:53,019 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=84a08f9d-e905-447e-be3d-ccb5f87f8193 reported by 4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15)
recon_1  | 2023-03-18 09:23:53,019 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=66de196e-0a1c-4e0b-be4e-6f7919707118 reported by 4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15)
recon_1  | 2023-03-18 09:23:56,610 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=84a08f9d-e905-447e-be3d-ccb5f87f8193 reported by 7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18)
recon_1  | 2023-03-18 09:23:56,610 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=66de196e-0a1c-4e0b-be4e-6f7919707118 reported by 7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18)
recon_1  | 2023-03-18 09:23:56,694 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=84a08f9d-e905-447e-be3d-ccb5f87f8193 reported by 04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19)
recon_1  | 2023-03-18 09:23:56,695 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=66de196e-0a1c-4e0b-be4e-6f7919707118 reported by 04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19)
recon_1  | 2023-03-18 09:23:57,726 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=84a08f9d-e905-447e-be3d-ccb5f87f8193 reported by 4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15)
recon_1  | 2023-03-18 09:23:57,726 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=66de196e-0a1c-4e0b-be4e-6f7919707118 reported by 4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15)
recon_1  | 2023-03-18 09:23:57,726 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 66de196e-0a1c-4e0b-be4e-6f7919707118, Nodes: 7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18)04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19)4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:4ba198f1-a013-49ce-b059-f73788d69001, CreationTimestamp2023-03-18T09:23:43.610Z[UTC]] moved to OPEN state
recon_1  | 2023-03-18 09:23:58,172 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=84a08f9d-e905-447e-be3d-ccb5f87f8193 reported by 04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19)
recon_1  | 2023-03-18 09:23:58,173 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 84a08f9d-e905-447e-be3d-ccb5f87f8193, Nodes: 04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19)7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18)4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:04b88c83-026f-4630-aada-108e37835bee, CreationTimestamp2023-03-18T09:23:43.612Z[UTC]] moved to OPEN state
recon_1  | 2023-03-18 09:24:06,938 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1  | 2023-03-18 09:24:06,938 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1  | 2023-03-18 09:24:06,938 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 151 
recon_1  | 2023-03-18 09:24:06,955 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
dn4_1    | 2023-03-18 09:23:52,376 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-03-18 09:23:52,377 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-03-18 09:23:52,377 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-03-18 09:23:52,377 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-03-18 09:23:52,377 [pool-38-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/84a08f9d-e905-447e-be3d-ccb5f87f8193 does not exist. Creating ...
dn4_1    | 2023-03-18 09:23:52,379 [pool-38-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/84a08f9d-e905-447e-be3d-ccb5f87f8193/in_use.lock acquired by nodename 7@23f39be4c8d3
dn4_1    | 2023-03-18 09:23:52,381 [pool-38-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/84a08f9d-e905-447e-be3d-ccb5f87f8193 has been successfully formatted.
dn4_1    | 2023-03-18 09:23:52,383 [pool-38-thread-1] INFO ratis.ContainerStateMachine: group-CCB5F87F8193: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn4_1    | 2023-03-18 09:23:52,383 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-03-18 09:23:52,383 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-03-18 09:23:52,383 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-18 09:23:52,383 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-03-18 09:23:52,383 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-03-18 09:23:52,384 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-03-18 09:23:52,387 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-03-18 09:23:52,388 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-03-18 09:23:52,388 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: new 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/84a08f9d-e905-447e-be3d-ccb5f87f8193
dn4_1    | 2023-03-18 09:23:52,388 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-03-18 09:23:52,396 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-03-18 09:23:52,396 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-03-18 09:23:52,396 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-03-18 09:23:52,397 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-03-18 09:23:52,397 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-03-18 09:23:52,397 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-03-18 09:23:52,397 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-03-18 09:23:52,397 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-03-18 09:23:52,402 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-18 09:23:52,764 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-7c9e7a7d-2593-458c-8a46-0313d77c3278: Detected pause in JVM or host machine (eg GC): pause of approximately 278243393ns.
dn4_1    | GC pool 'ParNew' had collection(s): count=1 time=358ms
dn4_1    | 2023-03-18 09:23:52,792 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-03-18 09:23:52,792 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-03-18 09:23:52,793 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-03-18 09:23:52,794 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-03-18 09:23:52,795 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-03-18 09:23:52,795 [pool-38-thread-1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193: start as a follower, conf=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:23:52,796 [pool-38-thread-1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn4_1    | 2023-03-18 09:23:52,796 [pool-38-thread-1] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: start 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-FollowerState
dn4_1    | 2023-03-18 09:23:52,801 [pool-38-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-CCB5F87F8193,id=7c9e7a7d-2593-458c-8a46-0313d77c3278
dn4_1    | 2023-03-18 09:23:52,802 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-03-18 09:23:52,802 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-03-18 09:23:52,803 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-03-18 09:23:52,803 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-03-18 09:23:52,804 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-18 09:23:52,806 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=84a08f9d-e905-447e-be3d-ccb5f87f8193
dn4_1    | 2023-03-18 09:23:52,806 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-18 09:23:53,202 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=84a08f9d-e905-447e-be3d-ccb5f87f8193.
dn3_1    | 2023-03-18 09:23:21,395 [Command processor thread] INFO server.RaftServer: bae0cd8d-e50f-4d23-8974-b816860f8fc3: remove  FOLLOWER bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D:t7, leader=04b88c83-026f-4630-aada-108e37835bee, voted=04b88c83-026f-4630-aada-108e37835bee, raftlog=Memoized:bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-SegmentedRaftLog:OPENED:c44, conf=39: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn3_1    | 2023-03-18 09:23:21,397 [Command processor thread] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D: shutdown
dn3_1    | 2023-03-18 09:23:21,398 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-8191D6D2324D,id=bae0cd8d-e50f-4d23-8974-b816860f8fc3
dn3_1    | 2023-03-18 09:23:21,398 [Command processor thread] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: shutdown bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-FollowerState
dn3_1    | 2023-03-18 09:23:21,398 [Command processor thread] INFO impl.StateMachineUpdater: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-StateMachineUpdater: set stopIndex = 44
dn3_1    | 2023-03-18 09:23:21,398 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-8191D6D2324D: Taking a snapshot at:(t:7, i:44) file /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/sm/snapshot.7_44
dn3_1    | 2023-03-18 09:23:21,398 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-FollowerState] INFO impl.FollowerState: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-FollowerState was interrupted
dn3_1    | 2023-03-18 09:23:21,403 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-8191D6D2324D: Finished taking a snapshot at:(t:7, i:44) file:/data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d/sm/snapshot.7_44 took: 4 ms
dn3_1    | 2023-03-18 09:23:21,404 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-StateMachineUpdater] INFO impl.StateMachineUpdater: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-StateMachineUpdater: Took a snapshot at index 44
dn3_1    | 2023-03-18 09:23:21,404 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-StateMachineUpdater] INFO impl.StateMachineUpdater: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-StateMachineUpdater: snapshotIndex: updateIncreasingly 38 -> 44
dn3_1    | 2023-03-18 09:23:21,415 [Command processor thread] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D: closes. applyIndex: 44
dn3_1    | 2023-03-18 09:23:21,416 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn3_1    | 2023-03-18 09:23:21,416 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D-SegmentedRaftLogWorker close()
dn3_1    | 2023-03-18 09:23:21,425 [Command processor thread] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-8191D6D2324D: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d
dn3_1    | 2023-03-18 09:23:21,426 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=97be11a2-1597-458e-9379-8191d6d2324d command on datanode bae0cd8d-e50f-4d23-8974-b816860f8fc3.
dn3_1    | 2023-03-18 09:23:21,426 [Command processor thread] INFO server.RaftServer: bae0cd8d-e50f-4d23-8974-b816860f8fc3: remove    LEADER bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C:t6, leader=bae0cd8d-e50f-4d23-8974-b816860f8fc3, voted=bae0cd8d-e50f-4d23-8974-b816860f8fc3, raftlog=Memoized:bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-SegmentedRaftLog:OPENED:c33, conf=28: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn3_1    | 2023-03-18 09:23:21,426 [Command processor thread] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C: shutdown
dn3_1    | 2023-03-18 09:23:21,427 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-80313208139C,id=bae0cd8d-e50f-4d23-8974-b816860f8fc3
dn3_1    | 2023-03-18 09:23:21,427 [Command processor thread] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: shutdown bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-LeaderStateImpl
dn3_1    | 2023-03-18 09:23:21,432 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C->7c9e7a7d-2593-458c-8a46-0313d77c3278-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C->7c9e7a7d-2593-458c-8a46-0313d77c3278-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
dn3_1    | 2023-03-18 09:23:21,444 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C->04b88c83-026f-4630-aada-108e37835bee-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C->04b88c83-026f-4630-aada-108e37835bee-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
dn3_1    | 2023-03-18 09:23:21,451 [grpc-default-executor-2] INFO server.GrpcLogAppender: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C->7c9e7a7d-2593-458c-8a46-0313d77c3278-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn3_1    | 2023-03-18 09:23:21,453 [grpc-default-executor-2] INFO leader.FollowerInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C->7c9e7a7d-2593-458c-8a46-0313d77c3278: nextIndex: updateUnconditionally 34 -> 33
dn3_1    | 2023-03-18 09:23:21,458 [grpc-default-executor-2] INFO server.GrpcLogAppender: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C->04b88c83-026f-4630-aada-108e37835bee-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn3_1    | 2023-03-18 09:23:21,458 [grpc-default-executor-2] INFO leader.FollowerInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C->04b88c83-026f-4630-aada-108e37835bee: nextIndex: updateUnconditionally 34 -> 33
dn3_1    | 2023-03-18 09:23:21,459 [Command processor thread] INFO impl.PendingRequests: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-PendingRequests: sendNotLeaderResponses
dn3_1    | 2023-03-18 09:23:21,459 [grpc-default-executor-4] INFO server.GrpcLogAppender: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C->7c9e7a7d-2593-458c-8a46-0313d77c3278-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn3_1    | 2023-03-18 09:23:21,475 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: bae0cd8d-e50f-4d23-8974-b816860f8fc3: Completed APPEND_ENTRIES, lastRequest: 04b88c83-026f-4630-aada-108e37835bee->bae0cd8d-e50f-4d23-8974-b816860f8fc3#271-t7,previous=(t:7, i:43),leaderCommit=43,initializing? true,entries: size=1, first=(t:7, i:44), METADATAENTRY(c:43)
dn3_1    | 2023-03-18 09:23:21,480 [grpc-default-executor-6] INFO server.GrpcServerProtocolService: bae0cd8d-e50f-4d23-8974-b816860f8fc3: Completed APPEND_ENTRIES, lastRequest: null
dn3_1    | 2023-03-18 09:23:21,459 [grpc-default-executor-4] INFO leader.FollowerInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C->7c9e7a7d-2593-458c-8a46-0313d77c3278: nextIndex: updateUnconditionally 33 -> 32
dn3_1    | 2023-03-18 09:23:21,482 [Command processor thread] INFO impl.StateMachineUpdater: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-StateMachineUpdater: set stopIndex = 33
dn3_1    | 2023-03-18 09:23:21,495 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-80313208139C: Taking a snapshot at:(t:6, i:33) file /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/sm/snapshot.6_33
dn3_1    | 2023-03-18 09:23:21,495 [grpc-default-executor-5] INFO server.GrpcLogAppender: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C->04b88c83-026f-4630-aada-108e37835bee-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn3_1    | 2023-03-18 09:23:21,496 [grpc-default-executor-5] INFO leader.FollowerInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C->04b88c83-026f-4630-aada-108e37835bee: nextIndex: updateUnconditionally 33 -> 32
dn3_1    | 2023-03-18 09:23:21,499 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-80313208139C: Finished taking a snapshot at:(t:6, i:33) file:/data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/sm/snapshot.6_33 took: 4 ms
dn3_1    | 2023-03-18 09:23:21,501 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-StateMachineUpdater] INFO impl.StateMachineUpdater: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-StateMachineUpdater: Took a snapshot at index 33
dn3_1    | 2023-03-18 09:23:21,503 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-StateMachineUpdater] INFO impl.StateMachineUpdater: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-StateMachineUpdater: snapshotIndex: updateIncreasingly 27 -> 33
dn3_1    | 2023-03-18 09:23:21,503 [Command processor thread] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C: closes. applyIndex: 33
dn3_1    | 2023-03-18 09:23:21,504 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn3_1    | 2023-03-18 09:23:21,505 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C-SegmentedRaftLogWorker close()
dn3_1    | 2023-03-18 09:23:21,507 [Command processor thread] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-80313208139C: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c
dn3_1    | 2023-03-18 09:23:21,508 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=e319c2ba-524b-42f7-8b03-80313208139c command on datanode bae0cd8d-e50f-4d23-8974-b816860f8fc3.
dn3_1    | 2023-03-18 09:23:21,508 [Command processor thread] INFO server.RaftServer: bae0cd8d-e50f-4d23-8974-b816860f8fc3: remove    LEADER bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B:t4, leader=bae0cd8d-e50f-4d23-8974-b816860f8fc3, voted=bae0cd8d-e50f-4d23-8974-b816860f8fc3, raftlog=Memoized:bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-SegmentedRaftLog:OPENED:c6, conf=5: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn3_1    | 2023-03-18 09:23:21,508 [Command processor thread] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B: shutdown
dn3_1    | 2023-03-18 09:23:21,508 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-D1111D92119B,id=bae0cd8d-e50f-4d23-8974-b816860f8fc3
dn3_1    | 2023-03-18 09:23:21,508 [Command processor thread] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: shutdown bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-LeaderStateImpl
dn3_1    | 2023-03-18 09:23:21,508 [Command processor thread] INFO impl.PendingRequests: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-PendingRequests: sendNotLeaderResponses
dn3_1    | 2023-03-18 09:23:21,509 [Command processor thread] INFO impl.StateMachineUpdater: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-StateMachineUpdater: set stopIndex = 6
dn3_1    | 2023-03-18 09:23:21,510 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-D1111D92119B: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/d1279162-045f-45cf-95f2-d1111d92119b/sm/snapshot.4_6
dn3_1    | 2023-03-18 09:23:21,514 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-D1111D92119B: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/d1279162-045f-45cf-95f2-d1111d92119b/sm/snapshot.4_6 took: 3 ms
dn3_1    | 2023-03-18 09:23:21,517 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-StateMachineUpdater] INFO impl.StateMachineUpdater: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-StateMachineUpdater: Took a snapshot at index 6
dn3_1    | 2023-03-18 09:23:21,518 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-StateMachineUpdater] INFO impl.StateMachineUpdater: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
dn3_1    | 2023-03-18 09:23:21,519 [Command processor thread] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B: closes. applyIndex: 6
dn3_1    | 2023-03-18 09:23:21,519 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn3_1    | 2023-03-18 09:23:21,520 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B-SegmentedRaftLogWorker close()
dn3_1    | 2023-03-18 09:23:21,521 [Command processor thread] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-D1111D92119B: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/d1279162-045f-45cf-95f2-d1111d92119b
dn3_1    | 2023-03-18 09:23:21,521 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=d1279162-045f-45cf-95f2-d1111d92119b command on datanode bae0cd8d-e50f-4d23-8974-b816860f8fc3.
dn3_1    | 2023-03-18 09:23:22,731 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@459003a0] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(0),2(0)], numOfContainers=2, numOfBlocks=3
dn3_1    | 2023-03-18 09:23:51,467 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-03-18 09:24:10,916 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 3/4997 blocks from 2 candidate containers.
dn3_1    | 2023-03-18 09:24:10,936 [BlockDeletingService#0] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/current/containerDir0/2/chunks/111677748019200002.block
dn3_1    | 2023-03-18 09:24:10,941 [BlockDeletingService#0] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/current/containerDir0/2/chunks/111677748019200003.block
dn3_1    | 2023-03-18 09:24:10,942 [BlockDeletingService#2] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/current/containerDir0/1/chunks/111677748019200001.block
dn3_1    | 2023-03-18 09:24:21,482 [pool-38-thread-1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3: new RaftServerImpl for group-2009D4EDBE4A:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn3_1    | 2023-03-18 09:24:21,482 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-03-18 09:24:21,482 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-03-18 09:24:21,482 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-03-18 09:24:21,482 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-03-18 09:24:21,483 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-03-18 09:24:21,483 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-03-18 09:24:21,483 [pool-38-thread-1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A: ConfigurationManager, init=-1: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-03-18 09:24:21,483 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-03-18 09:24:21,484 [Command processor thread] INFO server.RaftServer: bae0cd8d-e50f-4d23-8974-b816860f8fc3: addNew group-2009D4EDBE4A:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] returns group-2009D4EDBE4A:java.util.concurrent.CompletableFuture@5e4e0c91[Not completed]
dn3_1    | 2023-03-18 09:24:21,485 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-03-18 09:24:21,485 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-03-18 09:24:21,486 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-03-18 09:24:21,486 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-03-18 09:24:21,486 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-03-18 09:24:21,487 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-03-18 09:24:21,487 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-03-18 09:24:21,487 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-03-18 09:24:21,487 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:24:06,959 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:24:06,960 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:24:06,961 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 2, SequenceNumber diff: 3, SequenceNumber Lag from OM 0.
recon_1  | 2023-03-18 09:24:06,961 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 3 records
recon_1  | 2023-03-18 09:24:13,233 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=d5743016-6c44-4b2d-ae26-d4e599c27fe1. Trying to get from SCM.
recon_1  | 2023-03-18 09:24:13,247 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: d5743016-6c44-4b2d-ae26-d4e599c27fe1, Nodes: 4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:4ba198f1-a013-49ce-b059-f73788d69001, CreationTimestamp2023-03-18T09:23:43.608Z[UTC]] to Recon pipeline metadata.
recon_1  | 2023-03-18 09:24:13,247 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d5743016-6c44-4b2d-ae26-d4e599c27fe1, Nodes: 4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:4ba198f1-a013-49ce-b059-f73788d69001, CreationTimestamp2023-03-18T09:23:43.608Z[UTC]].
recon_1  | 2023-03-18 09:24:14,884 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=747ad178-d87d-4bbd-985b-ed015d5b9b4c. Trying to get from SCM.
recon_1  | 2023-03-18 09:24:14,902 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 747ad178-d87d-4bbd-985b-ed015d5b9b4c, Nodes: 2b374f74-8596-4317-81ba-ce0a491ddd96(ha_dn2_1.ha_net/10.9.0.16), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:2b374f74-8596-4317-81ba-ce0a491ddd96, CreationTimestamp2023-03-18T09:23:46.608Z[UTC]] to Recon pipeline metadata.
recon_1  | 2023-03-18 09:24:14,903 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 747ad178-d87d-4bbd-985b-ed015d5b9b4c, Nodes: 2b374f74-8596-4317-81ba-ce0a491ddd96(ha_dn2_1.ha_net/10.9.0.16), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:2b374f74-8596-4317-81ba-ce0a491ddd96, CreationTimestamp2023-03-18T09:23:46.608Z[UTC]].
recon_1  | 2023-03-18 09:24:21,505 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=744854b1-789c-4dc8-9bd9-2009d4edbe4a. Trying to get from SCM.
om3_1    | 2023-03-18 09:22:20,410 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 104
om3_1    | 2023-03-18 09:22:20,412 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 68
om3_1    | 2023-03-18 09:22:20,709 [om3-impl-thread1] INFO raftlog.RaftLog: om3@group-D66704EFC61C-SegmentedRaftLog: commitIndex: updateToMax old=104, new=103, updated? false
om3_1    | 2023-03-18 09:22:20,711 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: start as a follower, conf=69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-03-18 09:22:20,711 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from      null to FOLLOWER at term 7 for startAsFollower
om3_1    | 2023-03-18 09:22:20,727 [om3-impl-thread1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
om3_1    | 2023-03-18 09:22:20,740 [om3-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om3
om3_1    | 2023-03-18 09:22:20,745 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om3_1    | 2023-03-18 09:22:20,745 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om3_1    | 2023-03-18 09:22:20,745 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-03-18 09:22:20,748 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-03-18 09:22:20,748 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om3_1    | 2023-03-18 09:22:20,749 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om3_1    | 2023-03-18 09:22:20,768 [Listener at om3/9862] INFO server.RaftServer: om3: start RPC server
om3_1    | 2023-03-18 09:22:20,914 [Listener at om3/9862] INFO server.GrpcService: om3: GrpcService started, listening on 9872
om3_1    | 2023-03-18 09:22:20,920 [Listener at om3/9862] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
om3_1    | 2023-03-18 09:22:20,923 [Listener at om3/9862] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
om3_1    | 2023-03-18 09:22:20,923 [Listener at om3/9862] INFO om.OzoneManager: Version File has different layout version (0) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om3_1    | 2023-03-18 09:22:20,935 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om3: Started
om3_1    | 2023-03-18 09:22:21,102 [Listener at om3/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om3_1    | 2023-03-18 09:22:21,104 [Listener at om3/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om3_1    | 2023-03-18 09:22:21,161 [Listener at om3/9862] INFO util.log: Logging initialized @65677ms to org.eclipse.jetty.util.log.Slf4jLog
om3_1    | 2023-03-18 09:22:21,464 [Listener at om3/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om3_1    | 2023-03-18 09:22:21,493 [Listener at om3/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om3_1    | 2023-03-18 09:22:21,520 [Listener at om3/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om3_1    | 2023-03-18 09:22:21,526 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om3_1    | 2023-03-18 09:22:21,526 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om3_1    | 2023-03-18 09:22:21,527 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om3_1    | 2023-03-18 09:22:21,661 [Listener at om3/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om3_1    | 2023-03-18 09:22:21,667 [Listener at om3/9862] INFO http.HttpServer2: Jetty bound to port 9874
om3_1    | 2023-03-18 09:22:21,669 [Listener at om3/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om3_1    | 2023-03-18 09:22:21,984 [Listener at om3/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om3_1    | 2023-03-18 09:22:21,984 [Listener at om3/9862] INFO server.session: No SessionScavenger set, using defaults
om3_1    | 2023-03-18 09:22:21,992 [Listener at om3/9862] INFO server.session: node0 Scavenging every 600000ms
om3_1    | 2023-03-18 09:22:22,088 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@23f60b7d{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om3_1    | 2023-03-18 09:22:22,093 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@52d96367{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om3_1    | 2023-03-18 09:22:22,781 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@faec277{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-3918337022555678905/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om3_1    | 2023-03-18 09:22:22,845 [Listener at om3/9862] INFO server.AbstractConnector: Started ServerConnector@18026052{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om3_1    | 2023-03-18 09:22:22,846 [Listener at om3/9862] INFO server.Server: Started @67362ms
om3_1    | 2023-03-18 09:22:22,853 [Listener at om3/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om3_1    | 2023-03-18 09:22:22,853 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om3_1    | 2023-03-18 09:22:22,854 [Listener at om3/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om3_1    | 2023-03-18 09:22:22,855 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om3_1    | 2023-03-18 09:22:22,856 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om3_1    | 2023-03-18 09:22:23,073 [Listener at om3/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om3_1    | 2023-03-18 09:22:23,104 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@62faf77] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om3_1    | 2023-03-18 09:22:25,773 [om3@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om3@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5046635277ns, electionTimeout:5024ms
om3_1    | 2023-03-18 09:22:25,774 [om3@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-FollowerState
recon_1  | 2023-03-18 09:24:21,512 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 744854b1-789c-4dc8-9bd9-2009d4edbe4a, Nodes: bae0cd8d-e50f-4d23-8974-b816860f8fc3(ha_dn3_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:bae0cd8d-e50f-4d23-8974-b816860f8fc3, CreationTimestamp2023-03-18T09:23:52.608Z[UTC]] to Recon pipeline metadata.
recon_1  | 2023-03-18 09:24:21,513 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 744854b1-789c-4dc8-9bd9-2009d4edbe4a, Nodes: bae0cd8d-e50f-4d23-8974-b816860f8fc3(ha_dn3_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:bae0cd8d-e50f-4d23-8974-b816860f8fc3, CreationTimestamp2023-03-18T09:23:52.608Z[UTC]].
recon_1  | 2023-03-18 09:25:06,968 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1  | 2023-03-18 09:25:06,969 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1  | 2023-03-18 09:25:06,969 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 154 
recon_1  | 2023-03-18 09:25:06,984 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:25:06,984 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 1, SequenceNumber diff: 1, SequenceNumber Lag from OM 0.
recon_1  | 2023-03-18 09:25:06,984 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 1 records
recon_1  | 2023-03-18 09:25:21,266 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #2001 got from ha_dn4_1.ha_net.
recon_1  | 2023-03-18 09:25:21,299 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Successfully added container #2001 to Recon.
recon_1  | 2023-03-18 09:25:33,113 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #2002 got from ha_dn4_1.ha_net.
recon_1  | 2023-03-18 09:25:33,105 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #2002 got from ha_dn5_1.ha_net.
recon_1  | 2023-03-18 09:25:33,116 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: New container #2002 got from ha_dn1_1.ha_net.
recon_1  | 2023-03-18 09:25:33,156 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: Successfully added container #2002 to Recon.
recon_1  | 2023-03-18 09:25:33,165 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Successfully added container #2002 to Recon.
recon_1  | 2023-03-18 09:25:33,167 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: Successfully added container #2002 to Recon.
recon_1  | 2023-03-18 09:26:06,987 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1  | 2023-03-18 09:26:06,987 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1  | 2023-03-18 09:26:06,987 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 155 
recon_1  | 2023-03-18 09:26:06,993 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:06,995 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
om3_1    | 2023-03-18 09:22:25,775 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 7 for changeToCandidate
om3_1    | 2023-03-18 09:22:25,777 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om3_1    | 2023-03-18 09:22:25,777 [om3@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-LeaderElection1
om3_1    | 2023-03-18 09:22:25,790 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 7 for 69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-03-18 09:22:25,814 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-03-18 09:22:25,823 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-03-18 09:22:25,820 [om3@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
om3_1    | 2023-03-18 09:22:25,834 [om3@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
om3_1    | 2023-03-18 09:22:26,978 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(PRE_VOTE, om2, group-D66704EFC61C, 7, (t:7, i:104))
om3_1    | 2023-03-18 09:22:26,993 [grpc-default-executor-0] INFO impl.VoteContext: om3@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om2: our priority 0 <= candidate's priority 0
om3_1    | 2023-03-18 09:22:27,017 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to PRE_VOTE vote request: om2<-om3#0:OK-t7. Peer's state: om3@group-D66704EFC61C:t7, leader=null, voted=om3, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c104, conf=69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-03-18 09:22:27,081 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
om3_1    | 2023-03-18 09:22:27,081 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om3<-om2#0:OK-t7
om3_1    | 2023-03-18 09:22:27,081 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result PASSED
om3_1    | 2023-03-18 09:22:27,085 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 ELECTION round 0: submit vote requests at term 8 for 69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-03-18 09:22:27,101 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-03-18 09:22:27,101 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-03-18 09:22:27,110 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(ELECTION, om2, group-D66704EFC61C, 8, (t:7, i:104))
om3_1    | 2023-03-18 09:22:27,125 [grpc-default-executor-0] INFO impl.VoteContext: om3@group-D66704EFC61C-CANDIDATE: reject ELECTION from om2: already has voted for om3 at current term 8
om3_1    | 2023-03-18 09:22:27,125 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to ELECTION vote request: om2<-om3#0:FAIL-t8. Peer's state: om3@group-D66704EFC61C:t8, leader=null, voted=om3, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c104, conf=69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-03-18 09:22:27,481 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1: ELECTION PASSED received 2 response(s) and 0 exception(s):
om3_1    | 2023-03-18 09:22:27,481 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om3<-om1#0:OK-t8
om3_1    | 2023-03-18 09:22:27,481 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 1: om3<-om2#0:FAIL-t8
om3_1    | 2023-03-18 09:22:27,481 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 ELECTION round 0: result PASSED
om3_1    | 2023-03-18 09:22:27,484 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-LeaderElection1
om3_1    | 2023-03-18 09:22:27,484 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from CANDIDATE to LEADER at term 8 for changeToLeader
om3_1    | 2023-03-18 09:22:27,484 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: change Leader from null to om3 at term 8 for becomeLeader, leader elected after 12899ms
om3_1    | 2023-03-18 09:22:27,495 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om3_1    | 2023-03-18 09:22:27,506 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om3_1    | 2023-03-18 09:22:27,510 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om3_1    | 2023-03-18 09:22:27,523 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(PRE_VOTE, om1, group-D66704EFC61C, 7, (t:7, i:104))
om3_1    | 2023-03-18 09:22:27,525 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om3_1    | 2023-03-18 09:22:27,526 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om3_1    | 2023-03-18 09:22:27,528 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om3_1    | 2023-03-18 09:22:27,538 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om3_1    | 2023-03-18 09:22:27,542 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om3_1    | 2023-03-18 09:22:27,566 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om3_1    | 2023-03-18 09:22:27,568 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-03-18 09:22:27,568 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
dn5_1    | 2023-03-18 09:23:21,501 [grpc-default-executor-3] INFO leader.FollowerInfo: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D->bae0cd8d-e50f-4d23-8974-b816860f8fc3: nextIndex: updateUnconditionally 44 -> 43
dn5_1    | 2023-03-18 09:23:21,501 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-StateMachineUpdater] INFO impl.StateMachineUpdater: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-StateMachineUpdater: snapshotIndex: updateIncreasingly 38 -> 44
dn5_1    | 2023-03-18 09:23:21,509 [Command processor thread] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D: closes. applyIndex: 44
dn5_1    | 2023-03-18 09:23:21,510 [04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn5_1    | 2023-03-18 09:23:21,516 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D-SegmentedRaftLogWorker close()
dn5_1    | 2023-03-18 09:23:21,525 [Command processor thread] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-8191D6D2324D: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/97be11a2-1597-458e-9379-8191d6d2324d
dn5_1    | 2023-03-18 09:23:21,526 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=97be11a2-1597-458e-9379-8191d6d2324d command on datanode 04b88c83-026f-4630-aada-108e37835bee.
dn5_1    | 2023-03-18 09:23:21,528 [Command processor thread] INFO server.RaftServer: 04b88c83-026f-4630-aada-108e37835bee: remove    LEADER 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58:t4, leader=04b88c83-026f-4630-aada-108e37835bee, voted=04b88c83-026f-4630-aada-108e37835bee, raftlog=Memoized:04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-SegmentedRaftLog:OPENED:c6, conf=5: peers:[04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn5_1    | 2023-03-18 09:23:21,528 [Command processor thread] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58: shutdown
dn5_1    | 2023-03-18 09:23:21,528 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-3212C22EDF58,id=04b88c83-026f-4630-aada-108e37835bee
dn5_1    | 2023-03-18 09:23:21,528 [Command processor thread] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: shutdown 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-LeaderStateImpl
dn5_1    | 2023-03-18 09:23:21,529 [Command processor thread] INFO impl.PendingRequests: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-PendingRequests: sendNotLeaderResponses
dn5_1    | 2023-03-18 09:23:21,531 [Command processor thread] INFO impl.StateMachineUpdater: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-StateMachineUpdater: set stopIndex = 6
dn5_1    | 2023-03-18 09:23:21,535 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-3212C22EDF58: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/b5ca130e-c58f-4d3d-8bda-3212c22edf58/sm/snapshot.4_6
dn5_1    | 2023-03-18 09:23:21,541 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-3212C22EDF58: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/b5ca130e-c58f-4d3d-8bda-3212c22edf58/sm/snapshot.4_6 took: 5 ms
dn5_1    | 2023-03-18 09:23:21,542 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-StateMachineUpdater] INFO impl.StateMachineUpdater: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-StateMachineUpdater: Took a snapshot at index 6
dn5_1    | 2023-03-18 09:23:21,542 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-StateMachineUpdater] INFO impl.StateMachineUpdater: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
dn5_1    | 2023-03-18 09:23:21,543 [Command processor thread] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58: closes. applyIndex: 6
dn5_1    | 2023-03-18 09:23:21,545 [04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn5_1    | 2023-03-18 09:23:21,546 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58-SegmentedRaftLogWorker close()
dn5_1    | 2023-03-18 09:23:21,547 [Command processor thread] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-3212C22EDF58: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/b5ca130e-c58f-4d3d-8bda-3212c22edf58
dn5_1    | 2023-03-18 09:23:21,547 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=b5ca130e-c58f-4d3d-8bda-3212c22edf58 command on datanode 04b88c83-026f-4630-aada-108e37835bee.
dn5_1    | 2023-03-18 09:23:21,548 [Command processor thread] INFO server.RaftServer: 04b88c83-026f-4630-aada-108e37835bee: remove  FOLLOWER 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C:t6, leader=bae0cd8d-e50f-4d23-8974-b816860f8fc3, voted=bae0cd8d-e50f-4d23-8974-b816860f8fc3, raftlog=Memoized:04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-SegmentedRaftLog:OPENED:c33, conf=28: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn5_1    | 2023-03-18 09:23:21,548 [Command processor thread] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C: shutdown
dn5_1    | 2023-03-18 09:23:21,548 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-80313208139C,id=04b88c83-026f-4630-aada-108e37835bee
dn5_1    | 2023-03-18 09:23:21,549 [Command processor thread] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: shutdown 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-FollowerState
dn5_1    | 2023-03-18 09:23:21,549 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-80313208139C: Taking a snapshot at:(t:6, i:33) file /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/sm/snapshot.6_33
dn5_1    | 2023-03-18 09:23:21,550 [Command processor thread] INFO impl.StateMachineUpdater: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-StateMachineUpdater: set stopIndex = 33
dn5_1    | 2023-03-18 09:23:21,550 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-FollowerState] INFO impl.FollowerState: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-FollowerState was interrupted
dn5_1    | 2023-03-18 09:23:21,554 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-80313208139C: Finished taking a snapshot at:(t:6, i:33) file:/data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c/sm/snapshot.6_33 took: 4 ms
dn4_1    | 2023-03-18 09:23:56,589 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-FollowerState] INFO impl.FollowerState: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5114225254ns, electionTimeout:5092ms
dn4_1    | 2023-03-18 09:23:56,589 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-FollowerState] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: shutdown 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-FollowerState
dn4_1    | 2023-03-18 09:23:56,589 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-FollowerState] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn4_1    | 2023-03-18 09:23:56,590 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | 2023-03-18 09:23:56,590 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-FollowerState] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: start 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4
dn4_1    | 2023-03-18 09:23:56,592 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4] INFO impl.LeaderElection: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:23:56,592 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4] INFO impl.LeaderElection: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4 PRE_VOTE round 0: result PASSED (term=0)
dn4_1    | 2023-03-18 09:23:56,594 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4] INFO impl.LeaderElection: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4 ELECTION round 0: submit vote requests at term 1 for -1: peers:[7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:23:56,594 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4] INFO impl.LeaderElection: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4 ELECTION round 0: result PASSED (term=1)
dn4_1    | 2023-03-18 09:23:56,594 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: shutdown 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4
dn4_1    | 2023-03-18 09:23:56,594 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn4_1    | 2023-03-18 09:23:56,594 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-BDE6DABFBCFE with new leaderId: 7c9e7a7d-2593-458c-8a46-0313d77c3278
dn4_1    | 2023-03-18 09:23:56,594 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE: change Leader from null to 7c9e7a7d-2593-458c-8a46-0313d77c3278 at term 1 for becomeLeader, leader elected after 5158ms
dn4_1    | 2023-03-18 09:23:56,595 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn4_1    | 2023-03-18 09:23:56,595 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-03-18 09:23:56,595 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn4_1    | 2023-03-18 09:23:56,595 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn4_1    | 2023-03-18 09:23:56,595 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn4_1    | 2023-03-18 09:23:56,595 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn4_1    | 2023-03-18 09:23:56,595 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-03-18 09:23:56,595 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn4_1    | 2023-03-18 09:23:56,595 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: start 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderStateImpl
dn4_1    | 2023-03-18 09:23:56,596 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-03-18 09:23:56,598 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/3538aed8-d817-47ae-96b3-bde6dabfbcfe/current/log_inprogress_0
dn4_1    | 2023-03-18 09:23:56,611 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE-LeaderElection4] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-BDE6DABFBCFE: set configuration 0: peers:[7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:23:56,739 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-FollowerState] INFO impl.FollowerState: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5051018300ns, electionTimeout:5027ms
dn4_1    | 2023-03-18 09:23:56,739 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-FollowerState] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: shutdown 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-FollowerState
dn4_1    | 2023-03-18 09:23:56,739 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-FollowerState] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn4_1    | 2023-03-18 09:23:56,739 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | 2023-03-18 09:23:56,739 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-FollowerState] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: start 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-LeaderElection5
dn5_1    | 2023-03-18 09:23:21,554 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-StateMachineUpdater] INFO impl.StateMachineUpdater: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-StateMachineUpdater: Took a snapshot at index 33
dn5_1    | 2023-03-18 09:23:21,555 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-StateMachineUpdater] INFO impl.StateMachineUpdater: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-StateMachineUpdater: snapshotIndex: updateIncreasingly 27 -> 33
dn5_1    | 2023-03-18 09:23:21,555 [Command processor thread] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C: closes. applyIndex: 33
dn5_1    | 2023-03-18 09:23:21,556 [04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn5_1    | 2023-03-18 09:23:21,559 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C-SegmentedRaftLogWorker close()
dn5_1    | 2023-03-18 09:23:21,561 [Command processor thread] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-80313208139C: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/e319c2ba-524b-42f7-8b03-80313208139c
dn5_1    | 2023-03-18 09:23:21,561 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=e319c2ba-524b-42f7-8b03-80313208139c command on datanode 04b88c83-026f-4630-aada-108e37835bee.
dn5_1    | 2023-03-18 09:23:22,356 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@245253d8] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(0),2(0)], numOfContainers=2, numOfBlocks=3
dn5_1    | 2023-03-18 09:23:51,514 [pool-38-thread-1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee: new RaftServerImpl for group-5DA9883170E4:[04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn5_1    | 2023-03-18 09:23:51,515 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-03-18 09:23:51,515 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-03-18 09:23:51,515 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-03-18 09:23:51,515 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-03-18 09:23:51,515 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-03-18 09:23:51,515 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-03-18 09:23:51,516 [pool-38-thread-1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4: ConfigurationManager, init=-1: peers:[04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-03-18 09:23:51,516 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-03-18 09:23:51,518 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-03-18 09:23:51,518 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-03-18 09:23:51,519 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-03-18 09:23:51,519 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-03-18 09:23:51,519 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-03-18 09:23:51,520 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-03-18 09:23:51,520 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-03-18 09:23:51,520 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-03-18 09:23:51,521 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-03-18 09:23:51,521 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-03-18 09:23:51,522 [Command processor thread] INFO server.RaftServer: 04b88c83-026f-4630-aada-108e37835bee: addNew group-5DA9883170E4:[04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] returns      null 04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4:t0, leader=null, voted=null, raftlog=UNINITIALIZED, conf=-1: peers:[04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null NEW
dn5_1    | 2023-03-18 09:23:51,523 [pool-38-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/087cc0b6-0f7f-40d8-82ec-5da9883170e4 does not exist. Creating ...
dn5_1    | 2023-03-18 09:23:51,524 [pool-38-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/087cc0b6-0f7f-40d8-82ec-5da9883170e4/in_use.lock acquired by nodename 7@e51427cbfe02
dn5_1    | 2023-03-18 09:23:51,529 [pool-38-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/087cc0b6-0f7f-40d8-82ec-5da9883170e4 has been successfully formatted.
dn5_1    | 2023-03-18 09:23:51,530 [pool-38-thread-1] INFO ratis.ContainerStateMachine: group-5DA9883170E4: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn5_1    | 2023-03-18 09:23:51,530 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-03-18 09:23:51,531 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-03-18 09:23:51,531 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-18 09:23:51,531 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-03-18 09:23:51,531 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-03-18 09:23:51,531 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-03-18 09:23:51,532 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-03-18 09:23:51,533 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-03-18 09:23:51,534 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: new 04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/087cc0b6-0f7f-40d8-82ec-5da9883170e4
dn5_1    | 2023-03-18 09:23:51,535 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-03-18 09:23:51,535 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-03-18 09:24:21,487 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-03-18 09:24:21,488 [pool-38-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/744854b1-789c-4dc8-9bd9-2009d4edbe4a does not exist. Creating ...
dn3_1    | 2023-03-18 09:24:21,490 [pool-38-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/744854b1-789c-4dc8-9bd9-2009d4edbe4a/in_use.lock acquired by nodename 7@ac7a47589fa9
dn3_1    | 2023-03-18 09:24:21,496 [pool-38-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/744854b1-789c-4dc8-9bd9-2009d4edbe4a has been successfully formatted.
dn3_1    | 2023-03-18 09:24:21,497 [pool-38-thread-1] INFO ratis.ContainerStateMachine: group-2009D4EDBE4A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-03-18 09:24:21,498 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-03-18 09:24:21,498 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-03-18 09:24:21,498 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-03-18 09:24:21,500 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-03-18 09:24:21,500 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-03-18 09:24:21,501 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-03-18 09:24:21,504 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-03-18 09:24:21,504 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-03-18 09:24:21,505 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: new bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/744854b1-789c-4dc8-9bd9-2009d4edbe4a
dn3_1    | 2023-03-18 09:24:21,506 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-03-18 09:24:21,508 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-03-18 09:24:21,508 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-03-18 09:24:21,508 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-03-18 09:24:21,508 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-03-18 09:24:21,509 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-03-18 09:24:21,509 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-03-18 09:24:21,509 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-03-18 09:24:21,511 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-03-18 09:24:21,514 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-03-18 09:24:21,520 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-03-18 09:24:21,521 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-03-18 09:24:21,522 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-03-18 09:24:21,522 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-03-18 09:24:21,522 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-03-18 09:24:21,523 [pool-38-thread-1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A: start as a follower, conf=-1: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:24:21,524 [pool-38-thread-1] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn3_1    | 2023-03-18 09:24:21,524 [pool-38-thread-1] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: start bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-FollowerState
dn3_1    | 2023-03-18 09:24:21,525 [pool-38-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-2009D4EDBE4A,id=bae0cd8d-e50f-4d23-8974-b816860f8fc3
dn3_1    | 2023-03-18 09:24:21,530 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-03-18 09:24:21,530 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-03-18 09:24:21,530 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-03-18 09:24:21,532 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-18 09:24:21,532 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-03-18 09:24:21,532 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-03-18 09:24:21,533 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=744854b1-789c-4dc8-9bd9-2009d4edbe4a
dn3_1    | 2023-03-18 09:24:21,534 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=744854b1-789c-4dc8-9bd9-2009d4edbe4a.
dn3_1    | 2023-03-18 09:24:26,563 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-FollowerState] INFO impl.FollowerState: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5038799624ns, electionTimeout:5031ms
dn3_1    | 2023-03-18 09:24:26,563 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-FollowerState] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: shutdown bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-FollowerState
dn3_1    | 2023-03-18 09:24:26,564 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-FollowerState] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn3_1    | 2023-03-18 09:24:26,564 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-03-18 09:24:26,564 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-FollowerState] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: start bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4
dn3_1    | 2023-03-18 09:24:26,569 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4] INFO impl.LeaderElection: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:24:26,569 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4] INFO impl.LeaderElection: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4 PRE_VOTE round 0: result PASSED (term=0)
dn3_1    | 2023-03-18 09:24:26,572 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4] INFO impl.LeaderElection: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4 ELECTION round 0: submit vote requests at term 1 for -1: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:24:26,572 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4] INFO impl.LeaderElection: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4 ELECTION round 0: result PASSED (term=1)
dn3_1    | 2023-03-18 09:24:26,572 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: shutdown bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4
dn3_1    | 2023-03-18 09:24:26,572 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn3_1    | 2023-03-18 09:24:26,572 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-2009D4EDBE4A with new leaderId: bae0cd8d-e50f-4d23-8974-b816860f8fc3
dn3_1    | 2023-03-18 09:24:26,573 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A: change Leader from null to bae0cd8d-e50f-4d23-8974-b816860f8fc3 at term 1 for becomeLeader, leader elected after 5086ms
dn3_1    | 2023-03-18 09:24:26,573 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn3_1    | 2023-03-18 09:24:26,575 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-03-18 09:24:26,575 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-03-18 09:24:26,577 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-03-18 09:24:26,577 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-03-18 09:24:26,577 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn3_1    | 2023-03-18 09:24:26,577 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-03-18 09:24:26,577 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn3_1    | 2023-03-18 09:24:26,578 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4] INFO impl.RoleInfo: bae0cd8d-e50f-4d23-8974-b816860f8fc3: start bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderStateImpl
dn3_1    | 2023-03-18 09:24:26,578 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-SegmentedRaftLogWorker: Starting segment from index:0
dn3_1    | 2023-03-18 09:24:26,583 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-LeaderElection4] INFO server.RaftServer$Division: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A: set configuration 0: peers:[bae0cd8d-e50f-4d23-8974-b816860f8fc3|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-18 09:24:26,584 [bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: bae0cd8d-e50f-4d23-8974-b816860f8fc3@group-2009D4EDBE4A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/744854b1-789c-4dc8-9bd9-2009d4edbe4a/current/log_inprogress_0
dn3_1    | 2023-03-18 09:25:10,936 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-03-18 09:26:10,936 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1    | 2023-03-18 09:22:36,321 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #1001, current state: CLOSING
scm_1    | 2023-03-18 09:22:36,327 [IPC Server handler 1 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 97be11a2-1597-458e-9379-8191d6d2324d, Nodes: 04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19)bae0cd8d-e50f-4d23-8974-b816860f8fc3(ha_dn3_1.ha_net/10.9.0.17)7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:04b88c83-026f-4630-aada-108e37835bee, CreationTimestamp2023-03-18T09:21:59.325102Z[UTC]] moved to CLOSED state
scm_1    | 2023-03-18 09:22:36,331 [IPC Server handler 1 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 67747242-de40-4a6c-83d8-e43a5a79ab67, Nodes: 4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:4ba198f1-a013-49ce-b059-f73788d69001, CreationTimestamp2023-03-18T09:21:59.309606Z[UTC]] moved to CLOSED state
scm_1    | 2023-03-18 09:22:36,332 [IPC Server handler 1 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: b5ca130e-c58f-4d3d-8bda-3212c22edf58, Nodes: 04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:04b88c83-026f-4630-aada-108e37835bee, CreationTimestamp2023-03-18T09:21:59.340827Z[UTC]] moved to CLOSED state
scm_1    | 2023-03-18 09:22:36,333 [IPC Server handler 1 on default port 9860] INFO pipeline.PipelineManagerImpl: Container #1 closed for pipeline=PipelineID=e319c2ba-524b-42f7-8b03-80313208139c
scm_1    | 2023-03-18 09:22:36,334 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #1, current state: CLOSING
scm_1    | 2023-03-18 09:22:36,338 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #1002, current state: CLOSING
scm_1    | 2023-03-18 09:22:36,339 [IPC Server handler 1 on default port 9860] INFO pipeline.PipelineManagerImpl: Container #1002 closed for pipeline=PipelineID=e319c2ba-524b-42f7-8b03-80313208139c
scm_1    | 2023-03-18 09:22:36,340 [IPC Server handler 1 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e319c2ba-524b-42f7-8b03-80313208139c, Nodes: bae0cd8d-e50f-4d23-8974-b816860f8fc3(ha_dn3_1.ha_net/10.9.0.17)04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19)7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:bae0cd8d-e50f-4d23-8974-b816860f8fc3, CreationTimestamp2023-03-18T09:21:59.341637Z[UTC]] moved to CLOSED state
scm_1    | 2023-03-18 09:22:36,342 [IPC Server handler 1 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: c7fe038c-d6a9-4840-a081-bf56057ff98f, Nodes: 2b374f74-8596-4317-81ba-ce0a491ddd96(ha_dn2_1.ha_net/10.9.0.16), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:2b374f74-8596-4317-81ba-ce0a491ddd96, CreationTimestamp2023-03-18T09:21:59.341011Z[UTC]] moved to CLOSED state
scm_1    | 2023-03-18 09:22:36,343 [IPC Server handler 1 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d1279162-045f-45cf-95f2-d1111d92119b, Nodes: bae0cd8d-e50f-4d23-8974-b816860f8fc3(ha_dn3_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:bae0cd8d-e50f-4d23-8974-b816860f8fc3, CreationTimestamp2023-03-18T09:21:59.341357Z[UTC]] moved to CLOSED state
scm_1    | 2023-03-18 09:22:36,343 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer:   Existing pipelines and containers will be closed during Upgrade.
scm_1    |   New pipelines creation will remain frozen until Upgrade is finalized.
scm_1    | 2023-03-18 09:22:36,343 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: ERASURE_CODED_STORAGE_SUPPORT.
scm_1    | 2023-03-18 09:22:36,356 [IPC Server handler 1 on default port 9860] INFO upgrade.AbstractLayoutVersionManager: Layout feature ERASURE_CODED_STORAGE_SUPPORT has been finalized.
scm_1    | 2023-03-18 09:22:36,356 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: DATANODE_SCHEMA_V3.
scm_1    | 2023-03-18 09:22:36,357 [IPC Server handler 1 on default port 9860] INFO upgrade.AbstractLayoutVersionManager: Layout feature DATANODE_SCHEMA_V3 has been finalized.
scm_1    | 2023-03-18 09:22:36,357 [IPC Server handler 1 on default port 9860] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
scm_1    | 2023-03-18 09:22:36,359 [IPC Server handler 1 on default port 9860] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm_1    | 2023-03-18 09:22:36,359 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15) moved to HEALTHY READONLY state.
scm_1    | 2023-03-18 09:22:36,359 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=67747242-de40-4a6c-83d8-e43a5a79ab67 in state CLOSED which uses HEALTHY_READONLY datanode 4ba198f1-a013-49ce-b059-f73788d69001. This will send close commands for its containers.
scm_1    | 2023-03-18 09:22:36,360 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 2b374f74-8596-4317-81ba-ce0a491ddd96(ha_dn2_1.ha_net/10.9.0.16) moved to HEALTHY READONLY state.
scm_1    | 2023-03-18 09:22:36,360 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=c7fe038c-d6a9-4840-a081-bf56057ff98f in state CLOSED which uses HEALTHY_READONLY datanode 2b374f74-8596-4317-81ba-ce0a491ddd96. This will send close commands for its containers.
scm_1    | 2023-03-18 09:22:36,360 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18) moved to HEALTHY READONLY state.
scm_1    | 2023-03-18 09:22:36,360 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=a2f44939-53c6-4b4c-afd2-40b9ce60bd6c in state CLOSED which uses HEALTHY_READONLY datanode 7c9e7a7d-2593-458c-8a46-0313d77c3278. This will send close commands for its containers.
scm_1    | 2023-03-18 09:22:36,360 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=97be11a2-1597-458e-9379-8191d6d2324d in state CLOSED which uses HEALTHY_READONLY datanode 7c9e7a7d-2593-458c-8a46-0313d77c3278. This will send close commands for its containers.
scm_1    | 2023-03-18 09:22:36,360 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=e319c2ba-524b-42f7-8b03-80313208139c in state CLOSED which uses HEALTHY_READONLY datanode 7c9e7a7d-2593-458c-8a46-0313d77c3278. This will send close commands for its containers.
scm_1    | 2023-03-18 09:22:36,360 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19) moved to HEALTHY READONLY state.
scm_1    | 2023-03-18 09:22:36,360 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=97be11a2-1597-458e-9379-8191d6d2324d in state CLOSED which uses HEALTHY_READONLY datanode 04b88c83-026f-4630-aada-108e37835bee. This will send close commands for its containers.
scm_1    | 2023-03-18 09:22:36,360 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=b5ca130e-c58f-4d3d-8bda-3212c22edf58 in state CLOSED which uses HEALTHY_READONLY datanode 04b88c83-026f-4630-aada-108e37835bee. This will send close commands for its containers.
scm_1    | 2023-03-18 09:22:36,360 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=e319c2ba-524b-42f7-8b03-80313208139c in state CLOSED which uses HEALTHY_READONLY datanode 04b88c83-026f-4630-aada-108e37835bee. This will send close commands for its containers.
scm_1    | 2023-03-18 09:22:36,360 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode bae0cd8d-e50f-4d23-8974-b816860f8fc3(ha_dn3_1.ha_net/10.9.0.17) moved to HEALTHY READONLY state.
scm_1    | 2023-03-18 09:22:36,360 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=97be11a2-1597-458e-9379-8191d6d2324d in state CLOSED which uses HEALTHY_READONLY datanode bae0cd8d-e50f-4d23-8974-b816860f8fc3. This will send close commands for its containers.
scm_1    | 2023-03-18 09:22:36,361 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=e319c2ba-524b-42f7-8b03-80313208139c in state CLOSED which uses HEALTHY_READONLY datanode bae0cd8d-e50f-4d23-8974-b816860f8fc3. This will send close commands for its containers.
scm_1    | 2023-03-18 09:22:36,361 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d1279162-045f-45cf-95f2-d1111d92119b in state CLOSED which uses HEALTHY_READONLY datanode bae0cd8d-e50f-4d23-8974-b816860f8fc3. This will send close commands for its containers.
scm_1    | 2023-03-18 09:22:36,364 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer: SCM Finalization has crossed checkpoint MLV_EQUALS_SLV
scm_1    | 2023-03-18 09:22:36,364 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm_1    | 2023-03-18 09:22:36,365 [RatisPipelineUtilsThread - 0] ERROR pipeline.PipelinePlacementPolicy: No healthy node found to allocate container.
scm_1    | 2023-03-18 09:22:41,364 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm_1    | 2023-03-18 09:22:42,112 [IPC Server handler 2 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:43,796 [IPC Server handler 1 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:44,733 [IPC Server handler 3 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:45,748 [IPC Server handler 3 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:45,792 [IPC Server handler 1 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:45,812 [IPC Server handler 2 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:45,822 [IPC Server handler 0 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:46,365 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm_1    | 2023-03-18 09:22:48,541 [IPC Server handler 24 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:48,592 [IPC Server handler 99 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:49,560 [IPC Server handler 4 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:49,604 [IPC Server handler 3 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:49,700 [IPC Server handler 1 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:49,748 [IPC Server handler 2 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:49,947 [IPC Server handler 62 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:49,948 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode bae0cd8d-e50f-4d23-8974-b816860f8fc3(ha_dn3_1.ha_net/10.9.0.17) reported CLOSED replica.
scm_1    | 2023-03-18 09:22:49,970 [IPC Server handler 63 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:49,988 [IPC Server handler 95 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:50,049 [IPC Server handler 78 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:50,142 [IPC Server handler 97 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:50,142 [FixedThreadPoolWithAffinityExecutor-8-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode 7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18) reported CLOSED replica.
scm_1    | 2023-03-18 09:22:50,182 [IPC Server handler 80 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:50,235 [IPC Server handler 98 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:50,293 [IPC Server handler 92 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:50,294 [IPC Server handler 83 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:50,297 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1002 to CLOSED state, datanode bae0cd8d-e50f-4d23-8974-b816860f8fc3(ha_dn3_1.ha_net/10.9.0.17) reported CLOSED replica.
scm_1    | 2023-03-18 09:22:50,336 [IPC Server handler 69 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:50,357 [IPC Server handler 91 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:50,401 [IPC Server handler 89 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:50,423 [IPC Server handler 94 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:50,423 [FixedThreadPoolWithAffinityExecutor-8-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to CLOSED state, datanode 7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18) reported CLOSED replica.
scm_1    | 2023-03-18 09:22:50,476 [IPC Server handler 31 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:50,494 [IPC Server handler 43 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:22:51,365 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm_1    | 2023-03-18 09:22:56,366 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm_1    | 2023-03-18 09:22:59,479 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=a2f44939-53c6-4b4c-afd2-40b9ce60bd6c since it stays at CLOSED stage.
scm_1    | 2023-03-18 09:22:59,480 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a2f44939-53c6-4b4c-afd2-40b9ce60bd6c close command to datanode 7c9e7a7d-2593-458c-8a46-0313d77c3278
scm_1    | 2023-03-18 09:22:59,482 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: a2f44939-53c6-4b4c-afd2-40b9ce60bd6c, Nodes: 7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:7c9e7a7d-2593-458c-8a46-0313d77c3278, CreationTimestamp2023-03-18T09:21:59.336504Z[UTC]] removed.
scm_1    | 2023-03-18 09:22:59,482 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=97be11a2-1597-458e-9379-8191d6d2324d since it stays at CLOSED stage.
scm_1    | 2023-03-18 09:22:59,482 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=97be11a2-1597-458e-9379-8191d6d2324d close command to datanode 04b88c83-026f-4630-aada-108e37835bee
scm_1    | 2023-03-18 09:22:59,482 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=97be11a2-1597-458e-9379-8191d6d2324d close command to datanode bae0cd8d-e50f-4d23-8974-b816860f8fc3
scm_1    | 2023-03-18 09:22:59,482 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=97be11a2-1597-458e-9379-8191d6d2324d close command to datanode 7c9e7a7d-2593-458c-8a46-0313d77c3278
scm_1    | 2023-03-18 09:22:59,482 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 97be11a2-1597-458e-9379-8191d6d2324d, Nodes: 04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19)bae0cd8d-e50f-4d23-8974-b816860f8fc3(ha_dn3_1.ha_net/10.9.0.17)7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:04b88c83-026f-4630-aada-108e37835bee, CreationTimestamp2023-03-18T09:21:59.325102Z[UTC]] removed.
scm_1    | 2023-03-18 09:22:59,482 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=67747242-de40-4a6c-83d8-e43a5a79ab67 since it stays at CLOSED stage.
scm_1    | 2023-03-18 09:22:59,482 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=67747242-de40-4a6c-83d8-e43a5a79ab67 close command to datanode 4ba198f1-a013-49ce-b059-f73788d69001
scm_1    | 2023-03-18 09:22:59,482 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 67747242-de40-4a6c-83d8-e43a5a79ab67, Nodes: 4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:4ba198f1-a013-49ce-b059-f73788d69001, CreationTimestamp2023-03-18T09:21:59.309606Z[UTC]] removed.
scm_1    | 2023-03-18 09:22:59,483 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=b5ca130e-c58f-4d3d-8bda-3212c22edf58 since it stays at CLOSED stage.
scm_1    | 2023-03-18 09:22:59,483 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=b5ca130e-c58f-4d3d-8bda-3212c22edf58 close command to datanode 04b88c83-026f-4630-aada-108e37835bee
scm_1    | 2023-03-18 09:22:59,483 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: b5ca130e-c58f-4d3d-8bda-3212c22edf58, Nodes: 04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:04b88c83-026f-4630-aada-108e37835bee, CreationTimestamp2023-03-18T09:21:59.340827Z[UTC]] removed.
scm_1    | 2023-03-18 09:22:59,483 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=e319c2ba-524b-42f7-8b03-80313208139c since it stays at CLOSED stage.
scm_1    | 2023-03-18 09:22:59,483 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=e319c2ba-524b-42f7-8b03-80313208139c close command to datanode bae0cd8d-e50f-4d23-8974-b816860f8fc3
scm_1    | 2023-03-18 09:22:59,483 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=e319c2ba-524b-42f7-8b03-80313208139c close command to datanode 04b88c83-026f-4630-aada-108e37835bee
scm_1    | 2023-03-18 09:22:59,483 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=e319c2ba-524b-42f7-8b03-80313208139c close command to datanode 7c9e7a7d-2593-458c-8a46-0313d77c3278
dn4_1    | 2023-03-18 09:23:56,744 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-LeaderElection5] INFO impl.LeaderElection: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-LeaderElection5 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:23:56,744 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-18 09:23:56,744 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-LeaderElection5-1] INFO server.GrpcServerProtocolClient: Build channel for 4ba198f1-a013-49ce-b059-f73788d69001
dn4_1    | 2023-03-18 09:23:56,745 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-18 09:23:56,783 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-LeaderElection5] INFO impl.LeaderElection: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-LeaderElection5: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
dn4_1    | 2023-03-18 09:23:56,783 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-LeaderElection5] INFO impl.LeaderElection:   Response 0: 7c9e7a7d-2593-458c-8a46-0313d77c3278<-4ba198f1-a013-49ce-b059-f73788d69001#0:FAIL-t0
dn4_1    | 2023-03-18 09:23:56,783 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-LeaderElection5] INFO impl.LeaderElection:   Response 1: 7c9e7a7d-2593-458c-8a46-0313d77c3278<-04b88c83-026f-4630-aada-108e37835bee#0:OK-t0
dn4_1    | 2023-03-18 09:23:56,783 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-LeaderElection5] INFO impl.LeaderElection: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-LeaderElection5 PRE_VOTE round 0: result REJECTED
dn4_1    | 2023-03-18 09:23:56,783 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-LeaderElection5] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
dn4_1    | 2023-03-18 09:23:56,784 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-LeaderElection5] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: shutdown 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-LeaderElection5
dn4_1    | 2023-03-18 09:23:56,784 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-LeaderElection5] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: start 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-FollowerState
dn4_1    | 2023-03-18 09:23:56,784 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-18 09:23:56,785 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-18 09:23:57,672 [grpc-default-executor-4] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118: receive requestVote(PRE_VOTE, 4ba198f1-a013-49ce-b059-f73788d69001, group-6F7919707118, 0, (t:0, i:0))
dn4_1    | 2023-03-18 09:23:57,672 [grpc-default-executor-4] INFO impl.VoteContext: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-FOLLOWER: accept PRE_VOTE from 4ba198f1-a013-49ce-b059-f73788d69001: our priority 0 <= candidate's priority 1
dn4_1    | 2023-03-18 09:23:57,672 [grpc-default-executor-4] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118 replies to PRE_VOTE vote request: 4ba198f1-a013-49ce-b059-f73788d69001<-7c9e7a7d-2593-458c-8a46-0313d77c3278#0:OK-t0. Peer's state: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118:t0, leader=null, voted=, raftlog=Memoized:7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:23:57,701 [grpc-default-executor-4] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118: receive requestVote(ELECTION, 4ba198f1-a013-49ce-b059-f73788d69001, group-6F7919707118, 1, (t:0, i:0))
dn4_1    | 2023-03-18 09:23:57,702 [grpc-default-executor-4] INFO impl.VoteContext: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-FOLLOWER: accept ELECTION from 4ba198f1-a013-49ce-b059-f73788d69001: our priority 0 <= candidate's priority 1
dn4_1    | 2023-03-18 09:23:57,702 [grpc-default-executor-4] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:4ba198f1-a013-49ce-b059-f73788d69001
dn4_1    | 2023-03-18 09:23:57,702 [grpc-default-executor-4] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: shutdown 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-FollowerState
dn4_1    | 2023-03-18 09:23:57,702 [grpc-default-executor-4] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: start 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-FollowerState
dn4_1    | 2023-03-18 09:23:57,702 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-FollowerState] INFO impl.FollowerState: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-FollowerState was interrupted
dn4_1    | 2023-03-18 09:23:57,712 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-18 09:23:57,712 [grpc-default-executor-4] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118 replies to ELECTION vote request: 4ba198f1-a013-49ce-b059-f73788d69001<-7c9e7a7d-2593-458c-8a46-0313d77c3278#0:OK-t1. Peer's state: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118:t1, leader=null, voted=4ba198f1-a013-49ce-b059-f73788d69001, raftlog=Memoized:7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:23:57,713 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-18 09:23:57,826 [7c9e7a7d-2593-458c-8a46-0313d77c3278-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6F7919707118 with new leaderId: 4ba198f1-a013-49ce-b059-f73788d69001
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:06,996 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:06,996 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,037 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
dn4_1    | 2023-03-18 09:23:57,827 [7c9e7a7d-2593-458c-8a46-0313d77c3278-server-thread1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118: change Leader from null to 4ba198f1-a013-49ce-b059-f73788d69001 at term 1 for appendEntries, leader elected after 6319ms
dn4_1    | 2023-03-18 09:23:57,845 [7c9e7a7d-2593-458c-8a46-0313d77c3278-server-thread2] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118: set configuration 0: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:23:57,846 [7c9e7a7d-2593-458c-8a46-0313d77c3278-server-thread2] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-03-18 09:23:57,848 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-6F7919707118-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/66de196e-0a1c-4e0b-be4e-6f7919707118/current/log_inprogress_0
dn4_1    | 2023-03-18 09:23:57,862 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-FollowerState] INFO impl.FollowerState: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5065638715ns, electionTimeout:5037ms
dn4_1    | 2023-03-18 09:23:57,862 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-FollowerState] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: shutdown 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-FollowerState
dn4_1    | 2023-03-18 09:23:57,862 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-FollowerState] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn4_1    | 2023-03-18 09:23:57,863 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | 2023-03-18 09:23:57,863 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-FollowerState] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: start 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-LeaderElection6
dn4_1    | 2023-03-18 09:23:57,864 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-LeaderElection6] INFO impl.LeaderElection: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-LeaderElection6 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:23:57,871 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-18 09:23:57,872 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-18 09:23:57,885 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-LeaderElection6] INFO impl.LeaderElection: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-LeaderElection6: PRE_VOTE REJECTED received 1 response(s) and 0 exception(s):
dn4_1    | 2023-03-18 09:23:57,885 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-LeaderElection6] INFO impl.LeaderElection:   Response 0: 7c9e7a7d-2593-458c-8a46-0313d77c3278<-04b88c83-026f-4630-aada-108e37835bee#0:FAIL-t0
dn4_1    | 2023-03-18 09:23:57,886 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-LeaderElection6] INFO impl.LeaderElection: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-LeaderElection6 PRE_VOTE round 0: result REJECTED
dn4_1    | 2023-03-18 09:23:57,886 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-LeaderElection6] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
dn4_1    | 2023-03-18 09:23:57,886 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-LeaderElection6] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: shutdown 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-LeaderElection6
dn4_1    | 2023-03-18 09:23:57,886 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-LeaderElection6] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: start 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-FollowerState
dn4_1    | 2023-03-18 09:23:57,896 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-18 09:23:57,900 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-18 09:23:58,141 [grpc-default-executor-4] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193: receive requestVote(PRE_VOTE, 04b88c83-026f-4630-aada-108e37835bee, group-CCB5F87F8193, 0, (t:0, i:0))
dn4_1    | 2023-03-18 09:23:58,141 [grpc-default-executor-4] INFO impl.VoteContext: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-FOLLOWER: accept PRE_VOTE from 04b88c83-026f-4630-aada-108e37835bee: our priority 0 <= candidate's priority 1
dn4_1    | 2023-03-18 09:23:58,141 [grpc-default-executor-4] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193 replies to PRE_VOTE vote request: 04b88c83-026f-4630-aada-108e37835bee<-7c9e7a7d-2593-458c-8a46-0313d77c3278#0:OK-t0. Peer's state: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193:t0, leader=null, voted=, raftlog=Memoized:7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:23:58,158 [grpc-default-executor-4] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193: receive requestVote(ELECTION, 04b88c83-026f-4630-aada-108e37835bee, group-CCB5F87F8193, 1, (t:0, i:0))
dn4_1    | 2023-03-18 09:23:58,158 [grpc-default-executor-4] INFO impl.VoteContext: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-FOLLOWER: accept ELECTION from 04b88c83-026f-4630-aada-108e37835bee: our priority 0 <= candidate's priority 1
dn4_1    | 2023-03-18 09:23:58,158 [grpc-default-executor-4] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:04b88c83-026f-4630-aada-108e37835bee
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,038 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,039 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,040 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,040 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
dn4_1    | 2023-03-18 09:23:58,158 [grpc-default-executor-4] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: shutdown 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-FollowerState
dn4_1    | 2023-03-18 09:23:58,158 [grpc-default-executor-4] INFO impl.RoleInfo: 7c9e7a7d-2593-458c-8a46-0313d77c3278: start 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-FollowerState
dn4_1    | 2023-03-18 09:23:58,159 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-FollowerState] INFO impl.FollowerState: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-FollowerState was interrupted
dn4_1    | 2023-03-18 09:23:58,165 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-18 09:23:58,166 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-18 09:23:58,166 [grpc-default-executor-4] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193 replies to ELECTION vote request: 04b88c83-026f-4630-aada-108e37835bee<-7c9e7a7d-2593-458c-8a46-0313d77c3278#0:OK-t1. Peer's state: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193:t1, leader=null, voted=04b88c83-026f-4630-aada-108e37835bee, raftlog=Memoized:7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:23:58,230 [7c9e7a7d-2593-458c-8a46-0313d77c3278-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-CCB5F87F8193 with new leaderId: 04b88c83-026f-4630-aada-108e37835bee
dn4_1    | 2023-03-18 09:23:58,230 [7c9e7a7d-2593-458c-8a46-0313d77c3278-server-thread1] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193: change Leader from null to 04b88c83-026f-4630-aada-108e37835bee at term 1 for appendEntries, leader elected after 5858ms
dn4_1    | 2023-03-18 09:23:58,242 [7c9e7a7d-2593-458c-8a46-0313d77c3278-server-thread2] INFO server.RaftServer$Division: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193: set configuration 0: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-18 09:23:58,243 [7c9e7a7d-2593-458c-8a46-0313d77c3278-server-thread2] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-03-18 09:23:58,245 [7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7c9e7a7d-2593-458c-8a46-0313d77c3278@group-CCB5F87F8193-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/84a08f9d-e905-447e-be3d-ccb5f87f8193/current/log_inprogress_0
dn4_1    | 2023-03-18 09:24:10,114 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 3/4997 blocks from 2 candidate containers.
dn4_1    | 2023-03-18 09:24:10,139 [BlockDeletingService#0] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/current/containerDir0/2/chunks/111677748019200002.block
dn4_1    | 2023-03-18 09:24:10,141 [BlockDeletingService#0] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/current/containerDir0/2/chunks/111677748019200003.block
dn4_1    | 2023-03-18 09:24:10,140 [BlockDeletingService#2] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/current/containerDir0/1/chunks/111677748019200001.block
dn4_1    | 2023-03-18 09:25:10,126 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn4_1    | 2023-03-18 09:26:10,126 [BlockDeletingService#4] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1    | 2023-03-18 09:22:59,484 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: e319c2ba-524b-42f7-8b03-80313208139c, Nodes: bae0cd8d-e50f-4d23-8974-b816860f8fc3(ha_dn3_1.ha_net/10.9.0.17)04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19)7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:bae0cd8d-e50f-4d23-8974-b816860f8fc3, CreationTimestamp2023-03-18T09:21:59.341637Z[UTC]] removed.
scm_1    | 2023-03-18 09:22:59,484 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=c7fe038c-d6a9-4840-a081-bf56057ff98f since it stays at CLOSED stage.
scm_1    | 2023-03-18 09:22:59,484 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=c7fe038c-d6a9-4840-a081-bf56057ff98f close command to datanode 2b374f74-8596-4317-81ba-ce0a491ddd96
scm_1    | 2023-03-18 09:22:59,484 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: c7fe038c-d6a9-4840-a081-bf56057ff98f, Nodes: 2b374f74-8596-4317-81ba-ce0a491ddd96(ha_dn2_1.ha_net/10.9.0.16), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:2b374f74-8596-4317-81ba-ce0a491ddd96, CreationTimestamp2023-03-18T09:21:59.341011Z[UTC]] removed.
scm_1    | 2023-03-18 09:22:59,484 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=d1279162-045f-45cf-95f2-d1111d92119b since it stays at CLOSED stage.
scm_1    | 2023-03-18 09:22:59,484 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=d1279162-045f-45cf-95f2-d1111d92119b close command to datanode bae0cd8d-e50f-4d23-8974-b816860f8fc3
scm_1    | 2023-03-18 09:22:59,484 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: d1279162-045f-45cf-95f2-d1111d92119b, Nodes: bae0cd8d-e50f-4d23-8974-b816860f8fc3(ha_dn3_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:bae0cd8d-e50f-4d23-8974-b816860f8fc3, CreationTimestamp2023-03-18T09:21:59.341357Z[UTC]] removed.
scm_1    | 2023-03-18 09:23:01,366 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm_1    | 2023-03-18 09:23:03,333 [SCMBlockDeletingService#0] INFO block.SCMBlockDeletingService: Totally added 9 blocks to be deleted for 3 datanodes, task elapsed time: 6ms
scm_1    | 2023-03-18 09:23:06,366 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm_1    | 2023-03-18 09:23:06,367 [RatisPipelineUtilsThread - 0] ERROR pipeline.PipelinePlacementPolicy: No healthy node found to allocate container.
scm_1    | 2023-03-18 09:23:11,366 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm_1    | 2023-03-18 09:23:12,106 [IPC Server handler 97 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:23:12,109 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=67747242-de40-4a6c-83d8-e43a5a79ab67 is not found
scm_1    | 2023-03-18 09:23:13,794 [IPC Server handler 0 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:23:16,367 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm_1    | 2023-03-18 09:23:20,503 [IPC Server handler 43 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-18 09:23:21,368 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm_1    | 2023-03-18 09:23:22,606 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18) moved to HEALTHY state.
scm_1    | 2023-03-18 09:23:22,608 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1    | 2023-03-18 09:23:22,608 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19) moved to HEALTHY state.
scm_1    | 2023-03-18 09:23:22,609 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1    | 2023-03-18 09:23:22,611 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=3538aed8-d817-47ae-96b3-bde6dabfbcfe to datanode:7c9e7a7d-2593-458c-8a46-0313d77c3278
scm_1    | 2023-03-18 09:23:22,612 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 3538aed8-d817-47ae-96b3-bde6dabfbcfe, Nodes: 7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-18T09:23:22.610Z[UTC]].
scm_1    | 2023-03-18 09:23:22,615 [RatisPipelineUtilsThread - 0] ERROR scm.SCMCommonPlacementPolicy: Unable to find enough nodes that meet the space requirement of 1073741824 bytes for metadata and 1073741824 bytes for data in healthy node set. Required 3. Found 2.
scm_1    | 2023-03-18 09:23:22,616 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=087cc0b6-0f7f-40d8-82ec-5da9883170e4 to datanode:04b88c83-026f-4630-aada-108e37835bee
scm_1    | 2023-03-18 09:23:22,616 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 087cc0b6-0f7f-40d8-82ec-5da9883170e4, Nodes: 04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-18T09:23:22.616Z[UTC]].
scm_1    | 2023-03-18 09:23:22,617 [RatisPipelineUtilsThread - 0] ERROR scm.SCMCommonPlacementPolicy: Unable to find enough nodes that meet the space requirement of 1073741824 bytes for metadata and 1073741824 bytes for data in healthy node set. Required 3. Found 2.
scm_1    | 2023-03-18 09:23:26,368 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm_1    | 2023-03-18 09:23:31,369 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm_1    | 2023-03-18 09:23:36,369 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm_1    | 2023-03-18 09:23:41,369 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm_1    | 2023-03-18 09:23:43,607 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15) moved to HEALTHY state.
scm_1    | 2023-03-18 09:23:43,607 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1    | 2023-03-18 09:23:43,608 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d5743016-6c44-4b2d-ae26-d4e599c27fe1 to datanode:4ba198f1-a013-49ce-b059-f73788d69001
scm_1    | 2023-03-18 09:23:43,609 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d5743016-6c44-4b2d-ae26-d4e599c27fe1, Nodes: 4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-18T09:23:43.608Z[UTC]].
scm_1    | 2023-03-18 09:23:43,610 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=66de196e-0a1c-4e0b-be4e-6f7919707118 to datanode:7c9e7a7d-2593-458c-8a46-0313d77c3278
scm_1    | 2023-03-18 09:23:43,611 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=66de196e-0a1c-4e0b-be4e-6f7919707118 to datanode:04b88c83-026f-4630-aada-108e37835bee
scm_1    | 2023-03-18 09:23:43,611 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=66de196e-0a1c-4e0b-be4e-6f7919707118 to datanode:4ba198f1-a013-49ce-b059-f73788d69001
scm_1    | 2023-03-18 09:23:43,611 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 66de196e-0a1c-4e0b-be4e-6f7919707118, Nodes: 7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18)04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19)4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-18T09:23:43.610Z[UTC]].
scm_1    | 2023-03-18 09:23:43,612 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=84a08f9d-e905-447e-be3d-ccb5f87f8193 to datanode:04b88c83-026f-4630-aada-108e37835bee
scm_1    | 2023-03-18 09:23:43,612 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=84a08f9d-e905-447e-be3d-ccb5f87f8193 to datanode:7c9e7a7d-2593-458c-8a46-0313d77c3278
scm_1    | 2023-03-18 09:23:43,612 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=84a08f9d-e905-447e-be3d-ccb5f87f8193 to datanode:4ba198f1-a013-49ce-b059-f73788d69001
scm_1    | 2023-03-18 09:23:43,613 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 84a08f9d-e905-447e-be3d-ccb5f87f8193, Nodes: 04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19)7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18)4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-18T09:23:43.612Z[UTC]].
scm_1    | 2023-03-18 09:23:43,613 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=84a08f9d-e905-447e-be3d-ccb5f87f8193 contains same datanodes as previous pipelines: PipelineID=66de196e-0a1c-4e0b-be4e-6f7919707118 nodeIds: 04b88c83-026f-4630-aada-108e37835bee, 7c9e7a7d-2593-458c-8a46-0313d77c3278, 4ba198f1-a013-49ce-b059-f73788d69001
scm_1    | 2023-03-18 09:23:43,613 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0.
scm_1    | 2023-03-18 09:23:46,369 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm_1    | 2023-03-18 09:23:46,607 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 2b374f74-8596-4317-81ba-ce0a491ddd96(ha_dn2_1.ha_net/10.9.0.16) moved to HEALTHY state.
scm_1    | 2023-03-18 09:23:46,607 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1    | 2023-03-18 09:23:46,608 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=747ad178-d87d-4bbd-985b-ed015d5b9b4c to datanode:2b374f74-8596-4317-81ba-ce0a491ddd96
scm_1    | 2023-03-18 09:23:46,609 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 747ad178-d87d-4bbd-985b-ed015d5b9b4c, Nodes: 2b374f74-8596-4317-81ba-ce0a491ddd96(ha_dn2_1.ha_net/10.9.0.16), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-18T09:23:46.608Z[UTC]].
scm_1    | 2023-03-18 09:23:46,609 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1.
scm_1    | 2023-03-18 09:23:51,369 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm_1    | 2023-03-18 09:23:51,450 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 3538aed8-d817-47ae-96b3-bde6dabfbcfe, Nodes: 7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:7c9e7a7d-2593-458c-8a46-0313d77c3278, CreationTimestamp2023-03-18T09:23:22.610Z[UTC]] moved to OPEN state
scm_1    | 2023-03-18 09:23:51,543 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 087cc0b6-0f7f-40d8-82ec-5da9883170e4, Nodes: 04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:04b88c83-026f-4630-aada-108e37835bee, CreationTimestamp2023-03-18T09:23:22.616Z[UTC]] moved to OPEN state
scm_1    | 2023-03-18 09:23:52,608 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode bae0cd8d-e50f-4d23-8974-b816860f8fc3(ha_dn3_1.ha_net/10.9.0.17) moved to HEALTHY state.
scm_1    | 2023-03-18 09:23:52,608 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1    | 2023-03-18 09:23:52,609 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=744854b1-789c-4dc8-9bd9-2009d4edbe4a to datanode:bae0cd8d-e50f-4d23-8974-b816860f8fc3
scm_1    | 2023-03-18 09:23:52,609 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 744854b1-789c-4dc8-9bd9-2009d4edbe4a, Nodes: bae0cd8d-e50f-4d23-8974-b816860f8fc3(ha_dn3_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-18T09:23:52.608Z[UTC]].
scm_1    | 2023-03-18 09:23:52,610 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
scm_1    | 2023-03-18 09:23:56,370 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm_1    | 2023-03-18 09:23:57,722 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 66de196e-0a1c-4e0b-be4e-6f7919707118, Nodes: 7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18)04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19)4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:4ba198f1-a013-49ce-b059-f73788d69001, CreationTimestamp2023-03-18T09:23:43.610Z[UTC]] moved to OPEN state
om3_1    | 2023-03-18 09:22:27,574 [om3@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om3_1    | 2023-03-18 09:22:27,576 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1    | 2023-03-18 09:22:27,576 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1    | 2023-03-18 09:22:27,577 [om3@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om3_1    | 2023-03-18 09:22:27,577 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om3_1    | 2023-03-18 09:22:27,582 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om3_1    | 2023-03-18 09:22:27,582 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-03-18 09:22:27,582 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om3_1    | 2023-03-18 09:22:27,584 [om3@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om3_1    | 2023-03-18 09:22:27,584 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1    | 2023-03-18 09:22:27,584 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1    | 2023-03-18 09:22:27,584 [om3@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om3_1    | 2023-03-18 09:22:27,584 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om3_1    | 2023-03-18 09:22:27,587 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-LeaderStateImpl
om3_1    | 2023-03-18 09:22:27,592 [om3@group-D66704EFC61C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: Rolling segment log-69_104 to index:104
om3_1    | 2023-03-18 09:22:27,596 [om3@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_69 to /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_69-104
om3_1    | 2023-03-18 09:22:27,617 [om3@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_105
om3_1    | 2023-03-18 09:22:27,630 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 105: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-03-18 09:22:27,630 [grpc-default-executor-0] INFO impl.VoteContext: om3@group-D66704EFC61C-LEADER: reject PRE_VOTE from om1: this server is the leader and still has leadership
om3_1    | 2023-03-18 09:22:27,631 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to PRE_VOTE vote request: om1<-om3#0:FAIL-t8. Peer's state: om3@group-D66704EFC61C:t8, leader=om3, voted=om3, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c104, conf=105: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-03-18 09:22:28,340 [om3@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om3_1    | [id: "om1"
om3_1    | address: "om1:9872"
om3_1    | startupRole: FOLLOWER
om3_1    | , id: "om3"
om3_1    | address: "om3:9872"
om3_1    | startupRole: FOLLOWER
om3_1    | , id: "om2"
om3_1    | address: "om2:9872"
om3_1    | startupRole: FOLLOWER
om3_1    | ]
om3_1    | 2023-03-18 09:23:06,584 [IPC Server handler 2 on default port 9862] WARN db.RDBStore: Unable to get delta updates since sequenceNumber 110. This exception will be thrown to the client
om3_1    | org.apache.hadoop.hdds.utils.db.SequenceNumberNotFoundException: Invalid transaction log iterator when getting updates since sequence number 110
om3_1    | 	at org.apache.hadoop.hdds.utils.db.RDBStore.getUpdatesSince(RDBStore.java:375)
om3_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.getDBUpdates(OzoneManager.java:3986)
om3_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getOMDBUpdates(OzoneManagerRequestHandler.java:354)
om3_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleReadRequest(OzoneManagerRequestHandler.java:233)
om3_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:223)
om3_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:177)
om3_1    | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
om3_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
om3_1    | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om3_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om3_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om3_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om3_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om3_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om3_1    | 2023-03-18 09:23:06,793 [qtp655183968-135] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
om3_1    | 2023-03-18 09:23:06,798 [qtp655183968-135] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.checkpoints/om.db_checkpoint_1679131386794 in 3 milliseconds
om3_1    | 2023-03-18 09:23:06,820 [qtp655183968-135] INFO db.RDBCheckpointManager: Waited for 21 milliseconds for checkpoint directory /data/metadata/db.checkpoints/om.db_checkpoint_1679131386794 availability.
om3_1    | 2023-03-18 09:23:06,849 [qtp655183968-135] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 28 milliseconds
dn5_1    | 2023-03-18 09:23:51,535 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-03-18 09:23:51,535 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-03-18 09:23:51,535 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-03-18 09:23:51,536 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-03-18 09:23:51,536 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-03-18 09:23:51,536 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-03-18 09:23:51,537 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-03-18 09:23:51,544 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-18 09:23:51,560 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-03-18 09:23:51,560 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-03-18 09:23:51,562 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-03-18 09:23:51,562 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-03-18 09:23:51,562 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-03-18 09:23:51,564 [pool-38-thread-1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4: start as a follower, conf=-1: peers:[04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:23:51,564 [pool-38-thread-1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn5_1    | 2023-03-18 09:23:51,564 [pool-38-thread-1] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: start 04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-FollowerState
dn5_1    | 2023-03-18 09:23:51,569 [pool-38-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-5DA9883170E4,id=04b88c83-026f-4630-aada-108e37835bee
dn5_1    | 2023-03-18 09:23:51,569 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-03-18 09:23:51,571 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-03-18 09:23:51,569 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-18 09:23:51,585 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-18 09:23:51,585 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-03-18 09:23:51,585 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-03-18 09:23:51,586 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=087cc0b6-0f7f-40d8-82ec-5da9883170e4
dn5_1    | 2023-03-18 09:23:51,587 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=087cc0b6-0f7f-40d8-82ec-5da9883170e4.
dn5_1    | 2023-03-18 09:23:51,587 [Command processor thread] INFO server.RaftServer: 04b88c83-026f-4630-aada-108e37835bee: addNew group-6F7919707118:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER] returns group-6F7919707118:java.util.concurrent.CompletableFuture@12641501[Not completed]
dn5_1    | 2023-03-18 09:23:51,589 [pool-38-thread-1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee: new RaftServerImpl for group-6F7919707118:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn5_1    | 2023-03-18 09:23:51,589 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-03-18 09:23:51,589 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-03-18 09:23:51,589 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-03-18 09:23:51,589 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-03-18 09:23:51,589 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-03-18 09:23:51,590 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-03-18 09:23:51,590 [pool-38-thread-1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118: ConfigurationManager, init=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-03-18 09:23:51,590 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-03-18 09:23:51,590 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-03-18 09:23:51,590 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-03-18 09:23:51,590 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-03-18 09:23:51,591 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,040 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.delete(OMDBUpdatesHandler.java:77)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,040 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,040 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
dn5_1    | 2023-03-18 09:23:51,591 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-03-18 09:23:51,591 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-03-18 09:23:51,592 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-03-18 09:23:51,592 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-03-18 09:23:51,592 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-03-18 09:23:51,592 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-03-18 09:23:51,592 [pool-38-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/66de196e-0a1c-4e0b-be4e-6f7919707118 does not exist. Creating ...
dn5_1    | 2023-03-18 09:23:51,594 [pool-38-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/66de196e-0a1c-4e0b-be4e-6f7919707118/in_use.lock acquired by nodename 7@e51427cbfe02
dn5_1    | 2023-03-18 09:23:51,595 [pool-38-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/66de196e-0a1c-4e0b-be4e-6f7919707118 has been successfully formatted.
dn5_1    | 2023-03-18 09:23:51,596 [pool-38-thread-1] INFO ratis.ContainerStateMachine: group-6F7919707118: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn5_1    | 2023-03-18 09:23:51,596 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-03-18 09:23:51,596 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-03-18 09:23:51,597 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-18 09:23:51,597 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-03-18 09:23:51,597 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-03-18 09:23:51,597 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-03-18 09:23:51,609 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-03-18 09:23:51,609 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-03-18 09:23:51,610 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: new 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/66de196e-0a1c-4e0b-be4e-6f7919707118
dn5_1    | 2023-03-18 09:23:51,612 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-03-18 09:23:51,612 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-03-18 09:23:51,613 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-03-18 09:23:51,613 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-03-18 09:23:51,613 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-03-18 09:23:51,613 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-03-18 09:23:51,613 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-03-18 09:23:51,613 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-03-18 09:23:51,614 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-03-18 09:23:51,614 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-18 09:23:51,710 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-03-18 09:23:51,710 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-03-18 09:23:51,710 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-03-18 09:23:51,710 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-03-18 09:23:51,710 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-03-18 09:23:51,754 [pool-38-thread-1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118: start as a follower, conf=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:23:51,754 [pool-38-thread-1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn5_1    | 2023-03-18 09:23:51,754 [pool-38-thread-1] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: start 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118-FollowerState
dn5_1    | 2023-03-18 09:23:51,784 [04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-18 09:23:51,786 [04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-18 09:23:51,784 [pool-38-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6F7919707118,id=04b88c83-026f-4630-aada-108e37835bee
dn5_1    | 2023-03-18 09:23:51,787 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-03-18 09:23:51,787 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-03-18 09:23:51,787 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-03-18 09:23:51,787 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-03-18 09:23:51,788 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=66de196e-0a1c-4e0b-be4e-6f7919707118
dn5_1    | 2023-03-18 09:23:52,577 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=66de196e-0a1c-4e0b-be4e-6f7919707118.
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,041 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,041 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,041 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,041 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
om3_1    | 2023-03-18 09:23:06,849 [qtp655183968-135] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1679131386794
om3_1    | 2023-03-18 09:24:04,633 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization started.
om3_1    | 2023-03-18 09:24:04,634 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: ERASURE_CODED_STORAGE_SUPPORT.
om3_1    | 2023-03-18 09:24:04,635 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature ERASURE_CODED_STORAGE_SUPPORT has been finalized.
om3_1    | 2023-03-18 09:24:04,636 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: BUCKET_LAYOUT_SUPPORT.
om3_1    | 2023-03-18 09:24:04,637 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature BUCKET_LAYOUT_SUPPORT has been finalized.
om3_1    | 2023-03-18 09:24:04,637 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: MULTITENANCY_SCHEMA.
om3_1    | 2023-03-18 09:24:04,638 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature MULTITENANCY_SCHEMA has been finalized.
om3_1    | 2023-03-18 09:24:04,638 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
om3_1    | 2023-03-18 09:24:04,638 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization is done.
om3_1    | 2023-03-18 09:24:04,643 [OMDoubleBufferFlushThread] INFO upgrade.OMFinalizeUpgradeResponse: Layout version to persist to DB : 3
om3_1    | 2023-03-18 09:25:13,750 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:new2-volume for user:hadoop
om3_1    | 2023-03-18 09:25:16,388 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: new2-volume
om3_1    | 2023-03-18 09:25:24,641 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: s3v
om3_1    | 2023-03-18 09:25:32,164 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:new2-bucket in volume:s3v
om3_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om3_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:207)
om3_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:337)
om3_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om3_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om3_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om3_1    | 2023-03-18 09:25:51,357 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:ectest-new for user:hadoop
om3_1    | 2023-03-18 09:25:53,922 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ectest of layout FILE_SYSTEM_OPTIMIZED in volume: ectest-new
om3_1    | 2023-03-18 09:25:59,151 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: testpropchange of layout FILE_SYSTEM_OPTIMIZED in volume: ectest-new
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,044 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,045 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,045 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
dn5_1    | 2023-03-18 09:23:52,585 [pool-38-thread-1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee: new RaftServerImpl for group-CCB5F87F8193:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn5_1    | 2023-03-18 09:23:52,589 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-03-18 09:23:52,590 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-03-18 09:23:52,591 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-03-18 09:23:52,591 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-03-18 09:23:52,591 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-03-18 09:23:52,591 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-03-18 09:23:52,592 [pool-38-thread-1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193: ConfigurationManager, init=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-03-18 09:23:52,593 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-03-18 09:23:52,594 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-03-18 09:23:52,594 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-03-18 09:23:52,595 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-03-18 09:23:52,595 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-03-18 09:23:52,595 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-03-18 09:23:52,600 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-03-18 09:23:52,601 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-03-18 09:23:52,601 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-03-18 09:23:52,603 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-03-18 09:23:52,603 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-03-18 09:23:52,616 [Command processor thread] INFO server.RaftServer: 04b88c83-026f-4630-aada-108e37835bee: addNew group-CCB5F87F8193:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] returns      null 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193:t0, leader=null, voted=null, raftlog=UNINITIALIZED, conf=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null NEW
dn5_1    | 2023-03-18 09:23:52,618 [pool-38-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/84a08f9d-e905-447e-be3d-ccb5f87f8193 does not exist. Creating ...
dn5_1    | 2023-03-18 09:23:52,619 [pool-38-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/84a08f9d-e905-447e-be3d-ccb5f87f8193/in_use.lock acquired by nodename 7@e51427cbfe02
dn5_1    | 2023-03-18 09:23:52,624 [pool-38-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/84a08f9d-e905-447e-be3d-ccb5f87f8193 has been successfully formatted.
dn5_1    | 2023-03-18 09:23:52,625 [pool-38-thread-1] INFO ratis.ContainerStateMachine: group-CCB5F87F8193: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn5_1    | 2023-03-18 09:23:52,625 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-03-18 09:23:52,625 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-03-18 09:23:52,626 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-18 09:23:52,626 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-03-18 09:23:52,626 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-03-18 09:23:52,626 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-03-18 09:23:52,627 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-03-18 09:23:52,627 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-03-18 09:23:52,628 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: new 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/84a08f9d-e905-447e-be3d-ccb5f87f8193
dn5_1    | 2023-03-18 09:23:52,628 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-03-18 09:23:52,628 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-03-18 09:23:52,628 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-03-18 09:23:52,629 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-03-18 09:23:52,629 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-03-18 09:23:52,629 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1    | 2023-03-18 09:23:58,171 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 84a08f9d-e905-447e-be3d-ccb5f87f8193, Nodes: 04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19)7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18)4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:04b88c83-026f-4630-aada-108e37835bee, CreationTimestamp2023-03-18T09:23:43.612Z[UTC]] moved to OPEN state
scm_1    | 2023-03-18 09:24:01,370 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer: Open pipeline found after SCM finalization
scm_1    | 2023-03-18 09:24:01,371 [IPC Server handler 1 on default port 9860] INFO upgrade.UpgradeFinalizer: Finalization is done.
scm_1    | 2023-03-18 09:24:13,226 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d5743016-6c44-4b2d-ae26-d4e599c27fe1, Nodes: 4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:4ba198f1-a013-49ce-b059-f73788d69001, CreationTimestamp2023-03-18T09:23:43.608Z[UTC]] moved to OPEN state
scm_1    | 2023-03-18 09:24:14,863 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 747ad178-d87d-4bbd-985b-ed015d5b9b4c, Nodes: 2b374f74-8596-4317-81ba-ce0a491ddd96(ha_dn2_1.ha_net/10.9.0.16), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:2b374f74-8596-4317-81ba-ce0a491ddd96, CreationTimestamp2023-03-18T09:23:46.608Z[UTC]] moved to OPEN state
scm_1    | 2023-03-18 09:24:21,503 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 744854b1-789c-4dc8-9bd9-2009d4edbe4a, Nodes: bae0cd8d-e50f-4d23-8974-b816860f8fc3(ha_dn3_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:bae0cd8d-e50f-4d23-8974-b816860f8fc3, CreationTimestamp2023-03-18T09:23:52.608Z[UTC]] moved to OPEN state
scm_1    | 2023-03-18 09:24:22,613 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
scm_1    | 2023-03-18 09:24:52,614 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
scm_1    | 2023-03-18 09:25:19,155 [IPC Server handler 0 on default port 9863] WARN ha.SequenceIdGenerator: Failed to allocate a batch for containerId, expected lastId is 0, actual lastId is 2000.
scm_1    | 2023-03-18 09:25:19,156 [IPC Server handler 0 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 2000 to 3000.
scm_1    | 2023-03-18 09:25:19,158 [IPC Server handler 0 on default port 9863] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019203000.
scm_1    | 2023-03-18 09:25:19,158 [IPC Server handler 0 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019203000 to 111677748019204000.
scm_1    | 2023-03-18 09:25:22,617 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
scm_1    | 2023-03-18 09:25:52,618 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
scm_1    | 2023-03-18 09:26:07,163 [IPC Server handler 0 on default port 9863] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 4c5c51d5-7f71-4265-ae16-de8d73f2a2b1, Nodes: 2b374f74-8596-4317-81ba-ce0a491ddd96(ha_dn2_1.ha_net/10.9.0.16)4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15)7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18)bae0cd8d-e50f-4d23-8974-b816860f8fc3(ha_dn3_1.ha_net/10.9.0.17)04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19), ReplicationConfig: EC{rs-3-2-1048576}, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-18T09:26:07.156Z[UTC]].
scm_1    | 2023-03-18 09:26:07,168 [IPC Server handler 0 on default port 9863] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 4c5c51d5-7f71-4265-ae16-de8d73f2a2b1, Nodes: 2b374f74-8596-4317-81ba-ce0a491ddd96(ha_dn2_1.ha_net/10.9.0.16)4ba198f1-a013-49ce-b059-f73788d69001(ha_dn1_1.ha_net/10.9.0.15)7c9e7a7d-2593-458c-8a46-0313d77c3278(ha_dn4_1.ha_net/10.9.0.18)bae0cd8d-e50f-4d23-8974-b816860f8fc3(ha_dn3_1.ha_net/10.9.0.17)04b88c83-026f-4630-aada-108e37835bee(ha_dn5_1.ha_net/10.9.0.19), ReplicationConfig: EC{rs-3-2-1048576}, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-18T09:26:07.156Z[UTC]] moved to OPEN state
dn5_1    | 2023-03-18 09:23:52,634 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-03-18 09:23:52,634 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-03-18 09:23:52,636 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-03-18 09:23:52,637 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-18 09:23:52,902 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-03-18 09:23:52,903 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-03-18 09:23:52,903 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-03-18 09:23:52,903 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-03-18 09:23:52,903 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-03-18 09:23:52,915 [pool-38-thread-1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193: start as a follower, conf=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:23:52,916 [pool-38-thread-1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn5_1    | 2023-03-18 09:23:52,916 [pool-38-thread-1] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: start 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-FollowerState
dn5_1    | 2023-03-18 09:23:52,916 [pool-38-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-CCB5F87F8193,id=04b88c83-026f-4630-aada-108e37835bee
dn5_1    | 2023-03-18 09:23:52,916 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-03-18 09:23:52,917 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-03-18 09:23:52,917 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-03-18 09:23:52,917 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-03-18 09:23:52,917 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-18 09:23:52,918 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=84a08f9d-e905-447e-be3d-ccb5f87f8193
dn5_1    | 2023-03-18 09:23:52,925 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-18 09:23:53,177 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=84a08f9d-e905-447e-be3d-ccb5f87f8193.
dn5_1    | 2023-03-18 09:23:56,688 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-FollowerState] INFO impl.FollowerState: 04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5123596870ns, electionTimeout:5103ms
dn5_1    | 2023-03-18 09:23:56,688 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-FollowerState] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: shutdown 04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-FollowerState
dn5_1    | 2023-03-18 09:23:56,688 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-FollowerState] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn5_1    | 2023-03-18 09:23:56,689 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-03-18 09:23:56,689 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-FollowerState] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: start 04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4
dn5_1    | 2023-03-18 09:23:56,689 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4] INFO impl.LeaderElection: 04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:23:56,689 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4] INFO impl.LeaderElection: 04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4 PRE_VOTE round 0: result PASSED (term=0)
dn5_1    | 2023-03-18 09:23:56,691 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4] INFO impl.LeaderElection: 04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4 ELECTION round 0: submit vote requests at term 1 for -1: peers:[04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:23:56,691 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4] INFO impl.LeaderElection: 04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4 ELECTION round 0: result PASSED (term=1)
dn5_1    | 2023-03-18 09:23:56,691 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: shutdown 04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4
dn5_1    | 2023-03-18 09:23:56,691 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn5_1    | 2023-03-18 09:23:56,691 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-5DA9883170E4 with new leaderId: 04b88c83-026f-4630-aada-108e37835bee
dn5_1    | 2023-03-18 09:23:56,693 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4: change Leader from null to 04b88c83-026f-4630-aada-108e37835bee at term 1 for becomeLeader, leader elected after 5172ms
dn5_1    | 2023-03-18 09:23:56,693 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,045 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.delete(OMDBUpdatesHandler.java:77)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,045 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,045 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,046 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
dn5_1    | 2023-03-18 09:23:56,694 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-03-18 09:23:56,695 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn5_1    | 2023-03-18 09:23:56,697 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn5_1    | 2023-03-18 09:23:56,697 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 2023-03-18 09:23:56,697 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn5_1    | 2023-03-18 09:23:56,698 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-03-18 09:23:56,698 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn5_1    | 2023-03-18 09:23:56,698 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: start 04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderStateImpl
dn5_1    | 2023-03-18 09:23:56,698 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 2023-03-18 09:23:56,699 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/087cc0b6-0f7f-40d8-82ec-5da9883170e4/current/log_inprogress_0
dn5_1    | 2023-03-18 09:23:56,701 [04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4-LeaderElection4] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-5DA9883170E4: set configuration 0: peers:[04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:23:56,757 [grpc-default-executor-2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118: receive requestVote(PRE_VOTE, 7c9e7a7d-2593-458c-8a46-0313d77c3278, group-6F7919707118, 0, (t:0, i:0))
dn5_1    | 2023-03-18 09:23:56,757 [grpc-default-executor-2] INFO impl.VoteContext: 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118-FOLLOWER: accept PRE_VOTE from 7c9e7a7d-2593-458c-8a46-0313d77c3278: our priority 0 <= candidate's priority 0
dn5_1    | 2023-03-18 09:23:56,758 [grpc-default-executor-2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118 replies to PRE_VOTE vote request: 7c9e7a7d-2593-458c-8a46-0313d77c3278<-04b88c83-026f-4630-aada-108e37835bee#0:OK-t0. Peer's state: 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118:t0, leader=null, voted=, raftlog=Memoized:04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:23:56,953 [04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-18 09:23:56,953 [04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-18 09:23:57,681 [grpc-default-executor-2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118: receive requestVote(PRE_VOTE, 4ba198f1-a013-49ce-b059-f73788d69001, group-6F7919707118, 0, (t:0, i:0))
dn5_1    | 2023-03-18 09:23:57,682 [grpc-default-executor-2] INFO impl.VoteContext: 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118-FOLLOWER: accept PRE_VOTE from 4ba198f1-a013-49ce-b059-f73788d69001: our priority 0 <= candidate's priority 1
dn5_1    | 2023-03-18 09:23:57,682 [grpc-default-executor-2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118 replies to PRE_VOTE vote request: 4ba198f1-a013-49ce-b059-f73788d69001<-04b88c83-026f-4630-aada-108e37835bee#0:OK-t0. Peer's state: 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118:t0, leader=null, voted=, raftlog=Memoized:04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:23:57,705 [grpc-default-executor-2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118: receive requestVote(ELECTION, 4ba198f1-a013-49ce-b059-f73788d69001, group-6F7919707118, 1, (t:0, i:0))
dn5_1    | 2023-03-18 09:23:57,705 [grpc-default-executor-2] INFO impl.VoteContext: 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118-FOLLOWER: accept ELECTION from 4ba198f1-a013-49ce-b059-f73788d69001: our priority 0 <= candidate's priority 1
dn5_1    | 2023-03-18 09:23:57,705 [grpc-default-executor-2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:4ba198f1-a013-49ce-b059-f73788d69001
dn5_1    | 2023-03-18 09:23:57,705 [grpc-default-executor-2] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: shutdown 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118-FollowerState
dn5_1    | 2023-03-18 09:23:57,705 [04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118-FollowerState] INFO impl.FollowerState: 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118-FollowerState was interrupted
dn5_1    | 2023-03-18 09:23:57,705 [grpc-default-executor-2] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: start 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118-FollowerState
dn5_1    | 2023-03-18 09:23:57,707 [04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-18 09:23:57,707 [04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-18 09:23:57,708 [grpc-default-executor-2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118 replies to ELECTION vote request: 4ba198f1-a013-49ce-b059-f73788d69001<-04b88c83-026f-4630-aada-108e37835bee#0:OK-t1. Peer's state: 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118:t1, leader=null, voted=4ba198f1-a013-49ce-b059-f73788d69001, raftlog=Memoized:04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:23:57,851 [04b88c83-026f-4630-aada-108e37835bee-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6F7919707118 with new leaderId: 4ba198f1-a013-49ce-b059-f73788d69001
dn5_1    | 2023-03-18 09:23:57,851 [04b88c83-026f-4630-aada-108e37835bee-server-thread1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118: change Leader from null to 4ba198f1-a013-49ce-b059-f73788d69001 at term 1 for appendEntries, leader elected after 6260ms
dn5_1    | 2023-03-18 09:23:57,851 [04b88c83-026f-4630-aada-108e37835bee-server-thread1] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118: set configuration 0: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:23:57,853 [04b88c83-026f-4630-aada-108e37835bee-server-thread1] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 2023-03-18 09:23:57,854 [04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-6F7919707118-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/66de196e-0a1c-4e0b-be4e-6f7919707118/current/log_inprogress_0
dn5_1    | 2023-03-18 09:23:57,879 [grpc-default-executor-2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193: receive requestVote(PRE_VOTE, 7c9e7a7d-2593-458c-8a46-0313d77c3278, group-CCB5F87F8193, 0, (t:0, i:0))
dn5_1    | 2023-03-18 09:23:57,879 [grpc-default-executor-2] INFO impl.VoteContext: 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-FOLLOWER: reject PRE_VOTE from 7c9e7a7d-2593-458c-8a46-0313d77c3278: our priority 1 > candidate's priority 0
dn5_1    | 2023-03-18 09:23:57,879 [grpc-default-executor-2] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193 replies to PRE_VOTE vote request: 7c9e7a7d-2593-458c-8a46-0313d77c3278<-04b88c83-026f-4630-aada-108e37835bee#0:FAIL-t0. Peer's state: 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193:t0, leader=null, voted=, raftlog=Memoized:04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:23:58,117 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-FollowerState] INFO impl.FollowerState: 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5200789929ns, electionTimeout:5191ms
dn5_1    | 2023-03-18 09:23:58,117 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-FollowerState] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: shutdown 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-FollowerState
dn5_1    | 2023-03-18 09:23:58,118 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-FollowerState] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn5_1    | 2023-03-18 09:23:58,118 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-03-18 09:23:58,119 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-FollowerState] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: start 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5
dn5_1    | 2023-03-18 09:23:58,128 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO impl.LeaderElection: 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:23:58,130 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5-1] INFO server.GrpcServerProtocolClient: Build channel for 4ba198f1-a013-49ce-b059-f73788d69001
dn5_1    | 2023-03-18 09:23:58,137 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-18 09:23:58,138 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-18 09:23:58,146 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO impl.LeaderElection: 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn5_1    | 2023-03-18 09:23:58,147 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO impl.LeaderElection:   Response 0: 04b88c83-026f-4630-aada-108e37835bee<-7c9e7a7d-2593-458c-8a46-0313d77c3278#0:OK-t0
dn5_1    | 2023-03-18 09:23:58,147 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO impl.LeaderElection: 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5 PRE_VOTE round 0: result PASSED
dn5_1    | 2023-03-18 09:23:58,148 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO impl.LeaderElection: 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5 ELECTION round 0: submit vote requests at term 1 for -1: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:23:58,150 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-18 09:23:58,150 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-18 09:23:58,163 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO impl.LeaderElection: 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn5_1    | 2023-03-18 09:23:58,163 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO impl.LeaderElection:   Response 0: 04b88c83-026f-4630-aada-108e37835bee<-4ba198f1-a013-49ce-b059-f73788d69001#0:OK-t1
dn5_1    | 2023-03-18 09:23:58,163 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO impl.LeaderElection: 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5 ELECTION round 0: result PASSED
dn5_1    | 2023-03-18 09:23:58,163 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: shutdown 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5
dn5_1    | 2023-03-18 09:23:58,164 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn5_1    | 2023-03-18 09:23:58,164 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-CCB5F87F8193 with new leaderId: 04b88c83-026f-4630-aada-108e37835bee
dn5_1    | 2023-03-18 09:23:58,164 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193: change Leader from null to 04b88c83-026f-4630-aada-108e37835bee at term 1 for becomeLeader, leader elected after 5568ms
dn5_1    | 2023-03-18 09:23:58,164 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn5_1    | 2023-03-18 09:23:58,165 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-03-18 09:23:58,165 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn5_1    | 2023-03-18 09:23:58,166 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn5_1    | 2023-03-18 09:23:58,166 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 2023-03-18 09:23:58,166 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn5_1    | 2023-03-18 09:23:58,167 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-03-18 09:23:58,167 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn5_1    | 2023-03-18 09:23:58,168 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn5_1    | 2023-03-18 09:23:58,168 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-18 09:23:58,168 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn5_1    | 2023-03-18 09:23:58,168 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn5_1    | 2023-03-18 09:23:58,171 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-03-18 09:23:58,171 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-03-18 09:23:58,171 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-03-18 09:23:58,171 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn5_1    | 2023-03-18 09:23:58,175 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn5_1    | 2023-03-18 09:23:58,175 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-18 09:23:58,175 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn5_1    | 2023-03-18 09:23:58,175 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn5_1    | 2023-03-18 09:23:58,178 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-03-18 09:23:58,178 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-03-18 09:23:58,178 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-03-18 09:23:58,178 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn5_1    | 2023-03-18 09:23:58,179 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO impl.RoleInfo: 04b88c83-026f-4630-aada-108e37835bee: start 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderStateImpl
dn5_1    | 2023-03-18 09:23:58,179 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 2023-03-18 09:23:58,182 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/84a08f9d-e905-447e-be3d-ccb5f87f8193/current/log_inprogress_0
dn5_1    | 2023-03-18 09:23:58,216 [04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193-LeaderElection5] INFO server.RaftServer$Division: 04b88c83-026f-4630-aada-108e37835bee@group-CCB5F87F8193: set configuration 0: peers:[4ba198f1-a013-49ce-b059-f73788d69001|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:0|startupRole:FOLLOWER, 7c9e7a7d-2593-458c-8a46-0313d77c3278|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 04b88c83-026f-4630-aada-108e37835bee|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-18 09:24:11,157 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 3/4997 blocks from 2 candidate containers.
dn5_1    | 2023-03-18 09:24:11,202 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/current/containerDir0/1/chunks/111677748019200001.block
dn5_1    | 2023-03-18 09:24:11,203 [BlockDeletingService#0] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/current/containerDir0/2/chunks/111677748019200002.block
dn5_1    | 2023-03-18 09:24:11,204 [BlockDeletingService#0] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-628f975d-bdc4-481c-a483-62db7ebce017/current/containerDir0/2/chunks/111677748019200003.block
dn5_1    | 2023-03-18 09:25:11,187 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 2023-03-18 09:26:11,187 [BlockDeletingService#4] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,046 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,046 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,046 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,047 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,047 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.delete(OMDBUpdatesHandler.java:77)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,047 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,047 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,047 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,048 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,048 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,048 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,048 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,049 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,050 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,050 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,050 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,051 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,051 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:407)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:641)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-18 09:26:07,051 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 15, SequenceNumber diff: 43, SequenceNumber Lag from OM 0.
recon_1  | 2023-03-18 09:26:07,051 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 43 records
recon_1  | 2023-03-18 09:26:07,060 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
recon_1  | 2023-03-18 09:26:07,060 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
recon_1  | 2023-03-18 09:26:07,410 [pool-49-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
recon_1  | 2023-03-18 09:26:07,411 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 0 OM DB update event(s).
recon_1  | 2023-03-18 09:26:07,412 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
recon_1  | 2023-03-18 09:26:09,076 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #2003 got from ha_dn5_1.ha_net.
recon_1  | 2023-03-18 09:26:09,082 [FixedThreadPoolWithAffinityExecutor-1-0] WARN scm.ReconContainerManager: Pipeline PipelineID=4c5c51d5-7f71-4265-ae16-de8d73f2a2b1 not found. Cannot add container #2003
recon_1  | 2023-03-18 09:26:09,082 [FixedThreadPoolWithAffinityExecutor-1-0] WARN scm.ReconIncrementalContainerReportHandler: Container 2003 not found!
recon_1  | 2023-03-18 09:26:09,142 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: New container #2003 got from ha_dn3_1.ha_net.
recon_1  | 2023-03-18 09:26:09,151 [FixedThreadPoolWithAffinityExecutor-9-0] WARN scm.ReconContainerManager: Pipeline PipelineID=4c5c51d5-7f71-4265-ae16-de8d73f2a2b1 not found. Cannot add container #2003
recon_1  | 2023-03-18 09:26:09,151 [FixedThreadPoolWithAffinityExecutor-9-0] WARN scm.ReconIncrementalContainerReportHandler: Container 2003 not found!
recon_1  | 2023-03-18 09:26:09,426 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: New container #2003 got from ha_dn2_1.ha_net.
recon_1  | 2023-03-18 09:26:09,428 [FixedThreadPoolWithAffinityExecutor-9-0] WARN scm.ReconContainerManager: Pipeline PipelineID=4c5c51d5-7f71-4265-ae16-de8d73f2a2b1 not found. Cannot add container #2003
recon_1  | 2023-03-18 09:26:09,428 [FixedThreadPoolWithAffinityExecutor-9-0] WARN scm.ReconIncrementalContainerReportHandler: Container 2003 not found!
recon_1  | 2023-03-18 09:26:09,510 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: New container #2003 got from ha_dn1_1.ha_net.
recon_1  | 2023-03-18 09:26:09,513 [FixedThreadPoolWithAffinityExecutor-9-0] WARN scm.ReconContainerManager: Pipeline PipelineID=4c5c51d5-7f71-4265-ae16-de8d73f2a2b1 not found. Cannot add container #2003
recon_1  | 2023-03-18 09:26:09,513 [FixedThreadPoolWithAffinityExecutor-9-0] WARN scm.ReconIncrementalContainerReportHandler: Container 2003 not found!
recon_1  | 2023-03-18 09:26:09,601 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #2003 got from ha_dn4_1.ha_net.
recon_1  | 2023-03-18 09:26:09,620 [FixedThreadPoolWithAffinityExecutor-8-0] WARN scm.ReconContainerManager: Pipeline PipelineID=4c5c51d5-7f71-4265-ae16-de8d73f2a2b1 not found. Cannot add container #2003
recon_1  | 2023-03-18 09:26:09,620 [FixedThreadPoolWithAffinityExecutor-8-0] WARN scm.ReconIncrementalContainerReportHandler: Container 2003 not found!
