Attaching to ha_om2_1, ha_om1_1, ha_s3g_1, ha_dn2_1, ha_dn5_1, ha_scm_1, ha_dn4_1, ha_om3_1, ha_dn3_1, ha_dn1_1, ha_recon_1
dn1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn1_1    | 2023-03-10 22:51:10,599 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn1_1    | /************************************************************
dn1_1    | STARTUP_MSG: Starting HddsDatanodeService
dn1_1    | STARTUP_MSG:   host = 516736b05cb5/10.9.0.15
dn1_1    | STARTUP_MSG:   args = []
dn1_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/354d3dc5a16eade52bdcbd566bfb952ac3a1d8b6 ; compiled by 'runner' on 2023-03-10T22:24Z
dn1_1    | STARTUP_MSG:   java = 11.0.14.1
dn1_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=1440, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.ssl.keystore.reload.interval=60s, hdds.security.ssl.truststore.reload.interval=60s, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4))$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.sst.filtering.service.timeout=300000ms, ozone.tags.system=ACL,BALANCER,CLIENT,CONTAINER,DATANODE,DATASTREAM,DEBUG,DELETION,DEPRECATED,FREON,HA,HDDS,KERBEROS,MANAGEMENT,OM,OPERATION,OZONE,OZONEFS,PERFORMANCE,PIPELINE,RATIS,RECON,REQUIRED,S3GATEWAY,SCM,SECURITY,STORAGE,TLS,TOKEN,UPGRADE,X509, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn1_1    | ************************************************************/
dn1_1    | 2023-03-10 22:51:10,778 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn1_1    | 2023-03-10 22:51:11,224 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn1_1    | 2023-03-10 22:51:12,121 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn1_1    | 2023-03-10 22:51:14,313 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn1_1    | 2023-03-10 22:51:14,313 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn1_1    | 2023-03-10 22:51:15,424 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:516736b05cb5 ip:10.9.0.15
dn1_1    | 2023-03-10 22:51:18,111 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn1_1    | 2023-03-10 22:51:20,167 [main] INFO reflections.Reflections: Reflections took 1693 ms to scan 2 urls, producing 103 keys and 227 values 
dn1_1    | 2023-03-10 22:51:20,685 [main] INFO upgrade.HDDSLayoutVersionManager: Registering Upgrade Action : DatanodeSchemaV3FinalizeAction
dn1_1    | 2023-03-10 22:51:21,014 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
dn1_1    | 2023-03-10 22:51:22,531 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 8414 at 2023-03-10T22:50:46.059Z
dn1_1    | 2023-03-10 22:51:22,663 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn1_1    | 2023-03-10 22:51:22,703 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn1_1    | 2023-03-10 22:51:22,718 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn1_1    | 2023-03-10 22:51:22,914 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn1_1    | 2023-03-10 22:51:23,107 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-03-10 22:51:23,115 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-03-10T22:50:46.074Z
dn1_1    | 2023-03-10 22:51:23,162 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn1_1    | 2023-03-10 22:51:23,165 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn1_1    | 2023-03-10 22:51:23,166 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn1_1    | 2023-03-10 22:51:23,378 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn1_1    | 2023-03-10 22:51:25,040 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn1_1    | 2023-03-10 22:51:25,040 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 1s
dn1_1    | 2023-03-10 22:51:39,698 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn1_1    | 2023-03-10 22:51:40,274 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-03-10 22:51:41,347 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn1_1    | 2023-03-10 22:51:42,520 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-03-10 22:51:42,557 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn1_1    | 2023-03-10 22:51:42,561 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-03-10 22:51:42,567 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn1_1    | 2023-03-10 22:51:42,567 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn1_1    | 2023-03-10 22:51:42,569 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn1_1    | 2023-03-10 22:51:42,570 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn1_1    | 2023-03-10 22:51:42,578 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-03-10 22:51:42,663 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn1_1    | 2023-03-10 22:51:42,741 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-03-10 22:51:42,911 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 2023-03-10 22:51:42,932 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn1_1    | 2023-03-10 22:51:42,932 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn2_1    | 2023-03-10 22:51:12,190 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn2_1    | /************************************************************
dn2_1    | STARTUP_MSG: Starting HddsDatanodeService
dn2_1    | STARTUP_MSG:   host = f1622084839d/10.9.0.16
dn2_1    | STARTUP_MSG:   args = []
dn2_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/354d3dc5a16eade52bdcbd566bfb952ac3a1d8b6 ; compiled by 'runner' on 2023-03-10T22:24Z
dn2_1    | STARTUP_MSG:   java = 11.0.14.1
dn2_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=1440, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.ssl.keystore.reload.interval=60s, hdds.security.ssl.truststore.reload.interval=60s, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4))$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.sst.filtering.service.timeout=300000ms, ozone.tags.system=ACL,BALANCER,CLIENT,CONTAINER,DATANODE,DATASTREAM,DEBUG,DELETION,DEPRECATED,FREON,HA,HDDS,KERBEROS,MANAGEMENT,OM,OPERATION,OZONE,OZONEFS,PERFORMANCE,PIPELINE,RATIS,RECON,REQUIRED,S3GATEWAY,SCM,SECURITY,STORAGE,TLS,TOKEN,UPGRADE,X509, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn2_1    | ************************************************************/
dn2_1    | 2023-03-10 22:51:12,390 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn2_1    | 2023-03-10 22:51:12,723 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn2_1    | 2023-03-10 22:51:13,790 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn2_1    | 2023-03-10 22:51:15,504 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn2_1    | 2023-03-10 22:51:15,505 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn2_1    | 2023-03-10 22:51:17,150 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:f1622084839d ip:10.9.0.16
dn2_1    | 2023-03-10 22:51:20,081 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn2_1    | 2023-03-10 22:51:22,263 [main] INFO reflections.Reflections: Reflections took 1817 ms to scan 2 urls, producing 103 keys and 227 values 
dn2_1    | 2023-03-10 22:51:22,785 [main] INFO upgrade.HDDSLayoutVersionManager: Registering Upgrade Action : DatanodeSchemaV3FinalizeAction
dn2_1    | 2023-03-10 22:51:23,025 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
dn2_1    | 2023-03-10 22:51:24,478 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 8259 at 2023-03-10T22:50:45.910Z
dn2_1    | 2023-03-10 22:51:24,634 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn2_1    | 2023-03-10 22:51:24,679 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn2_1    | 2023-03-10 22:51:24,687 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn2_1    | 2023-03-10 22:51:24,969 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn2_1    | 2023-03-10 22:51:25,178 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-03-10 22:51:25,236 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-03-10T22:50:45.909Z
dn2_1    | 2023-03-10 22:51:25,242 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn2_1    | 2023-03-10 22:51:25,245 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn2_1    | 2023-03-10 22:51:25,246 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn2_1    | 2023-03-10 22:51:25,475 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn2_1    | 2023-03-10 22:51:27,252 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn2_1    | 2023-03-10 22:51:27,266 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 1s
dn2_1    | 2023-03-10 22:51:41,541 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn2_1    | 2023-03-10 22:51:42,147 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-03-10 22:51:42,748 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn2_1    | 2023-03-10 22:51:44,302 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-03-10 22:51:44,349 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn2_1    | 2023-03-10 22:51:44,365 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-03-10 22:51:44,368 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn2_1    | 2023-03-10 22:51:44,388 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn2_1    | 2023-03-10 22:51:44,389 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn2_1    | 2023-03-10 22:51:44,394 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn2_1    | 2023-03-10 22:51:44,432 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-03-10 22:51:44,436 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn2_1    | 2023-03-10 22:51:44,470 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-03-10 22:51:44,745 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-03-10 22:51:44,759 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn2_1    | 2023-03-10 22:51:44,787 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn4_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn4_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn4_1    | 2023-03-10 22:51:08,496 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn4_1    | /************************************************************
dn4_1    | STARTUP_MSG: Starting HddsDatanodeService
dn4_1    | STARTUP_MSG:   host = b0c55c35d5b9/10.9.0.18
dn4_1    | STARTUP_MSG:   args = []
dn4_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn4_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn4_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/354d3dc5a16eade52bdcbd566bfb952ac3a1d8b6 ; compiled by 'runner' on 2023-03-10T22:24Z
dn4_1    | STARTUP_MSG:   java = 11.0.14.1
dn4_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=1440, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.ssl.keystore.reload.interval=60s, hdds.security.ssl.truststore.reload.interval=60s, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4))$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.sst.filtering.service.timeout=300000ms, ozone.tags.system=ACL,BALANCER,CLIENT,CONTAINER,DATANODE,DATASTREAM,DEBUG,DELETION,DEPRECATED,FREON,HA,HDDS,KERBEROS,MANAGEMENT,OM,OPERATION,OZONE,OZONEFS,PERFORMANCE,PIPELINE,RATIS,RECON,REQUIRED,S3GATEWAY,SCM,SECURITY,STORAGE,TLS,TOKEN,UPGRADE,X509, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn4_1    | ************************************************************/
dn4_1    | 2023-03-10 22:51:08,638 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn4_1    | 2023-03-10 22:51:09,063 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn4_1    | 2023-03-10 22:51:09,986 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn4_1    | 2023-03-10 22:51:11,641 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn4_1    | 2023-03-10 22:51:11,645 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
om1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1    | 2023-03-10 22:51:14,355 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1    | /************************************************************
om1_1    | STARTUP_MSG: Starting OzoneManager
om1_1    | STARTUP_MSG:   host = adbba98a828c/10.9.0.11
om1_1    | STARTUP_MSG:   args = [--upgrade]
om1_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/354d3dc5a16eade52bdcbd566bfb952ac3a1d8b6 ; compiled by 'runner' on 2023-03-10T22:24Z
om1_1    | STARTUP_MSG:   java = 11.0.14.1
om1_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.ssl.keystore.reload.interval=60s, hdds.security.ssl.truststore.reload.interval=60s, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4))$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.sst.filtering.service.timeout=300000ms, ozone.tags.system=ACL,BALANCER,CLIENT,CONTAINER,DATANODE,DATASTREAM,DEBUG,DELETION,DEPRECATED,FREON,HA,HDDS,KERBEROS,MANAGEMENT,OM,OPERATION,OZONE,OZONEFS,PERFORMANCE,PIPELINE,RATIS,RECON,REQUIRED,S3GATEWAY,SCM,SECURITY,STORAGE,TLS,TOKEN,UPGRADE,X509, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om1_1    | ************************************************************/
om1_1    | 2023-03-10 22:51:14,434 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1    | 2023-03-10 22:51:24,772 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1    | 2023-03-10 22:51:29,794 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om1_1    | 2023-03-10 22:51:30,357 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1    | 2023-03-10 22:51:30,365 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
om1_1    | 2023-03-10 22:51:30,399 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-03-10 22:51:30,829 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = INITIAL_VERSION (version = 0), software layout = MULTITENANCY_SCHEMA (version = 3)
om1_1    | 2023-03-10 22:51:34,105 [main] INFO reflections.Reflections: Reflections took 2595 ms to scan 1 urls, producing 126 keys and 365 values [using 2 cores]
om1_1    | 2023-03-10 22:51:34,307 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-03-10 22:51:34,401 [main] WARN om.OzoneManager: ozone.default.bucket.layout configured to non-legacy bucket layout FILE_SYSTEM_OPTIMIZED when Ozone Manager is pre-finalized for bucket layout support. Legacy buckets will be created by default until Ozone Manager is finalized.
om1_1    | 2023-03-10 22:51:36,437 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863]
om1_1    | 2023-03-10 22:51:36,956 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863]
om1_1    | 2023-03-10 22:51:39,730 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From adbba98a828c/10.9.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-03-10 22:51:41,732 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From adbba98a828c/10.9.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-03-10 22:51:43,734 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From adbba98a828c/10.9.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-03-10 22:51:45,736 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From adbba98a828c/10.9.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-03-10 22:51:47,737 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From adbba98a828c/10.9.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-03-10 22:51:49,739 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From adbba98a828c/10.9.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-03-10 22:51:51,749 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From adbba98a828c/10.9.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-03-10 22:51:53,751 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From adbba98a828c/10.9.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-03-10 22:51:55,753 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From adbba98a828c/10.9.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-03-10 22:51:57,757 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From adbba98a828c/10.9.0.11 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-03-10 22:52:06,026 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om1_1    | 2023-03-10 22:52:06,203 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-03-10 22:52:07,593 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om1_1    | 2023-03-10 22:52:07,623 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om1_1    | 2023-03-10 22:52:09,005 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om1_1    | 2023-03-10 22:52:09,251 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-03-10 22:52:10,140 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1    | 2023-03-10 22:52:10,142 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om1_1    | 2023-03-10 22:52:10,208 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om1_1    | 2023-03-10 22:52:11,440 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om1_1    | 2023-03-10 22:52:11,488 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1    | 2023-03-10 22:52:11,727 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om1:9872, om3:9872, om2:9872
om1_1    | 2023-03-10 22:52:11,816 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:5, i:102)
om1_1    | 2023-03-10 22:52:11,983 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om1_1    | 2023-03-10 22:52:12,368 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om1_1    | 2023-03-10 22:52:12,376 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om1_1    | 2023-03-10 22:52:12,381 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om1_1    | 2023-03-10 22:52:12,384 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om1_1    | 2023-03-10 22:52:12,384 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om1_1    | 2023-03-10 22:52:12,385 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om1_1    | 2023-03-10 22:52:12,387 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om1_1    | 2023-03-10 22:52:12,399 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-03-10 22:52:12,404 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om1_1    | 2023-03-10 22:52:12,407 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1    | 2023-03-10 22:52:12,494 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1    | 2023-03-10 22:52:12,519 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om1_1    | 2023-03-10 22:52:12,524 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om1_1    | 2023-03-10 22:52:14,763 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om1_1    | 2023-03-10 22:52:14,812 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om1_1    | 2023-03-10 22:52:14,828 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om1_1    | 2023-03-10 22:52:14,840 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1    | 2023-03-10 22:52:14,840 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | 2023-03-10 22:52:14,914 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1    | 2023-03-10 22:52:14,962 [om1-impl-thread1] INFO server.RaftServer: om1: found a subdirectory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om1_1    | 2023-03-10 22:52:15,011 [main] INFO server.RaftServer: om1: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@4844930a[Not completed]
om1_1    | 2023-03-10 22:52:15,012 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om1_1    | 2023-03-10 22:52:15,057 [main] INFO om.OzoneManager: Creating RPC Server
om1_1    | 2023-03-10 22:52:15,350 [pool-29-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om1_1    | 2023-03-10 22:52:15,412 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om1_1    | 2023-03-10 22:52:15,412 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om1_1    | 2023-03-10 22:52:15,412 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om1_1    | 2023-03-10 22:52:15,417 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1    | 2023-03-10 22:52:15,422 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | 2023-03-10 22:52:15,430 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-03-10 22:51:13,076 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:b0c55c35d5b9 ip:10.9.0.18
dn4_1    | 2023-03-10 22:51:15,639 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn4_1    | 2023-03-10 22:51:17,648 [main] INFO reflections.Reflections: Reflections took 1667 ms to scan 2 urls, producing 103 keys and 227 values 
dn4_1    | 2023-03-10 22:51:18,150 [main] INFO upgrade.HDDSLayoutVersionManager: Registering Upgrade Action : DatanodeSchemaV3FinalizeAction
dn4_1    | 2023-03-10 22:51:18,455 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
dn4_1    | 2023-03-10 22:51:19,943 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 8414 at 2023-03-10T22:50:45.956Z
dn4_1    | 2023-03-10 22:51:20,099 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn4_1    | 2023-03-10 22:51:20,125 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn4_1    | 2023-03-10 22:51:20,126 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn4_1    | 2023-03-10 22:51:20,361 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn4_1    | 2023-03-10 22:51:20,561 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn4_1    | 2023-03-10 22:51:20,571 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-03-10T22:50:45.989Z
dn4_1    | 2023-03-10 22:51:20,609 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn4_1    | 2023-03-10 22:51:20,613 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn4_1    | 2023-03-10 22:51:20,613 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn4_1    | 2023-03-10 22:51:20,793 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn4_1    | 2023-03-10 22:51:22,304 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn4_1    | 2023-03-10 22:51:22,347 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 1s
dn4_1    | 2023-03-10 22:51:37,786 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn4_1    | 2023-03-10 22:51:38,458 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn4_1    | 2023-03-10 22:51:39,078 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn4_1    | 2023-03-10 22:51:40,298 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn4_1    | 2023-03-10 22:51:40,305 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn4_1    | 2023-03-10 22:51:40,324 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn4_1    | 2023-03-10 22:51:40,337 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn4_1    | 2023-03-10 22:51:40,345 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn4_1    | 2023-03-10 22:51:40,345 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn4_1    | 2023-03-10 22:51:40,347 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn4_1    | 2023-03-10 22:51:40,350 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-10 22:51:40,359 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn4_1    | 2023-03-10 22:51:40,421 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-03-10 22:51:40,551 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-03-10 22:51:40,587 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn4_1    | 2023-03-10 22:51:40,590 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn1_1    | 2023-03-10 22:51:45,351 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn1_1    | 2023-03-10 22:51:45,374 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn1_1    | 2023-03-10 22:51:45,375 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn1_1    | 2023-03-10 22:51:45,375 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-03-10 22:51:45,396 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-03-10 22:51:45,422 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-03-10 22:51:45,443 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServer: e3e4587c-aa42-4e86-ae9a-d3e448365275: found a subdirectory /data/metadata/ratis/d1fa3d0d-6abe-48bc-b456-d0b7aa0d159e
dn1_1    | 2023-03-10 22:51:45,496 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServer: e3e4587c-aa42-4e86-ae9a-d3e448365275: addNew group-D0B7AA0D159E:[] returns group-D0B7AA0D159E:java.util.concurrent.CompletableFuture@3a4bde63[Not completed]
dn1_1    | 2023-03-10 22:51:45,505 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServer: e3e4587c-aa42-4e86-ae9a-d3e448365275: found a subdirectory /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3
dn1_1    | 2023-03-10 22:51:45,506 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServer: e3e4587c-aa42-4e86-ae9a-d3e448365275: addNew group-0049FBBC23B3:[] returns group-0049FBBC23B3:java.util.concurrent.CompletableFuture@37293e46[Not completed]
dn1_1    | 2023-03-10 22:51:45,509 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServer: e3e4587c-aa42-4e86-ae9a-d3e448365275: found a subdirectory /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575
dn1_1    | 2023-03-10 22:51:45,513 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServer: e3e4587c-aa42-4e86-ae9a-d3e448365275: addNew group-CBC13A60F575:[] returns group-CBC13A60F575:java.util.concurrent.CompletableFuture@7a7fbb8e[Not completed]
dn1_1    | 2023-03-10 22:51:45,770 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn1_1    | 2023-03-10 22:51:46,331 [pool-38-thread-1] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275: new RaftServerImpl for group-D0B7AA0D159E:[] with ContainerStateMachine:uninitialized
dn1_1    | 2023-03-10 22:51:46,358 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-03-10 22:51:46,359 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-03-10 22:51:46,359 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-03-10 22:51:46,359 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-03-10 22:51:46,359 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-03-10 22:51:46,359 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-03-10 22:51:46,531 [pool-38-thread-1] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-03-10 22:51:46,532 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-03-10 22:51:46,690 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-03-10 22:51:46,697 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-03-10 22:51:46,975 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-03-10 22:51:47,007 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-03-10 22:51:47,071 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-03-10 22:51:47,958 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-03-10 22:51:47,959 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-03-10 22:51:47,968 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-03-10 22:51:47,995 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-03-10 22:51:48,001 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-03-10 22:51:48,111 [pool-38-thread-1] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275: new RaftServerImpl for group-0049FBBC23B3:[] with ContainerStateMachine:uninitialized
dn1_1    | 2023-03-10 22:51:48,131 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-03-10 22:51:48,145 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-03-10 22:51:48,145 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-03-10 22:51:48,199 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-03-10 22:51:48,209 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-03-10 22:51:48,210 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-03-10 22:51:48,210 [pool-38-thread-1] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-03-10 22:51:48,210 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-03-10 22:51:48,210 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-03-10 22:51:48,217 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-03-10 22:51:48,217 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-03-10 22:51:48,218 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-03-10 22:51:48,218 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-03-10 22:51:48,226 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-03-10 22:51:48,226 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-03-10 22:51:48,226 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-03-10 22:51:48,227 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1    | 2023-03-10 22:51:14,081 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1    | /************************************************************
om2_1    | STARTUP_MSG: Starting OzoneManager
om2_1    | STARTUP_MSG:   host = d08309057e39/10.9.0.12
om2_1    | STARTUP_MSG:   args = [--upgrade]
om2_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/354d3dc5a16eade52bdcbd566bfb952ac3a1d8b6 ; compiled by 'runner' on 2023-03-10T22:24Z
om2_1    | STARTUP_MSG:   java = 11.0.14.1
dn4_1    | 2023-03-10 22:51:42,924 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn4_1    | 2023-03-10 22:51:42,974 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn4_1    | 2023-03-10 22:51:43,019 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn4_1    | 2023-03-10 22:51:43,031 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-03-10 22:51:43,039 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-03-10 22:51:43,124 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-03-10 22:51:43,160 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServer: e78c5ce1-46ab-4889-a0cd-5903ae46614d: found a subdirectory /data/metadata/ratis/8581ae3d-c50a-4a49-9d70-d98e5419b7a9
dn4_1    | 2023-03-10 22:51:43,326 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServer: e78c5ce1-46ab-4889-a0cd-5903ae46614d: addNew group-D98E5419B7A9:[] returns group-D98E5419B7A9:java.util.concurrent.CompletableFuture@73c22b74[Not completed]
dn4_1    | 2023-03-10 22:51:43,333 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServer: e78c5ce1-46ab-4889-a0cd-5903ae46614d: found a subdirectory /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3
dn4_1    | 2023-03-10 22:51:43,333 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServer: e78c5ce1-46ab-4889-a0cd-5903ae46614d: addNew group-0049FBBC23B3:[] returns group-0049FBBC23B3:java.util.concurrent.CompletableFuture@6cff298b[Not completed]
dn4_1    | 2023-03-10 22:51:43,335 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServer: e78c5ce1-46ab-4889-a0cd-5903ae46614d: found a subdirectory /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575
dn4_1    | 2023-03-10 22:51:43,335 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServer: e78c5ce1-46ab-4889-a0cd-5903ae46614d: addNew group-CBC13A60F575:[] returns group-CBC13A60F575:java.util.concurrent.CompletableFuture@5bcb3f86[Not completed]
dn4_1    | 2023-03-10 22:51:43,559 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn4_1    | 2023-03-10 22:51:43,971 [pool-38-thread-1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d: new RaftServerImpl for group-D98E5419B7A9:[] with ContainerStateMachine:uninitialized
dn4_1    | 2023-03-10 22:51:43,974 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-03-10 22:51:43,986 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-03-10 22:51:43,989 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-03-10 22:51:43,995 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-03-10 22:51:44,000 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-03-10 22:51:44,001 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-03-10 22:51:44,147 [pool-38-thread-1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-03-10 22:51:44,159 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-03-10 22:51:44,254 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-03-10 22:51:44,286 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-03-10 22:51:44,555 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-03-10 22:51:44,640 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-03-10 22:51:44,661 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-03-10 22:51:45,590 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-03-10 22:51:45,594 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-03-10 22:51:45,601 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-03-10 22:51:45,607 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-03-10 22:51:45,645 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-03-10 22:51:45,688 [pool-38-thread-1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d: new RaftServerImpl for group-0049FBBC23B3:[] with ContainerStateMachine:uninitialized
dn4_1    | 2023-03-10 22:51:45,707 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-03-10 22:51:45,708 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-03-10 22:51:45,708 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-03-10 22:51:45,710 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-03-10 22:51:45,750 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-03-10 22:51:45,754 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-03-10 22:51:45,754 [pool-38-thread-1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-03-10 22:51:45,757 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-03-10 22:51:45,759 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-03-10 22:51:45,767 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-03-10 22:51:45,769 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-03-10 22:51:45,771 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-03-10 22:51:45,772 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-03-10 22:51:45,780 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-03-10 22:51:45,795 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-03-10 22:51:45,795 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-03-10 22:51:45,796 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-03-10 22:51:48,227 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-03-10 22:51:48,233 [pool-38-thread-1] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275: new RaftServerImpl for group-CBC13A60F575:[] with ContainerStateMachine:uninitialized
dn1_1    | 2023-03-10 22:51:48,237 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-03-10 22:51:48,237 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-03-10 22:51:48,239 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-03-10 22:51:48,239 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-03-10 22:51:48,240 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-03-10 22:51:48,244 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-03-10 22:51:48,245 [pool-38-thread-1] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-03-10 22:51:48,247 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-03-10 22:51:48,248 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-03-10 22:51:48,248 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-03-10 22:51:48,249 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-03-10 22:51:48,265 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-03-10 22:51:48,280 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-03-10 22:51:48,289 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-03-10 22:51:48,297 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-03-10 22:51:48,300 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-03-10 22:51:48,300 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-03-10 22:51:48,301 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-03-10 22:51:48,359 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn4_1    | 2023-03-10 22:51:45,817 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-03-10 22:51:45,886 [pool-38-thread-1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d: new RaftServerImpl for group-CBC13A60F575:[] with ContainerStateMachine:uninitialized
dn4_1    | 2023-03-10 22:51:45,895 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-03-10 22:51:45,901 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-03-10 22:51:45,917 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-03-10 22:51:45,919 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-03-10 22:51:45,923 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-03-10 22:51:45,923 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-03-10 22:51:45,924 [pool-38-thread-1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-03-10 22:51:45,964 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-03-10 22:51:45,966 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-03-10 22:51:45,973 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-03-10 22:51:45,974 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-03-10 22:51:45,974 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-03-10 22:51:45,975 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-03-10 22:51:45,998 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-03-10 22:51:46,001 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-03-10 22:51:46,001 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-03-10 22:51:46,002 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-03-10 22:51:46,002 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-03-10 22:51:46,043 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn4_1    | 2023-03-10 22:51:46,157 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn4_1    | 2023-03-10 22:51:46,909 [main] INFO util.log: Logging initialized @50587ms to org.eclipse.jetty.util.log.Slf4jLog
dn4_1    | 2023-03-10 22:51:47,944 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn4_1    | 2023-03-10 22:51:47,984 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn4_1    | 2023-03-10 22:51:48,084 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn4_1    | 2023-03-10 22:51:48,095 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn4_1    | 2023-03-10 22:51:48,096 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn4_1    | 2023-03-10 22:51:48,096 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn4_1    | 2023-03-10 22:51:48,319 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn4_1    | 2023-03-10 22:51:48,370 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn4_1    | 2023-03-10 22:51:48,382 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn4_1    | 2023-03-10 22:51:48,698 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn4_1    | 2023-03-10 22:51:48,698 [main] INFO server.session: No SessionScavenger set, using defaults
dn4_1    | 2023-03-10 22:51:48,713 [main] INFO server.session: node0 Scavenging every 600000ms
dn4_1    | 2023-03-10 22:51:49,003 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1653b84e{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn4_1    | 2023-03-10 22:51:49,004 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7b14c61{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn4_1    | 2023-03-10 22:51:49,966 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@69a5c6be{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-5589477375772779434/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn4_1    | 2023-03-10 22:51:50,050 [main] INFO server.AbstractConnector: Started ServerConnector@3c81cd82{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn4_1    | 2023-03-10 22:51:50,093 [main] INFO server.Server: Started @53728ms
dn4_1    | 2023-03-10 22:51:50,116 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn4_1    | 2023-03-10 22:51:50,116 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn4_1    | 2023-03-10 22:51:50,123 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn4_1    | 2023-03-10 22:51:50,147 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn4_1    | 2023-03-10 22:51:50,207 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn4_1    | 2023-03-10 22:51:50,229 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn4_1    | 2023-03-10 22:51:50,317 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@35f07ab8] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn4_1    | 2023-03-10 22:51:51,007 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.20:9891
dn4_1    | 2023-03-10 22:51:51,114 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn4_1    | 2023-03-10 22:51:53,830 [EndpointStateMachine task thread for recon/10.9.0.20:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.20:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-03-10 22:51:53,853 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-03-10 22:51:48,490 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn1_1    | 2023-03-10 22:51:48,735 [main] INFO util.log: Logging initialized @51120ms to org.eclipse.jetty.util.log.Slf4jLog
dn1_1    | 2023-03-10 22:51:49,715 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn1_1    | 2023-03-10 22:51:49,790 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn1_1    | 2023-03-10 22:51:49,833 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn1_1    | 2023-03-10 22:51:49,850 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn1_1    | 2023-03-10 22:51:49,861 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn1_1    | 2023-03-10 22:51:49,861 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn1_1    | 2023-03-10 22:51:50,217 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn1_1    | 2023-03-10 22:51:50,299 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn1_1    | 2023-03-10 22:51:50,346 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn1_1    | 2023-03-10 22:51:51,046 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn1_1    | 2023-03-10 22:51:51,046 [main] INFO server.session: No SessionScavenger set, using defaults
dn1_1    | 2023-03-10 22:51:51,061 [main] INFO server.session: node0 Scavenging every 600000ms
dn1_1    | 2023-03-10 22:51:51,222 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5d71b500{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn1_1    | 2023-03-10 22:51:51,230 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@437c4b25{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn1_1    | 2023-03-10 22:51:52,233 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@120aa40b{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-189282979243553759/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn1_1    | 2023-03-10 22:51:52,337 [main] INFO server.AbstractConnector: Started ServerConnector@21f7e537{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn1_1    | 2023-03-10 22:51:52,340 [main] INFO server.Server: Started @54723ms
dn1_1    | 2023-03-10 22:51:52,353 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn1_1    | 2023-03-10 22:51:52,354 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn1_1    | 2023-03-10 22:51:52,368 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn1_1    | 2023-03-10 22:51:52,401 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn1_1    | 2023-03-10 22:51:52,582 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn1_1    | 2023-03-10 22:51:52,611 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn1_1    | 2023-03-10 22:51:52,783 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@268d865e] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn1_1    | 2023-03-10 22:51:53,219 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.20:9891
dn1_1    | 2023-03-10 22:51:53,270 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn1_1    | 2023-03-10 22:51:56,220 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-03-10 22:51:56,808 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn1_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:321)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	... 1 more
dn1_1    | 2023-03-10 22:51:57,221 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-03-10 22:51:58,222 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-03-10 22:51:59,223 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-03-10 22:52:00,387 [EndpointStateMachine task thread for recon/10.9.0.20:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From 516736b05cb5/10.9.0.15 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.15:34038 remote=recon/10.9.0.20:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn1_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.15:34038 remote=recon/10.9.0.20:9891]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
om1_1    | 2023-03-10 22:52:15,650 [pool-29-thread-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om1_1    | 2023-03-10 22:52:15,674 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1    | 2023-03-10 22:52:15,741 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om1_1    | 2023-03-10 22:52:15,745 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om1_1    | 2023-03-10 22:52:16,304 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om1_1    | 2023-03-10 22:52:16,336 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om1_1    | 2023-03-10 22:52:16,388 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om1_1    | 2023-03-10 22:52:18,015 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1    | 2023-03-10 22:52:18,061 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om1_1    | 2023-03-10 22:52:18,062 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om1_1    | 2023-03-10 22:52:18,062 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om1_1    | 2023-03-10 22:52:18,109 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om1_1    | 2023-03-10 22:52:21,221 [main] INFO reflections.Reflections: Reflections took 5530 ms to scan 8 urls, producing 23 keys and 581 values [using 2 cores]
om1_1    | 2023-03-10 22:52:22,739 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om1_1    | 2023-03-10 22:52:22,790 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om1_1    | 2023-03-10 22:52:23,679 [Listener at om1/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om1_1    | 2023-03-10 22:52:23,758 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om1_1    | 2023-03-10 22:52:23,760 [Listener at om1/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om1_1    | 2023-03-10 22:52:24,006 [Listener at om1/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om1/10.9.0.11:9862
om1_1    | 2023-03-10 22:52:24,011 [Listener at om1/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om1_1    | 2023-03-10 22:52:24,042 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@adbba98a828c
om1_1    | 2023-03-10 22:52:24,076 [om1-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=5, votedFor=om1} from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/raft-meta
om1_1    | 2023-03-10 22:52:24,281 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-03-10 22:52:24,287 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om1_1    | 2023-03-10 22:52:24,337 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1    | 2023-03-10 22:52:24,337 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-03-10 22:52:24,360 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om1_1    | 2023-03-10 22:52:24,362 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om1_1    | 2023-03-10 22:52:24,384 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1    | 2023-03-10 22:52:24,415 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om1_1    | 2023-03-10 22:52:24,416 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1    | 2023-03-10 22:52:24,466 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om1_1    | 2023-03-10 22:52:24,467 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om1_1    | 2023-03-10 22:52:24,467 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om1_1    | 2023-03-10 22:52:24,479 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1    | 2023-03-10 22:52:24,480 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om1_1    | 2023-03-10 22:52:24,486 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1    | 2023-03-10 22:52:24,557 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om1_1    | 2023-03-10 22:52:24,563 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om1_1    | 2023-03-10 22:52:24,567 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om1_1    | 2023-03-10 22:52:24,637 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om1_1    | 2023-03-10 22:52:24,638 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-03-10 22:52:24,715 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om1_1    | 2023-03-10 22:52:24,716 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om1_1    | 2023-03-10 22:52:24,719 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om1_1    | 2023-03-10 22:52:24,869 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-03-10 22:52:24,919 [om1-impl-thread1] INFO segmented.LogSegment: Successfully read 21 entries from segment file /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_0-20
om1_1    | 2023-03-10 22:52:24,939 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 21: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-03-10 22:52:24,959 [om1-impl-thread1] INFO segmented.LogSegment: Successfully read 46 entries from segment file /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_21-66
om1_1    | 2023-03-10 22:52:24,964 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-03-10 22:52:24,974 [om1-impl-thread1] INFO segmented.LogSegment: Successfully read 36 entries from segment file /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_67
om1_1    | 2023-03-10 22:52:24,982 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 102
om1_1    | 2023-03-10 22:52:24,982 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 66
om1_1    | 2023-03-10 22:52:25,171 [om1-impl-thread1] INFO raftlog.RaftLog: om1@group-D66704EFC61C-SegmentedRaftLog: commitIndex: updateToMax old=102, new=101, updated? false
om1_1    | 2023-03-10 22:52:25,171 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: start as a follower, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-03-10 22:52:25,171 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from      null to FOLLOWER at term 5 for startAsFollower
om1_1    | 2023-03-10 22:52:25,177 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
om1_1    | 2023-03-10 22:52:25,194 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1    | 2023-03-10 22:52:25,198 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 2023-03-10 22:52:25,209 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om1
om1_1    | 2023-03-10 22:52:25,218 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om1_1    | 2023-03-10 22:52:25,220 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om1_1    | 2023-03-10 22:52:25,221 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om1_1    | 2023-03-10 22:52:25,225 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om1_1    | 2023-03-10 22:52:25,278 [Listener at om1/9862] INFO server.RaftServer: om1: start RPC server
om1_1    | 2023-03-10 22:52:25,531 [Listener at om1/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om1_1    | 2023-03-10 22:52:25,546 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om1_1    | 2023-03-10 22:52:25,566 [Listener at om1/9862] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
om1_1    | 2023-03-10 22:52:25,582 [Listener at om1/9862] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
om1_1    | 2023-03-10 22:52:25,583 [Listener at om1/9862] INFO om.OzoneManager: Version File has different layout version (0) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om1_1    | 2023-03-10 22:52:25,875 [Listener at om1/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om1_1    | 2023-03-10 22:52:25,875 [Listener at om1/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om1_1    | 2023-03-10 22:52:26,019 [Listener at om1/9862] INFO util.log: Logging initialized @85765ms to org.eclipse.jetty.util.log.Slf4jLog
om1_1    | 2023-03-10 22:52:26,759 [Listener at om1/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om1_1    | 2023-03-10 22:52:26,815 [Listener at om1/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om1_1    | 2023-03-10 22:52:26,888 [Listener at om1/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om1_1    | 2023-03-10 22:52:26,906 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om1_1    | 2023-03-10 22:52:26,913 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om1_1    | 2023-03-10 22:52:26,915 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om1_1    | 2023-03-10 22:52:27,336 [Listener at om1/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om1_1    | 2023-03-10 22:52:27,347 [Listener at om1/9862] INFO http.HttpServer2: Jetty bound to port 9874
om1_1    | 2023-03-10 22:52:27,353 [Listener at om1/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om1_1    | 2023-03-10 22:52:27,612 [Listener at om1/9862] INFO server.session: DefaultSessionIdManager workerName=node0
dn2_1    | 2023-03-10 22:51:46,710 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn2_1    | 2023-03-10 22:51:46,747 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn2_1    | 2023-03-10 22:51:46,748 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn2_1    | 2023-03-10 22:51:46,756 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-03-10 22:51:46,757 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-03-10 22:51:46,760 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-03-10 22:51:46,798 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServer: 3892a4e1-c878-42af-adb7-db66a90d61f4: found a subdirectory /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00
dn2_1    | 2023-03-10 22:51:46,831 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServer: 3892a4e1-c878-42af-adb7-db66a90d61f4: addNew group-5B98FDC36F00:[] returns group-5B98FDC36F00:java.util.concurrent.CompletableFuture@5690a46e[Not completed]
dn2_1    | 2023-03-10 22:51:46,832 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServer: 3892a4e1-c878-42af-adb7-db66a90d61f4: found a subdirectory /data/metadata/ratis/2d4f1397-2cdc-4336-a562-676bab171c02
dn2_1    | 2023-03-10 22:51:46,832 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServer: 3892a4e1-c878-42af-adb7-db66a90d61f4: addNew group-676BAB171C02:[] returns group-676BAB171C02:java.util.concurrent.CompletableFuture@1dac78b7[Not completed]
dn2_1    | 2023-03-10 22:51:46,832 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServer: 3892a4e1-c878-42af-adb7-db66a90d61f4: found a subdirectory /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575
dn2_1    | 2023-03-10 22:51:46,833 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServer: 3892a4e1-c878-42af-adb7-db66a90d61f4: addNew group-CBC13A60F575:[] returns group-CBC13A60F575:java.util.concurrent.CompletableFuture@6fe25e0f[Not completed]
dn2_1    | 2023-03-10 22:51:47,102 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn2_1    | 2023-03-10 22:51:47,420 [pool-34-thread-1] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4: new RaftServerImpl for group-5B98FDC36F00:[] with ContainerStateMachine:uninitialized
dn2_1    | 2023-03-10 22:51:47,526 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-03-10 22:51:47,538 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-03-10 22:51:47,540 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-03-10 22:51:47,545 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-03-10 22:51:47,545 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-03-10 22:51:47,550 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-03-10 22:51:47,909 [pool-34-thread-1] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-03-10 22:51:47,935 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-03-10 22:51:48,060 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-03-10 22:51:48,081 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-03-10 22:51:48,418 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-03-10 22:51:48,476 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-03-10 22:51:48,490 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-03-10 22:51:49,131 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn2_1    | 2023-03-10 22:51:49,223 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-03-10 22:51:49,271 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-03-10 22:51:49,291 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-03-10 22:51:49,297 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-03-10 22:51:49,303 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-03-10 22:51:49,302 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn2_1    | 2023-03-10 22:51:49,371 [pool-34-thread-1] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4: new RaftServerImpl for group-676BAB171C02:[] with ContainerStateMachine:uninitialized
dn2_1    | 2023-03-10 22:51:49,375 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-03-10 22:51:49,375 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-03-10 22:51:49,380 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-03-10 22:51:49,380 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-03-10 22:51:49,381 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-03-10 22:51:49,381 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-03-10 22:51:49,382 [pool-34-thread-1] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-03-10 22:51:49,385 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-03-10 22:51:49,389 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-03-10 22:51:49,389 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-03-10 22:51:49,401 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-03-10 22:51:49,408 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-03-10 22:51:49,409 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-03-10 22:51:49,411 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-03-10 22:51:49,448 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-03-10 22:52:00,812 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn1_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:321)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.util.concurrent.TimeoutException
dn3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn3_1    | 2023-03-10 22:51:12,514 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn3_1    | /************************************************************
dn3_1    | STARTUP_MSG: Starting HddsDatanodeService
dn3_1    | STARTUP_MSG:   host = 0de9850545d7/10.9.0.17
dn3_1    | STARTUP_MSG:   args = []
dn3_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn5_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn5_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn5_1    | 2023-03-10 22:51:12,237 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn5_1    | /************************************************************
dn5_1    | STARTUP_MSG: Starting HddsDatanodeService
dn5_1    | STARTUP_MSG:   host = 82dcecb82990/10.9.0.19
dn5_1    | STARTUP_MSG:   args = []
dn5_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn5_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn5_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/354d3dc5a16eade52bdcbd566bfb952ac3a1d8b6 ; compiled by 'runner' on 2023-03-10T22:24Z
dn5_1    | STARTUP_MSG:   java = 11.0.14.1
om1_1    | 2023-03-10 22:52:27,612 [Listener at om1/9862] INFO server.session: No SessionScavenger set, using defaults
om1_1    | 2023-03-10 22:52:27,615 [Listener at om1/9862] INFO server.session: node0 Scavenging every 600000ms
om1_1    | 2023-03-10 22:52:27,646 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2296127{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om1_1    | 2023-03-10 22:52:27,646 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7cb29ea8{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om1_1    | 2023-03-10 22:52:28,349 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7cca31fc{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-15628246878003860458/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om1_1    | 2023-03-10 22:52:28,376 [Listener at om1/9862] INFO server.AbstractConnector: Started ServerConnector@67b560fe{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om1_1    | 2023-03-10 22:52:28,376 [Listener at om1/9862] INFO server.Server: Started @88123ms
om1_1    | 2023-03-10 22:52:28,379 [Listener at om1/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om1_1    | 2023-03-10 22:52:28,380 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om1_1    | 2023-03-10 22:52:28,383 [Listener at om1/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om1_1    | 2023-03-10 22:52:28,384 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1    | 2023-03-10 22:52:28,388 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om1_1    | 2023-03-10 22:52:28,657 [Listener at om1/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om1_1    | 2023-03-10 22:52:28,688 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@18e6b72b] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om1_1    | 2023-03-10 22:52:30,348 [om1@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om1@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5170634170ns, electionTimeout:5143ms
om1_1    | 2023-03-10 22:52:30,349 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-FollowerState
om2_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.ssl.keystore.reload.interval=60s, hdds.security.ssl.truststore.reload.interval=60s, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4))$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.sst.filtering.service.timeout=300000ms, ozone.tags.system=ACL,BALANCER,CLIENT,CONTAINER,DATANODE,DATASTREAM,DEBUG,DELETION,DEPRECATED,FREON,HA,HDDS,KERBEROS,MANAGEMENT,OM,OPERATION,OZONE,OZONEFS,PERFORMANCE,PIPELINE,RATIS,RECON,REQUIRED,S3GATEWAY,SCM,SECURITY,STORAGE,TLS,TOKEN,UPGRADE,X509, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om2_1    | ************************************************************/
om2_1    | 2023-03-10 22:51:14,168 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1    | 2023-03-10 22:51:25,054 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1    | 2023-03-10 22:51:30,225 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om2_1    | 2023-03-10 22:51:30,765 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1    | 2023-03-10 22:51:30,766 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
om2_1    | 2023-03-10 22:51:30,876 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-03-10 22:51:31,294 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = INITIAL_VERSION (version = 0), software layout = MULTITENANCY_SCHEMA (version = 3)
om2_1    | 2023-03-10 22:51:34,462 [main] INFO reflections.Reflections: Reflections took 2535 ms to scan 1 urls, producing 126 keys and 365 values [using 2 cores]
om2_1    | 2023-03-10 22:51:34,624 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-03-10 22:51:34,696 [main] WARN om.OzoneManager: ozone.default.bucket.layout configured to non-legacy bucket layout FILE_SYSTEM_OPTIMIZED when Ozone Manager is pre-finalized for bucket layout support. Legacy buckets will be created by default until Ozone Manager is finalized.
om2_1    | 2023-03-10 22:51:37,977 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863]
om2_1    | 2023-03-10 22:51:38,361 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863]
om2_1    | 2023-03-10 22:51:41,350 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d08309057e39/10.9.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-03-10 22:51:43,352 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d08309057e39/10.9.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-03-10 22:51:45,355 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d08309057e39/10.9.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-03-10 22:51:47,357 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d08309057e39/10.9.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-03-10 22:51:49,359 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d08309057e39/10.9.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-03-10 22:51:51,360 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d08309057e39/10.9.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-03-10 22:51:53,362 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d08309057e39/10.9.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-03-10 22:51:55,364 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d08309057e39/10.9.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
dn3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/354d3dc5a16eade52bdcbd566bfb952ac3a1d8b6 ; compiled by 'runner' on 2023-03-10T22:24Z
dn3_1    | STARTUP_MSG:   java = 11.0.14.1
dn3_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=1440, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.ssl.keystore.reload.interval=60s, hdds.security.ssl.truststore.reload.interval=60s, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4))$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.sst.filtering.service.timeout=300000ms, ozone.tags.system=ACL,BALANCER,CLIENT,CONTAINER,DATANODE,DATASTREAM,DEBUG,DELETION,DEPRECATED,FREON,HA,HDDS,KERBEROS,MANAGEMENT,OM,OPERATION,OZONE,OZONEFS,PERFORMANCE,PIPELINE,RATIS,RECON,REQUIRED,S3GATEWAY,SCM,SECURITY,STORAGE,TLS,TOKEN,UPGRADE,X509, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn3_1    | ************************************************************/
dn3_1    | 2023-03-10 22:51:12,648 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn3_1    | 2023-03-10 22:51:13,056 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn3_1    | 2023-03-10 22:51:14,072 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn3_1    | 2023-03-10 22:51:15,939 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn3_1    | 2023-03-10 22:51:15,939 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn3_1    | 2023-03-10 22:51:17,132 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:0de9850545d7 ip:10.9.0.17
dn3_1    | 2023-03-10 22:51:19,575 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn3_1    | 2023-03-10 22:51:21,571 [main] INFO reflections.Reflections: Reflections took 1670 ms to scan 2 urls, producing 103 keys and 227 values 
dn3_1    | 2023-03-10 22:51:22,107 [main] INFO upgrade.HDDSLayoutVersionManager: Registering Upgrade Action : DatanodeSchemaV3FinalizeAction
dn3_1    | 2023-03-10 22:51:22,352 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
dn3_1    | 2023-03-10 22:51:23,867 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 8347 at 2023-03-10T22:50:46.062Z
dn3_1    | 2023-03-10 22:51:23,975 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn3_1    | 2023-03-10 22:51:24,011 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn3_1    | 2023-03-10 22:51:24,034 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn3_1    | 2023-03-10 22:51:24,243 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn3_1    | 2023-03-10 22:51:24,476 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-03-10 22:51:24,559 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-03-10T22:50:46.063Z
dn3_1    | 2023-03-10 22:51:24,567 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn3_1    | 2023-03-10 22:51:24,568 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn3_1    | 2023-03-10 22:51:24,593 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn3_1    | 2023-03-10 22:51:24,878 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn3_1    | 2023-03-10 22:51:26,470 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn3_1    | 2023-03-10 22:51:26,487 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 1s
dn3_1    | 2023-03-10 22:51:41,588 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn3_1    | 2023-03-10 22:51:42,090 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-03-10 22:51:43,202 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn3_1    | 2023-03-10 22:51:44,158 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-03-10 22:51:44,205 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn3_1    | 2023-03-10 22:51:44,240 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-03-10 22:51:44,250 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn3_1    | 2023-03-10 22:51:44,251 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn3_1    | 2023-03-10 22:51:44,251 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn3_1    | 2023-03-10 22:51:44,252 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn3_1    | 2023-03-10 22:51:44,278 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-03-10 22:51:44,282 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn3_1    | 2023-03-10 22:51:44,285 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-03-10 22:51:44,394 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-03-10 22:51:44,420 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn3_1    | 2023-03-10 22:51:44,613 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn5_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=1440, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.ssl.keystore.reload.interval=60s, hdds.security.ssl.truststore.reload.interval=60s, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4))$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.sst.filtering.service.timeout=300000ms, ozone.tags.system=ACL,BALANCER,CLIENT,CONTAINER,DATANODE,DATASTREAM,DEBUG,DELETION,DEPRECATED,FREON,HA,HDDS,KERBEROS,MANAGEMENT,OM,OPERATION,OZONE,OZONEFS,PERFORMANCE,PIPELINE,RATIS,RECON,REQUIRED,S3GATEWAY,SCM,SECURITY,STORAGE,TLS,TOKEN,UPGRADE,X509, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn5_1    | ************************************************************/
dn5_1    | 2023-03-10 22:51:12,318 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn5_1    | 2023-03-10 22:51:12,842 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn5_1    | 2023-03-10 22:51:13,625 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn5_1    | 2023-03-10 22:51:15,226 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn5_1    | 2023-03-10 22:51:15,226 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn5_1    | 2023-03-10 22:51:16,649 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:82dcecb82990 ip:10.9.0.19
dn5_1    | 2023-03-10 22:51:19,164 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn5_1    | 2023-03-10 22:51:20,747 [main] INFO reflections.Reflections: Reflections took 1229 ms to scan 2 urls, producing 103 keys and 227 values 
dn5_1    | 2023-03-10 22:51:21,222 [main] INFO upgrade.HDDSLayoutVersionManager: Registering Upgrade Action : DatanodeSchemaV3FinalizeAction
dn5_1    | 2023-03-10 22:51:21,505 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
dn5_1    | 2023-03-10 22:51:22,929 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 8192 at 2023-03-10T22:50:46.003Z
dn5_1    | 2023-03-10 22:51:23,048 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn5_1    | 2023-03-10 22:51:23,111 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn5_1    | 2023-03-10 22:51:23,111 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn5_1    | 2023-03-10 22:51:23,374 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn5_1    | 2023-03-10 22:51:23,555 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 2023-03-10 22:51:23,573 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-03-10T22:50:45.994Z
dn5_1    | 2023-03-10 22:51:23,607 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn5_1    | 2023-03-10 22:51:23,607 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn5_1    | 2023-03-10 22:51:23,607 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn5_1    | 2023-03-10 22:51:23,846 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn5_1    | 2023-03-10 22:51:24,641 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn5_1    | 2023-03-10 22:51:24,641 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn5_1    | 2023-03-10 22:51:40,003 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn5_1    | 2023-03-10 22:51:40,617 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 2023-03-10 22:51:41,667 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn5_1    | 2023-03-10 22:51:42,190 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-03-10 22:51:42,232 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn5_1    | 2023-03-10 22:51:42,237 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-03-10 22:51:42,240 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn5_1    | 2023-03-10 22:51:42,244 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn5_1    | 2023-03-10 22:51:42,245 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn5_1    | 2023-03-10 22:51:42,254 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn5_1    | 2023-03-10 22:51:42,256 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-10 22:51:42,278 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn5_1    | 2023-03-10 22:51:42,298 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-03-10 22:51:42,368 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-03-10 22:51:42,401 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn5_1    | 2023-03-10 22:51:42,426 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om2_1    | 2023-03-10 22:51:57,366 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d08309057e39/10.9.0.12 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-03-10 22:52:05,774 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om2_1    | 2023-03-10 22:52:05,924 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-03-10 22:52:06,846 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om2_1    | 2023-03-10 22:52:06,850 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om2_1    | 2023-03-10 22:52:08,840 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om2_1    | 2023-03-10 22:52:09,047 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-03-10 22:52:10,026 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1    | 2023-03-10 22:52:10,026 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om2_1    | 2023-03-10 22:52:10,211 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om2_1    | 2023-03-10 22:52:11,505 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om2_1    | 2023-03-10 22:52:11,628 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1    | 2023-03-10 22:52:11,980 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om2:9872, om1:9872, om3:9872
om2_1    | 2023-03-10 22:52:12,070 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:5, i:102)
om2_1    | 2023-03-10 22:52:12,491 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om2_1    | 2023-03-10 22:52:13,052 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om2_1    | 2023-03-10 22:52:13,064 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om2_1    | 2023-03-10 22:52:13,068 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om2_1    | 2023-03-10 22:52:13,070 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om2_1    | 2023-03-10 22:52:13,086 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om2_1    | 2023-03-10 22:52:13,087 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om2_1    | 2023-03-10 22:52:13,088 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om2_1    | 2023-03-10 22:52:13,110 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-03-10 22:52:13,115 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om2_1    | 2023-03-10 22:52:13,119 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1    | 2023-03-10 22:52:13,212 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1    | 2023-03-10 22:52:13,234 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om2_1    | 2023-03-10 22:52:13,236 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om2_1    | 2023-03-10 22:52:14,137 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om2_1    | 2023-03-10 22:52:14,158 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om2_1    | 2023-03-10 22:52:14,160 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om2_1    | 2023-03-10 22:52:14,180 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1    | 2023-03-10 22:52:14,181 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | 2023-03-10 22:52:14,214 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1    | 2023-03-10 22:52:14,231 [om2-impl-thread1] INFO server.RaftServer: om2: found a subdirectory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om2_1    | 2023-03-10 22:52:14,272 [main] INFO server.RaftServer: om2: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@8c12524[Not completed]
om2_1    | 2023-03-10 22:52:14,276 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om2_1    | 2023-03-10 22:52:14,369 [main] INFO om.OzoneManager: Creating RPC Server
om2_1    | 2023-03-10 22:52:14,581 [pool-29-thread-1] INFO server.RaftServer$Division: om2: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om2_1    | 2023-03-10 22:52:14,610 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om2_1    | 2023-03-10 22:52:14,622 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-03-10 22:51:49,449 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-03-10 22:51:49,450 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-03-10 22:51:49,452 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-03-10 22:51:49,490 [pool-34-thread-1] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4: new RaftServerImpl for group-CBC13A60F575:[] with ContainerStateMachine:uninitialized
dn2_1    | 2023-03-10 22:51:49,500 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-03-10 22:51:49,533 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-03-10 22:51:49,538 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-03-10 22:51:49,554 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-03-10 22:51:49,557 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-03-10 22:51:49,561 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-03-10 22:51:49,564 [pool-34-thread-1] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-03-10 22:51:49,564 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-03-10 22:51:49,565 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-03-10 22:51:49,567 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-03-10 22:51:49,580 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-03-10 22:51:49,597 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-03-10 22:51:49,600 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-03-10 22:51:49,602 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-03-10 22:51:49,614 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-03-10 22:51:49,615 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-03-10 22:51:49,615 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-03-10 22:51:49,615 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-03-10 22:51:49,705 [main] INFO util.log: Logging initialized @49279ms to org.eclipse.jetty.util.log.Slf4jLog
dn2_1    | 2023-03-10 22:51:50,737 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn2_1    | 2023-03-10 22:51:50,829 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn2_1    | 2023-03-10 22:51:50,883 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn2_1    | 2023-03-10 22:51:50,906 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn2_1    | 2023-03-10 22:51:50,909 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn2_1    | 2023-03-10 22:51:50,912 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn2_1    | 2023-03-10 22:51:51,284 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn2_1    | 2023-03-10 22:51:51,334 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn2_1    | 2023-03-10 22:51:51,351 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn2_1    | 2023-03-10 22:51:51,649 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn2_1    | 2023-03-10 22:51:51,653 [main] INFO server.session: No SessionScavenger set, using defaults
dn2_1    | 2023-03-10 22:51:51,655 [main] INFO server.session: node0 Scavenging every 660000ms
dn2_1    | 2023-03-10 22:51:51,755 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@580902cd{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn2_1    | 2023-03-10 22:51:51,777 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6c075e9d{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn2_1    | 2023-03-10 22:51:52,830 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1373e3ee{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-4446204316349901308/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn2_1    | 2023-03-10 22:51:52,917 [main] INFO server.AbstractConnector: Started ServerConnector@6af65f29{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn2_1    | 2023-03-10 22:51:52,923 [main] INFO server.Server: Started @52497ms
dn2_1    | 2023-03-10 22:51:52,944 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn2_1    | 2023-03-10 22:51:52,951 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn2_1    | 2023-03-10 22:51:52,974 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn2_1    | 2023-03-10 22:51:52,996 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn2_1    | 2023-03-10 22:51:53,119 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn2_1    | 2023-03-10 22:51:53,120 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn2_1    | 2023-03-10 22:51:53,182 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@6ffcbd66] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn2_1    | 2023-03-10 22:51:53,576 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.20:9891
dn2_1    | 2023-03-10 22:51:53,611 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn2_1    | 2023-03-10 22:51:57,023 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-03-10 22:51:58,024 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-03-10 22:51:54,831 [EndpointStateMachine task thread for recon/10.9.0.20:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.20:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-03-10 22:51:54,861 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-03-10 22:51:55,865 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-03-10 22:51:56,866 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-03-10 22:51:57,869 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-03-10 22:51:58,870 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-03-10 22:51:59,957 [EndpointStateMachine task thread for recon/10.9.0.20:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn4_1    | java.net.SocketTimeoutException: Call From b0c55c35d5b9/10.9.0.18 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:46298 remote=recon/10.9.0.20:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn4_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:46298 remote=recon/10.9.0.20:9891]
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn4_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn4_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn4_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn4_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn4_1    | 2023-03-10 22:52:01,550 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn4_1    | 2023-03-10 22:52:01,581 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn4_1    | 2023-03-10 22:52:02,287 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn4_1    | 2023-03-10 22:52:02,292 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn4_1    | 2023-03-10 22:52:02,492 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/in_use.lock acquired by nodename 6@b0c55c35d5b9
dn4_1    | 2023-03-10 22:52:02,496 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/8581ae3d-c50a-4a49-9d70-d98e5419b7a9/in_use.lock acquired by nodename 6@b0c55c35d5b9
dn4_1    | 2023-03-10 22:52:02,505 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/in_use.lock acquired by nodename 6@b0c55c35d5b9
dn4_1    | 2023-03-10 22:52:02,741 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=10, votedFor=e3e4587c-aa42-4e86-ae9a-d3e448365275} from /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/current/raft-meta
dn4_1    | 2023-03-10 22:52:02,747 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=e78c5ce1-46ab-4889-a0cd-5903ae46614d} from /data/metadata/ratis/8581ae3d-c50a-4a49-9d70-d98e5419b7a9/current/raft-meta
dn4_1    | 2023-03-10 22:52:02,747 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=6, votedFor=e78c5ce1-46ab-4889-a0cd-5903ae46614d} from /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/current/raft-meta
dn4_1    | 2023-03-10 22:52:03,035 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9: set configuration 3: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:03,030 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575: set configuration 12: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:03,045 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3: set configuration 22: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:03,145 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO ratis.ContainerStateMachine: group-0049FBBC23B3: Setting the last applied index to (t:10, i:35)
dn4_1    | 2023-03-10 22:52:03,154 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO ratis.ContainerStateMachine: group-CBC13A60F575: Setting the last applied index to (t:6, i:20)
dn4_1    | 2023-03-10 22:52:03,170 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO ratis.ContainerStateMachine: group-D98E5419B7A9: Setting the last applied index to (t:3, i:4)
dn4_1    | 2023-03-10 22:52:04,536 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-03-10 22:52:04,594 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-03-10 22:52:04,594 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-03-10 22:52:04,638 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-03-10 22:52:04,644 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-10 22:52:04,644 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-03-10 22:52:04,649 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-10 22:52:04,644 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-03-10 22:52:04,649 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-10 22:52:04,659 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-03-10 22:52:04,661 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-03-10 22:51:59,025 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-03-10 22:52:01,081 [EndpointStateMachine task thread for recon/10.9.0.20:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From f1622084839d/10.9.0.16 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.16:36784 remote=recon/10.9.0.20:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn2_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.16:36784 remote=recon/10.9.0.20:9891]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn2_1    | 2023-03-10 22:52:01,623 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn2_1    | 2023-03-10 22:52:01,667 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn2_1    | 2023-03-10 22:52:02,332 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn2_1    | 2023-03-10 22:52:02,351 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 3892a4e1-c878-42af-adb7-db66a90d61f4
dn2_1    | 2023-03-10 22:52:02,505 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/in_use.lock acquired by nodename 7@f1622084839d
dn2_1    | 2023-03-10 22:52:02,561 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/2d4f1397-2cdc-4336-a562-676bab171c02/in_use.lock acquired by nodename 7@f1622084839d
dn2_1    | 2023-03-10 22:52:02,563 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/in_use.lock acquired by nodename 7@f1622084839d
dn2_1    | 2023-03-10 22:52:02,578 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=3892a4e1-c878-42af-adb7-db66a90d61f4} from /data/metadata/ratis/2d4f1397-2cdc-4336-a562-676bab171c02/current/raft-meta
dn2_1    | 2023-03-10 22:52:02,622 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=11, votedFor=3892a4e1-c878-42af-adb7-db66a90d61f4} from /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/current/raft-meta
dn2_1    | 2023-03-10 22:52:02,594 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=6, votedFor=e78c5ce1-46ab-4889-a0cd-5903ae46614d} from /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/current/raft-meta
dn2_1    | 2023-03-10 22:52:02,937 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00: set configuration 11: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:02,947 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02: set configuration 3: peers:[3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-03-10 22:52:30,355 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 5 for changeToCandidate
om1_1    | 2023-03-10 22:52:30,358 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om1_1    | 2023-03-10 22:52:30,359 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-LeaderElection1
om1_1    | 2023-03-10 22:52:30,375 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 5 for 67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-03-10 22:52:30,449 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1    | 2023-03-10 22:52:30,450 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 2023-03-10 22:52:30,466 [om1@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om3
om1_1    | 2023-03-10 22:52:30,468 [om1@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
om1_1    | 2023-03-10 22:52:32,384 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(PRE_VOTE, om2, group-D66704EFC61C, 5, (t:5, i:102))
om1_1    | 2023-03-10 22:52:32,413 [grpc-default-executor-0] INFO impl.VoteContext: om1@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om2: our priority 0 <= candidate's priority 0
om1_1    | 2023-03-10 22:52:32,461 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to PRE_VOTE vote request: om2<-om1#0:OK-t5. Peer's state: om1@group-D66704EFC61C:t5, leader=null, voted=om1, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c102, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-03-10 22:52:32,504 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
om1_1    | 2023-03-10 22:52:32,505 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om1<-om2#0:OK-t5
om1_1    | 2023-03-10 22:52:32,505 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result PASSED
om1_1    | 2023-03-10 22:52:32,520 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 ELECTION round 0: submit vote requests at term 6 for 67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-03-10 22:52:32,530 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1    | 2023-03-10 22:52:32,531 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 2023-03-10 22:52:32,569 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(ELECTION, om2, group-D66704EFC61C, 6, (t:5, i:102))
om1_1    | 2023-03-10 22:52:32,572 [grpc-default-executor-0] INFO impl.VoteContext: om1@group-D66704EFC61C-CANDIDATE: reject ELECTION from om2: already has voted for om1 at current term 6
om1_1    | 2023-03-10 22:52:32,572 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to ELECTION vote request: om2<-om1#0:FAIL-t6. Peer's state: om1@group-D66704EFC61C:t6, leader=null, voted=om1, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c102, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-03-10 22:52:32,923 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(PRE_VOTE, om3, group-D66704EFC61C, 5, (t:5, i:102))
om1_1    | 2023-03-10 22:52:32,923 [grpc-default-executor-0] INFO impl.VoteContext: om1@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om3: our priority 0 <= candidate's priority 0
om1_1    | 2023-03-10 22:52:32,923 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to PRE_VOTE vote request: om3<-om1#0:OK-t6. Peer's state: om1@group-D66704EFC61C:t6, leader=null, voted=om1, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c102, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-03-10 22:52:32,932 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1: ELECTION PASSED received 2 response(s) and 0 exception(s):
om1_1    | 2023-03-10 22:52:32,935 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om1<-om3#0:OK-t6
om1_1    | 2023-03-10 22:52:32,935 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 1: om1<-om2#0:FAIL-t6
om1_1    | 2023-03-10 22:52:32,936 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 ELECTION round 0: result PASSED
om1_1    | 2023-03-10 22:52:32,937 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-LeaderElection1
om1_1    | 2023-03-10 22:52:32,938 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from CANDIDATE to LEADER at term 6 for changeToLeader
om1_1    | 2023-03-10 22:52:32,939 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: change Leader from null to om1 at term 6 for becomeLeader, leader elected after 16635ms
om1_1    | 2023-03-10 22:52:32,973 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om1_1    | 2023-03-10 22:52:33,002 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om1_1    | 2023-03-10 22:52:33,004 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om1_1    | 2023-03-10 22:52:33,033 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om1_1    | 2023-03-10 22:52:33,035 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om1_1    | 2023-03-10 22:52:33,036 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om1_1    | 2023-03-10 22:52:33,065 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om1_1    | 2023-03-10 22:52:33,083 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om1_1    | 2023-03-10 22:52:33,130 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om1_1    | 2023-03-10 22:52:33,131 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-03-10 22:52:33,143 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om1_1    | 2023-03-10 22:52:33,158 [om1@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om1_1    | 2023-03-10 22:52:33,160 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1    | 2023-03-10 22:52:33,161 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1    | 2023-03-10 22:52:33,161 [om1@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1    | 2023-03-10 22:52:33,162 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om1_1    | 2023-03-10 22:52:33,187 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om1_1    | 2023-03-10 22:52:33,190 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-03-10 22:52:33,190 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om1_1    | 2023-03-10 22:52:33,205 [om1@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om1_1    | 2023-03-10 22:52:33,205 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1    | 2023-03-10 22:52:33,205 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1    | 2023-03-10 22:52:33,206 [om1@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1    | 2023-03-10 22:52:33,206 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om1_1    | 2023-03-10 22:52:33,217 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-LeaderStateImpl
om1_1    | 2023-03-10 22:52:33,247 [om1@group-D66704EFC61C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: Rolling segment log-67_102 to index:102
om1_1    | 2023-03-10 22:52:33,259 [om1@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_67 to /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_67-102
om1_1    | 2023-03-10 22:52:33,285 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 103: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	... 1 more
dn1_1    | 2023-03-10 22:52:01,574 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn1_1    | 2023-03-10 22:52:01,596 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn1_1    | 2023-03-10 22:52:02,392 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn1_1    | 2023-03-10 22:52:02,455 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis e3e4587c-aa42-4e86-ae9a-d3e448365275
dn1_1    | 2023-03-10 22:52:02,616 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/in_use.lock acquired by nodename 7@516736b05cb5
dn1_1    | 2023-03-10 22:52:02,622 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d1fa3d0d-6abe-48bc-b456-d0b7aa0d159e/in_use.lock acquired by nodename 7@516736b05cb5
dn1_1    | 2023-03-10 22:52:02,687 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/in_use.lock acquired by nodename 7@516736b05cb5
dn1_1    | 2023-03-10 22:52:02,730 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=e3e4587c-aa42-4e86-ae9a-d3e448365275} from /data/metadata/ratis/d1fa3d0d-6abe-48bc-b456-d0b7aa0d159e/current/raft-meta
dn1_1    | 2023-03-10 22:52:02,755 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=6, votedFor=e78c5ce1-46ab-4889-a0cd-5903ae46614d} from /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/current/raft-meta
dn1_1    | 2023-03-10 22:52:02,783 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=10, votedFor=e3e4587c-aa42-4e86-ae9a-d3e448365275} from /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/current/raft-meta
dn1_1    | 2023-03-10 22:52:02,821 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn1_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:321)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	... 1 more
dn1_1    | 2023-03-10 22:52:02,998 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E: set configuration 3: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:03,034 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575: set configuration 12: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:03,035 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3: set configuration 22: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:03,154 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO ratis.ContainerStateMachine: group-D0B7AA0D159E: Setting the last applied index to (t:3, i:4)
dn1_1    | 2023-03-10 22:52:03,173 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO ratis.ContainerStateMachine: group-CBC13A60F575: Setting the last applied index to (t:6, i:20)
dn1_1    | 2023-03-10 22:52:03,196 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO ratis.ContainerStateMachine: group-0049FBBC23B3: Setting the last applied index to (t:10, i:35)
dn1_1    | 2023-03-10 22:52:04,153 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-03-10 22:52:04,162 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-03-10 22:52:04,189 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-03-10 22:52:04,262 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-03-10 22:52:04,274 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-03-10 22:52:04,276 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-03-10 22:52:04,292 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-03-10 22:52:04,339 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-03-10 22:52:04,276 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-03-10 22:52:04,387 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-03-10 22:52:04,388 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-03-10 22:52:04,388 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-03-10 22:52:04,300 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-03-10 22:52:04,419 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-03-10 22:52:04,419 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-03-10 22:52:04,421 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-03-10 22:52:04,421 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-03-10 22:52:04,421 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-03-10 22:52:04,481 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-03-10 22:52:04,482 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-03-10 22:52:04,490 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-03-10 22:52:04,490 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-03-10 22:51:44,744 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn5_1    | 2023-03-10 22:51:44,792 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn5_1    | 2023-03-10 22:51:44,817 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn5_1    | 2023-03-10 22:51:44,818 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-03-10 22:51:44,818 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-03-10 22:51:44,878 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-03-10 22:51:44,893 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServer: 178b30e1-b74d-4f4d-a142-c930eee71455: found a subdirectory /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00
dn5_1    | 2023-03-10 22:51:44,988 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServer: 178b30e1-b74d-4f4d-a142-c930eee71455: addNew group-5B98FDC36F00:[] returns group-5B98FDC36F00:java.util.concurrent.CompletableFuture@6a4fb719[Not completed]
dn5_1    | 2023-03-10 22:51:44,988 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServer: 178b30e1-b74d-4f4d-a142-c930eee71455: found a subdirectory /data/metadata/ratis/b1358d3e-7575-4e13-af6d-8e287caccc68
dn5_1    | 2023-03-10 22:51:44,989 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServer: 178b30e1-b74d-4f4d-a142-c930eee71455: addNew group-8E287CACCC68:[] returns group-8E287CACCC68:java.util.concurrent.CompletableFuture@3212422e[Not completed]
dn5_1    | 2023-03-10 22:51:45,283 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn5_1    | 2023-03-10 22:51:45,784 [pool-26-thread-1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455: new RaftServerImpl for group-5B98FDC36F00:[] with ContainerStateMachine:uninitialized
dn5_1    | 2023-03-10 22:51:45,816 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-03-10 22:51:45,849 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-03-10 22:51:45,850 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-03-10 22:51:45,850 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-03-10 22:51:45,850 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-03-10 22:51:45,854 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-03-10 22:51:45,963 [pool-26-thread-1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-03-10 22:51:45,988 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-03-10 22:51:46,067 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-03-10 22:51:46,113 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-03-10 22:51:46,362 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-03-10 22:51:46,457 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-03-10 22:51:46,461 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-03-10 22:51:47,321 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-03-10 22:51:47,375 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-03-10 22:51:47,389 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-03-10 22:51:47,399 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-03-10 22:51:47,421 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-03-10 22:51:47,537 [pool-26-thread-1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455: new RaftServerImpl for group-8E287CACCC68:[] with ContainerStateMachine:uninitialized
dn5_1    | 2023-03-10 22:51:47,546 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-03-10 22:51:47,551 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-03-10 22:51:47,551 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-03-10 22:51:47,553 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-03-10 22:51:47,553 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-03-10 22:51:47,554 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-03-10 22:51:47,554 [pool-26-thread-1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-03-10 22:51:47,556 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-03-10 22:51:47,560 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-03-10 22:51:47,571 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-03-10 22:51:47,571 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-03-10 22:51:47,571 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-03-10 22:51:47,572 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-03-10 22:51:47,797 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-03-10 22:51:47,802 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-03-10 22:51:47,802 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-03-10 22:51:47,803 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-03-10 22:51:47,803 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-03-10 22:51:47,855 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn5_1    | 2023-03-10 22:51:48,017 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn5_1    | 2023-03-10 22:51:48,231 [main] INFO util.log: Logging initialized @48070ms to org.eclipse.jetty.util.log.Slf4jLog
dn5_1    | 2023-03-10 22:51:49,126 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn5_1    | 2023-03-10 22:51:49,161 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn5_1    | 2023-03-10 22:51:49,289 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn5_1    | 2023-03-10 22:51:49,295 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn5_1    | 2023-03-10 22:51:49,321 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn5_1    | 2023-03-10 22:51:49,322 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn5_1    | 2023-03-10 22:51:49,752 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn5_1    | 2023-03-10 22:51:49,918 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn5_1    | 2023-03-10 22:51:49,920 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn2_1    | 2023-03-10 22:52:02,965 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575: set configuration 12: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:03,164 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO ratis.ContainerStateMachine: group-676BAB171C02: Setting the last applied index to (t:3, i:4)
dn2_1    | 2023-03-10 22:52:03,185 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO ratis.ContainerStateMachine: group-5B98FDC36F00: Setting the last applied index to (t:11, i:16)
dn2_1    | 2023-03-10 22:52:03,185 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO ratis.ContainerStateMachine: group-CBC13A60F575: Setting the last applied index to (t:6, i:20)
dn2_1    | 2023-03-10 22:52:04,601 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-03-10 22:52:04,621 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-03-10 22:52:04,685 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-03-10 22:52:04,727 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-03-10 22:52:04,734 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-03-10 22:52:04,734 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-03-10 22:52:04,746 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-03-10 22:52:04,730 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-03-10 22:52:04,762 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-03-10 22:52:04,799 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-03-10 22:52:04,805 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-03-10 22:52:04,809 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-03-10 22:52:04,811 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-03-10 22:52:04,864 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-03-10 22:52:04,879 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-03-10 22:52:04,909 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-03-10 22:52:04,916 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-03-10 22:52:04,916 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-03-10 22:52:04,973 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-03-10 22:52:04,980 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-03-10 22:52:04,986 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-03-10 22:52:05,020 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-03-10 22:52:05,038 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-03-10 22:52:05,019 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-03-10 22:52:05,075 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575
dn2_1    | 2023-03-10 22:52:05,081 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00
dn2_1    | 2023-03-10 22:52:05,086 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-03-10 22:52:05,098 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-03-10 22:52:05,109 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-03-10 22:52:05,110 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-03-10 22:52:05,113 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-03-10 22:52:05,113 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-03-10 22:52:05,116 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-03-10 22:52:05,116 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-03-10 22:52:05,119 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-03-10 22:52:05,120 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-03-10 22:52:05,130 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/2d4f1397-2cdc-4336-a562-676bab171c02
dn2_1    | 2023-03-10 22:52:05,137 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-03-10 22:52:05,137 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
om1_1    | 2023-03-10 22:52:33,330 [om1@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_103
om1_1    | 2023-03-10 22:52:34,353 [om1@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om1_1    | [id: "om1"
om1_1    | address: "om1:9872"
om1_1    | startupRole: FOLLOWER
om1_1    | , id: "om3"
om1_1    | address: "om3:9872"
om1_1    | startupRole: FOLLOWER
om1_1    | , id: "om2"
om1_1    | address: "om2:9872"
om1_1    | startupRole: FOLLOWER
om1_1    | ]
om1_1    | 2023-03-10 22:53:03,961 [IPC Server handler 0 on default port 9862] WARN db.RDBStore: Unable to get delta updates since sequenceNumber 102. This exception will be thrown to the client
om1_1    | org.apache.hadoop.hdds.utils.db.SequenceNumberNotFoundException: Invalid transaction log iterator when getting updates since sequence number 102
om1_1    | 	at org.apache.hadoop.hdds.utils.db.RDBStore.getUpdatesSince(RDBStore.java:375)
om1_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.getDBUpdates(OzoneManager.java:3975)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getOMDBUpdates(OzoneManagerRequestHandler.java:354)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleReadRequest(OzoneManagerRequestHandler.java:233)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:223)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:177)
om1_1    | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
om1_1    | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om1_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om1_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om1_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om1_1    | 2023-03-10 22:53:04,226 [qtp371037638-124] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
om1_1    | 2023-03-10 22:53:04,233 [qtp371037638-124] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.checkpoints/om.db_checkpoint_1678488784227 in 5 milliseconds
om1_1    | 2023-03-10 22:53:04,274 [qtp371037638-124] INFO db.RDBCheckpointManager: Waited for 40 milliseconds for checkpoint directory /data/metadata/db.checkpoints/om.db_checkpoint_1678488784227 availability.
om1_1    | 2023-03-10 22:53:04,332 [qtp371037638-124] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 57 milliseconds
dn3_1    | 2023-03-10 22:51:46,594 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn3_1    | 2023-03-10 22:51:46,618 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn3_1    | 2023-03-10 22:51:46,641 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn3_1    | 2023-03-10 22:51:46,642 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-03-10 22:51:46,644 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-03-10 22:51:46,693 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-03-10 22:51:46,705 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServer: 1a6d358d-6662-4447-914c-d709a67ff716: found a subdirectory /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00
dn3_1    | 2023-03-10 22:51:46,768 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServer: 1a6d358d-6662-4447-914c-d709a67ff716: addNew group-5B98FDC36F00:[] returns group-5B98FDC36F00:java.util.concurrent.CompletableFuture@6a2f5b96[Not completed]
dn3_1    | 2023-03-10 22:51:46,773 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServer: 1a6d358d-6662-4447-914c-d709a67ff716: found a subdirectory /data/metadata/ratis/7572f4a9-8e86-407b-b624-5c511402a23e
dn3_1    | 2023-03-10 22:51:46,773 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServer: 1a6d358d-6662-4447-914c-d709a67ff716: addNew group-5C511402A23E:[] returns group-5C511402A23E:java.util.concurrent.CompletableFuture@29b63fe3[Not completed]
dn3_1    | 2023-03-10 22:51:46,773 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServer: 1a6d358d-6662-4447-914c-d709a67ff716: found a subdirectory /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3
dn3_1    | 2023-03-10 22:51:46,774 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServer: 1a6d358d-6662-4447-914c-d709a67ff716: addNew group-0049FBBC23B3:[] returns group-0049FBBC23B3:java.util.concurrent.CompletableFuture@4afe5fda[Not completed]
dn3_1    | 2023-03-10 22:51:47,140 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn3_1    | 2023-03-10 22:51:47,600 [pool-34-thread-1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716: new RaftServerImpl for group-5B98FDC36F00:[] with ContainerStateMachine:uninitialized
dn3_1    | 2023-03-10 22:51:47,651 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-03-10 22:51:47,735 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-03-10 22:51:47,735 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-03-10 22:51:47,735 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-03-10 22:51:47,736 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-03-10 22:51:47,747 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-03-10 22:51:47,876 [pool-34-thread-1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-03-10 22:51:47,917 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-03-10 22:51:47,991 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-03-10 22:51:48,005 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-03-10 22:51:48,441 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-03-10 22:51:48,498 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-03-10 22:51:48,535 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-03-10 22:51:49,404 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-03-10 22:51:49,448 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-03-10 22:51:49,450 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-03-10 22:51:49,498 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-03-10 22:51:49,509 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-03-10 22:51:49,536 [pool-34-thread-1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716: new RaftServerImpl for group-5C511402A23E:[] with ContainerStateMachine:uninitialized
dn3_1    | 2023-03-10 22:51:49,537 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-03-10 22:51:49,534 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn3_1    | 2023-03-10 22:51:49,574 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-03-10 22:51:49,575 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-03-10 22:51:49,575 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-03-10 22:51:49,575 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-03-10 22:51:49,575 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-03-10 22:51:49,577 [pool-34-thread-1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-03-10 22:51:49,585 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-03-10 22:51:49,590 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-03-10 22:51:49,591 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-03-10 22:51:49,597 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-03-10 22:51:49,612 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-03-10 22:51:49,613 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-03-10 22:51:49,626 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-03-10 22:51:49,642 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-03-10 22:51:49,643 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-03-10 22:51:49,643 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-03-10 22:51:49,643 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-03-10 22:51:49,680 [pool-34-thread-1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716: new RaftServerImpl for group-0049FBBC23B3:[] with ContainerStateMachine:uninitialized
dn3_1    | 2023-03-10 22:51:49,740 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-03-10 22:51:49,748 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-03-10 22:51:49,748 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-03-10 22:51:49,748 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-03-10 22:51:49,748 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | 2023-03-10 22:52:14,623 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om2_1    | 2023-03-10 22:52:14,623 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1    | 2023-03-10 22:52:14,623 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | 2023-03-10 22:52:14,623 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1    | 2023-03-10 22:52:14,731 [pool-29-thread-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om2_1    | 2023-03-10 22:52:14,731 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1    | 2023-03-10 22:52:14,794 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1    | 2023-03-10 22:52:14,829 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om2_1    | 2023-03-10 22:52:15,112 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om2_1    | 2023-03-10 22:52:15,187 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om2_1    | 2023-03-10 22:52:15,188 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om2_1    | 2023-03-10 22:52:16,832 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1    | 2023-03-10 22:52:16,833 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om2_1    | 2023-03-10 22:52:16,833 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om2_1    | 2023-03-10 22:52:16,834 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om2_1    | 2023-03-10 22:52:16,859 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om2_1    | 2023-03-10 22:52:18,980 [main] INFO reflections.Reflections: Reflections took 4273 ms to scan 8 urls, producing 23 keys and 581 values [using 2 cores]
om2_1    | 2023-03-10 22:52:20,501 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om2_1    | 2023-03-10 22:52:20,848 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om2_1    | 2023-03-10 22:52:22,759 [Listener at om2/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om2_1    | 2023-03-10 22:52:22,831 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om2_1    | 2023-03-10 22:52:22,831 [Listener at om2/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om2_1    | 2023-03-10 22:52:23,205 [Listener at om2/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om2/10.9.0.12:9862
om2_1    | 2023-03-10 22:52:23,206 [Listener at om2/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om2 at port 9872
om2_1    | 2023-03-10 22:52:23,253 [om2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@d08309057e39
om2_1    | 2023-03-10 22:52:23,301 [om2-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=5, votedFor=om1} from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/raft-meta
om2_1    | 2023-03-10 22:52:23,463 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-03-10 22:52:23,480 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om2_1    | 2023-03-10 22:52:23,536 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om2_1    | 2023-03-10 22:52:23,536 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-03-10 22:52:23,542 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om2_1    | 2023-03-10 22:52:23,547 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om2_1    | 2023-03-10 22:52:23,566 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1    | 2023-03-10 22:52:23,671 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om2_1    | 2023-03-10 22:52:23,673 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om2_1    | 2023-03-10 22:52:23,755 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om2@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om2_1    | 2023-03-10 22:52:23,756 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om2_1    | 2023-03-10 22:52:23,757 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om2_1    | 2023-03-10 22:52:23,766 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1    | 2023-03-10 22:52:23,767 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om2_1    | 2023-03-10 22:52:23,767 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om2_1    | 2023-03-10 22:52:23,769 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om2_1    | 2023-03-10 22:52:23,770 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om2_1    | 2023-03-10 22:52:23,771 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om2_1    | 2023-03-10 22:52:23,833 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om2_1    | 2023-03-10 22:52:23,836 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-03-10 22:52:23,893 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om2_1    | 2023-03-10 22:52:23,896 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om2_1    | 2023-03-10 22:52:23,899 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om2_1    | 2023-03-10 22:52:24,130 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-03-10 22:52:24,176 [om2-impl-thread1] INFO segmented.LogSegment: Successfully read 21 entries from segment file /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_0-20
dn3_1    | 2023-03-10 22:51:49,748 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-03-10 22:51:49,749 [pool-34-thread-1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-03-10 22:51:49,749 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-03-10 22:51:49,751 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-03-10 22:51:49,751 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-03-10 22:51:49,751 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-03-10 22:51:49,754 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-03-10 22:51:49,757 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-03-10 22:51:49,754 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn3_1    | 2023-03-10 22:51:49,817 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-03-10 22:51:49,817 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-03-10 22:51:49,817 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-03-10 22:51:49,818 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-03-10 22:51:49,818 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-03-10 22:51:50,032 [main] INFO util.log: Logging initialized @50679ms to org.eclipse.jetty.util.log.Slf4jLog
dn3_1    | 2023-03-10 22:51:51,295 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn3_1    | 2023-03-10 22:51:51,357 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn3_1    | 2023-03-10 22:51:51,445 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn3_1    | 2023-03-10 22:51:51,471 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn3_1    | 2023-03-10 22:51:51,473 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn3_1    | 2023-03-10 22:51:51,485 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn3_1    | 2023-03-10 22:51:52,123 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn3_1    | 2023-03-10 22:51:52,218 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn3_1    | 2023-03-10 22:51:52,245 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn3_1    | 2023-03-10 22:51:52,548 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn3_1    | 2023-03-10 22:51:52,569 [main] INFO server.session: No SessionScavenger set, using defaults
dn3_1    | 2023-03-10 22:51:52,589 [main] INFO server.session: node0 Scavenging every 660000ms
dn3_1    | 2023-03-10 22:51:52,718 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@14b275bd{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn3_1    | 2023-03-10 22:51:52,727 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4b957db0{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn3_1    | 2023-03-10 22:51:53,959 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6842c101{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-1170840278427099190/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn3_1    | 2023-03-10 22:51:54,012 [main] INFO server.AbstractConnector: Started ServerConnector@b5aa65b{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn3_1    | 2023-03-10 22:51:54,012 [main] INFO server.Server: Started @54658ms
dn3_1    | 2023-03-10 22:51:54,021 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn3_1    | 2023-03-10 22:51:54,021 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn3_1    | 2023-03-10 22:51:54,026 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn3_1    | 2023-03-10 22:51:54,045 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn3_1    | 2023-03-10 22:51:54,103 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn3_1    | 2023-03-10 22:51:54,107 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn3_1    | 2023-03-10 22:51:54,125 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2bc40fec] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn3_1    | 2023-03-10 22:51:54,618 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.20:9891
dn3_1    | 2023-03-10 22:51:54,669 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn3_1    | 2023-03-10 22:51:57,403 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-03-10 22:51:58,404 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1    | 2023-03-10 22:51:12,599 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1    | /************************************************************
om3_1    | STARTUP_MSG: Starting OzoneManager
om3_1    | STARTUP_MSG:   host = b8bcefb332b8/10.9.0.13
om3_1    | STARTUP_MSG:   args = [--upgrade]
om3_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/354d3dc5a16eade52bdcbd566bfb952ac3a1d8b6 ; compiled by 'runner' on 2023-03-10T22:24Z
om3_1    | STARTUP_MSG:   java = 11.0.14.1
dn2_1    | 2023-03-10 22:52:05,139 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-03-10 22:52:05,141 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-03-10 22:52:05,144 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-03-10 22:52:05,144 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-03-10 22:52:05,145 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-03-10 22:52:05,141 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-03-10 22:52:05,144 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-03-10 22:52:05,215 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-03-10 22:52:05,215 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-03-10 22:52:05,215 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-03-10 22:52:05,216 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-03-10 22:52:05,215 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-03-10 22:52:05,340 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-03-10 22:52:05,350 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-03-10 22:52:05,345 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-03-10 22:52:05,368 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-03-10 22:52:05,372 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-03-10 22:52:06,497 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-03-10 22:52:06,525 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-03-10 22:52:06,525 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-03-10 22:52:06,533 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-03-10 22:52:06,726 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-03-10 22:52:06,726 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-03-10 22:52:06,727 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-03-10 22:52:06,728 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-03-10 22:52:06,733 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-03-10 22:52:06,734 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-03-10 22:52:07,722 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00: set configuration 0: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:07,754 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575: set configuration 0: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:01,447 [EndpointStateMachine task thread for recon/10.9.0.20:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From 0de9850545d7/10.9.0.17 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:48158 remote=recon/10.9.0.20:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn3_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:48158 remote=recon/10.9.0.20:9891]
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn3_1    | 2023-03-10 22:52:01,530 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn3_1    | 2023-03-10 22:52:01,561 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn3_1    | 2023-03-10 22:52:02,335 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn3_1    | 2023-03-10 22:52:02,343 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 1a6d358d-6662-4447-914c-d709a67ff716
dn3_1    | 2023-03-10 22:52:02,535 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/7572f4a9-8e86-407b-b624-5c511402a23e/in_use.lock acquired by nodename 7@0de9850545d7
dn3_1    | 2023-03-10 22:52:02,544 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/in_use.lock acquired by nodename 7@0de9850545d7
dn3_1    | 2023-03-10 22:52:02,550 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/in_use.lock acquired by nodename 7@0de9850545d7
dn3_1    | 2023-03-10 22:52:02,592 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=11, votedFor=3892a4e1-c878-42af-adb7-db66a90d61f4} from /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/current/raft-meta
dn3_1    | 2023-03-10 22:52:02,596 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=10, votedFor=e3e4587c-aa42-4e86-ae9a-d3e448365275} from /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/current/raft-meta
dn3_1    | 2023-03-10 22:52:02,690 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=1a6d358d-6662-4447-914c-d709a67ff716} from /data/metadata/ratis/7572f4a9-8e86-407b-b624-5c511402a23e/current/raft-meta
dn3_1    | 2023-03-10 22:52:02,831 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E: set configuration 3: peers:[1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:02,830 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00: set configuration 11: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:02,853 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3: set configuration 22: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-03-10 22:52:24,184 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 21: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-03-10 22:52:24,215 [om2-impl-thread1] INFO segmented.LogSegment: Successfully read 46 entries from segment file /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_21-66
om2_1    | 2023-03-10 22:52:24,223 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-03-10 22:52:24,268 [om2-impl-thread1] INFO segmented.LogSegment: Successfully read 36 entries from segment file /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_67
om2_1    | 2023-03-10 22:52:24,297 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 102
om2_1    | 2023-03-10 22:52:24,304 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 66
om2_1    | 2023-03-10 22:52:24,755 [om2-impl-thread1] INFO raftlog.RaftLog: om2@group-D66704EFC61C-SegmentedRaftLog: commitIndex: updateToMax old=102, new=101, updated? false
dn5_1    | 2023-03-10 22:51:50,519 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn5_1    | 2023-03-10 22:51:50,524 [main] INFO server.session: No SessionScavenger set, using defaults
dn5_1    | 2023-03-10 22:51:50,527 [main] INFO server.session: node0 Scavenging every 660000ms
dn5_1    | 2023-03-10 22:51:50,653 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@273fa9e{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn5_1    | 2023-03-10 22:51:50,654 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3757e8e2{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn5_1    | 2023-03-10 22:51:51,656 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@78b03788{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-2409668453200422240/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn5_1    | 2023-03-10 22:51:51,742 [main] INFO server.AbstractConnector: Started ServerConnector@58f254b1{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn5_1    | 2023-03-10 22:51:51,745 [main] INFO server.Server: Started @51583ms
dn5_1    | 2023-03-10 22:51:51,755 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn5_1    | 2023-03-10 22:51:51,755 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn5_1    | 2023-03-10 22:51:51,765 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn5_1    | 2023-03-10 22:51:51,769 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn5_1    | 2023-03-10 22:51:51,958 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn5_1    | 2023-03-10 22:51:51,966 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn5_1    | 2023-03-10 22:51:52,030 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@62efe67a] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn5_1    | 2023-03-10 22:51:52,680 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.20:9891
dn5_1    | 2023-03-10 22:51:52,852 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn5_1    | 2023-03-10 22:51:55,401 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-03-10 22:51:55,415 [EndpointStateMachine task thread for recon/10.9.0.20:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.20:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-03-10 22:51:56,404 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-03-10 22:51:57,405 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-03-10 22:51:58,406 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-03-10 22:52:00,570 [EndpointStateMachine task thread for recon/10.9.0.20:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn5_1    | java.net.SocketTimeoutException: Call From 82dcecb82990/10.9.0.19 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:44848 remote=recon/10.9.0.20:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn5_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:44848 remote=recon/10.9.0.20:9891]
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn5_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn5_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn5_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn5_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn5_1    | 2023-03-10 22:52:01,355 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn5_1    | 2023-03-10 22:52:01,413 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn5_1    | 2023-03-10 22:52:02,141 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn5_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:321)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	... 1 more
dn5_1    | 2023-03-10 22:52:02,341 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn5_1    | 2023-03-10 22:52:02,375 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 178b30e1-b74d-4f4d-a142-c930eee71455
dn5_1    | 2023-03-10 22:52:02,563 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/in_use.lock acquired by nodename 7@82dcecb82990
dn5_1    | 2023-03-10 22:52:02,614 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/b1358d3e-7575-4e13-af6d-8e287caccc68/in_use.lock acquired by nodename 7@82dcecb82990
dn5_1    | 2023-03-10 22:52:02,639 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=11, votedFor=3892a4e1-c878-42af-adb7-db66a90d61f4} from /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/current/raft-meta
dn5_1    | 2023-03-10 22:52:02,644 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=178b30e1-b74d-4f4d-a142-c930eee71455} from /data/metadata/ratis/b1358d3e-7575-4e13-af6d-8e287caccc68/current/raft-meta
om2_1    | 2023-03-10 22:52:24,757 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: start as a follower, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-03-10 22:52:24,759 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from      null to FOLLOWER at term 5 for startAsFollower
om2_1    | 2023-03-10 22:52:24,776 [om2-impl-thread1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
om2_1    | 2023-03-10 22:52:24,801 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-03-10 22:52:24,821 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-03-10 22:52:24,822 [om2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om2
om2_1    | 2023-03-10 22:52:24,832 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om2_1    | 2023-03-10 22:52:24,844 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om2_1    | 2023-03-10 22:52:24,852 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om2_1    | 2023-03-10 22:52:24,856 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om2_1    | 2023-03-10 22:52:24,863 [Listener at om2/9862] INFO server.RaftServer: om2: start RPC server
om2_1    | 2023-03-10 22:52:25,173 [Listener at om2/9862] INFO server.GrpcService: om2: GrpcService started, listening on 9872
om2_1    | 2023-03-10 22:52:25,180 [Listener at om2/9862] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
om2_1    | 2023-03-10 22:52:25,188 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om2: Started
om2_1    | 2023-03-10 22:52:25,195 [Listener at om2/9862] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
om2_1    | 2023-03-10 22:52:25,197 [Listener at om2/9862] INFO om.OzoneManager: Version File has different layout version (0) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om2_1    | 2023-03-10 22:52:25,788 [Listener at om2/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om2_1    | 2023-03-10 22:52:25,788 [Listener at om2/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om2_1    | 2023-03-10 22:52:25,974 [Listener at om2/9862] INFO util.log: Logging initialized @85641ms to org.eclipse.jetty.util.log.Slf4jLog
om2_1    | 2023-03-10 22:52:26,513 [Listener at om2/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om2_1    | 2023-03-10 22:52:26,541 [Listener at om2/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om2_1    | 2023-03-10 22:52:26,566 [Listener at om2/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om2_1    | 2023-03-10 22:52:26,571 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om2_1    | 2023-03-10 22:52:26,572 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om2_1    | 2023-03-10 22:52:26,573 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om2_1    | 2023-03-10 22:52:26,760 [Listener at om2/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om2_1    | 2023-03-10 22:52:26,770 [Listener at om2/9862] INFO http.HttpServer2: Jetty bound to port 9874
om2_1    | 2023-03-10 22:52:26,785 [Listener at om2/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om2_1    | 2023-03-10 22:52:27,030 [Listener at om2/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om2_1    | 2023-03-10 22:52:27,050 [Listener at om2/9862] INFO server.session: No SessionScavenger set, using defaults
om2_1    | 2023-03-10 22:52:27,072 [Listener at om2/9862] INFO server.session: node0 Scavenging every 600000ms
om2_1    | 2023-03-10 22:52:27,182 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7be38eba{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om2_1    | 2023-03-10 22:52:27,187 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@69eb544f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om2_1    | 2023-03-10 22:52:27,966 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6cceb281{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-16274464093340004891/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om2_1    | 2023-03-10 22:52:28,004 [Listener at om2/9862] INFO server.AbstractConnector: Started ServerConnector@5e385b6f{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om2_1    | 2023-03-10 22:52:28,005 [Listener at om2/9862] INFO server.Server: Started @87675ms
om2_1    | 2023-03-10 22:52:28,025 [Listener at om2/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om2_1    | 2023-03-10 22:52:28,025 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om2_1    | 2023-03-10 22:52:28,028 [Listener at om2/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om2_1    | 2023-03-10 22:52:28,040 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om2_1    | 2023-03-10 22:52:28,058 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om2_1    | 2023-03-10 22:52:28,142 [Listener at om2/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om2_1    | 2023-03-10 22:52:28,267 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@176333ee] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om2_1    | 2023-03-10 22:52:29,950 [om2@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om2@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5174877724ns, electionTimeout:5120ms
om2_1    | 2023-03-10 22:52:29,952 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-FollowerState
om2_1    | 2023-03-10 22:52:29,953 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 5 for changeToCandidate
om2_1    | 2023-03-10 22:52:29,956 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om2_1    | 2023-03-10 22:52:29,957 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-LeaderElection1
om2_1    | 2023-03-10 22:52:29,963 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 5 for 67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:04,482 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-03-10 22:52:04,517 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-03-10 22:52:04,621 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d1fa3d0d-6abe-48bc-b456-d0b7aa0d159e
dn1_1    | 2023-03-10 22:52:04,625 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575
dn1_1    | 2023-03-10 22:52:04,629 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-03-10 22:52:04,629 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-03-10 22:52:04,645 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-03-10 22:52:04,649 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-03-10 22:52:04,650 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-03-10 22:52:04,656 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-03-10 22:52:04,656 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-03-10 22:52:04,664 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-03-10 22:52:04,664 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-03-10 22:52:04,665 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-03-10 22:52:04,668 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-03-10 22:52:04,670 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-03-10 22:52:04,671 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-03-10 22:52:04,671 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-03-10 22:52:04,672 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-03-10 22:52:04,675 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-03-10 22:52:04,720 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3
dn1_1    | 2023-03-10 22:52:04,728 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-03-10 22:52:04,728 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-03-10 22:52:04,738 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-03-10 22:52:04,738 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-03-10 22:52:04,739 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-03-10 22:52:02,996 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00: set configuration 11: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:52:02,991 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68: set configuration 3: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:52:03,184 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO ratis.ContainerStateMachine: group-5B98FDC36F00: Setting the last applied index to (t:11, i:16)
dn5_1    | 2023-03-10 22:52:03,239 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO ratis.ContainerStateMachine: group-8E287CACCC68: Setting the last applied index to (t:3, i:4)
dn5_1    | 2023-03-10 22:52:04,698 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-03-10 22:52:04,710 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-03-10 22:52:04,835 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-03-10 22:52:04,839 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-10 22:52:04,842 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-03-10 22:52:04,855 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-03-10 22:52:04,850 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om3_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.ssl.keystore.reload.interval=60s, hdds.security.ssl.truststore.reload.interval=60s, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4))$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.sst.filtering.service.timeout=300000ms, ozone.tags.system=ACL,BALANCER,CLIENT,CONTAINER,DATANODE,DATASTREAM,DEBUG,DELETION,DEPRECATED,FREON,HA,HDDS,KERBEROS,MANAGEMENT,OM,OPERATION,OZONE,OZONEFS,PERFORMANCE,PIPELINE,RATIS,RECON,REQUIRED,S3GATEWAY,SCM,SECURITY,STORAGE,TLS,TOKEN,UPGRADE,X509, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om3_1    | ************************************************************/
om3_1    | 2023-03-10 22:51:12,706 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1    | 2023-03-10 22:51:23,391 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1    | 2023-03-10 22:51:28,812 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om3_1    | 2023-03-10 22:51:29,432 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1    | 2023-03-10 22:51:29,432 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om3_1    | 2023-03-10 22:51:29,520 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-03-10 22:51:30,060 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = INITIAL_VERSION (version = 0), software layout = MULTITENANCY_SCHEMA (version = 3)
om3_1    | 2023-03-10 22:51:32,724 [main] INFO reflections.Reflections: Reflections took 2046 ms to scan 1 urls, producing 126 keys and 365 values [using 2 cores]
om3_1    | 2023-03-10 22:51:32,970 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-03-10 22:51:33,041 [main] WARN om.OzoneManager: ozone.default.bucket.layout configured to non-legacy bucket layout FILE_SYSTEM_OPTIMIZED when Ozone Manager is pre-finalized for bucket layout support. Legacy buckets will be created by default until Ozone Manager is finalized.
om3_1    | 2023-03-10 22:51:36,123 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863]
om3_1    | 2023-03-10 22:51:36,454 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863]
om3_1    | 2023-03-10 22:51:39,053 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b8bcefb332b8/10.9.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-03-10 22:51:41,055 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b8bcefb332b8/10.9.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-03-10 22:51:43,056 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b8bcefb332b8/10.9.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-03-10 22:51:45,058 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b8bcefb332b8/10.9.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-03-10 22:51:47,059 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b8bcefb332b8/10.9.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-03-10 22:51:49,061 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b8bcefb332b8/10.9.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-03-10 22:51:51,066 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b8bcefb332b8/10.9.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-03-10 22:51:53,068 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b8bcefb332b8/10.9.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-03-10 22:53:04,337 [qtp371037638-124] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1678488784227
om1_1    | 2023-03-10 22:54:06,423 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization started.
om1_1    | 2023-03-10 22:54:06,429 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: ERASURE_CODED_STORAGE_SUPPORT.
om1_1    | 2023-03-10 22:54:06,433 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature ERASURE_CODED_STORAGE_SUPPORT has been finalized.
om1_1    | 2023-03-10 22:54:06,438 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: BUCKET_LAYOUT_SUPPORT.
om1_1    | 2023-03-10 22:54:06,442 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature BUCKET_LAYOUT_SUPPORT has been finalized.
om1_1    | 2023-03-10 22:54:06,442 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: MULTITENANCY_SCHEMA.
om1_1    | 2023-03-10 22:54:06,445 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature MULTITENANCY_SCHEMA has been finalized.
om1_1    | 2023-03-10 22:54:06,445 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
om1_1    | 2023-03-10 22:54:06,445 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization is done.
om1_1    | 2023-03-10 22:54:06,461 [OMDoubleBufferFlushThread] INFO upgrade.OMFinalizeUpgradeResponse: Layout version to persist to DB : 3
om1_1    | 2023-03-10 22:55:44,721 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:new2-volume for user:hadoop
om1_1    | 2023-03-10 22:55:49,193 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: new2-volume
om1_1    | 2023-03-10 22:56:02,104 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: s3v
om1_1    | 2023-03-10 22:56:13,904 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:new2-bucket in volume:s3v
om1_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om1_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:207)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:337)
om1_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om1_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om1_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | 2023-03-10 22:56:43,289 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:ectest-new for user:hadoop
om1_1    | 2023-03-10 22:56:47,865 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ectest of layout FILE_SYSTEM_OPTIMIZED in volume: ectest-new
om1_1    | 2023-03-10 22:56:56,810 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: testpropchange of layout FILE_SYSTEM_OPTIMIZED in volume: ectest-new
dn1_1    | 2023-03-10 22:52:04,739 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-03-10 22:52:04,742 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-03-10 22:52:04,748 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-03-10 22:52:04,822 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-03-10 22:52:04,846 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-03-10 22:52:04,853 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-03-10 22:52:04,857 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-03-10 22:52:04,894 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-03-10 22:52:04,896 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-03-10 22:52:06,047 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-03-10 22:52:06,167 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-03-10 22:52:06,170 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-03-10 22:52:06,172 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-03-10 22:52:06,172 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-03-10 22:52:06,193 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-03-10 22:52:06,173 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-03-10 22:52:06,222 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-03-10 22:52:06,223 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-03-10 22:52:06,491 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575: set configuration 0: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:06,521 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3: set configuration 0: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:06,523 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/current/log_0-0
dn1_1    | 2023-03-10 22:52:06,564 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575: set configuration 1: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:06,566 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E: set configuration 0: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:06,586 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/current/log_1-2
dn1_1    | 2023-03-10 22:52:06,605 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/d1fa3d0d-6abe-48bc-b456-d0b7aa0d159e/current/log_0-0
dn1_1    | 2023-03-10 22:52:06,615 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E: set configuration 1: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:06,615 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/d1fa3d0d-6abe-48bc-b456-d0b7aa0d159e/current/log_1-2
dn1_1    | 2023-03-10 22:52:06,629 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO segmented.LogSegment: Successfully read 4 entries from segment file /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/current/log_0-3
dn1_1    | 2023-03-10 22:52:06,630 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E: set configuration 3: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:04,662 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-03-10 22:52:04,665 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-03-10 22:52:04,665 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-03-10 22:52:04,697 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-03-10 22:52:04,705 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-03-10 22:52:04,708 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-03-10 22:52:04,732 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-03-10 22:52:04,768 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-03-10 22:52:04,774 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-03-10 22:52:04,779 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-03-10 22:52:04,784 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-03-10 22:52:04,800 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-03-10 22:52:04,802 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-03-10 22:52:04,869 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/8581ae3d-c50a-4a49-9d70-d98e5419b7a9
dn4_1    | 2023-03-10 22:52:04,873 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-03-10 22:52:04,884 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-03-10 22:52:04,898 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3
dn4_1    | 2023-03-10 22:52:04,903 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-03-10 22:52:04,903 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-03-10 22:52:04,905 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-03-10 22:52:04,905 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-03-10 22:52:04,912 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-03-10 22:52:04,917 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-03-10 22:52:04,924 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-03-10 22:52:04,928 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-03-10 22:52:04,946 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-03-10 22:52:04,952 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-03-10 22:52:04,953 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-03-10 22:52:04,956 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-03-10 22:52:04,968 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575
dn4_1    | 2023-03-10 22:52:04,969 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-03-10 22:52:04,969 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-03-10 22:52:04,969 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-03-10 22:52:04,970 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-03-10 22:52:04,974 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-03-10 22:52:04,983 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-03-10 22:52:04,983 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-03-10 22:52:04,985 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-03-10 22:52:04,985 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-03-10 22:52:04,998 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-03-10 22:52:05,112 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-03-10 22:52:05,118 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-03-10 22:52:05,119 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-10 22:52:05,125 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-10 22:52:06,408 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-03-10 22:52:06,408 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-03-10 22:52:06,409 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-03-10 22:52:06,444 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-03-10 22:52:06,450 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-10 22:52:06,479 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@35f07ab8] INFO util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1013ms
dn4_1    | GC pool 'ParNew' had collection(s): count=1 time=168ms
dn4_1    | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=941ms
dn4_1    | 2023-03-10 22:52:06,598 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-03-10 22:52:06,598 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-03-10 22:52:06,615 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-03-10 22:52:06,657 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-03-10 22:52:06,658 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om2_1    | 2023-03-10 22:52:30,001 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-03-10 22:52:30,003 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-03-10 22:52:30,013 [om2@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
om2_1    | 2023-03-10 22:52:30,015 [om2@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om3
om2_1    | 2023-03-10 22:52:32,361 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(PRE_VOTE, om1, group-D66704EFC61C, 5, (t:5, i:102))
om2_1    | 2023-03-10 22:52:32,373 [grpc-default-executor-0] INFO impl.VoteContext: om2@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om1: our priority 0 <= candidate's priority 0
om2_1    | 2023-03-10 22:52:32,396 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to PRE_VOTE vote request: om1<-om2#0:OK-t5. Peer's state: om2@group-D66704EFC61C:t5, leader=null, voted=om1, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c102, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-03-10 22:52:32,498 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
om2_1    | 2023-03-10 22:52:32,499 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om2<-om1#0:OK-t5
om2_1    | 2023-03-10 22:52:32,507 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result PASSED
om2_1    | 2023-03-10 22:52:32,521 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 ELECTION round 0: submit vote requests at term 6 for 67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-03-10 22:52:32,539 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-03-10 22:52:32,540 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-03-10 22:52:32,584 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(ELECTION, om1, group-D66704EFC61C, 6, (t:5, i:102))
om2_1    | 2023-03-10 22:52:32,587 [grpc-default-executor-0] INFO impl.VoteContext: om2@group-D66704EFC61C-CANDIDATE: reject ELECTION from om1: already has voted for om2 at current term 6
om2_1    | 2023-03-10 22:52:32,588 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to ELECTION vote request: om1<-om2#0:FAIL-t6. Peer's state: om2@group-D66704EFC61C:t6, leader=null, voted=om2, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c102, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-03-10 22:52:32,703 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(PRE_VOTE, om3, group-D66704EFC61C, 5, (t:5, i:102))
om2_1    | 2023-03-10 22:52:32,703 [grpc-default-executor-0] INFO impl.VoteContext: om2@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om3: our priority 0 <= candidate's priority 0
om2_1    | 2023-03-10 22:52:32,703 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to PRE_VOTE vote request: om3<-om2#0:OK-t6. Peer's state: om2@group-D66704EFC61C:t6, leader=null, voted=om2, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c102, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-03-10 22:52:32,937 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1: ELECTION REJECTED received 2 response(s) and 0 exception(s):
om2_1    | 2023-03-10 22:52:32,943 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om2<-om1#0:FAIL-t6
om2_1    | 2023-03-10 22:52:32,946 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 1: om2<-om3#0:FAIL-t6
om2_1    | 2023-03-10 22:52:32,946 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 ELECTION round 0: result REJECTED
om2_1    | 2023-03-10 22:52:32,958 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from CANDIDATE to FOLLOWER at term 6 for REJECTED
om2_1    | 2023-03-10 22:52:32,958 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-LeaderElection1
om2_1    | 2023-03-10 22:52:32,960 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
om2_1    | 2023-03-10 22:52:32,970 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-03-10 22:52:33,011 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-03-10 22:52:33,431 [om2-server-thread2] INFO server.RaftServer$Division: om2@group-D66704EFC61C: change Leader from null to om1 at term 6 for appendEntries, leader elected after 18319ms
om2_1    | 2023-03-10 22:52:33,455 [om2-server-thread2] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 103: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-03-10 22:52:33,473 [om2-server-thread2] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: Rolling segment log-67_102 to index:102
om2_1    | 2023-03-10 22:52:33,501 [om2@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_67 to /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_67-102
om3_1    | 2023-03-10 22:51:55,070 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b8bcefb332b8/10.9.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
dn2_1    | 2023-03-10 22:52:07,755 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/current/log_0-0
dn2_1    | 2023-03-10 22:52:07,777 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02: set configuration 0: peers:[3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:07,862 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575: set configuration 1: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:07,858 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/2d4f1397-2cdc-4336-a562-676bab171c02/current/log_0-0
dn2_1    | 2023-03-10 22:52:08,028 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02: set configuration 1: peers:[3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:08,028 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/2d4f1397-2cdc-4336-a562-676bab171c02/current/log_1-2
dn2_1    | 2023-03-10 22:52:08,040 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02: set configuration 3: peers:[3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:08,041 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/2d4f1397-2cdc-4336-a562-676bab171c02/current/log_inprogress_3
dn2_1    | 2023-03-10 22:52:08,057 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO segmented.LogSegment: Successfully read 9 entries from segment file /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/current/log_0-8
dn2_1    | 2023-03-10 22:52:08,058 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/current/log_1-2
dn2_1    | 2023-03-10 22:52:08,099 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575: set configuration 3: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:08,100 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO segmented.LogSegment: Successfully read 9 entries from segment file /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/current/log_3-11
dn2_1    | 2023-03-10 22:52:08,127 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00: set configuration 9: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:08,215 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/current/log_9-10
dn2_1    | 2023-03-10 22:52:08,222 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00: set configuration 11: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:08,206 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn2_1    | 2023-03-10 22:52:08,262 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn2_1    | 2023-03-10 22:52:08,239 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO segmented.LogSegment: Successfully read 6 entries from segment file /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/current/log_inprogress_11
dn2_1    | 2023-03-10 22:52:08,311 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 16
dn2_1    | 2023-03-10 22:52:08,316 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 10
dn2_1    | 2023-03-10 22:52:08,231 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575: set configuration 12: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:08,342 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO segmented.LogSegment: Successfully read 9 entries from segment file /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/current/log_inprogress_12
dn2_1    | 2023-03-10 22:52:08,343 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 20
dn2_1    | 2023-03-10 22:52:08,343 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 11
dn2_1    | 2023-03-10 22:52:09,365 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO raftlog.RaftLog: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-SegmentedRaftLog: commitIndex: updateToMax old=16, new=15, updated? false
dn2_1    | 2023-03-10 22:52:09,451 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00: start as a follower, conf=11: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:09,460 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00: changes role from      null to FOLLOWER at term 11 for startAsFollower
dn2_1    | 2023-03-10 22:52:09,405 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO raftlog.RaftLog: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-SegmentedRaftLog: commitIndex: updateToMax old=4, new=3, updated? false
dn2_1    | 2023-03-10 22:52:09,507 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02: start as a follower, conf=3: peers:[3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-03-10 22:51:57,072 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b8bcefb332b8/10.9.0.13 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/10.9.0.14:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-03-10 22:52:06,078 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om3_1    | 2023-03-10 22:52:06,363 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-03-10 22:52:07,981 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om3_1    | 2023-03-10 22:52:08,014 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om3_1    | 2023-03-10 22:52:09,471 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om3_1    | 2023-03-10 22:52:09,784 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-03-10 22:52:10,314 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1    | 2023-03-10 22:52:10,319 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om3_1    | 2023-03-10 22:52:10,414 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om3_1    | 2023-03-10 22:52:11,666 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om3_1    | 2023-03-10 22:52:11,795 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1    | 2023-03-10 22:52:12,073 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om3:9872, om1:9872, om2:9872
om3_1    | 2023-03-10 22:52:12,173 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:5, i:102)
om3_1    | 2023-03-10 22:52:12,404 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om3_1    | 2023-03-10 22:52:12,748 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om3_1    | 2023-03-10 22:52:12,791 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om3_1    | 2023-03-10 22:52:12,818 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om3_1    | 2023-03-10 22:52:12,825 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om3_1    | 2023-03-10 22:52:12,828 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om3_1    | 2023-03-10 22:52:12,829 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om3_1    | 2023-03-10 22:52:12,836 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om3_1    | 2023-03-10 22:52:12,859 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-03-10 22:52:12,872 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om3_1    | 2023-03-10 22:52:12,873 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1    | 2023-03-10 22:52:12,928 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om3_1    | 2023-03-10 22:52:12,953 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om3_1    | 2023-03-10 22:52:12,953 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om3_1    | 2023-03-10 22:52:14,162 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om3_1    | 2023-03-10 22:52:14,236 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om3_1    | 2023-03-10 22:52:14,254 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om3_1    | 2023-03-10 22:52:14,259 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1    | 2023-03-10 22:52:14,269 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1    | 2023-03-10 22:52:14,346 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1    | 2023-03-10 22:52:14,429 [om3-impl-thread1] INFO server.RaftServer: om3: found a subdirectory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om3_1    | 2023-03-10 22:52:14,474 [main] INFO server.RaftServer: om3: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@f723cdb[Not completed]
om3_1    | 2023-03-10 22:52:14,476 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
dn4_1    | 2023-03-10 22:52:06,658 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-03-10 22:52:06,922 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3: set configuration 0: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:06,930 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575: set configuration 0: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:06,951 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/current/log_0-0
dn4_1    | 2023-03-10 22:52:06,930 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9: set configuration 0: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:07,112 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/8581ae3d-c50a-4a49-9d70-d98e5419b7a9/current/log_0-0
dn4_1    | 2023-03-10 22:52:07,118 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9: set configuration 1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:07,124 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/8581ae3d-c50a-4a49-9d70-d98e5419b7a9/current/log_1-2
dn4_1    | 2023-03-10 22:52:07,055 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575: set configuration 1: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:07,129 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/current/log_1-2
dn4_1    | 2023-03-10 22:52:07,131 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575: set configuration 3: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:07,032 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO segmented.LogSegment: Successfully read 4 entries from segment file /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/current/log_0-3
dn4_1    | 2023-03-10 22:52:07,150 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9: set configuration 3: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:07,167 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/8581ae3d-c50a-4a49-9d70-d98e5419b7a9/current/log_inprogress_3
dn4_1    | 2023-03-10 22:52:07,171 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO segmented.LogSegment: Successfully read 9 entries from segment file /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/current/log_3-11
dn4_1    | 2023-03-10 22:52:07,177 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3: set configuration 4: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:07,185 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn4_1    | 2023-03-10 22:52:07,193 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn4_1    | 2023-03-10 22:52:08,614 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@35f07ab8] INFO util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1119ms
dn4_1    | GC pool 'ParNew' had collection(s): count=1 time=1389ms
dn4_1    | 2023-03-10 22:52:08,636 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575: set configuration 12: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:08,614 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn4_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:321)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.util.concurrent.TimeoutException
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	... 1 more
dn3_1    | 2023-03-10 22:52:02,960 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO ratis.ContainerStateMachine: group-5B98FDC36F00: Setting the last applied index to (t:11, i:16)
dn3_1    | 2023-03-10 22:52:02,988 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO ratis.ContainerStateMachine: group-5C511402A23E: Setting the last applied index to (t:3, i:4)
dn3_1    | 2023-03-10 22:52:02,979 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO ratis.ContainerStateMachine: group-0049FBBC23B3: Setting the last applied index to (t:10, i:35)
dn3_1    | 2023-03-10 22:52:04,441 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-03-10 22:52:04,447 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-03-10 22:52:04,483 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-03-10 22:52:04,569 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-03-10 22:52:04,570 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-03-10 22:52:04,572 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-03-10 22:52:04,582 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-03-10 22:52:04,599 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-03-10 22:52:04,604 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-03-10 22:52:04,621 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-03-10 22:52:04,627 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-03-10 22:52:04,623 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-03-10 22:52:04,632 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-03-10 22:52:04,636 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-03-10 22:52:04,636 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-03-10 22:52:04,635 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-03-10 22:52:04,676 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-03-10 22:52:04,703 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-03-10 22:52:04,742 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-03-10 22:52:04,747 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-03-10 22:52:04,749 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-03-10 22:52:04,752 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-03-10 22:52:04,759 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-03-10 22:52:04,761 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-03-10 22:52:04,892 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/7572f4a9-8e86-407b-b624-5c511402a23e
dn3_1    | 2023-03-10 22:52:04,892 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3
dn3_1    | 2023-03-10 22:52:04,900 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-03-10 22:52:04,902 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-03-10 22:52:04,909 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-03-10 22:52:04,914 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-03-10 22:52:04,927 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-03-10 22:52:04,928 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-03-10 22:52:04,929 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00
dn3_1    | 2023-03-10 22:52:04,930 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-03-10 22:52:04,930 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-03-10 22:52:04,930 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-03-10 22:52:04,931 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-03-10 22:52:08,614 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO segmented.LogSegment: Successfully read 18 entries from segment file /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/current/log_4-21
dn4_1    | 2023-03-10 22:52:08,713 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3: set configuration 22: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:08,715 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO segmented.LogSegment: Successfully read 14 entries from segment file /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/current/log_inprogress_22
dn4_1    | 2023-03-10 22:52:08,723 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 35
dn4_1    | 2023-03-10 22:52:08,712 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO segmented.LogSegment: Successfully read 9 entries from segment file /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/current/log_inprogress_12
dn4_1    | 2023-03-10 22:52:08,749 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 20
dn4_1    | 2023-03-10 22:52:08,750 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 11
dn4_1    | 2023-03-10 22:52:08,723 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 21
dn4_1    | 2023-03-10 22:52:10,454 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO raftlog.RaftLog: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-SegmentedRaftLog: commitIndex: updateToMax old=20, new=19, updated? false
dn4_1    | 2023-03-10 22:52:10,460 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575: start as a follower, conf=12: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:10,460 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575: changes role from      null to FOLLOWER at term 6 for startAsFollower
dn4_1    | 2023-03-10 22:52:10,468 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: start e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-FollowerState
dn4_1    | 2023-03-10 22:52:10,478 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO raftlog.RaftLog: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-SegmentedRaftLog: commitIndex: updateToMax old=4, new=3, updated? false
dn4_1    | 2023-03-10 22:52:10,507 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9: start as a follower, conf=3: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:10,509 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn4_1    | 2023-03-10 22:52:10,503 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-CBC13A60F575,id=e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn4_1    | 2023-03-10 22:52:10,481 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO raftlog.RaftLog: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-SegmentedRaftLog: commitIndex: updateToMax old=35, new=33, updated? false
dn4_1    | 2023-03-10 22:52:10,509 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3: start as a follower, conf=22: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:10,510 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3: changes role from      null to FOLLOWER at term 10 for startAsFollower
dn4_1    | 2023-03-10 22:52:10,518 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-03-10 22:52:10,510 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: start e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-FollowerState
dn4_1    | 2023-03-10 22:52:10,509 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: start e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-FollowerState
dn4_1    | 2023-03-10 22:52:10,525 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-03-10 22:52:10,526 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-03-10 22:52:10,532 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-03-10 22:52:10,562 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-10 22:52:10,563 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-10 22:52:10,567 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-10 22:52:10,572 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-03-10 22:52:33,577 [om2@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_103
om2_1    | 2023-03-10 22:52:34,144 [om2@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om2_1    | [id: "om1"
om2_1    | address: "om1:9872"
om2_1    | startupRole: FOLLOWER
om2_1    | , id: "om3"
om2_1    | address: "om3:9872"
om2_1    | startupRole: FOLLOWER
om2_1    | , id: "om2"
om2_1    | address: "om2:9872"
om2_1    | startupRole: FOLLOWER
om2_1    | ]
om2_1    | 2023-03-10 22:54:06,456 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization started.
om2_1    | 2023-03-10 22:54:06,462 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: ERASURE_CODED_STORAGE_SUPPORT.
om2_1    | 2023-03-10 22:54:06,467 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature ERASURE_CODED_STORAGE_SUPPORT has been finalized.
om2_1    | 2023-03-10 22:54:06,467 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: BUCKET_LAYOUT_SUPPORT.
om2_1    | 2023-03-10 22:54:06,469 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature BUCKET_LAYOUT_SUPPORT has been finalized.
om2_1    | 2023-03-10 22:54:06,470 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: MULTITENANCY_SCHEMA.
om2_1    | 2023-03-10 22:54:06,471 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature MULTITENANCY_SCHEMA has been finalized.
om2_1    | 2023-03-10 22:54:06,473 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
om2_1    | 2023-03-10 22:54:06,474 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization is done.
om2_1    | 2023-03-10 22:54:06,478 [OMDoubleBufferFlushThread] INFO upgrade.OMFinalizeUpgradeResponse: Layout version to persist to DB : 3
om2_1    | 2023-03-10 22:55:44,784 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:new2-volume for user:hadoop
om2_1    | 2023-03-10 22:55:49,203 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: new2-volume
om2_1    | 2023-03-10 22:56:02,151 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: s3v
om2_1    | 2023-03-10 22:56:13,926 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:new2-bucket in volume:s3v
om2_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om2_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:207)
om2_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:337)
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om2_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1    | 2023-03-10 22:56:43,299 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:ectest-new for user:hadoop
om2_1    | 2023-03-10 22:56:47,861 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ectest of layout FILE_SYSTEM_OPTIMIZED in volume: ectest-new
om2_1    | 2023-03-10 22:56:56,819 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: testpropchange of layout FILE_SYSTEM_OPTIMIZED in volume: ectest-new
dn1_1    | 2023-03-10 22:52:06,648 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3: set configuration 4: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:06,653 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/d1fa3d0d-6abe-48bc-b456-d0b7aa0d159e/current/log_inprogress_3
dn1_1    | 2023-03-10 22:52:06,656 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575: set configuration 3: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:06,672 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO segmented.LogSegment: Successfully read 9 entries from segment file /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/current/log_3-11
dn1_1    | 2023-03-10 22:52:06,675 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO segmented.LogSegment: Successfully read 18 entries from segment file /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/current/log_4-21
dn1_1    | 2023-03-10 22:52:06,738 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn1_1    | 2023-03-10 22:52:06,738 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn1_1    | 2023-03-10 22:52:06,748 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3: set configuration 22: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:06,770 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO segmented.LogSegment: Successfully read 14 entries from segment file /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/current/log_inprogress_22
dn1_1    | 2023-03-10 22:52:06,780 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575: set configuration 12: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:06,796 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 35
dn1_1    | 2023-03-10 22:52:06,796 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 21
dn1_1    | 2023-03-10 22:52:06,809 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO segmented.LogSegment: Successfully read 9 entries from segment file /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/current/log_inprogress_12
dn1_1    | 2023-03-10 22:52:06,825 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 20
dn1_1    | 2023-03-10 22:52:06,825 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 11
dn1_1    | 2023-03-10 22:52:08,587 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO raftlog.RaftLog: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-SegmentedRaftLog: commitIndex: updateToMax old=20, new=19, updated? false
dn1_1    | 2023-03-10 22:52:08,588 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575: start as a follower, conf=12: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:08,597 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575: changes role from      null to FOLLOWER at term 6 for startAsFollower
dn1_1    | 2023-03-10 22:52:08,665 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: start e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-FollowerState
dn1_1    | 2023-03-10 22:52:08,730 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@268d865e] INFO util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1023ms
dn1_1    | GC pool 'ParNew' had collection(s): count=1 time=1217ms
dn1_1    | 2023-03-10 22:52:08,587 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO raftlog.RaftLog: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-SegmentedRaftLog: commitIndex: updateToMax old=4, new=3, updated? false
dn1_1    | 2023-03-10 22:52:08,831 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E: start as a follower, conf=3: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:08,832 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn1_1    | 2023-03-10 22:52:08,587 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO raftlog.RaftLog: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-SegmentedRaftLog: commitIndex: updateToMax old=35, new=33, updated? false
dn1_1    | 2023-03-10 22:52:08,835 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3: start as a follower, conf=22: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:08,837 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3: changes role from      null to FOLLOWER at term 10 for startAsFollower
dn1_1    | 2023-03-10 22:52:08,769 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-03-10 22:52:08,769 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-CBC13A60F575,id=e3e4587c-aa42-4e86-ae9a-d3e448365275
dn1_1    | 2023-03-10 22:52:08,861 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-03-10 22:52:08,838 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: start e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-FollowerState
dn1_1    | 2023-03-10 22:52:08,833 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: start e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-FollowerState
dn1_1    | 2023-03-10 22:52:08,871 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-03-10 22:52:08,880 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-03-10 22:52:08,894 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-03-10 22:52:08,893 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-03-10 22:52:08,926 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-03-10 22:52:08,933 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-03-10 22:52:09,017 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-03-10 22:52:09,018 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D0B7AA0D159E,id=e3e4587c-aa42-4e86-ae9a-d3e448365275
dn1_1    | 2023-03-10 22:52:09,018 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-03-10 22:52:09,018 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-03-10 22:52:09,018 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-03-10 22:52:09,018 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-03-10 22:52:09,018 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0049FBBC23B3,id=e3e4587c-aa42-4e86-ae9a-d3e448365275
dn1_1    | 2023-03-10 22:52:09,023 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-03-10 22:52:09,023 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-03-10 22:52:09,025 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-03-10 22:52:09,026 [e3e4587c-aa42-4e86-ae9a-d3e448365275-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-03-10 22:52:09,029 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.RaftServer: e3e4587c-aa42-4e86-ae9a-d3e448365275: start RPC server
dn1_1    | 2023-03-10 22:52:09,041 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-03-10 22:52:09,160 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: e3e4587c-aa42-4e86-ae9a-d3e448365275: GrpcService started, listening on 9858
dn1_1    | 2023-03-10 22:52:09,233 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: e3e4587c-aa42-4e86-ae9a-d3e448365275: GrpcService started, listening on 9856
dn1_1    | 2023-03-10 22:52:09,413 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: e3e4587c-aa42-4e86-ae9a-d3e448365275: GrpcService started, listening on 9857
dn1_1    | 2023-03-10 22:52:09,445 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis e3e4587c-aa42-4e86-ae9a-d3e448365275 is started using port 9858 for RATIS
dn1_1    | 2023-03-10 22:52:09,445 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis e3e4587c-aa42-4e86-ae9a-d3e448365275 is started using port 9857 for RATIS_ADMIN
dn1_1    | 2023-03-10 22:52:09,445 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis e3e4587c-aa42-4e86-ae9a-d3e448365275 is started using port 9856 for RATIS_SERVER
dn1_1    | 2023-03-10 22:52:09,451 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-e3e4587c-aa42-4e86-ae9a-d3e448365275: Started
dn1_1    | 2023-03-10 22:52:09,958 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-03-10 22:52:13,982 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-FollowerState] INFO impl.FollowerState: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5317218182ns, electionTimeout:5086ms
dn1_1    | 2023-03-10 22:52:13,984 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-FollowerState] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: shutdown e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-FollowerState
dn1_1    | 2023-03-10 22:52:13,984 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-FollowerState] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575: changes role from  FOLLOWER to CANDIDATE at term 6 for changeToCandidate
dn1_1    | 2023-03-10 22:52:13,987 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-03-10 22:52:04,870 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-10 22:52:04,871 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-03-10 22:52:04,874 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-03-10 22:52:04,928 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-03-10 22:52:04,928 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-03-10 22:52:05,025 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-03-10 22:52:05,032 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-03-10 22:52:05,043 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-03-10 22:52:05,054 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-03-10 22:52:05,123 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00
dn5_1    | 2023-03-10 22:52:05,131 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-03-10 22:52:05,136 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-03-10 22:52:05,150 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-03-10 22:52:05,155 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-03-10 22:52:05,162 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-03-10 22:52:05,174 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-03-10 22:52:05,178 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-03-10 22:52:05,174 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/b1358d3e-7575-4e13-af6d-8e287caccc68
dn5_1    | 2023-03-10 22:52:05,185 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-03-10 22:52:05,187 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-03-10 22:52:05,187 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-03-10 22:52:05,190 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-03-10 22:52:05,191 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-03-10 22:52:05,196 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-03-10 22:52:05,202 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-03-10 22:52:05,205 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-03-10 22:52:05,191 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-03-10 22:52:05,423 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-03-10 22:52:05,425 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-10 22:52:05,455 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-03-10 22:52:05,493 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-10 22:52:06,855 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@62efe67a] INFO util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1239ms
dn5_1    | GC pool 'ParNew' had collection(s): count=1 time=162ms
dn5_1    | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1103ms
dn5_1    | 2023-03-10 22:52:06,883 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-03-10 22:52:06,886 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-03-10 22:52:06,886 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-03-10 22:52:06,900 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-03-10 22:52:06,900 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-03-10 22:52:06,900 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-03-10 22:52:07,332 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00: set configuration 0: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:52:07,400 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68: set configuration 0: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:52:07,456 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/b1358d3e-7575-4e13-af6d-8e287caccc68/current/log_0-0
dn5_1    | 2023-03-10 22:52:07,516 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68: set configuration 1: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:52:07,541 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/b1358d3e-7575-4e13-af6d-8e287caccc68/current/log_1-2
dn5_1    | 2023-03-10 22:52:07,533 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO segmented.LogSegment: Successfully read 9 entries from segment file /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/current/log_0-8
dn5_1    | 2023-03-10 22:52:07,569 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00: set configuration 9: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:52:07,572 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/current/log_9-10
dn5_1    | 2023-03-10 22:52:07,575 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68: set configuration 3: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:13,987 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-FollowerState] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: start e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-LeaderElection1
dn4_1    | 2023-03-10 22:52:10,642 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-10 22:52:10,642 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D98E5419B7A9,id=e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn4_1    | 2023-03-10 22:52:10,646 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-03-10 22:52:10,642 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0049FBBC23B3,id=e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn4_1    | 2023-03-10 22:52:10,649 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-03-10 22:52:10,650 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-03-10 22:52:10,650 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-03-10 22:52:10,651 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-03-10 22:52:10,651 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-03-10 22:52:10,651 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-03-10 22:52:10,652 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-03-10 22:52:10,655 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-10 22:52:10,700 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.RaftServer: e78c5ce1-46ab-4889-a0cd-5903ae46614d: start RPC server
dn4_1    | 2023-03-10 22:52:10,709 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn4_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:321)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.util.concurrent.TimeoutException
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	... 1 more
dn4_1    | 2023-03-10 22:52:10,728 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: e78c5ce1-46ab-4889-a0cd-5903ae46614d: GrpcService started, listening on 9858
dn4_1    | 2023-03-10 22:52:10,743 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: e78c5ce1-46ab-4889-a0cd-5903ae46614d: GrpcService started, listening on 9856
dn4_1    | 2023-03-10 22:52:10,818 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: e78c5ce1-46ab-4889-a0cd-5903ae46614d: GrpcService started, listening on 9857
dn4_1    | 2023-03-10 22:52:10,845 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis e78c5ce1-46ab-4889-a0cd-5903ae46614d is started using port 9858 for RATIS
dn4_1    | 2023-03-10 22:52:10,852 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis e78c5ce1-46ab-4889-a0cd-5903ae46614d is started using port 9857 for RATIS_ADMIN
dn4_1    | 2023-03-10 22:52:10,855 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis e78c5ce1-46ab-4889-a0cd-5903ae46614d is started using port 9856 for RATIS_SERVER
dn4_1    | 2023-03-10 22:52:10,901 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-e78c5ce1-46ab-4889-a0cd-5903ae46614d: Started
dn4_1    | 2023-03-10 22:52:11,121 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn4_1    | 2023-03-10 22:52:15,678 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-FollowerState] INFO impl.FollowerState: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5167982614ns, electionTimeout:5091ms
dn4_1    | 2023-03-10 22:52:15,681 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-FollowerState] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: shutdown e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-FollowerState
dn4_1    | 2023-03-10 22:52:15,682 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-FollowerState] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3: changes role from  FOLLOWER to CANDIDATE at term 10 for changeToCandidate
dn4_1    | 2023-03-10 22:52:15,690 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | 2023-03-10 22:52:15,701 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-FollowerState] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: start e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-LeaderElection1
dn1_1    | 2023-03-10 22:52:13,998 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-LeaderElection1] INFO impl.LeaderElection: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 6 for 12: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:14,012 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-FollowerState] INFO impl.FollowerState: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5174091740ns, electionTimeout:5077ms
dn1_1    | 2023-03-10 22:52:14,022 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-FollowerState] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: shutdown e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-FollowerState
dn1_1    | 2023-03-10 22:52:14,022 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-FollowerState] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3: changes role from  FOLLOWER to CANDIDATE at term 10 for changeToCandidate
dn1_1    | 2023-03-10 22:52:14,031 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-03-10 22:52:14,032 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-FollowerState] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: start e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2
dn1_1    | 2023-03-10 22:52:14,088 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO impl.LeaderElection: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 10 for 22: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:14,128 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-FollowerState] INFO impl.FollowerState: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5295062458ns, electionTimeout:5058ms
dn1_1    | 2023-03-10 22:52:14,129 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-FollowerState] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: shutdown e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-FollowerState
dn1_1    | 2023-03-10 22:52:14,129 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-FollowerState] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn1_1    | 2023-03-10 22:52:14,138 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-03-10 22:52:14,138 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-FollowerState] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: start e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3
dn1_1    | 2023-03-10 22:52:14,166 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-03-10 22:52:14,189 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-03-10 22:52:14,172 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-03-10 22:52:14,190 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-03-10 22:52:14,172 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 3892a4e1-c878-42af-adb7-db66a90d61f4
dn1_1    | 2023-03-10 22:52:14,252 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3] INFO impl.LeaderElection: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:14,253 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3] INFO impl.LeaderElection: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3 PRE_VOTE round 0: result PASSED (term=3)
dn1_1    | 2023-03-10 22:52:14,177 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn1_1    | 2023-03-10 22:52:14,282 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 1a6d358d-6662-4447-914c-d709a67ff716
dn1_1    | 2023-03-10 22:52:14,465 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3] INFO impl.LeaderElection: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3 ELECTION round 0: submit vote requests at term 4 for 3: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:14,465 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3] INFO impl.LeaderElection: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3 ELECTION round 0: result PASSED (term=4)
dn1_1    | 2023-03-10 22:52:14,466 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: shutdown e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3
dn1_1    | 2023-03-10 22:52:14,468 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn1_1    | 2023-03-10 22:52:14,474 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D0B7AA0D159E with new leaderId: e3e4587c-aa42-4e86-ae9a-d3e448365275
dn1_1    | 2023-03-10 22:52:14,503 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E: change Leader from null to e3e4587c-aa42-4e86-ae9a-d3e448365275 at term 4 for becomeLeader, leader elected after 27499ms
dn1_1    | 2023-03-10 22:52:14,608 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om3_1    | 2023-03-10 22:52:14,494 [main] INFO om.OzoneManager: Creating RPC Server
om3_1    | 2023-03-10 22:52:14,866 [pool-29-thread-1] INFO server.RaftServer$Division: om3: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om3_1    | 2023-03-10 22:52:14,921 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om3_1    | 2023-03-10 22:52:14,944 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om3_1    | 2023-03-10 22:52:14,956 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om3_1    | 2023-03-10 22:52:14,963 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1    | 2023-03-10 22:52:14,963 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1    | 2023-03-10 22:52:14,964 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om3_1    | 2023-03-10 22:52:15,281 [pool-29-thread-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om3_1    | 2023-03-10 22:52:15,297 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1    | 2023-03-10 22:52:15,428 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om3_1    | 2023-03-10 22:52:15,438 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om3_1    | 2023-03-10 22:52:16,002 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om3_1    | 2023-03-10 22:52:16,079 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om3_1    | 2023-03-10 22:52:16,085 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om3_1    | 2023-03-10 22:52:18,496 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1    | 2023-03-10 22:52:18,526 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om3_1    | 2023-03-10 22:52:18,529 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om3_1    | 2023-03-10 22:52:18,533 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om3_1    | 2023-03-10 22:52:18,542 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om3_1    | 2023-03-10 22:52:22,179 [main] INFO reflections.Reflections: Reflections took 7386 ms to scan 8 urls, producing 23 keys and 581 values [using 2 cores]
om3_1    | 2023-03-10 22:52:23,096 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1    | 2023-03-10 22:52:23,151 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om3_1    | 2023-03-10 22:52:23,892 [Listener at om3/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om3_1    | 2023-03-10 22:52:23,981 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om3_1    | 2023-03-10 22:52:23,982 [Listener at om3/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om3_1    | 2023-03-10 22:52:24,306 [Listener at om3/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om3/10.9.0.13:9862
om3_1    | 2023-03-10 22:52:24,312 [Listener at om3/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om3 at port 9872
om3_1    | 2023-03-10 22:52:24,346 [om3-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 8@b8bcefb332b8
om3_1    | 2023-03-10 22:52:24,372 [om3-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=5, votedFor=om1} from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/raft-meta
om3_1    | 2023-03-10 22:52:24,443 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-03-10 22:52:24,448 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om3_1    | 2023-03-10 22:52:24,482 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om3_1    | 2023-03-10 22:52:24,490 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-03-10 22:52:24,508 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om3_1    | 2023-03-10 22:52:24,529 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om3_1    | 2023-03-10 22:52:24,568 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1    | 2023-03-10 22:52:24,619 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om3_1    | 2023-03-10 22:52:24,620 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om3_1    | 2023-03-10 22:52:24,649 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om3@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om3_1    | 2023-03-10 22:52:24,653 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om3_1    | 2023-03-10 22:52:24,655 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om3_1    | 2023-03-10 22:52:24,658 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1    | 2023-03-10 22:52:24,664 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om3_1    | 2023-03-10 22:52:24,666 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om3_1    | 2023-03-10 22:52:24,669 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om3_1    | 2023-03-10 22:52:24,672 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om3_1    | 2023-03-10 22:52:24,677 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om3_1    | 2023-03-10 22:52:24,741 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om3_1    | 2023-03-10 22:52:24,745 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-03-10 22:52:24,785 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om3_1    | 2023-03-10 22:52:24,786 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om3_1    | 2023-03-10 22:52:24,787 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om3_1    | 2023-03-10 22:52:24,858 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-03-10 22:52:24,875 [om3-impl-thread1] INFO segmented.LogSegment: Successfully read 21 entries from segment file /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_0-20
om3_1    | 2023-03-10 22:52:24,880 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 21: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-03-10 22:52:24,896 [om3-impl-thread1] INFO segmented.LogSegment: Successfully read 46 entries from segment file /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_21-66
om3_1    | 2023-03-10 22:52:24,898 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-03-10 22:52:24,904 [om3-impl-thread1] INFO segmented.LogSegment: Successfully read 36 entries from segment file /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_67
dn1_1    | 2023-03-10 22:52:14,689 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-03-10 22:52:14,690 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn1_1    | 2023-03-10 22:52:14,814 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn1_1    | 2023-03-10 22:52:14,816 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn1_1    | 2023-03-10 22:52:14,819 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn1_1    | 2023-03-10 22:52:14,965 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-03-10 22:52:14,993 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn1_1    | 2023-03-10 22:52:15,050 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: start e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderStateImpl
dn1_1    | 2023-03-10 22:52:15,636 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-e3e4587c-aa42-4e86-ae9a-d3e448365275: Detected pause in JVM or host machine (eg GC): pause of approximately 166030642ns. No GCs detected.
dn1_1    | 2023-03-10 22:52:15,646 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn1_1    | 2023-03-10 22:52:15,959 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderElection3] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E: set configuration 5: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:15,973 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/d1fa3d0d-6abe-48bc-b456-d0b7aa0d159e/current/log_inprogress_3 to /data/metadata/ratis/d1fa3d0d-6abe-48bc-b456-d0b7aa0d159e/current/log_3-4
dn1_1    | 2023-03-10 22:52:16,101 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d1fa3d0d-6abe-48bc-b456-d0b7aa0d159e/current/log_inprogress_5
dn1_1    | 2023-03-10 22:52:19,326 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO impl.LeaderElection: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2: PRE_VOTE TIMEOUT received 0 response(s) and 0 exception(s):
dn1_1    | 2023-03-10 22:52:19,327 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO impl.LeaderElection: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2 PRE_VOTE round 0: result TIMEOUT
dn1_1    | 2023-03-10 22:52:19,327 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO impl.LeaderElection: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2 PRE_VOTE round 1: submit vote requests at term 10 for 22: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:19,346 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-LeaderElection1] INFO impl.LeaderElection: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-LeaderElection1: PRE_VOTE TIMEOUT received 0 response(s) and 0 exception(s):
dn1_1    | 2023-03-10 22:52:19,346 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-LeaderElection1] INFO impl.LeaderElection: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-LeaderElection1 PRE_VOTE round 0: result TIMEOUT
dn1_1    | 2023-03-10 22:52:19,346 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-LeaderElection1] INFO impl.LeaderElection: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-LeaderElection1 PRE_VOTE round 1: submit vote requests at term 6 for 12: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:19,360 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-03-10 22:52:19,386 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-03-10 22:52:19,425 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-03-10 22:52:19,455 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-03-10 22:52:20,001 [grpc-default-executor-0] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575: receive requestVote(PRE_VOTE, 3892a4e1-c878-42af-adb7-db66a90d61f4, group-CBC13A60F575, 6, (t:6, i:20))
dn1_1    | 2023-03-10 22:52:20,039 [grpc-default-executor-0] INFO impl.VoteContext: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-CANDIDATE: accept PRE_VOTE from 3892a4e1-c878-42af-adb7-db66a90d61f4: our priority 0 <= candidate's priority 0
dn1_1    | 2023-03-10 22:52:20,044 [grpc-default-executor-2] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3: receive requestVote(PRE_VOTE, e78c5ce1-46ab-4889-a0cd-5903ae46614d, group-0049FBBC23B3, 10, (t:10, i:35))
dn1_1    | 2023-03-10 22:52:20,071 [grpc-default-executor-1] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575: receive requestVote(PRE_VOTE, e78c5ce1-46ab-4889-a0cd-5903ae46614d, group-CBC13A60F575, 6, (t:6, i:20))
dn1_1    | 2023-03-10 22:52:20,059 [grpc-default-executor-2] INFO impl.VoteContext: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-CANDIDATE: reject PRE_VOTE from e78c5ce1-46ab-4889-a0cd-5903ae46614d: our priority 1 > candidate's priority 0
dn4_1    | 2023-03-10 22:52:15,716 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-LeaderElection1] INFO impl.LeaderElection: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 10 for 22: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:15,757 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-FollowerState] INFO impl.FollowerState: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5291163236ns, electionTimeout:5179ms
dn4_1    | 2023-03-10 22:52:15,759 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-FollowerState] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: shutdown e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-FollowerState
dn4_1    | 2023-03-10 22:52:15,764 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-FollowerState] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575: changes role from  FOLLOWER to CANDIDATE at term 6 for changeToCandidate
dn4_1    | 2023-03-10 22:52:15,765 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | 2023-03-10 22:52:15,773 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-FollowerState] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: start e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2
dn4_1    | 2023-03-10 22:52:15,779 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-FollowerState] INFO impl.FollowerState: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5269742280ns, electionTimeout:5123ms
dn4_1    | 2023-03-10 22:52:15,810 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-FollowerState] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: shutdown e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-FollowerState
dn4_1    | 2023-03-10 22:52:15,810 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-FollowerState] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn4_1    | 2023-03-10 22:52:15,813 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | 2023-03-10 22:52:15,813 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-FollowerState] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: start e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3
dn4_1    | 2023-03-10 22:52:15,822 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO impl.LeaderElection: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 6 for 12: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:15,874 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3] INFO impl.LeaderElection: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:15,875 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3] INFO impl.LeaderElection: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3 PRE_VOTE round 0: result PASSED (term=3)
dn4_1    | 2023-03-10 22:52:16,043 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for e3e4587c-aa42-4e86-ae9a-d3e448365275
dn4_1    | 2023-03-10 22:52:16,057 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 3892a4e1-c878-42af-adb7-db66a90d61f4
dn4_1    | 2023-03-10 22:52:16,146 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-10 22:52:16,199 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-10 22:52:16,057 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-10 22:52:16,218 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-10 22:52:16,221 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 1a6d358d-6662-4447-914c-d709a67ff716
dn4_1    | 2023-03-10 22:52:16,296 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3] INFO impl.LeaderElection: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3 ELECTION round 0: submit vote requests at term 4 for 3: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:16,309 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3] INFO impl.LeaderElection: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3 ELECTION round 0: result PASSED (term=4)
dn4_1    | 2023-03-10 22:52:16,313 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: shutdown e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3
dn4_1    | 2023-03-10 22:52:16,314 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn4_1    | 2023-03-10 22:52:16,330 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D98E5419B7A9 with new leaderId: e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn4_1    | 2023-03-10 22:52:16,340 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9: change Leader from null to e78c5ce1-46ab-4889-a0cd-5903ae46614d at term 4 for becomeLeader, leader elected after 31776ms
dn4_1    | 2023-03-10 22:52:16,481 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn4_1    | 2023-03-10 22:52:16,648 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-03-10 22:52:16,672 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn4_1    | 2023-03-10 22:52:16,765 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
recon_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1  | 2023-03-10 22:51:12,323 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1  | /************************************************************
recon_1  | STARTUP_MSG: Starting ReconServer
recon_1  | STARTUP_MSG:   host = ecabc86923e0/10.9.0.20
recon_1  | STARTUP_MSG:   args = []
recon_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.23.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.3.23.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.23.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.23.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.23.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
recon_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/354d3dc5a16eade52bdcbd566bfb952ac3a1d8b6 ; compiled by 'runner' on 2023-03-10T22:24Z
recon_1  | STARTUP_MSG:   java = 11.0.14.1
s3g_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1    | 2023-03-10 22:51:11,472 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1    | 2023-03-10 22:51:11,501 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1    | 2023-03-10 22:51:11,719 [main] INFO util.log: Logging initialized @12740ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1    | 2023-03-10 22:51:13,082 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1    | 2023-03-10 22:51:13,407 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1    | 2023-03-10 22:51:13,476 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1    | 2023-03-10 22:51:13,499 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1    | 2023-03-10 22:51:13,525 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1    | 2023-03-10 22:51:13,525 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1    | 2023-03-10 22:51:14,089 [main] INFO http.BaseHttpServer: HTTP server of s3gateway uses base directory /data/metadata/webserver
s3g_1    | 2023-03-10 22:51:15,992 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1    | /************************************************************
s3g_1    | STARTUP_MSG: Starting Gateway
s3g_1    | STARTUP_MSG:   host = c95a36ddd115/10.9.0.21
s3g_1    | STARTUP_MSG:   args = []
s3g_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
s3g_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-shaded-3.1.9.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/cdi-api-2.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
s3g_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/354d3dc5a16eade52bdcbd566bfb952ac3a1d8b6 ; compiled by 'runner' on 2023-03-10T22:24Z
s3g_1    | STARTUP_MSG:   java = 11.0.14.1
scm_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1    | 2023-03-10 22:51:25,672 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1    | /************************************************************
scm_1    | STARTUP_MSG: Starting StorageContainerManager
scm_1    | STARTUP_MSG:   host = bd16cef192ca/10.9.0.14
scm_1    | STARTUP_MSG:   args = []
scm_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/354d3dc5a16eade52bdcbd566bfb952ac3a1d8b6 ; compiled by 'runner' on 2023-03-10T22:24Z
scm_1    | STARTUP_MSG:   java = 11.0.14.1
scm_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.timeout=30m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=1440, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.command.deadline.factor=0.9, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=true, hdds.scm.replication.event.timeout=30m, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.ssl.keystore.reload.interval=60s, hdds.security.ssl.truststore.reload.interval=60s, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4))$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.sst.filtering.service.timeout=300000ms, ozone.tags.system=ACL,BALANCER,CLIENT,CONTAINER,DATANODE,DATASTREAM,DEBUG,DELETION,DEPRECATED,FREON,HA,HDDS,KERBEROS,MANAGEMENT,OM,OPERATION,OZONE,OZONEFS,PERFORMANCE,PIPELINE,RATIS,RECON,REQUIRED,S3GATEWAY,SCM,SECURITY,STORAGE,TLS,TOKEN,UPGRADE,X509, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm_1    | ************************************************************/
scm_1    | 2023-03-10 22:51:25,861 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1    | 2023-03-10 22:51:27,975 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1    | 2023-03-10 22:51:28,559 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1    | 2023-03-10 22:51:29,233 [main] WARN ha.SCMHANodeDetails: Invalid config ozone.scm.ratis.enable. The config was not specified, but the default value true conflicts with the expected config value false. Falling back to the expected value. Current State of SCM: SCM is running in Non-HA without Ratis Ratis SCM -> Non Ratis SCM or Non HA SCM -> HA SCM is not supported
scm_1    | 2023-03-10 22:51:29,233 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1    | 2023-03-10 22:51:40,985 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1    | 2023-03-10 22:51:44,027 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1    | 2023-03-10 22:51:46,564 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm_1    | 2023-03-10 22:51:46,587 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1    | 2023-03-10 22:51:48,116 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1    | 2023-03-10 22:51:48,182 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = DATANODE_SCHEMA_V3 (version = 4)
scm_1    | 2023-03-10 22:51:52,398 [main] INFO reflections.Reflections: Reflections took 2218 ms to scan 3 urls, producing 128 keys and 283 values 
scm_1    | 2023-03-10 22:51:53,284 [main] INFO upgrade.HDDSLayoutVersionManager: Registering Upgrade Action : DatanodeSchemaV3FinalizeAction
scm_1    | 2023-03-10 22:51:53,466 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm_1    | 2023-03-10 22:51:54,116 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1    | 2023-03-10 22:51:54,274 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1    | 2023-03-10 22:51:54,352 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1    | 2023-03-10 22:51:55,270 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1    | 2023-03-10 22:51:55,276 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1    | 2023-03-10 22:51:55,324 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm_1    | 2023-03-10 22:51:55,355 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm_1    | 2023-03-10 22:51:55,412 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm_1    | 2023-03-10 22:51:55,442 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
dn2_1    | 2023-03-10 22:52:09,507 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn2_1    | 2023-03-10 22:52:09,405 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO raftlog.RaftLog: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-SegmentedRaftLog: commitIndex: updateToMax old=20, new=19, updated? false
dn2_1    | 2023-03-10 22:52:09,507 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575: start as a follower, conf=12: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:09,507 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575: changes role from      null to FOLLOWER at term 6 for startAsFollower
dn2_1    | 2023-03-10 22:52:09,509 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: start 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-FollowerState
dn2_1    | 2023-03-10 22:52:09,819 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-5B98FDC36F00,id=3892a4e1-c878-42af-adb7-db66a90d61f4
dn2_1    | 2023-03-10 22:52:09,534 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: start 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-FollowerState
dn2_1    | 2023-03-10 22:52:09,534 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: start 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-FollowerState
dn2_1    | 2023-03-10 22:52:09,924 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-03-10 22:52:09,924 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-03-10 22:52:09,950 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-03-10 22:52:09,950 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-03-10 22:52:09,951 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-03-10 22:52:09,951 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-03-10 22:52:09,977 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-03-10 22:52:10,001 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-03-10 22:52:10,041 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-676BAB171C02,id=3892a4e1-c878-42af-adb7-db66a90d61f4
dn2_1    | 2023-03-10 22:52:10,045 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-03-10 22:52:10,046 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-03-10 22:52:10,046 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-03-10 22:52:10,041 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-CBC13A60F575,id=3892a4e1-c878-42af-adb7-db66a90d61f4
dn2_1    | 2023-03-10 22:52:10,053 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-03-10 22:52:10,054 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-03-10 22:52:10,054 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-03-10 22:52:10,068 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-03-10 22:52:10,068 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-03-10 22:52:10,069 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-03-10 22:52:10,069 [3892a4e1-c878-42af-adb7-db66a90d61f4-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-03-10 22:52:10,114 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 3892a4e1-c878-42af-adb7-db66a90d61f4: start RPC server
dn2_1    | 2023-03-10 22:52:10,124 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 3892a4e1-c878-42af-adb7-db66a90d61f4: GrpcService started, listening on 9858
dn2_1    | 2023-03-10 22:52:10,126 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 3892a4e1-c878-42af-adb7-db66a90d61f4: GrpcService started, listening on 9856
dn2_1    | 2023-03-10 22:52:10,134 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 3892a4e1-c878-42af-adb7-db66a90d61f4: GrpcService started, listening on 9857
dn2_1    | 2023-03-10 22:52:10,159 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3892a4e1-c878-42af-adb7-db66a90d61f4 is started using port 9858 for RATIS
dn2_1    | 2023-03-10 22:52:10,159 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3892a4e1-c878-42af-adb7-db66a90d61f4 is started using port 9857 for RATIS_ADMIN
dn2_1    | 2023-03-10 22:52:10,160 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3892a4e1-c878-42af-adb7-db66a90d61f4 is started using port 9856 for RATIS_SERVER
dn2_1    | 2023-03-10 22:52:10,166 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-3892a4e1-c878-42af-adb7-db66a90d61f4: Started
dn2_1    | 2023-03-10 22:52:10,302 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn2_1    | 2023-03-10 22:52:15,073 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-FollowerState] INFO impl.FollowerState: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5539040019ns, electionTimeout:5121ms
dn2_1    | 2023-03-10 22:52:15,075 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-FollowerState] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: shutdown 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-FollowerState
dn2_1    | 2023-03-10 22:52:15,075 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-FollowerState] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn2_1    | 2023-03-10 22:52:15,078 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn2_1    | 2023-03-10 22:52:15,078 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-FollowerState] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: start 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1
dn2_1    | 2023-03-10 22:52:15,081 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1] INFO impl.LeaderElection: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:15,082 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1] INFO impl.LeaderElection: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1 PRE_VOTE round 0: result PASSED (term=3)
dn2_1    | 2023-03-10 22:52:15,149 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-FollowerState] INFO impl.FollowerState: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5614586417ns, electionTimeout:5100ms
dn2_1    | 2023-03-10 22:52:15,151 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-FollowerState] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: shutdown 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-FollowerState
dn2_1    | 2023-03-10 22:52:15,155 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-FollowerState] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575: changes role from  FOLLOWER to CANDIDATE at term 6 for changeToCandidate
dn2_1    | 2023-03-10 22:52:15,156 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn2_1    | 2023-03-10 22:52:15,156 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-FollowerState] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: start 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-LeaderElection2
dn2_1    | 2023-03-10 22:52:15,159 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-FollowerState] INFO impl.FollowerState: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5650353520ns, electionTimeout:5156ms
dn2_1    | 2023-03-10 22:52:15,161 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-FollowerState] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: shutdown 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-FollowerState
dn2_1    | 2023-03-10 22:52:15,161 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-FollowerState] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00: changes role from  FOLLOWER to CANDIDATE at term 11 for changeToCandidate
dn2_1    | 2023-03-10 22:52:15,158 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-LeaderElection2] INFO impl.LeaderElection: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 6 for 12: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:15,163 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn2_1    | 2023-03-10 22:52:15,163 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-FollowerState] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: start 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3
dn2_1    | 2023-03-10 22:52:15,167 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1] INFO impl.LeaderElection: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1 ELECTION round 0: submit vote requests at term 4 for 3: peers:[3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:15,170 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1] INFO impl.LeaderElection: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1 ELECTION round 0: result PASSED (term=4)
dn2_1    | 2023-03-10 22:52:15,170 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: shutdown 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1
dn2_1    | 2023-03-10 22:52:15,170 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO impl.LeaderElection: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 11 for 11: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:15,172 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn2_1    | 2023-03-10 22:52:15,182 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-676BAB171C02 with new leaderId: 3892a4e1-c878-42af-adb7-db66a90d61f4
dn2_1    | 2023-03-10 22:52:15,202 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02: change Leader from null to 3892a4e1-c878-42af-adb7-db66a90d61f4 at term 4 for becomeLeader, leader elected after 25781ms
dn2_1    | 2023-03-10 22:52:15,373 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn2_1    | 2023-03-10 22:52:15,386 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-03-10 22:52:15,439 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-03-10 22:52:15,437 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-10 22:52:07,597 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/b1358d3e-7575-4e13-af6d-8e287caccc68/current/log_inprogress_3
dn5_1    | 2023-03-10 22:52:07,599 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00: set configuration 11: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:52:07,640 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO segmented.LogSegment: Successfully read 6 entries from segment file /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/current/log_inprogress_11
dn5_1    | 2023-03-10 22:52:07,642 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 16
dn5_1    | 2023-03-10 22:52:07,642 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 10
dn5_1    | 2023-03-10 22:52:07,641 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn5_1    | 2023-03-10 22:52:07,697 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn5_1    | 2023-03-10 22:52:08,444 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO raftlog.RaftLog: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-SegmentedRaftLog: commitIndex: updateToMax old=4, new=3, updated? false
dn5_1    | 2023-03-10 22:52:08,444 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68: start as a follower, conf=3: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:52:08,445 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn5_1    | 2023-03-10 22:52:08,447 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: start 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-FollowerState
dn5_1    | 2023-03-10 22:52:08,450 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8E287CACCC68,id=178b30e1-b74d-4f4d-a142-c930eee71455
dn5_1    | 2023-03-10 22:52:08,460 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO raftlog.RaftLog: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-SegmentedRaftLog: commitIndex: updateToMax old=16, new=15, updated? false
dn5_1    | 2023-03-10 22:52:08,518 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00: start as a follower, conf=11: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:52:08,532 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00: changes role from      null to FOLLOWER at term 11 for startAsFollower
dn5_1    | 2023-03-10 22:52:08,534 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: start 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-FollowerState
dn5_1    | 2023-03-10 22:52:08,566 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-5B98FDC36F00,id=178b30e1-b74d-4f4d-a142-c930eee71455
dn5_1    | 2023-03-10 22:52:08,569 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-10 22:52:08,607 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-10 22:52:08,601 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-03-10 22:52:08,598 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-10 22:52:08,608 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-10 22:52:08,600 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-03-10 22:52:08,612 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-03-10 22:52:08,615 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-03-10 22:52:08,619 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-03-10 22:52:08,638 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-03-10 22:52:08,639 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-03-10 22:52:08,630 [178b30e1-b74d-4f4d-a142-c930eee71455-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-03-10 22:52:08,754 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 178b30e1-b74d-4f4d-a142-c930eee71455: start RPC server
dn5_1    | 2023-03-10 22:52:08,761 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 178b30e1-b74d-4f4d-a142-c930eee71455: GrpcService started, listening on 9858
dn5_1    | 2023-03-10 22:52:08,763 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 178b30e1-b74d-4f4d-a142-c930eee71455: GrpcService started, listening on 9856
dn5_1    | 2023-03-10 22:52:08,783 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 178b30e1-b74d-4f4d-a142-c930eee71455: GrpcService started, listening on 9857
dn5_1    | 2023-03-10 22:52:08,798 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 178b30e1-b74d-4f4d-a142-c930eee71455 is started using port 9858 for RATIS
dn5_1    | 2023-03-10 22:52:08,801 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 178b30e1-b74d-4f4d-a142-c930eee71455 is started using port 9857 for RATIS_ADMIN
dn5_1    | 2023-03-10 22:52:08,801 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 178b30e1-b74d-4f4d-a142-c930eee71455 is started using port 9856 for RATIS_SERVER
dn5_1    | 2023-03-10 22:52:08,801 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-178b30e1-b74d-4f4d-a142-c930eee71455: Started
dn5_1    | 2023-03-10 22:52:08,883 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 2023-03-10 22:52:13,706 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-FollowerState] INFO impl.FollowerState: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5171215752ns, electionTimeout:5097ms
dn5_1    | 2023-03-10 22:52:13,707 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-FollowerState] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: shutdown 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-FollowerState
dn5_1    | 2023-03-10 22:52:13,708 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-FollowerState] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00: changes role from  FOLLOWER to CANDIDATE at term 11 for changeToCandidate
dn5_1    | 2023-03-10 22:52:13,715 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-03-10 22:52:13,715 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-FollowerState] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: start 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-LeaderElection1
dn5_1    | 2023-03-10 22:52:13,729 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-LeaderElection1] INFO impl.LeaderElection: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 11 for 11: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:52:13,783 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-FollowerState] INFO impl.FollowerState: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5336582019ns, electionTimeout:5171ms
dn5_1    | 2023-03-10 22:52:13,790 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-FollowerState] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: shutdown 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-FollowerState
dn5_1    | 2023-03-10 22:52:13,792 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-FollowerState] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn5_1    | 2023-03-10 22:52:13,800 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-03-10 22:52:13,801 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-FollowerState] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: start 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2
dn5_1    | 2023-03-10 22:52:13,817 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2] INFO impl.LeaderElection: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:52:13,829 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2] INFO impl.LeaderElection: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2 PRE_VOTE round 0: result PASSED (term=3)
dn5_1    | 2023-03-10 22:52:13,859 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-10 22:52:13,859 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-10 22:52:13,870 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 1a6d358d-6662-4447-914c-d709a67ff716
dn3_1    | 2023-03-10 22:52:04,931 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-03-10 22:52:04,933 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-03-10 22:52:04,953 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-03-10 22:52:04,958 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-03-10 22:52:04,929 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-03-10 22:52:04,976 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-03-10 22:52:04,979 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-03-10 22:52:04,980 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-03-10 22:52:04,982 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-03-10 22:52:04,983 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-03-10 22:52:04,949 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-03-10 22:52:04,988 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-03-10 22:52:04,992 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-03-10 22:52:04,992 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-03-10 22:52:05,143 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-03-10 22:52:05,149 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-03-10 22:52:05,148 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-03-10 22:52:05,178 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-03-10 22:52:05,152 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-03-10 22:52:05,185 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-03-10 22:52:06,445 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-03-10 22:52:06,470 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-03-10 22:52:06,542 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-03-10 22:52:06,454 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2bc40fec] INFO util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1233ms
dn3_1    | GC pool 'ParNew' had collection(s): count=1 time=142ms
dn3_1    | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1026ms
dn3_1    | 2023-03-10 22:52:06,748 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-03-10 22:52:06,749 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-03-10 22:52:06,749 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-03-10 22:52:06,766 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-03-10 22:52:06,767 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-03-10 22:52:06,767 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-03-10 22:52:08,411 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2bc40fec] INFO util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1344ms
dn3_1    | GC pool 'ParNew' had collection(s): count=1 time=1435ms
dn3_1    | 2023-03-10 22:52:08,527 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00: set configuration 0: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:08,529 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E: set configuration 0: peers:[1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:08,553 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3: set configuration 0: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:08,578 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/7572f4a9-8e86-407b-b624-5c511402a23e/current/log_0-0
dn3_1    | 2023-03-10 22:52:08,592 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E: set configuration 1: peers:[1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:08,668 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/7572f4a9-8e86-407b-b624-5c511402a23e/current/log_1-2
dn3_1    | 2023-03-10 22:52:08,671 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E: set configuration 3: peers:[1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.timeout=30m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=1440, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.command.deadline.factor=0.9, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=true, hdds.scm.replication.event.timeout=30m, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.ssl.keystore.reload.interval=60s, hdds.security.ssl.truststore.reload.interval=60s, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.sql.db.auto.commit=true, ozone.recon.sql.db.conn.idle.max.age=3600s, ozone.recon.sql.db.conn.idle.test=SELECT 1, ozone.recon.sql.db.conn.idle.test.period=60s, ozone.recon.sql.db.conn.max.active=5, ozone.recon.sql.db.conn.max.age=1800s, ozone.recon.sql.db.conn.timeout=30000ms, ozone.recon.sql.db.driver=org.apache.derby.jdbc.EmbeddedDriver, ozone.recon.sql.db.jdbc.url=jdbc:derby:/data/metadata/recon/ozone_recon_derby.db, ozone.recon.sql.db.jooq.dialect=DERBY, ozone.recon.task.missingcontainer.interval=300s, ozone.recon.task.pipelinesync.interval=300s, ozone.recon.task.safemode.wait.threshold=300s, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4))$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.sst.filtering.service.timeout=300000ms, ozone.tags.system=ACL,BALANCER,CLIENT,CONTAINER,DATANODE,DATASTREAM,DEBUG,DELETION,DEPRECATED,FREON,HA,HDDS,KERBEROS,MANAGEMENT,OM,OPERATION,OZONE,OZONEFS,PERFORMANCE,PIPELINE,RATIS,RECON,REQUIRED,S3GATEWAY,SCM,SECURITY,STORAGE,TLS,TOKEN,UPGRADE,X509, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
recon_1  | ************************************************************/
recon_1  | 2023-03-10 22:51:12,429 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1  | WARNING: An illegal reflective access operation has occurred
recon_1  | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1  | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1  | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1  | WARNING: All illegal access operations will be denied in a future release
recon_1  | 2023-03-10 22:51:18,244 [main] INFO reflections.Reflections: Reflections took 420 ms to scan 1 urls, producing 17 keys and 54 values 
recon_1  | 2023-03-10 22:51:21,838 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1  | 2023-03-10 22:51:21,851 [main] INFO impl.ReconDBProvider: Last known Recon DB : /data/metadata/recon/recon-container-key.db_1678488063884
recon_1  | 2023-03-10 22:51:23,778 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1  | 2023-03-10 22:51:30,444 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1  | 2023-03-10 22:51:34,234 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1  | 2023-03-10 22:51:34,240 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1  | 2023-03-10 22:51:34,259 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1  | 2023-03-10 22:51:38,510 [main] INFO codegen.SqlDbUtils: RECON_TASK_STATUS table already exists, skipping creation.
recon_1  | 2023-03-10 22:51:38,781 [main] INFO codegen.SqlDbUtils: GLOBAL_STATS table already exists, skipping creation.
recon_1  | 2023-03-10 22:51:42,967 [main] INFO codegen.SqlDbUtils: FILE_COUNT_BY_SIZE table already exists, skipping creation.
recon_1  | 2023-03-10 22:51:43,104 [main] INFO codegen.SqlDbUtils: CLUSTER_GROWTH_DAILY table already exists, skipping creation.
recon_1  | 2023-03-10 22:51:43,275 [main] INFO codegen.SqlDbUtils: UNHEALTHY_CONTAINERS table already exists, skipping creation.
recon_1  | 2023-03-10 22:51:43,823 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1  | 2023-03-10 22:51:43,977 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1  | 2023-03-10 22:51:44,110 [main] INFO util.log: Logging initialized @45539ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1  | 2023-03-10 22:51:44,850 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1  | 2023-03-10 22:51:44,900 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1  | 2023-03-10 22:51:44,950 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1  | 2023-03-10 22:51:44,963 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
scm_1    | 2023-03-10 22:51:55,493 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm_1    | 2023-03-10 22:51:55,499 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm_1    | 2023-03-10 22:51:55,865 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1    | 2023-03-10 22:51:56,075 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm_1    | 2023-03-10 22:51:56,270 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm_1    | 2023-03-10 22:51:56,357 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm_1    | 2023-03-10 22:51:56,358 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
scm_1    | WARNING: An illegal reflective access operation has occurred
scm_1    | WARNING: Illegal reflective access by org.apache.hadoop.hdds.utils.MetricsUtil (file:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar) to method java.lang.Class.annotationData()
scm_1    | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.hdds.utils.MetricsUtil
scm_1    | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
scm_1    | WARNING: All illegal access operations will be denied in a future release
scm_1    | 2023-03-10 22:51:56,399 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm_1    | 2023-03-10 22:51:56,410 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 3, healthy pipeline threshold count is 1
scm_1    | 2023-03-10 22:51:56,417 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 3, pipeline's with at least one datanode reported threshold count is 3
scm_1    | 2023-03-10 22:51:56,547 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
scm_1    | 2023-03-10 22:51:58,070 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1    | 2023-03-10 22:51:58,127 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1    | 2023-03-10 22:51:58,244 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1    | 2023-03-10 22:51:58,361 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1    | 2023-03-10 22:51:58,366 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1    | 2023-03-10 22:51:58,367 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1    | 2023-03-10 22:51:58,423 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1    | 2023-03-10 22:51:58,437 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1    | 2023-03-10 22:51:58,439 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1    | 2023-03-10 22:51:58,528 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm_1    | 2023-03-10 22:51:58,530 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm_1    | Container Balancer status:
scm_1    | Key                            Value
scm_1    | Running                        true
scm_1    | Container Balancer Configuration values:
scm_1    | Key                                                Value
scm_1    | Threshold                                          10
scm_1    | Max Datanodes to Involve per Iteration(percent)    20
scm_1    | Max Size to Move per Iteration                     500GB
scm_1    | Max Size Entering Target per Iteration             26GB
scm_1    | Max Size Leaving Source per Iteration              26GB
scm_1    | 
scm_1    | 2023-03-10 22:51:58,530 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm_1    | 2023-03-10 22:51:58,541 [Listener at 0.0.0.0/9860] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
scm_1    | 2023-03-10 22:51:58,561 [Listener at 0.0.0.0/9860] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
scm_1    | 2023-03-10 22:51:58,561 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1    | 2023-03-10 22:51:58,717 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1    | 2023-03-10 22:51:58,758 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1    | 2023-03-10 22:51:58,760 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1    | 2023-03-10 22:51:59,515 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1    | 2023-03-10 22:51:59,521 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1    | 2023-03-10 22:51:59,565 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1    | 2023-03-10 22:51:59,938 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1    | 2023-03-10 22:51:59,947 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1    | 2023-03-10 22:52:00,006 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1    | 2023-03-10 22:52:00,006 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1    | 2023-03-10 22:52:00,123 [Listener at 0.0.0.0/9860] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1    | 2023-03-10 22:52:00,124 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1    | 2023-03-10 22:52:00,127 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1    | 2023-03-10 22:52:00,206 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7b53b1ad] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm_1    | 2023-03-10 22:52:00,285 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1    | 2023-03-10 22:52:00,286 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1    | 2023-03-10 22:52:00,408 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @61222ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1    | 2023-03-10 22:52:02,418 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm_1    | 2023-03-10 22:52:02,571 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
s3g_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=1, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.ssl.keystore.reload.interval=60s, hdds.security.ssl.truststore.reload.interval=60s, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4))$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.sst.filtering.service.timeout=300000ms, ozone.tags.system=ACL,BALANCER,CLIENT,CONTAINER,DATANODE,DATASTREAM,DEBUG,DELETION,DEPRECATED,FREON,HA,HDDS,KERBEROS,MANAGEMENT,OM,OPERATION,OZONE,OZONEFS,PERFORMANCE,PIPELINE,RATIS,RECON,REQUIRED,S3GATEWAY,SCM,SECURITY,STORAGE,TLS,TOKEN,UPGRADE,X509, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
s3g_1    | ************************************************************/
s3g_1    | 2023-03-10 22:51:16,151 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1    | 2023-03-10 22:51:16,398 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1    | 2023-03-10 22:51:17,307 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1    | 2023-03-10 22:51:18,818 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1    | 2023-03-10 22:51:18,818 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1    | 2023-03-10 22:51:19,397 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1    | 2023-03-10 22:51:19,406 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
s3g_1    | 2023-03-10 22:51:19,745 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1    | 2023-03-10 22:51:19,767 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1    | 2023-03-10 22:51:19,794 [main] INFO server.session: node0 Scavenging every 660000ms
s3g_1    | 2023-03-10 22:51:19,974 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d898981{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1    | 2023-03-10 22:51:20,039 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5965be2d{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1    | WARNING: An illegal reflective access operation has occurred
s3g_1    | WARNING: Illegal reflective access by com.sun.xml.bind.v2.runtime.reflect.opt.Injector (file:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int)
s3g_1    | WARNING: Please consider reporting this to the maintainers of com.sun.xml.bind.v2.runtime.reflect.opt.Injector
s3g_1    | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1    | WARNING: All illegal access operations will be denied in a future release
s3g_1    | Mar 10, 2023 10:51:54 PM org.glassfish.jersey.internal.Errors logErrors
s3g_1    | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1    | 
s3g_1    | 2023-03-10 22:51:55,020 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@12fccff0{s3gateway,/,file:///data/metadata/webserver/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-11807854994142516248/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1    | 2023-03-10 22:51:55,146 [main] INFO server.AbstractConnector: Started ServerConnector@1339e7aa{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1    | 2023-03-10 22:51:55,146 [main] INFO server.Server: Started @56168ms
s3g_1    | 2023-03-10 22:51:55,161 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1    | 2023-03-10 22:51:55,161 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1    | 2023-03-10 22:51:55,192 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
s3g_1    | 2023-03-10 22:54:57,446 [qtp867988177-20] INFO audit.AuditLogger: Refresh DebugCmdSet for S3GAudit to [].
s3g_1    | 2023-03-10 22:54:57,498 [qtp867988177-20] INFO audit.AuditLogger: Refresh DebugCmdSet for S3GAudit to [].
s3g_1    | 2023-03-10 22:54:57,510 [qtp867988177-20] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
s3g_1    | 2023-03-10 22:54:57,510 [qtp867988177-20] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
s3g_1    | 2023-03-10 22:54:59,089 [qtp867988177-20] WARN impl.MetricsSystemImpl: S3Gateway metrics system already initialized!
s3g_1    | 2023-03-10 22:56:13,874 [qtp867988177-23] INFO rpc.RpcClient: Creating Bucket: s3v/new2-bucket, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false, Replication Type set to server-side default replication type, Namespace Quota set to -1, Space Quota set to -1 
s3g_1    | 2023-03-10 22:56:15,449 [qtp867988177-19] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn1_1    | 2023-03-10 22:52:20,137 [grpc-default-executor-0] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575 replies to PRE_VOTE vote request: 3892a4e1-c878-42af-adb7-db66a90d61f4<-e3e4587c-aa42-4e86-ae9a-d3e448365275#0:OK-t6. Peer's state: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575:t6, leader=null, voted=e78c5ce1-46ab-4889-a0cd-5903ae46614d, raftlog=Memoized:e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-SegmentedRaftLog:OPENED:c20, conf=12: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:20,165 [grpc-default-executor-1] INFO impl.VoteContext: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-CANDIDATE: accept PRE_VOTE from e78c5ce1-46ab-4889-a0cd-5903ae46614d: our priority 0 <= candidate's priority 1
dn1_1    | 2023-03-10 22:52:20,167 [grpc-default-executor-1] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575 replies to PRE_VOTE vote request: e78c5ce1-46ab-4889-a0cd-5903ae46614d<-e3e4587c-aa42-4e86-ae9a-d3e448365275#0:OK-t6. Peer's state: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575:t6, leader=null, voted=e78c5ce1-46ab-4889-a0cd-5903ae46614d, raftlog=Memoized:e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-SegmentedRaftLog:OPENED:c20, conf=12: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:20,149 [grpc-default-executor-2] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3 replies to PRE_VOTE vote request: e78c5ce1-46ab-4889-a0cd-5903ae46614d<-e3e4587c-aa42-4e86-ae9a-d3e448365275#0:FAIL-t10. Peer's state: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3:t10, leader=null, voted=e3e4587c-aa42-4e86-ae9a-d3e448365275, raftlog=Memoized:e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-SegmentedRaftLog:OPENED:c35, conf=22: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:20,293 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO impl.LeaderElection: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn1_1    | 2023-03-10 22:52:20,321 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO impl.LeaderElection:   Response 0: e3e4587c-aa42-4e86-ae9a-d3e448365275<-e78c5ce1-46ab-4889-a0cd-5903ae46614d#0:OK-t10
dn1_1    | 2023-03-10 22:52:20,321 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO impl.LeaderElection: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2 PRE_VOTE round 1: result PASSED
dn1_1    | 2023-03-10 22:52:20,332 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO impl.LeaderElection: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2 ELECTION round 0: submit vote requests at term 11 for 22: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:20,354 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-LeaderElection1] INFO impl.LeaderElection: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-LeaderElection1: PRE_VOTE REJECTED received 1 response(s) and 0 exception(s):
dn1_1    | 2023-03-10 22:52:20,404 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-03-10 22:52:20,465 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-03-10 22:52:20,460 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-LeaderElection1] INFO impl.LeaderElection:   Response 0: e3e4587c-aa42-4e86-ae9a-d3e448365275<-e78c5ce1-46ab-4889-a0cd-5903ae46614d#0:FAIL-t6
dn1_1    | 2023-03-10 22:52:20,469 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-LeaderElection1] INFO impl.LeaderElection: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-LeaderElection1 PRE_VOTE round 1: result REJECTED
dn1_1    | 2023-03-10 22:52:20,471 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-LeaderElection1] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575: changes role from CANDIDATE to FOLLOWER at term 6 for REJECTED
dn1_1    | 2023-03-10 22:52:20,472 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-LeaderElection1] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: shutdown e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-LeaderElection1
dn1_1    | 2023-03-10 22:52:20,473 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-LeaderElection1] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: start e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-FollowerState
dn1_1    | 2023-03-10 22:52:20,579 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-03-10 22:52:20,580 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-03-10 22:52:20,607 [grpc-default-executor-0] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575: receive requestVote(ELECTION, e78c5ce1-46ab-4889-a0cd-5903ae46614d, group-CBC13A60F575, 7, (t:6, i:20))
dn1_1    | 2023-03-10 22:52:20,612 [grpc-default-executor-0] INFO impl.VoteContext: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-FOLLOWER: accept ELECTION from e78c5ce1-46ab-4889-a0cd-5903ae46614d: our priority 0 <= candidate's priority 1
dn1_1    | 2023-03-10 22:52:20,612 [grpc-default-executor-0] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575: changes role from  FOLLOWER to FOLLOWER at term 7 for candidate:e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn1_1    | 2023-03-10 22:52:20,612 [grpc-default-executor-0] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: shutdown e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-FollowerState
dn1_1    | 2023-03-10 22:52:20,613 [grpc-default-executor-0] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: start e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-FollowerState
dn5_1    | 2023-03-10 22:52:13,874 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 3892a4e1-c878-42af-adb7-db66a90d61f4
dn5_1    | 2023-03-10 22:52:13,928 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2] INFO impl.LeaderElection: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2 ELECTION round 0: submit vote requests at term 4 for 3: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:52:13,929 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2] INFO impl.LeaderElection: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2 ELECTION round 0: result PASSED (term=4)
dn5_1    | 2023-03-10 22:52:13,930 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: shutdown 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2
dn5_1    | 2023-03-10 22:52:13,935 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn5_1    | 2023-03-10 22:52:13,937 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8E287CACCC68 with new leaderId: 178b30e1-b74d-4f4d-a142-c930eee71455
dn5_1    | 2023-03-10 22:52:13,944 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68: change Leader from null to 178b30e1-b74d-4f4d-a142-c930eee71455 at term 4 for becomeLeader, leader elected after 26364ms
dn5_1    | 2023-03-10 22:52:14,031 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn5_1    | 2023-03-10 22:52:14,090 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-03-10 22:52:14,098 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn5_1    | 2023-03-10 22:52:14,147 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn5_1    | 2023-03-10 22:52:14,215 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 2023-03-10 22:52:14,235 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn5_1    | 2023-03-10 22:52:14,293 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-03-10 22:52:14,332 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn5_1    | 2023-03-10 22:52:14,395 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: start 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderStateImpl
dn5_1    | 2023-03-10 22:52:14,541 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn5_1    | 2023-03-10 22:52:14,574 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/b1358d3e-7575-4e13-af6d-8e287caccc68/current/log_inprogress_3 to /data/metadata/ratis/b1358d3e-7575-4e13-af6d-8e287caccc68/current/log_3-4
dn5_1    | 2023-03-10 22:52:14,606 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/b1358d3e-7575-4e13-af6d-8e287caccc68/current/log_inprogress_5
dn5_1    | 2023-03-10 22:52:14,715 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderElection2] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68: set configuration 5: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:52:17,020 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-178b30e1-b74d-4f4d-a142-c930eee71455: Detected pause in JVM or host machine (eg GC): pause of approximately 677906380ns.
dn5_1    | GC pool 'ParNew' had collection(s): count=1 time=686ms
dn5_1    | 2023-03-10 22:52:18,960 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-LeaderElection1] INFO impl.LeaderElection: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-LeaderElection1: PRE_VOTE TIMEOUT received 0 response(s) and 0 exception(s):
dn5_1    | 2023-03-10 22:52:18,961 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-LeaderElection1] INFO impl.LeaderElection: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-LeaderElection1 PRE_VOTE round 0: result TIMEOUT
dn5_1    | 2023-03-10 22:52:18,964 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-LeaderElection1] INFO impl.LeaderElection: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-LeaderElection1 PRE_VOTE round 1: submit vote requests at term 11 for 11: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:52:18,998 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-10 22:52:18,998 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-10 22:52:19,729 [grpc-default-executor-0] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00: receive requestVote(PRE_VOTE, 3892a4e1-c878-42af-adb7-db66a90d61f4, group-5B98FDC36F00, 11, (t:11, i:16))
dn5_1    | 2023-03-10 22:52:19,753 [grpc-default-executor-0] INFO impl.VoteContext: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-CANDIDATE: accept PRE_VOTE from 3892a4e1-c878-42af-adb7-db66a90d61f4: our priority 0 <= candidate's priority 1
dn5_1    | 2023-03-10 22:52:19,825 [grpc-default-executor-0] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00 replies to PRE_VOTE vote request: 3892a4e1-c878-42af-adb7-db66a90d61f4<-178b30e1-b74d-4f4d-a142-c930eee71455#0:OK-t11. Peer's state: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00:t11, leader=null, voted=3892a4e1-c878-42af-adb7-db66a90d61f4, raftlog=Memoized:178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-SegmentedRaftLog:OPENED:c16, conf=11: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:52:20,383 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-LeaderElection1] INFO impl.LeaderElection: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-LeaderElection1: PRE_VOTE DISCOVERED_A_NEW_TERM (term=12) received 1 response(s) and 0 exception(s):
dn5_1    | 2023-03-10 22:52:20,384 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-LeaderElection1] INFO impl.LeaderElection:   Response 0: 178b30e1-b74d-4f4d-a142-c930eee71455<-3892a4e1-c878-42af-adb7-db66a90d61f4#0:FAIL-t12
dn5_1    | 2023-03-10 22:52:20,384 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-LeaderElection1] INFO impl.LeaderElection: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-LeaderElection1 PRE_VOTE round 1: result DISCOVERED_A_NEW_TERM (term=12)
dn5_1    | 2023-03-10 22:52:20,384 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-LeaderElection1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00: changes role from CANDIDATE to FOLLOWER at term 12 for DISCOVERED_A_NEW_TERM (term=12)
dn5_1    | 2023-03-10 22:52:20,384 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-LeaderElection1] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: shutdown 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-LeaderElection1
dn5_1    | 2023-03-10 22:52:20,385 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-LeaderElection1] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: start 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-FollowerState
dn5_1    | 2023-03-10 22:52:20,423 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-10 22:52:20,424 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-10 22:52:20,526 [grpc-default-executor-0] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00: receive requestVote(ELECTION, 3892a4e1-c878-42af-adb7-db66a90d61f4, group-5B98FDC36F00, 12, (t:11, i:16))
dn5_1    | 2023-03-10 22:52:20,528 [grpc-default-executor-0] INFO impl.VoteContext: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-FOLLOWER: accept ELECTION from 3892a4e1-c878-42af-adb7-db66a90d61f4: our priority 0 <= candidate's priority 1
dn5_1    | 2023-03-10 22:52:20,528 [grpc-default-executor-0] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00: changes role from  FOLLOWER to FOLLOWER at term 12 for candidate:3892a4e1-c878-42af-adb7-db66a90d61f4
dn5_1    | 2023-03-10 22:52:20,529 [grpc-default-executor-0] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: shutdown 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-FollowerState
dn5_1    | 2023-03-10 22:52:20,530 [grpc-default-executor-0] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: start 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-FollowerState
dn5_1    | 2023-03-10 22:52:20,530 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-FollowerState] INFO impl.FollowerState: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-FollowerState was interrupted
dn5_1    | 2023-03-10 22:52:20,552 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-10 22:52:20,567 [grpc-default-executor-0] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00 replies to ELECTION vote request: 3892a4e1-c878-42af-adb7-db66a90d61f4<-178b30e1-b74d-4f4d-a142-c930eee71455#0:OK-t12. Peer's state: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00:t12, leader=null, voted=3892a4e1-c878-42af-adb7-db66a90d61f4, raftlog=Memoized:178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-SegmentedRaftLog:OPENED:c16, conf=11: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:52:20,582 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-10 22:52:21,345 [grpc-default-executor-1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00: receive requestVote(PRE_VOTE, 1a6d358d-6662-4447-914c-d709a67ff716, group-5B98FDC36F00, 11, (t:11, i:16))
dn5_1    | 2023-03-10 22:52:21,351 [grpc-default-executor-1] INFO impl.VoteContext: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-FOLLOWER: accept PRE_VOTE from 1a6d358d-6662-4447-914c-d709a67ff716: our priority 0 <= candidate's priority 0
dn5_1    | 2023-03-10 22:52:21,361 [grpc-default-executor-1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00 replies to PRE_VOTE vote request: 1a6d358d-6662-4447-914c-d709a67ff716<-178b30e1-b74d-4f4d-a142-c930eee71455#0:OK-t12. Peer's state: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00:t12, leader=null, voted=3892a4e1-c878-42af-adb7-db66a90d61f4, raftlog=Memoized:178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-SegmentedRaftLog:OPENED:c16, conf=11: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:52:21,399 [178b30e1-b74d-4f4d-a142-c930eee71455-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-5B98FDC36F00 with new leaderId: 3892a4e1-c878-42af-adb7-db66a90d61f4
dn5_1    | 2023-03-10 22:52:21,401 [178b30e1-b74d-4f4d-a142-c930eee71455-server-thread1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00: change Leader from null to 3892a4e1-c878-42af-adb7-db66a90d61f4 at term 12 for appendEntries, leader elected after 35037ms
dn5_1    | 2023-03-10 22:52:21,523 [178b30e1-b74d-4f4d-a142-c930eee71455-server-thread1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00: set configuration 17: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:20,616 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-FollowerState] INFO impl.FollowerState: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-FollowerState was interrupted
dn1_1    | 2023-03-10 22:52:20,633 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO impl.LeaderElection: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn1_1    | 2023-03-10 22:52:20,635 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO impl.LeaderElection:   Response 0: e3e4587c-aa42-4e86-ae9a-d3e448365275<-e78c5ce1-46ab-4889-a0cd-5903ae46614d#0:OK-t11
dn1_1    | 2023-03-10 22:52:20,637 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO impl.LeaderElection: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2 ELECTION round 0: result PASSED
dn1_1    | 2023-03-10 22:52:20,637 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: shutdown e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2
dn1_1    | 2023-03-10 22:52:20,638 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3: changes role from CANDIDATE to LEADER at term 11 for changeToLeader
dn1_1    | 2023-03-10 22:52:20,639 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-0049FBBC23B3 with new leaderId: e3e4587c-aa42-4e86-ae9a-d3e448365275
dn1_1    | 2023-03-10 22:52:20,656 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3: change Leader from null to e3e4587c-aa42-4e86-ae9a-d3e448365275 at term 11 for becomeLeader, leader elected after 32421ms
dn1_1    | 2023-03-10 22:52:20,663 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn1_1    | 2023-03-10 22:52:20,669 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-03-10 22:52:20,675 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn1_1    | 2023-03-10 22:52:20,676 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn1_1    | 2023-03-10 22:52:20,678 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn1_1    | 2023-03-10 22:52:20,679 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn1_1    | 2023-03-10 22:52:20,680 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-03-10 22:52:20,686 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn1_1    | 2023-03-10 22:52:20,682 [grpc-default-executor-0] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575 replies to ELECTION vote request: e78c5ce1-46ab-4889-a0cd-5903ae46614d<-e3e4587c-aa42-4e86-ae9a-d3e448365275#0:OK-t7. Peer's state: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575:t7, leader=null, voted=e78c5ce1-46ab-4889-a0cd-5903ae46614d, raftlog=Memoized:e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-SegmentedRaftLog:OPENED:c20, conf=12: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:20,749 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-03-10 22:52:20,751 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-03-10 22:52:20,816 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn1_1    | 2023-03-10 22:52:20,840 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-03-10 22:52:20,844 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn1_1    | 2023-03-10 22:52:20,863 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn1_1    | 2023-03-10 22:52:20,868 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-03-10 22:52:20,871 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-03-10 22:52:20,871 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 2023-03-10 22:52:20,871 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn1_1    | 2023-03-10 22:52:20,901 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn1_1    | 2023-03-10 22:52:20,905 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-03-10 22:52:20,905 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn2_1    | 2023-03-10 22:52:15,415 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3-1] INFO server.GrpcServerProtocolClient: Build channel for 178b30e1-b74d-4f4d-a142-c930eee71455
dn2_1    | 2023-03-10 22:52:15,453 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn2_1    | 2023-03-10 22:52:15,458 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3-2] INFO server.GrpcServerProtocolClient: Build channel for 1a6d358d-6662-4447-914c-d709a67ff716
dn2_1    | 2023-03-10 22:52:15,448 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for e3e4587c-aa42-4e86-ae9a-d3e448365275
dn2_1    | 2023-03-10 22:52:15,464 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-03-10 22:52:15,454 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-03-10 22:52:15,499 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn2_1    | 2023-03-10 22:52:15,680 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn2_1    | 2023-03-10 22:52:15,681 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn2_1    | 2023-03-10 22:52:15,688 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn2_1    | 2023-03-10 22:52:15,749 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-03-10 22:52:15,795 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn2_1    | 2023-03-10 22:52:15,898 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: start 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderStateImpl
dn2_1    | 2023-03-10 22:52:16,035 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn2_1    | 2023-03-10 22:52:16,104 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/2d4f1397-2cdc-4336-a562-676bab171c02/current/log_inprogress_3 to /data/metadata/ratis/2d4f1397-2cdc-4336-a562-676bab171c02/current/log_3-4
dn2_1    | 2023-03-10 22:52:16,307 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/2d4f1397-2cdc-4336-a562-676bab171c02/current/log_inprogress_5
dn2_1    | 2023-03-10 22:52:16,439 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderElection1] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02: set configuration 5: peers:[3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:20,037 [grpc-default-executor-1] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575: receive requestVote(PRE_VOTE, e3e4587c-aa42-4e86-ae9a-d3e448365275, group-CBC13A60F575, 6, (t:6, i:20))
dn2_1    | 2023-03-10 22:52:20,051 [grpc-default-executor-1] INFO impl.VoteContext: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-CANDIDATE: accept PRE_VOTE from e3e4587c-aa42-4e86-ae9a-d3e448365275: our priority 0 <= candidate's priority 0
dn2_1    | 2023-03-10 22:52:20,064 [grpc-default-executor-0] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575: receive requestVote(PRE_VOTE, e3e4587c-aa42-4e86-ae9a-d3e448365275, group-CBC13A60F575, 6, (t:6, i:20))
dn2_1    | 2023-03-10 22:52:20,148 [grpc-default-executor-3] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575: receive requestVote(PRE_VOTE, e78c5ce1-46ab-4889-a0cd-5903ae46614d, group-CBC13A60F575, 6, (t:6, i:20))
dn2_1    | 2023-03-10 22:52:20,158 [grpc-default-executor-2] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00: receive requestVote(PRE_VOTE, 178b30e1-b74d-4f4d-a142-c930eee71455, group-5B98FDC36F00, 11, (t:11, i:16))
dn2_1    | 2023-03-10 22:52:20,161 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-LeaderElection2] INFO impl.LeaderElection: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-LeaderElection2: PRE_VOTE REJECTED received 1 response(s) and 0 exception(s):
dn2_1    | 2023-03-10 22:52:20,159 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO impl.LeaderElection: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn2_1    | 2023-03-10 22:52:20,164 [grpc-default-executor-2] INFO impl.VoteContext: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-CANDIDATE: reject PRE_VOTE from 178b30e1-b74d-4f4d-a142-c930eee71455: our priority 1 > candidate's priority 0
dn2_1    | 2023-03-10 22:52:20,182 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-LeaderElection2] INFO impl.LeaderElection:   Response 0: 3892a4e1-c878-42af-adb7-db66a90d61f4<-e78c5ce1-46ab-4889-a0cd-5903ae46614d#0:FAIL-t6
dn2_1    | 2023-03-10 22:52:20,187 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO impl.LeaderElection:   Response 0: 3892a4e1-c878-42af-adb7-db66a90d61f4<-178b30e1-b74d-4f4d-a142-c930eee71455#0:OK-t11
dn2_1    | 2023-03-10 22:52:20,272 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO impl.LeaderElection: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3 PRE_VOTE round 0: result PASSED
dn2_1    | 2023-03-10 22:52:20,187 [grpc-default-executor-2] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00 replies to PRE_VOTE vote request: 178b30e1-b74d-4f4d-a142-c930eee71455<-3892a4e1-c878-42af-adb7-db66a90d61f4#0:FAIL-t11. Peer's state: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00:t11, leader=null, voted=3892a4e1-c878-42af-adb7-db66a90d61f4, raftlog=Memoized:3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-SegmentedRaftLog:OPENED:c16, conf=11: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:20,282 [grpc-default-executor-4] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00: receive requestVote(PRE_VOTE, 178b30e1-b74d-4f4d-a142-c930eee71455, group-5B98FDC36F00, 11, (t:11, i:16))
dn2_1    | 2023-03-10 22:52:20,286 [grpc-default-executor-4] INFO impl.VoteContext: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-CANDIDATE: reject PRE_VOTE from 178b30e1-b74d-4f4d-a142-c930eee71455: our priority 1 > candidate's priority 0
dn2_1    | 2023-03-10 22:52:20,289 [grpc-default-executor-4] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00 replies to PRE_VOTE vote request: 178b30e1-b74d-4f4d-a142-c930eee71455<-3892a4e1-c878-42af-adb7-db66a90d61f4#0:FAIL-t12. Peer's state: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00:t12, leader=null, voted=3892a4e1-c878-42af-adb7-db66a90d61f4, raftlog=Memoized:3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-SegmentedRaftLog:OPENED:c16, conf=11: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:20,272 [grpc-default-executor-1] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575 replies to PRE_VOTE vote request: e3e4587c-aa42-4e86-ae9a-d3e448365275<-3892a4e1-c878-42af-adb7-db66a90d61f4#0:OK-t6. Peer's state: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575:t6, leader=null, voted=e78c5ce1-46ab-4889-a0cd-5903ae46614d, raftlog=Memoized:3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-SegmentedRaftLog:OPENED:c20, conf=12: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:20,299 [grpc-default-executor-3] INFO impl.VoteContext: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-CANDIDATE: accept PRE_VOTE from e78c5ce1-46ab-4889-a0cd-5903ae46614d: our priority 0 <= candidate's priority 1
dn2_1    | 2023-03-10 22:52:20,327 [grpc-default-executor-3] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575 replies to PRE_VOTE vote request: e78c5ce1-46ab-4889-a0cd-5903ae46614d<-3892a4e1-c878-42af-adb7-db66a90d61f4#0:OK-t6. Peer's state: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575:t6, leader=null, voted=e78c5ce1-46ab-4889-a0cd-5903ae46614d, raftlog=Memoized:3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-SegmentedRaftLog:OPENED:c20, conf=12: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:20,327 [grpc-default-executor-0] INFO impl.VoteContext: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-CANDIDATE: accept PRE_VOTE from e3e4587c-aa42-4e86-ae9a-d3e448365275: our priority 0 <= candidate's priority 0
dn2_1    | 2023-03-10 22:52:20,344 [grpc-default-executor-0] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575 replies to PRE_VOTE vote request: e3e4587c-aa42-4e86-ae9a-d3e448365275<-3892a4e1-c878-42af-adb7-db66a90d61f4#0:OK-t6. Peer's state: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575:t6, leader=null, voted=e78c5ce1-46ab-4889-a0cd-5903ae46614d, raftlog=Memoized:3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-SegmentedRaftLog:OPENED:c20, conf=12: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:20,283 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-LeaderElection2] INFO impl.LeaderElection: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-LeaderElection2 PRE_VOTE round 0: result REJECTED
dn2_1    | 2023-03-10 22:52:20,282 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO impl.LeaderElection: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3 ELECTION round 0: submit vote requests at term 12 for 11: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:20,392 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-LeaderElection2] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575: changes role from CANDIDATE to FOLLOWER at term 6 for REJECTED
dn2_1    | 2023-03-10 22:52:20,396 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-LeaderElection2] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: shutdown 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-LeaderElection2
dn2_1    | 2023-03-10 22:52:20,397 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-LeaderElection2] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: start 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-FollowerState
dn2_1    | 2023-03-10 22:52:20,420 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-03-10 22:52:20,460 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-03-10 22:52:20,531 [grpc-default-executor-3] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575: receive requestVote(ELECTION, e78c5ce1-46ab-4889-a0cd-5903ae46614d, group-CBC13A60F575, 7, (t:6, i:20))
dn2_1    | 2023-03-10 22:52:20,549 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-03-10 22:52:20,549 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-10 22:52:08,677 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/7572f4a9-8e86-407b-b624-5c511402a23e/current/log_inprogress_3
dn3_1    | 2023-03-10 22:52:08,717 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn3_1    | 2023-03-10 22:52:08,721 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn3_1    | 2023-03-10 22:52:08,667 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO segmented.LogSegment: Successfully read 4 entries from segment file /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/current/log_0-3
dn3_1    | 2023-03-10 22:52:08,754 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3: set configuration 4: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:08,759 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO segmented.LogSegment: Successfully read 18 entries from segment file /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/current/log_4-21
dn3_1    | 2023-03-10 22:52:08,761 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3: set configuration 22: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:08,672 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO segmented.LogSegment: Successfully read 9 entries from segment file /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/current/log_0-8
dn3_1    | 2023-03-10 22:52:08,792 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00: set configuration 9: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:08,797 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/current/log_9-10
dn3_1    | 2023-03-10 22:52:08,798 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00: set configuration 11: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:08,800 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO segmented.LogSegment: Successfully read 6 entries from segment file /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/current/log_inprogress_11
dn3_1    | 2023-03-10 22:52:08,802 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 16
dn3_1    | 2023-03-10 22:52:08,802 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 10
dn3_1    | 2023-03-10 22:52:08,770 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO segmented.LogSegment: Successfully read 14 entries from segment file /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/current/log_inprogress_22
dn3_1    | 2023-03-10 22:52:08,865 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 35
dn3_1    | 2023-03-10 22:52:08,867 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 21
dn3_1    | 2023-03-10 22:52:10,488 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO raftlog.RaftLog: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-SegmentedRaftLog: commitIndex: updateToMax old=35, new=33, updated? false
dn3_1    | 2023-03-10 22:52:10,497 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3: start as a follower, conf=22: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:10,499 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3: changes role from      null to FOLLOWER at term 10 for startAsFollower
dn3_1    | 2023-03-10 22:52:10,566 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO raftlog.RaftLog: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-SegmentedRaftLog: commitIndex: updateToMax old=4, new=3, updated? false
dn3_1    | 2023-03-10 22:52:10,649 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E: start as a follower, conf=3: peers:[1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:10,650 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn3_1    | 2023-03-10 22:52:10,575 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: start 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-FollowerState
dn3_1    | 2023-03-10 22:52:10,536 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO raftlog.RaftLog: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-SegmentedRaftLog: commitIndex: updateToMax old=16, new=15, updated? false
dn3_1    | 2023-03-10 22:52:10,670 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00: start as a follower, conf=11: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:10,671 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00: changes role from      null to FOLLOWER at term 11 for startAsFollower
dn3_1    | 2023-03-10 22:52:10,666 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: start 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-FollowerState
dn3_1    | 2023-03-10 22:52:10,735 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: start 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-FollowerState
dn3_1    | 2023-03-10 22:52:10,737 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-5C511402A23E,id=1a6d358d-6662-4447-914c-d709a67ff716
dn3_1    | 2023-03-10 22:52:10,769 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-03-10 22:52:10,770 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-03-10 22:52:10,772 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-03-10 22:52:10,772 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-03-10 22:52:10,781 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-10 22:52:10,785 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-03-10 22:52:10,861 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-5B98FDC36F00,id=1a6d358d-6662-4447-914c-d709a67ff716
dn3_1    | 2023-03-10 22:52:10,861 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0049FBBC23B3,id=1a6d358d-6662-4447-914c-d709a67ff716
dn3_1    | 2023-03-10 22:52:10,862 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-03-10 22:52:10,863 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-03-10 22:52:10,863 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-03-10 22:52:10,863 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-03-10 22:52:10,863 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-03-10 22:52:10,863 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-03-10 22:52:10,866 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-03-10 22:52:21,526 [178b30e1-b74d-4f4d-a142-c930eee71455-server-thread1] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-SegmentedRaftLogWorker: Rolling segment log-11_16 to index:16
dn5_1    | 2023-03-10 22:52:21,542 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/current/log_inprogress_11 to /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/current/log_11-16
dn5_1    | 2023-03-10 22:52:21,559 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/current/log_inprogress_17
dn5_1    | 2023-03-10 22:52:52,742 [ContainerOp-54b523a6-4d1d-4f07-a057-5b98fdc36f00-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 15.
dn5_1    | 2023-03-10 22:52:52,743 [ContainerOp-54b523a6-4d1d-4f07-a057-5b98fdc36f00-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 15.
dn5_1    | 2023-03-10 22:52:52,785 [ContainerOp-54b523a6-4d1d-4f07-a057-5b98fdc36f00-0] INFO keyvalue.KeyValueContainer: Container 1 is closed with bcsId 15.
dn5_1    | 2023-03-10 22:53:08,885 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 2023-03-10 22:53:14,945 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    | 2023-03-10 22:53:14,947 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn5_1    | 2023-03-10 22:53:14,947 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn5_1    | 2023-03-10 22:53:14,948 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: ERASURE_CODED_STORAGE_SUPPORT.
dn5_1    | 2023-03-10 22:53:14,950 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature ERASURE_CODED_STORAGE_SUPPORT has been finalized.
dn5_1    | 2023-03-10 22:53:14,951 [Command processor thread] INFO upgrade.UpgradeFinalizer: Running finalization actions for layout feature: DATANODE_SCHEMA_V3
dn5_1    | 2023-03-10 22:53:14,951 [Command processor thread] INFO upgrade.DatanodeSchemaV3FinalizeAction: Upgrading Datanode volume layout for Schema V3 support.
dn5_1    | 2023-03-10 22:53:15,006 [Command processor thread] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-65fdc528-544f-4492-8e62-cb2426903d78/container.db to cache
dn5_1    | 2023-03-10 22:53:15,006 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-65fdc528-544f-4492-8e62-cb2426903d78/container.db for volume DS-65fdc528-544f-4492-8e62-cb2426903d78
dn5_1    | 2023-03-10 22:53:15,008 [Command processor thread] INFO utils.DatanodeStoreCache: Removed db /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-65fdc528-544f-4492-8e62-cb2426903d78/container.db from cache
dn5_1    | 2023-03-10 22:53:15,008 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is stopped at /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-65fdc528-544f-4492-8e62-cb2426903d78/container.db for volume DS-65fdc528-544f-4492-8e62-cb2426903d78
dn5_1    | 2023-03-10 22:53:15,031 [Command processor thread] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-65fdc528-544f-4492-8e62-cb2426903d78/container.db to cache
dn5_1    | 2023-03-10 22:53:15,031 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-65fdc528-544f-4492-8e62-cb2426903d78/container.db for volume DS-65fdc528-544f-4492-8e62-cb2426903d78
dn5_1    | 2023-03-10 22:53:15,033 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature DATANODE_SCHEMA_V3 has been finalized.
dn5_1    | 2023-03-10 22:53:15,033 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn5_1    | 2023-03-10 22:53:15,033 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn5_1    | 2023-03-10 22:53:23,784 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    | 2023-03-10 22:53:23,801 [Command processor thread] INFO server.RaftServer: 178b30e1-b74d-4f4d-a142-c930eee71455: remove  FOLLOWER 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00:t12, leader=3892a4e1-c878-42af-adb7-db66a90d61f4, voted=3892a4e1-c878-42af-adb7-db66a90d61f4, raftlog=Memoized:178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-SegmentedRaftLog:OPENED:c20, conf=17: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn5_1    | 2023-03-10 22:53:23,803 [Command processor thread] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00: shutdown
dn5_1    | 2023-03-10 22:53:23,803 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-5B98FDC36F00,id=178b30e1-b74d-4f4d-a142-c930eee71455
dn5_1    | 2023-03-10 22:53:23,803 [Command processor thread] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: shutdown 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-FollowerState
dn5_1    | 2023-03-10 22:53:23,803 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-FollowerState] INFO impl.FollowerState: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-FollowerState was interrupted
dn5_1    | 2023-03-10 22:53:23,804 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-5B98FDC36F00: Taking a snapshot at:(t:12, i:20) file /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/sm/snapshot.12_20
dn5_1    | 2023-03-10 22:53:23,812 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-5B98FDC36F00: Finished taking a snapshot at:(t:12, i:20) file:/data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/sm/snapshot.12_20 took: 8 ms
dn5_1    | 2023-03-10 22:53:23,831 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-StateMachineUpdater] INFO impl.StateMachineUpdater: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-StateMachineUpdater: Took a snapshot at index 20
dn5_1    | 2023-03-10 22:53:23,834 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-StateMachineUpdater] INFO impl.StateMachineUpdater: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-StateMachineUpdater: snapshotIndex: updateIncreasingly 16 -> 20
dn5_1    | 2023-03-10 22:53:23,834 [Command processor thread] INFO impl.StateMachineUpdater: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-StateMachineUpdater: set stopIndex = 20
dn5_1    | 2023-03-10 22:53:23,842 [Command processor thread] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00: closes. applyIndex: 20
dn5_1    | 2023-03-10 22:53:23,843 [178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn5_1    | 2023-03-10 22:53:23,848 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00-SegmentedRaftLogWorker close()
dn5_1    | 2023-03-10 22:53:23,870 [Command processor thread] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-5B98FDC36F00: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00
dn5_1    | 2023-03-10 22:53:23,873 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=54b523a6-4d1d-4f07-a057-5b98fdc36f00 command on datanode 178b30e1-b74d-4f4d-a142-c930eee71455.
dn5_1    | 2023-03-10 22:53:23,873 [Command processor thread] INFO server.RaftServer: 178b30e1-b74d-4f4d-a142-c930eee71455: remove    LEADER 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68:t4, leader=178b30e1-b74d-4f4d-a142-c930eee71455, voted=178b30e1-b74d-4f4d-a142-c930eee71455, raftlog=Memoized:178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-SegmentedRaftLog:OPENED:c6, conf=5: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn5_1    | 2023-03-10 22:53:23,874 [Command processor thread] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68: shutdown
dn5_1    | 2023-03-10 22:53:23,874 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-8E287CACCC68,id=178b30e1-b74d-4f4d-a142-c930eee71455
dn5_1    | 2023-03-10 22:53:23,874 [Command processor thread] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: shutdown 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-LeaderStateImpl
dn5_1    | 2023-03-10 22:53:23,877 [Command processor thread] INFO impl.PendingRequests: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-PendingRequests: sendNotLeaderResponses
dn5_1    | 2023-03-10 22:53:23,881 [Command processor thread] INFO impl.StateMachineUpdater: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-StateMachineUpdater: set stopIndex = 6
dn5_1    | 2023-03-10 22:53:23,881 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-8E287CACCC68: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/b1358d3e-7575-4e13-af6d-8e287caccc68/sm/snapshot.4_6
dn5_1    | 2023-03-10 22:53:23,890 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-8E287CACCC68: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/b1358d3e-7575-4e13-af6d-8e287caccc68/sm/snapshot.4_6 took: 9 ms
dn5_1    | 2023-03-10 22:53:23,890 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-StateMachineUpdater] INFO impl.StateMachineUpdater: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-StateMachineUpdater: Took a snapshot at index 6
dn5_1    | 2023-03-10 22:53:23,892 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-StateMachineUpdater] INFO impl.StateMachineUpdater: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
dn5_1    | 2023-03-10 22:53:23,893 [Command processor thread] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68: closes. applyIndex: 6
dn5_1    | 2023-03-10 22:53:23,893 [178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn5_1    | 2023-03-10 22:53:23,894 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68-SegmentedRaftLogWorker close()
dn5_1    | 2023-03-10 22:53:23,900 [Command processor thread] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-8E287CACCC68: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/b1358d3e-7575-4e13-af6d-8e287caccc68
dn5_1    | 2023-03-10 22:53:23,900 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=b1358d3e-7575-4e13-af6d-8e287caccc68 command on datanode 178b30e1-b74d-4f4d-a142-c930eee71455.
dn5_1    | 2023-03-10 22:53:24,416 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 178b30e1-b74d-4f4d-a142-c930eee71455: Completed APPEND_ENTRIES, lastRequest: 3892a4e1-c878-42af-adb7-db66a90d61f4->178b30e1-b74d-4f4d-a142-c930eee71455#231-t12,previous=(t:12, i:19),leaderCommit=19,initializing? true,entries: size=1, first=(t:12, i:20), METADATAENTRY(c:19)
dn5_1    | 2023-03-10 22:53:24,420 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: 178b30e1-b74d-4f4d-a142-c930eee71455: Completed APPEND_ENTRIES, lastRequest: null
dn5_1    | 2023-03-10 22:53:24,985 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@2233cac0] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(0)], numOfContainers=1, numOfBlocks=2
dn5_1    | 2023-03-10 22:53:53,832 [pool-26-thread-1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455: new RaftServerImpl for group-6265FBDC9D88:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn5_1    | 2023-03-10 22:53:53,833 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-03-10 22:53:53,833 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-03-10 22:53:53,833 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-03-10 22:53:53,833 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-03-10 22:53:53,833 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-03-10 22:53:53,833 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-03-10 22:53:53,834 [pool-26-thread-1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88: ConfigurationManager, init=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-03-10 22:53:53,834 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-03-10 22:53:53,836 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-03-10 22:52:20,905 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn1_1    | 2023-03-10 22:52:20,908 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-03-10 22:52:20,913 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-03-10 22:52:20,914 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 2023-03-10 22:52:20,917 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn1_1    | 2023-03-10 22:52:20,963 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: start e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderStateImpl
dn1_1    | 2023-03-10 22:52:20,963 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-SegmentedRaftLogWorker: Rolling segment log-22_35 to index:35
dn1_1    | 2023-03-10 22:52:20,983 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/current/log_inprogress_22 to /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/current/log_22-35
dn1_1    | 2023-03-10 22:52:21,002 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/current/log_inprogress_36
dn1_1    | 2023-03-10 22:52:21,067 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderElection2] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3: set configuration 36: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:21,160 [grpc-default-executor-2] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3: receive requestVote(PRE_VOTE, 1a6d358d-6662-4447-914c-d709a67ff716, group-0049FBBC23B3, 10, (t:10, i:35))
dn1_1    | 2023-03-10 22:52:21,160 [grpc-default-executor-2] INFO impl.VoteContext: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LEADER: reject PRE_VOTE from 1a6d358d-6662-4447-914c-d709a67ff716: this server is the leader and still has leadership
dn1_1    | 2023-03-10 22:52:21,169 [grpc-default-executor-2] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3 replies to PRE_VOTE vote request: 1a6d358d-6662-4447-914c-d709a67ff716<-e3e4587c-aa42-4e86-ae9a-d3e448365275#0:FAIL-t11. Peer's state: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3:t11, leader=e3e4587c-aa42-4e86-ae9a-d3e448365275, voted=e3e4587c-aa42-4e86-ae9a-d3e448365275, raftlog=Memoized:e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-SegmentedRaftLog:OPENED:c35, conf=36: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:21,515 [e3e4587c-aa42-4e86-ae9a-d3e448365275-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-CBC13A60F575 with new leaderId: e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn1_1    | 2023-03-10 22:52:21,696 [e3e4587c-aa42-4e86-ae9a-d3e448365275-server-thread1] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575: change Leader from null to e78c5ce1-46ab-4889-a0cd-5903ae46614d at term 7 for appendEntries, leader elected after 33266ms
dn1_1    | 2023-03-10 22:52:21,756 [e3e4587c-aa42-4e86-ae9a-d3e448365275-server-thread2] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575: set configuration 21: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:52:21,776 [e3e4587c-aa42-4e86-ae9a-d3e448365275-server-thread2] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-SegmentedRaftLogWorker: Rolling segment log-12_20 to index:20
dn1_1    | 2023-03-10 22:52:21,869 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/current/log_inprogress_12 to /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/current/log_12-20
dn1_1    | 2023-03-10 22:52:21,908 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/current/log_inprogress_21
dn1_1    | 2023-03-10 22:52:52,329 [ContainerOp-cf0cdca9-af45-4cc0-9366-0049fbbc23b3-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 33.
dn1_1    | 2023-03-10 22:52:52,329 [ContainerOp-cf0cdca9-af45-4cc0-9366-0049fbbc23b3-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 33.
dn1_1    | 2023-03-10 22:52:52,364 [ContainerOp-cf0cdca9-af45-4cc0-9366-0049fbbc23b3-0] INFO keyvalue.KeyValueContainer: Container 2 is closed with bcsId 33.
dn1_1    | 2023-03-10 22:52:53,134 [ContainerOp-78b03a88-75c1-4060-9962-cbc13a60f575-0] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 10.
dn1_1    | 2023-03-10 22:52:53,139 [ContainerOp-78b03a88-75c1-4060-9962-cbc13a60f575-0] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 10.
dn1_1    | 2023-03-10 22:52:53,182 [ContainerOp-78b03a88-75c1-4060-9962-cbc13a60f575-0] INFO keyvalue.KeyValueContainer: Container 1002 is closed with bcsId 10.
dn1_1    | 2023-03-10 22:52:53,378 [ContainerOp-cf0cdca9-af45-4cc0-9366-0049fbbc23b3-1] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 20.
dn1_1    | 2023-03-10 22:52:53,379 [ContainerOp-cf0cdca9-af45-4cc0-9366-0049fbbc23b3-1] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 20.
dn1_1    | 2023-03-10 22:52:53,397 [ContainerOp-cf0cdca9-af45-4cc0-9366-0049fbbc23b3-1] INFO keyvalue.KeyValueContainer: Container 1001 is closed with bcsId 20.
dn1_1    | 2023-03-10 22:52:53,471 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn1_1    | 2023-03-10 22:52:53,474 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn1_1    | 2023-03-10 22:52:53,475 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn1_1    | 2023-03-10 22:52:53,476 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 2001 to close, current state is: CLOSING
dn1_1    | 2023-03-10 22:52:53,476 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
recon_1  | 2023-03-10 22:51:44,973 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1  | 2023-03-10 22:51:44,973 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1  | 2023-03-10 22:51:45,257 [main] INFO http.BaseHttpServer: HTTP server of recon uses base directory /data/metadata/webserver
recon_1  | 2023-03-10 22:51:45,906 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1  | 2023-03-10 22:51:46,246 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1  | 2023-03-10 22:51:46,284 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1  | 2023-03-10 22:51:46,334 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1  | 2023-03-10 22:51:46,397 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1  | 2023-03-10 22:51:46,398 [main] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
recon_1  | 2023-03-10 22:51:49,877 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-03-10 22:51:50,781 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-03-10 22:51:51,400 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1  | 2023-03-10 22:51:51,412 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1  | 2023-03-10 22:51:52,044 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-03-10 22:51:52,423 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
recon_1  | 2023-03-10 22:51:53,084 [main] INFO reflections.Reflections: Reflections took 646 ms to scan 3 urls, producing 128 keys and 283 values 
recon_1  | 2023-03-10 22:51:53,380 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1  | 2023-03-10 22:51:53,570 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1  | 2023-03-10 22:51:53,694 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/178b30e1-b74d-4f4d-a142-c930eee71455
recon_1  | 2023-03-10 22:51:53,701 [main] INFO node.SCMNodeManager: Registered Data node : 178b30e1-b74d-4f4d-a142-c930eee71455{ip: 10.9.0.19, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-03-10 22:51:53,717 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/1a6d358d-6662-4447-914c-d709a67ff716
recon_1  | 2023-03-10 22:51:53,718 [main] INFO node.SCMNodeManager: Registered Data node : 1a6d358d-6662-4447-914c-d709a67ff716{ip: 10.9.0.17, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-03-10 22:51:53,718 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3892a4e1-c878-42af-adb7-db66a90d61f4
recon_1  | 2023-03-10 22:51:53,722 [main] INFO node.SCMNodeManager: Registered Data node : 3892a4e1-c878-42af-adb7-db66a90d61f4{ip: 10.9.0.16, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-03-10 22:51:53,723 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/e3e4587c-aa42-4e86-ae9a-d3e448365275
recon_1  | 2023-03-10 22:51:53,725 [main] INFO node.SCMNodeManager: Registered Data node : e3e4587c-aa42-4e86-ae9a-d3e448365275{ip: 10.9.0.15, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-03-10 22:51:53,725 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/e78c5ce1-46ab-4889-a0cd-5903ae46614d
recon_1  | 2023-03-10 22:51:53,732 [main] INFO node.SCMNodeManager: Registered Data node : e78c5ce1-46ab-4889-a0cd-5903ae46614d{ip: 10.9.0.18, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-03-10 22:51:53,732 [main] INFO scm.ReconNodeManager: Loaded 5 nodes from node DB.
recon_1  | 2023-03-10 22:51:53,759 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1  | 2023-03-10 22:51:53,942 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1  | 2023-03-10 22:51:54,066 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1  | 2023-03-10 22:51:54,509 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1  | 2023-03-10 22:51:55,528 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
recon_1  | 2023-03-10 22:51:55,537 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1  | 2023-03-10 22:51:56,180 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1  | 2023-03-10 22:51:56,222 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1  | 2023-03-10 22:51:56,222 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1  | 2023-03-10 22:51:56,977 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1  | 2023-03-10 22:51:56,979 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
recon_1  | 2023-03-10 22:51:57,064 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1  | 2023-03-10 22:51:57,065 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1  | 2023-03-10 22:51:57,071 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 660000ms
recon_1  | 2023-03-10 22:51:57,100 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@55e1192{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1  | 2023-03-10 22:51:57,106 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@35787726{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1  | 2023-03-10 22:52:02,864 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@116efe65{recon,/,file:///data/metadata/webserver/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-17386387494026878254/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
recon_1  | 2023-03-10 22:52:02,969 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@6282b9f5{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1  | 2023-03-10 22:52:02,977 [Listener at 0.0.0.0/9891] INFO server.Server: Started @64407ms
recon_1  | 2023-03-10 22:52:02,991 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn2_1    | 2023-03-10 22:52:20,551 [grpc-default-executor-3] INFO impl.VoteContext: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-FOLLOWER: accept ELECTION from e78c5ce1-46ab-4889-a0cd-5903ae46614d: our priority 0 <= candidate's priority 1
dn2_1    | 2023-03-10 22:52:20,553 [grpc-default-executor-3] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575: changes role from  FOLLOWER to FOLLOWER at term 7 for candidate:e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn2_1    | 2023-03-10 22:52:20,554 [grpc-default-executor-3] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: shutdown 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-FollowerState
dn2_1    | 2023-03-10 22:52:20,554 [grpc-default-executor-3] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: start 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-FollowerState
dn2_1    | 2023-03-10 22:52:20,554 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-FollowerState] INFO impl.FollowerState: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-FollowerState was interrupted
dn2_1    | 2023-03-10 22:52:20,557 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-03-10 22:52:20,557 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-03-10 22:52:20,575 [grpc-default-executor-3] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575 replies to ELECTION vote request: e78c5ce1-46ab-4889-a0cd-5903ae46614d<-3892a4e1-c878-42af-adb7-db66a90d61f4#0:OK-t7. Peer's state: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575:t7, leader=null, voted=e78c5ce1-46ab-4889-a0cd-5903ae46614d, raftlog=Memoized:3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-SegmentedRaftLog:OPENED:c20, conf=12: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:20,617 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO impl.LeaderElection: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn2_1    | 2023-03-10 22:52:20,618 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO impl.LeaderElection:   Response 0: 3892a4e1-c878-42af-adb7-db66a90d61f4<-178b30e1-b74d-4f4d-a142-c930eee71455#0:OK-t12
dn2_1    | 2023-03-10 22:52:20,618 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO impl.LeaderElection: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3 ELECTION round 0: result PASSED
dn2_1    | 2023-03-10 22:52:20,618 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: shutdown 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3
dn2_1    | 2023-03-10 22:52:20,618 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00: changes role from CANDIDATE to LEADER at term 12 for changeToLeader
dn2_1    | 2023-03-10 22:52:20,619 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-5B98FDC36F00 with new leaderId: 3892a4e1-c878-42af-adb7-db66a90d61f4
dn2_1    | 2023-03-10 22:52:20,619 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00: change Leader from null to 3892a4e1-c878-42af-adb7-db66a90d61f4 at term 12 for becomeLeader, leader elected after 32200ms
dn2_1    | 2023-03-10 22:52:20,620 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn2_1    | 2023-03-10 22:52:20,620 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-03-10 22:52:20,620 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn2_1    | 2023-03-10 22:52:20,621 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn2_1    | 2023-03-10 22:52:20,621 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn2_1    | 2023-03-10 22:52:20,621 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn2_1    | 2023-03-10 22:52:20,623 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-03-10 22:52:20,623 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn2_1    | 2023-03-10 22:52:20,767 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn2_1    | 2023-03-10 22:52:20,770 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-03-10 22:52:20,772 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn2_1    | 2023-03-10 22:52:20,796 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn2_1    | 2023-03-10 22:52:20,798 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-03-10 22:52:20,800 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-03-10 22:52:20,801 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-03-10 22:52:20,802 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn2_1    | 2023-03-10 22:52:20,814 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn2_1    | 2023-03-10 22:52:20,818 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-03-10 22:52:20,819 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn1_1    | 2023-03-10 22:52:53,477 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn1_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | 2023-03-10 22:52:53,478 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn1_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | 2023-03-10 22:52:53,478 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn1_1    | 2023-03-10 22:52:53,478 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn1_1    | 2023-03-10 22:52:53,478 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn1_1    | 2023-03-10 22:52:53,478 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 2001 to close, current state is: CLOSING
dn1_1    | 2023-03-10 22:52:53,478 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn1_1    | 2023-03-10 22:52:53,479 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn1_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn2_1    | 2023-03-10 22:52:20,820 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn2_1    | 2023-03-10 22:52:20,822 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-03-10 22:52:20,823 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-03-10 22:52:20,824 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-03-10 22:52:20,826 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn2_1    | 2023-03-10 22:52:20,839 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: start 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderStateImpl
dn2_1    | 2023-03-10 22:52:20,846 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-SegmentedRaftLogWorker: Rolling segment log-11_16 to index:16
dn2_1    | 2023-03-10 22:52:20,856 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/current/log_inprogress_11 to /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/current/log_11-16
dn2_1    | 2023-03-10 22:52:20,877 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/current/log_inprogress_17
dn2_1    | 2023-03-10 22:52:20,909 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderElection3] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00: set configuration 17: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:21,415 [grpc-default-executor-0] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00: receive requestVote(PRE_VOTE, 1a6d358d-6662-4447-914c-d709a67ff716, group-5B98FDC36F00, 11, (t:11, i:16))
dn2_1    | 2023-03-10 22:52:21,416 [grpc-default-executor-0] INFO impl.VoteContext: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LEADER: reject PRE_VOTE from 1a6d358d-6662-4447-914c-d709a67ff716: this server is the leader and still has leadership
dn2_1    | 2023-03-10 22:52:21,441 [grpc-default-executor-0] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00 replies to PRE_VOTE vote request: 1a6d358d-6662-4447-914c-d709a67ff716<-3892a4e1-c878-42af-adb7-db66a90d61f4#0:FAIL-t12. Peer's state: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00:t12, leader=3892a4e1-c878-42af-adb7-db66a90d61f4, voted=3892a4e1-c878-42af-adb7-db66a90d61f4, raftlog=Memoized:3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-SegmentedRaftLog:OPENED:c16, conf=17: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:21,554 [3892a4e1-c878-42af-adb7-db66a90d61f4-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-CBC13A60F575 with new leaderId: e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn2_1    | 2023-03-10 22:52:21,577 [3892a4e1-c878-42af-adb7-db66a90d61f4-server-thread1] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575: change Leader from null to e78c5ce1-46ab-4889-a0cd-5903ae46614d at term 7 for appendEntries, leader elected after 31970ms
dn2_1    | 2023-03-10 22:52:21,579 [3892a4e1-c878-42af-adb7-db66a90d61f4-server-thread1] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575: set configuration 21: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:52:21,580 [3892a4e1-c878-42af-adb7-db66a90d61f4-server-thread1] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-SegmentedRaftLogWorker: Rolling segment log-12_20 to index:20
dn2_1    | 2023-03-10 22:52:21,631 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/current/log_inprogress_12 to /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/current/log_12-20
dn2_1    | 2023-03-10 22:52:21,673 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/current/log_inprogress_21
dn2_1    | 2023-03-10 22:52:31,520 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-3892a4e1-c878-42af-adb7-db66a90d61f4: Detected pause in JVM or host machine (eg GC): pause of approximately 224967488ns. No GCs detected.
dn2_1    | 2023-03-10 22:52:52,425 [ContainerOp-54b523a6-4d1d-4f07-a057-5b98fdc36f00-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 15.
dn2_1    | 2023-03-10 22:52:52,425 [ContainerOp-54b523a6-4d1d-4f07-a057-5b98fdc36f00-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 15.
dn2_1    | 2023-03-10 22:52:52,493 [ContainerOp-54b523a6-4d1d-4f07-a057-5b98fdc36f00-0] INFO keyvalue.KeyValueContainer: Container 1 is closed with bcsId 15.
dn2_1    | 2023-03-10 22:52:53,087 [ContainerOp-78b03a88-75c1-4060-9962-cbc13a60f575-0] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 10.
dn2_1    | 2023-03-10 22:52:53,088 [ContainerOp-78b03a88-75c1-4060-9962-cbc13a60f575-0] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 10.
dn2_1    | 2023-03-10 22:52:53,346 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | 2023-03-10 22:52:53,348 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn2_1    | 2023-03-10 22:52:53,348 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
om3_1    | 2023-03-10 22:52:24,907 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 102
om3_1    | 2023-03-10 22:52:24,907 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 66
om3_1    | 2023-03-10 22:52:25,687 [om3-impl-thread1] INFO raftlog.RaftLog: om3@group-D66704EFC61C-SegmentedRaftLog: commitIndex: updateToMax old=102, new=101, updated? false
om3_1    | 2023-03-10 22:52:25,687 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: start as a follower, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-03-10 22:52:25,687 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from      null to FOLLOWER at term 5 for startAsFollower
om3_1    | 2023-03-10 22:52:25,697 [om3-impl-thread1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
om3_1    | 2023-03-10 22:52:25,710 [om3-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om3
om3_1    | 2023-03-10 22:52:25,713 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om3_1    | 2023-03-10 22:52:25,713 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om3_1    | 2023-03-10 22:52:25,714 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om3_1    | 2023-03-10 22:52:25,714 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-03-10 22:52:25,714 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-03-10 22:52:25,715 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om3_1    | 2023-03-10 22:52:25,735 [Listener at om3/9862] INFO server.RaftServer: om3: start RPC server
om3_1    | 2023-03-10 22:52:26,213 [Listener at om3/9862] INFO server.GrpcService: om3: GrpcService started, listening on 9872
om3_1    | 2023-03-10 22:52:26,243 [Listener at om3/9862] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
om3_1    | 2023-03-10 22:52:26,255 [Listener at om3/9862] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
om3_1    | 2023-03-10 22:52:26,255 [Listener at om3/9862] INFO om.OzoneManager: Version File has different layout version (0) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om3_1    | 2023-03-10 22:52:26,260 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om3: Started
om3_1    | 2023-03-10 22:52:26,629 [Listener at om3/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om3_1    | 2023-03-10 22:52:26,630 [Listener at om3/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om3_1    | 2023-03-10 22:52:26,754 [Listener at om3/9862] INFO util.log: Logging initialized @87624ms to org.eclipse.jetty.util.log.Slf4jLog
om3_1    | 2023-03-10 22:52:27,490 [Listener at om3/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om3_1    | 2023-03-10 22:52:27,512 [Listener at om3/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om3_1    | 2023-03-10 22:52:27,546 [Listener at om3/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om3_1    | 2023-03-10 22:52:27,568 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om3_1    | 2023-03-10 22:52:27,568 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om3_1    | 2023-03-10 22:52:27,574 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om3_1    | 2023-03-10 22:52:27,862 [Listener at om3/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om3_1    | 2023-03-10 22:52:27,872 [Listener at om3/9862] INFO http.HttpServer2: Jetty bound to port 9874
om3_1    | 2023-03-10 22:52:27,881 [Listener at om3/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om3_1    | 2023-03-10 22:52:28,081 [Listener at om3/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om3_1    | 2023-03-10 22:52:28,081 [Listener at om3/9862] INFO server.session: No SessionScavenger set, using defaults
om3_1    | 2023-03-10 22:52:28,085 [Listener at om3/9862] INFO server.session: node0 Scavenging every 660000ms
om3_1    | 2023-03-10 22:52:28,153 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2296127{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om3_1    | 2023-03-10 22:52:28,156 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7cb29ea8{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om3_1    | 2023-03-10 22:52:28,639 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7cca31fc{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-14570930022660062729/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om3_1    | 2023-03-10 22:52:28,666 [Listener at om3/9862] INFO server.AbstractConnector: Started ServerConnector@67b560fe{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om3_1    | 2023-03-10 22:52:28,668 [Listener at om3/9862] INFO server.Server: Started @89538ms
om3_1    | 2023-03-10 22:52:28,677 [Listener at om3/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om3_1    | 2023-03-10 22:52:28,678 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om3_1    | 2023-03-10 22:52:28,682 [Listener at om3/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om3_1    | 2023-03-10 22:52:28,689 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om3_1    | 2023-03-10 22:52:28,712 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om3_1    | 2023-03-10 22:52:28,853 [Listener at om3/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om3_1    | 2023-03-10 22:52:28,887 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@18e6b72b] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om3_1    | 2023-03-10 22:52:30,863 [om3@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om3@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5166155635ns, electionTimeout:5143ms
om3_1    | 2023-03-10 22:52:30,864 [om3@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-FollowerState
dn4_1    | 2023-03-10 22:52:16,789 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn4_1    | 2023-03-10 22:52:16,791 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn4_1    | 2023-03-10 22:52:16,942 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-03-10 22:52:17,015 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn4_1    | 2023-03-10 22:52:17,141 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: start e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderStateImpl
dn4_1    | 2023-03-10 22:52:17,309 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn4_1    | 2023-03-10 22:52:17,388 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/8581ae3d-c50a-4a49-9d70-d98e5419b7a9/current/log_inprogress_3 to /data/metadata/ratis/8581ae3d-c50a-4a49-9d70-d98e5419b7a9/current/log_3-4
dn4_1    | 2023-03-10 22:52:17,569 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/8581ae3d-c50a-4a49-9d70-d98e5419b7a9/current/log_inprogress_5
dn4_1    | 2023-03-10 22:52:17,625 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderElection3] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9: set configuration 5: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:19,596 [grpc-default-executor-0] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575: receive requestVote(PRE_VOTE, 3892a4e1-c878-42af-adb7-db66a90d61f4, group-CBC13A60F575, 6, (t:6, i:20))
dn4_1    | 2023-03-10 22:52:19,606 [grpc-default-executor-0] INFO impl.VoteContext: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-CANDIDATE: reject PRE_VOTE from 3892a4e1-c878-42af-adb7-db66a90d61f4: our priority 1 > candidate's priority 0
dn4_1    | 2023-03-10 22:52:19,620 [grpc-default-executor-1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3: receive requestVote(PRE_VOTE, e3e4587c-aa42-4e86-ae9a-d3e448365275, group-0049FBBC23B3, 10, (t:10, i:35))
dn4_1    | 2023-03-10 22:52:19,627 [grpc-default-executor-2] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575: receive requestVote(PRE_VOTE, e3e4587c-aa42-4e86-ae9a-d3e448365275, group-CBC13A60F575, 6, (t:6, i:20))
dn4_1    | 2023-03-10 22:52:19,621 [grpc-default-executor-1] INFO impl.VoteContext: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-CANDIDATE: accept PRE_VOTE from e3e4587c-aa42-4e86-ae9a-d3e448365275: our priority 0 <= candidate's priority 1
dn4_1    | 2023-03-10 22:52:19,647 [grpc-default-executor-1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3 replies to PRE_VOTE vote request: e3e4587c-aa42-4e86-ae9a-d3e448365275<-e78c5ce1-46ab-4889-a0cd-5903ae46614d#0:OK-t10. Peer's state: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3:t10, leader=null, voted=e3e4587c-aa42-4e86-ae9a-d3e448365275, raftlog=Memoized:e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-SegmentedRaftLog:OPENED:c35, conf=22: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:19,660 [grpc-default-executor-0] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575 replies to PRE_VOTE vote request: 3892a4e1-c878-42af-adb7-db66a90d61f4<-e78c5ce1-46ab-4889-a0cd-5903ae46614d#0:FAIL-t6. Peer's state: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575:t6, leader=null, voted=e78c5ce1-46ab-4889-a0cd-5903ae46614d, raftlog=Memoized:e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-SegmentedRaftLog:OPENED:c20, conf=12: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:19,662 [grpc-default-executor-2] INFO impl.VoteContext: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-CANDIDATE: reject PRE_VOTE from e3e4587c-aa42-4e86-ae9a-d3e448365275: our priority 1 > candidate's priority 0
dn4_1    | 2023-03-10 22:52:19,679 [grpc-default-executor-2] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575 replies to PRE_VOTE vote request: e3e4587c-aa42-4e86-ae9a-d3e448365275<-e78c5ce1-46ab-4889-a0cd-5903ae46614d#0:FAIL-t6. Peer's state: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575:t6, leader=null, voted=e78c5ce1-46ab-4889-a0cd-5903ae46614d, raftlog=Memoized:e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-SegmentedRaftLog:OPENED:c20, conf=12: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:19,891 [grpc-default-executor-3] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3: receive requestVote(PRE_VOTE, e3e4587c-aa42-4e86-ae9a-d3e448365275, group-0049FBBC23B3, 10, (t:10, i:35))
dn4_1    | 2023-03-10 22:52:19,891 [grpc-default-executor-3] INFO impl.VoteContext: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-CANDIDATE: accept PRE_VOTE from e3e4587c-aa42-4e86-ae9a-d3e448365275: our priority 0 <= candidate's priority 1
recon_1  | 2023-03-10 22:52:02,991 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1  | 2023-03-10 22:52:03,003 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1  | 2023-03-10 22:52:03,003 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1  | 2023-03-10 22:52:03,125 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1  | 2023-03-10 22:52:03,125 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-03-10 22:52:03,137 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Last known snapshot for OM : /data/metadata/om.snapshot.db_1678488594284
recon_1  | 2023-03-10 22:52:03,191 [Listener at 0.0.0.0/9891] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-03-10 22:52:03,199 [Listener at 0.0.0.0/9891] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-03-10 22:52:03,516 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1678488594284.
recon_1  | 2023-03-10 22:52:03,916 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1  | 2023-03-10 22:52:03,931 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1  | 2023-03-10 22:52:05,981 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 8 pipelines from SCM.
recon_1  | 2023-03-10 22:52:05,982 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 8 pipelines in house.
recon_1  | 2023-03-10 22:52:06,263 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1  | 2023-03-10 22:52:06,356 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1  | 2023-03-10 22:52:06,511 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1  | 2023-03-10 22:52:06,601 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1  | 2023-03-10 22:52:09,621 [IPC Server handler 26 on default port 9891] WARN ipc.Server: IPC Server handler 26 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.16:36784: output error
recon_1  | 2023-03-10 22:52:09,773 [IPC Server handler 31 on default port 9891] WARN ipc.Server: IPC Server handler 31 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.16:36790: output error
recon_1  | 2023-03-10 22:52:09,775 [IPC Server handler 26 on default port 9891] INFO ipc.Server: IPC Server handler 26 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
dn4_1    | 2023-03-10 22:52:19,895 [grpc-default-executor-3] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3 replies to PRE_VOTE vote request: e3e4587c-aa42-4e86-ae9a-d3e448365275<-e78c5ce1-46ab-4889-a0cd-5903ae46614d#0:OK-t10. Peer's state: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3:t10, leader=null, voted=e3e4587c-aa42-4e86-ae9a-d3e448365275, raftlog=Memoized:e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-SegmentedRaftLog:OPENED:c35, conf=22: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:19,906 [grpc-default-executor-3] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575: receive requestVote(PRE_VOTE, e3e4587c-aa42-4e86-ae9a-d3e448365275, group-CBC13A60F575, 6, (t:6, i:20))
dn4_1    | 2023-03-10 22:52:19,906 [grpc-default-executor-3] INFO impl.VoteContext: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-CANDIDATE: reject PRE_VOTE from e3e4587c-aa42-4e86-ae9a-d3e448365275: our priority 1 > candidate's priority 0
dn4_1    | 2023-03-10 22:52:19,906 [grpc-default-executor-3] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575 replies to PRE_VOTE vote request: e3e4587c-aa42-4e86-ae9a-d3e448365275<-e78c5ce1-46ab-4889-a0cd-5903ae46614d#0:FAIL-t6. Peer's state: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575:t6, leader=null, voted=e78c5ce1-46ab-4889-a0cd-5903ae46614d, raftlog=Memoized:e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-SegmentedRaftLog:OPENED:c20, conf=12: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:20,401 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO impl.LeaderElection: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn4_1    | 2023-03-10 22:52:20,403 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO impl.LeaderElection:   Response 0: e78c5ce1-46ab-4889-a0cd-5903ae46614d<-e3e4587c-aa42-4e86-ae9a-d3e448365275#0:OK-t6
dn4_1    | 2023-03-10 22:52:20,404 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO impl.LeaderElection: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2 PRE_VOTE round 0: result PASSED
dn4_1    | 2023-03-10 22:52:20,413 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO impl.LeaderElection: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2 ELECTION round 0: submit vote requests at term 7 for 12: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:20,416 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-LeaderElection1] INFO impl.LeaderElection: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-LeaderElection1: PRE_VOTE REJECTED received 1 response(s) and 0 exception(s):
dn4_1    | 2023-03-10 22:52:20,463 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-LeaderElection1] INFO impl.LeaderElection:   Response 0: e78c5ce1-46ab-4889-a0cd-5903ae46614d<-e3e4587c-aa42-4e86-ae9a-d3e448365275#0:FAIL-t10
dn4_1    | 2023-03-10 22:52:20,463 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-LeaderElection1] INFO impl.LeaderElection: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-LeaderElection1 PRE_VOTE round 0: result REJECTED
dn4_1    | 2023-03-10 22:52:20,463 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-10 22:52:20,478 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-10 22:52:20,478 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-LeaderElection1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3: changes role from CANDIDATE to FOLLOWER at term 10 for REJECTED
dn4_1    | 2023-03-10 22:52:20,480 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-LeaderElection1] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: shutdown e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-LeaderElection1
dn4_1    | 2023-03-10 22:52:20,480 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-LeaderElection1] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: start e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-FollowerState
dn4_1    | 2023-03-10 22:52:20,527 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-10 22:52:20,528 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-10 22:52:20,547 [grpc-default-executor-3] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3: receive requestVote(ELECTION, e3e4587c-aa42-4e86-ae9a-d3e448365275, group-0049FBBC23B3, 11, (t:10, i:35))
dn4_1    | 2023-03-10 22:52:20,549 [grpc-default-executor-3] INFO impl.VoteContext: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-FOLLOWER: accept ELECTION from e3e4587c-aa42-4e86-ae9a-d3e448365275: our priority 0 <= candidate's priority 1
dn4_1    | 2023-03-10 22:52:20,550 [grpc-default-executor-3] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3: changes role from  FOLLOWER to FOLLOWER at term 11 for candidate:e3e4587c-aa42-4e86-ae9a-d3e448365275
dn4_1    | 2023-03-10 22:52:20,551 [grpc-default-executor-3] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: shutdown e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-FollowerState
dn4_1    | 2023-03-10 22:52:20,551 [grpc-default-executor-3] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: start e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-FollowerState
dn4_1    | 2023-03-10 22:52:20,567 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-FollowerState] INFO impl.FollowerState: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-FollowerState was interrupted
dn4_1    | 2023-03-10 22:52:20,598 [grpc-default-executor-3] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3 replies to ELECTION vote request: e3e4587c-aa42-4e86-ae9a-d3e448365275<-e78c5ce1-46ab-4889-a0cd-5903ae46614d#0:OK-t11. Peer's state: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3:t11, leader=null, voted=e3e4587c-aa42-4e86-ae9a-d3e448365275, raftlog=Memoized:e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-SegmentedRaftLog:OPENED:c35, conf=22: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:53:53,836 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-03-10 22:53:53,836 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-03-10 22:53:53,836 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-03-10 22:53:53,837 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-03-10 22:53:53,838 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-03-10 22:53:53,838 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-03-10 22:53:53,838 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-03-10 22:53:53,839 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-03-10 22:53:53,839 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-03-10 22:53:53,838 [Command processor thread] INFO server.RaftServer: 178b30e1-b74d-4f4d-a142-c930eee71455: addNew group-6265FBDC9D88:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:0|startupRole:FOLLOWER] returns group-6265FBDC9D88:java.util.concurrent.CompletableFuture@55fe5704[Not completed]
dn5_1    | 2023-03-10 22:53:53,841 [pool-26-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/a8f227f9-9826-491d-a836-6265fbdc9d88 does not exist. Creating ...
dn5_1    | 2023-03-10 22:53:53,846 [pool-26-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a8f227f9-9826-491d-a836-6265fbdc9d88/in_use.lock acquired by nodename 7@82dcecb82990
dn5_1    | 2023-03-10 22:53:53,851 [pool-26-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/a8f227f9-9826-491d-a836-6265fbdc9d88 has been successfully formatted.
dn5_1    | 2023-03-10 22:53:53,852 [pool-26-thread-1] INFO ratis.ContainerStateMachine: group-6265FBDC9D88: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn5_1    | 2023-03-10 22:53:53,852 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-03-10 22:53:53,852 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-03-10 22:53:53,853 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-10 22:53:53,853 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-03-10 22:53:53,853 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-03-10 22:53:53,853 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-03-10 22:53:53,854 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-03-10 22:53:53,854 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-03-10 22:53:53,854 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: new 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a8f227f9-9826-491d-a836-6265fbdc9d88
dn5_1    | 2023-03-10 22:53:53,854 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-03-10 22:53:53,854 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-03-10 22:53:53,855 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-03-10 22:53:53,855 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-03-10 22:53:53,855 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-03-10 22:53:53,855 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-03-10 22:53:53,855 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-03-10 22:53:53,856 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-03-10 22:53:53,858 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-03-10 22:53:53,868 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-10 22:53:53,907 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-03-10 22:53:53,908 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-03-10 22:53:53,908 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-03-10 22:53:53,909 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-03-10 22:53:53,909 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-03-10 22:53:53,910 [pool-26-thread-1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88: start as a follower, conf=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:53:53,911 [pool-26-thread-1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn5_1    | 2023-03-10 22:53:53,911 [pool-26-thread-1] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: start 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-FollowerState
dn5_1    | 2023-03-10 22:53:53,917 [pool-26-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6265FBDC9D88,id=178b30e1-b74d-4f4d-a142-c930eee71455
dn5_1    | 2023-03-10 22:53:53,917 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-03-10 22:53:53,917 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | 2023-03-10 22:53:53,917 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-03-10 22:53:53,918 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-03-10 22:53:53,919 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=a8f227f9-9826-491d-a836-6265fbdc9d88
dn5_1    | 2023-03-10 22:53:53,933 [178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-10 22:53:53,933 [178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-10 22:53:54,323 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=a8f227f9-9826-491d-a836-6265fbdc9d88.
dn5_1    | 2023-03-10 22:53:54,323 [Command processor thread] INFO server.RaftServer: 178b30e1-b74d-4f4d-a142-c930eee71455: addNew group-CD960C190469:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] returns group-CD960C190469:java.util.concurrent.CompletableFuture@3e9a1213[Not completed]
dn5_1    | 2023-03-10 22:53:54,325 [pool-26-thread-1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455: new RaftServerImpl for group-CD960C190469:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn5_1    | 2023-03-10 22:53:54,326 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-03-10 22:53:54,326 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-03-10 22:53:54,327 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-03-10 22:53:54,328 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-03-10 22:53:54,328 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-03-10 22:53:54,328 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-03-10 22:53:54,328 [pool-26-thread-1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469: ConfigurationManager, init=-1: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-03-10 22:53:54,329 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-03-10 22:53:54,330 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-03-10 22:53:54,331 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-03-10 22:53:54,331 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-03-10 22:53:54,332 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-03-10 22:53:54,333 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-03-10 22:53:54,337 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-03-10 22:53:54,338 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-03-10 22:53:54,339 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-03-10 22:53:54,339 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-03-10 22:53:54,341 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-03-10 22:53:54,341 [pool-26-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/47893e13-e605-460c-a18a-cd960c190469 does not exist. Creating ...
dn5_1    | 2023-03-10 22:53:54,345 [pool-26-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/47893e13-e605-460c-a18a-cd960c190469/in_use.lock acquired by nodename 7@82dcecb82990
dn5_1    | 2023-03-10 22:53:54,350 [pool-26-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/47893e13-e605-460c-a18a-cd960c190469 has been successfully formatted.
dn5_1    | 2023-03-10 22:53:54,351 [pool-26-thread-1] INFO ratis.ContainerStateMachine: group-CD960C190469: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn5_1    | 2023-03-10 22:53:54,370 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-03-10 22:53:54,379 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-03-10 22:53:54,379 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-10 22:53:54,380 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-03-10 22:53:54,380 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-03-10 22:53:54,381 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-03-10 22:53:54,387 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-03-10 22:53:54,388 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-03-10 22:53:54,388 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: new 178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/47893e13-e605-460c-a18a-cd960c190469
dn5_1    | 2023-03-10 22:53:54,388 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-03-10 22:53:54,388 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-03-10 22:53:54,388 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-03-10 22:53:54,389 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-03-10 22:53:54,389 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-03-10 22:53:54,389 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-03-10 22:53:54,389 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1    | 2023-03-10 22:52:02,713 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1    | 2023-03-10 22:52:02,732 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1    | 2023-03-10 22:52:02,747 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1    | 2023-03-10 22:52:02,750 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1    | 2023-03-10 22:52:03,483 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm_1    | 2023-03-10 22:52:03,531 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm_1    | 2023-03-10 22:52:03,552 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm_1    | 2023-03-10 22:52:04,296 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1    | 2023-03-10 22:52:04,322 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm_1    | 2023-03-10 22:52:04,340 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
scm_1    | 2023-03-10 22:52:04,866 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@423ed3b5{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1    | 2023-03-10 22:52:04,897 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@73893ec1{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm_1    | 2023-03-10 22:52:07,920 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@8641b7d{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-2811931035213802138/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm_1    | 2023-03-10 22:52:08,295 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@426131d7{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1    | 2023-03-10 22:52:08,313 [Listener at 0.0.0.0/9860] INFO server.Server: Started @69123ms
scm_1    | 2023-03-10 22:52:08,384 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1    | 2023-03-10 22:52:08,384 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1    | 2023-03-10 22:52:08,424 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1    | 2023-03-10 22:52:10,202 [IPC Server handler 77 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/178b30e1-b74d-4f4d-a142-c930eee71455
scm_1    | 2023-03-10 22:52:10,349 [IPC Server handler 77 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 178b30e1-b74d-4f4d-a142-c930eee71455{ip: 10.9.0.19, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm_1    | 2023-03-10 22:52:10,495 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1    | 2023-03-10 22:52:10,675 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 1, required at least one datanode reported per pipeline count is 3
scm_1    | 2023-03-10 22:52:10,631 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1    | 2023-03-10 22:52:10,631 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1    | 2023-03-10 22:52:10,716 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-03-10 22:52:10,890 [IPC Server handler 81 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/e3e4587c-aa42-4e86-ae9a-d3e448365275
scm_1    | 2023-03-10 22:52:10,913 [IPC Server handler 81 on default port 9861] INFO node.SCMNodeManager: Registered Data node : e3e4587c-aa42-4e86-ae9a-d3e448365275{ip: 10.9.0.15, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm_1    | 2023-03-10 22:52:10,939 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1    | 2023-03-10 22:52:10,939 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1    | 2023-03-10 22:52:10,939 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 3, required at least one datanode reported per pipeline count is 3
scm_1    | 2023-03-10 22:52:10,944 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-03-10 22:52:11,010 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1    | 2023-03-10 22:52:11,391 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-03-10 22:52:11,774 [IPC Server handler 81 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3892a4e1-c878-42af-adb7-db66a90d61f4
scm_1    | 2023-03-10 22:52:11,781 [IPC Server handler 81 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3892a4e1-c878-42af-adb7-db66a90d61f4{ip: 10.9.0.16, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm_1    | 2023-03-10 22:52:11,782 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1    | 2023-03-10 22:52:11,815 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1    | 2023-03-10 22:52:11,833 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1    | 2023-03-10 22:52:11,833 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1    | 2023-03-10 22:52:11,852 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
dn4_1    | 2023-03-10 22:52:20,605 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO impl.LeaderElection: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn4_1    | 2023-03-10 22:52:20,607 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO impl.LeaderElection:   Response 0: e78c5ce1-46ab-4889-a0cd-5903ae46614d<-3892a4e1-c878-42af-adb7-db66a90d61f4#0:OK-t7
dn4_1    | 2023-03-10 22:52:20,608 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO impl.LeaderElection: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2 ELECTION round 0: result PASSED
dn4_1    | 2023-03-10 22:52:20,608 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: shutdown e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2
dn4_1    | 2023-03-10 22:52:20,610 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575: changes role from CANDIDATE to LEADER at term 7 for changeToLeader
dn4_1    | 2023-03-10 22:52:20,611 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-CBC13A60F575 with new leaderId: e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn4_1    | 2023-03-10 22:52:20,615 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575: change Leader from null to e78c5ce1-46ab-4889-a0cd-5903ae46614d at term 7 for becomeLeader, leader elected after 34636ms
dn4_1    | 2023-03-10 22:52:20,615 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn4_1    | 2023-03-10 22:52:20,624 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-03-10 22:52:20,624 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn4_1    | 2023-03-10 22:52:20,628 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn4_1    | 2023-03-10 22:52:20,630 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn4_1    | 2023-03-10 22:52:20,630 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn4_1    | 2023-03-10 22:52:20,630 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-03-10 22:52:20,631 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn4_1    | 2023-03-10 22:52:20,664 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-10 22:52:20,694 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-10 22:52:20,725 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn4_1    | 2023-03-10 22:52:20,726 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-10 22:52:20,727 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn4_1    | 2023-03-10 22:52:20,757 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn4_1    | 2023-03-10 22:52:20,758 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-03-10 22:52:20,760 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-03-10 22:52:20,761 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-03-10 22:52:20,761 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn4_1    | 2023-03-10 22:52:20,780 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn4_1    | 2023-03-10 22:52:20,786 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-10 22:52:20,786 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn4_1    | 2023-03-10 22:52:20,787 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn4_1    | 2023-03-10 22:52:20,791 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-03-10 22:52:20,793 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-03-10 22:52:20,793 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-03-10 22:52:20,793 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn4_1    | 2023-03-10 22:52:20,806 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: start e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderStateImpl
dn4_1    | 2023-03-10 22:52:20,813 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-SegmentedRaftLogWorker: Rolling segment log-12_20 to index:20
dn4_1    | 2023-03-10 22:52:20,822 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/current/log_inprogress_12 to /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/current/log_12-20
dn3_1    | 2023-03-10 22:52:10,873 [1a6d358d-6662-4447-914c-d709a67ff716-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-03-10 22:52:10,891 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-03-10 22:52:10,897 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-10 22:52:10,974 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 1a6d358d-6662-4447-914c-d709a67ff716: start RPC server
dn3_1    | 2023-03-10 22:52:11,015 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-03-10 22:52:11,015 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-10 22:52:11,069 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 1a6d358d-6662-4447-914c-d709a67ff716: GrpcService started, listening on 9858
dn3_1    | 2023-03-10 22:52:11,123 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 1a6d358d-6662-4447-914c-d709a67ff716: GrpcService started, listening on 9856
dn3_1    | 2023-03-10 22:52:11,138 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 1a6d358d-6662-4447-914c-d709a67ff716: GrpcService started, listening on 9857
dn3_1    | 2023-03-10 22:52:11,162 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1a6d358d-6662-4447-914c-d709a67ff716 is started using port 9858 for RATIS
dn3_1    | 2023-03-10 22:52:11,164 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1a6d358d-6662-4447-914c-d709a67ff716 is started using port 9857 for RATIS_ADMIN
dn3_1    | 2023-03-10 22:52:11,164 [EndpointStateMachine task thread for scm/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1a6d358d-6662-4447-914c-d709a67ff716 is started using port 9856 for RATIS_SERVER
dn3_1    | 2023-03-10 22:52:11,168 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-1a6d358d-6662-4447-914c-d709a67ff716: Started
dn3_1    | 2023-03-10 22:52:11,293 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-03-10 22:52:15,934 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-FollowerState] INFO impl.FollowerState: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5199323496ns, electionTimeout:5036ms
dn3_1    | 2023-03-10 22:52:15,935 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-FollowerState] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: shutdown 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-FollowerState
dn2_1    | 2023-03-10 22:52:53,352 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 2001 to close, current state is: CLOSING
dn2_1    | 2023-03-10 22:52:53,352 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn2_1    | 2023-03-10 22:52:53,354 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn2_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 2023-03-10 22:52:53,355 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn5_1    | 2023-03-10 22:53:54,390 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-03-10 22:52:20,839 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/current/log_inprogress_21
dn4_1    | 2023-03-10 22:52:20,860 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderElection2] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575: set configuration 21: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:21,132 [grpc-default-executor-2] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3: receive requestVote(PRE_VOTE, 1a6d358d-6662-4447-914c-d709a67ff716, group-0049FBBC23B3, 10, (t:10, i:35))
dn4_1    | 2023-03-10 22:52:21,133 [grpc-default-executor-2] INFO impl.VoteContext: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-FOLLOWER: accept PRE_VOTE from 1a6d358d-6662-4447-914c-d709a67ff716: our priority 0 <= candidate's priority 0
dn4_1    | 2023-03-10 22:52:21,134 [grpc-default-executor-2] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3 replies to PRE_VOTE vote request: 1a6d358d-6662-4447-914c-d709a67ff716<-e78c5ce1-46ab-4889-a0cd-5903ae46614d#0:OK-t11. Peer's state: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3:t11, leader=null, voted=e3e4587c-aa42-4e86-ae9a-d3e448365275, raftlog=Memoized:e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-SegmentedRaftLog:OPENED:c35, conf=22: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:21,838 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-0049FBBC23B3 with new leaderId: e3e4587c-aa42-4e86-ae9a-d3e448365275
dn4_1    | 2023-03-10 22:52:21,876 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-server-thread1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3: change Leader from null to e3e4587c-aa42-4e86-ae9a-d3e448365275 at term 11 for appendEntries, leader elected after 36069ms
dn4_1    | 2023-03-10 22:52:21,882 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-server-thread2] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3: set configuration 36: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:52:22,039 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-server-thread2] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-SegmentedRaftLogWorker: Rolling segment log-22_35 to index:35
dn4_1    | 2023-03-10 22:52:22,181 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/current/log_inprogress_22 to /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/current/log_22-35
dn4_1    | 2023-03-10 22:52:22,199 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/current/log_inprogress_36
dn4_1    | 2023-03-10 22:52:53,139 [ContainerOp-cf0cdca9-af45-4cc0-9366-0049fbbc23b3-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 33.
dn4_1    | 2023-03-10 22:52:53,140 [ContainerOp-cf0cdca9-af45-4cc0-9366-0049fbbc23b3-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 33.
dn4_1    | 2023-03-10 22:52:53,199 [ContainerOp-78b03a88-75c1-4060-9962-cbc13a60f575-0] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 10.
dn4_1    | 2023-03-10 22:52:53,211 [ContainerOp-78b03a88-75c1-4060-9962-cbc13a60f575-0] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 10.
dn4_1    | 2023-03-10 22:52:53,247 [ContainerOp-cf0cdca9-af45-4cc0-9366-0049fbbc23b3-0] INFO keyvalue.KeyValueContainer: Container 2 is closed with bcsId 33.
dn4_1    | 2023-03-10 22:52:53,312 [ContainerOp-78b03a88-75c1-4060-9962-cbc13a60f575-0] INFO keyvalue.KeyValueContainer: Container 1002 is closed with bcsId 10.
dn4_1    | 2023-03-10 22:52:53,554 [ContainerOp-cf0cdca9-af45-4cc0-9366-0049fbbc23b3-1] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 20.
dn4_1    | 2023-03-10 22:52:53,554 [ContainerOp-cf0cdca9-af45-4cc0-9366-0049fbbc23b3-1] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 20.
dn4_1    | 2023-03-10 22:52:53,614 [ContainerOp-cf0cdca9-af45-4cc0-9366-0049fbbc23b3-1] INFO keyvalue.KeyValueContainer: Container 1001 is closed with bcsId 20.
dn4_1    | 2023-03-10 22:52:53,790 [ContainerOp-78b03a88-75c1-4060-9962-cbc13a60f575-1] INFO keyvalue.KeyValueContainer: Container 2001 is synced with bcsId 19.
dn4_1    | 2023-03-10 22:52:53,791 [ContainerOp-78b03a88-75c1-4060-9962-cbc13a60f575-1] INFO keyvalue.KeyValueContainer: Container 2001 is synced with bcsId 19.
dn4_1    | 2023-03-10 22:52:53,807 [ContainerOp-78b03a88-75c1-4060-9962-cbc13a60f575-1] INFO keyvalue.KeyValueContainer: Container 2001 is closed with bcsId 19.
dn4_1    | 2023-03-10 22:52:53,819 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-03-10 22:52:53,822 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn4_1    | 2023-03-10 22:52:53,823 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn4_1    | 2023-03-10 22:52:53,824 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: ERASURE_CODED_STORAGE_SUPPORT.
dn4_1    | 2023-03-10 22:52:53,833 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature ERASURE_CODED_STORAGE_SUPPORT has been finalized.
dn4_1    | 2023-03-10 22:52:53,833 [Command processor thread] INFO upgrade.UpgradeFinalizer: Running finalization actions for layout feature: DATANODE_SCHEMA_V3
dn4_1    | 2023-03-10 22:52:53,835 [Command processor thread] INFO upgrade.DatanodeSchemaV3FinalizeAction: Upgrading Datanode volume layout for Schema V3 support.
dn4_1    | 2023-03-10 22:52:53,940 [Command processor thread] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-efc75ae1-5bd9-4645-9a43-bc7861eb0b3e/container.db to cache
dn4_1    | 2023-03-10 22:52:53,941 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-efc75ae1-5bd9-4645-9a43-bc7861eb0b3e/container.db for volume DS-efc75ae1-5bd9-4645-9a43-bc7861eb0b3e
om3_1    | 2023-03-10 22:52:30,864 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 5 for changeToCandidate
om3_1    | 2023-03-10 22:52:30,882 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om3_1    | 2023-03-10 22:52:30,882 [om3@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-LeaderElection1
om3_1    | 2023-03-10 22:52:30,887 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 5 for 67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-03-10 22:52:31,041 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-03-10 22:52:31,072 [om3@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
om3_1    | 2023-03-10 22:52:31,072 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-03-10 22:52:31,085 [om3@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
om3_1    | 2023-03-10 22:52:32,785 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(PRE_VOTE, om2, group-D66704EFC61C, 5, (t:5, i:102))
om3_1    | 2023-03-10 22:52:32,789 [grpc-default-executor-0] INFO impl.VoteContext: om3@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om2: our priority 0 <= candidate's priority 0
om3_1    | 2023-03-10 22:52:32,797 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to PRE_VOTE vote request: om2<-om3#0:OK-t5. Peer's state: om3@group-D66704EFC61C:t5, leader=null, voted=om1, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c102, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-03-10 22:52:32,799 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(PRE_VOTE, om1, group-D66704EFC61C, 5, (t:5, i:102))
om3_1    | 2023-03-10 22:52:32,803 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1: PRE_VOTE DISCOVERED_A_NEW_TERM (term=6) received 1 response(s) and 0 exception(s):
om3_1    | 2023-03-10 22:52:32,800 [grpc-default-executor-1] INFO impl.VoteContext: om3@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om1: our priority 0 <= candidate's priority 0
om3_1    | 2023-03-10 22:52:32,836 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to PRE_VOTE vote request: om1<-om3#0:OK-t5. Peer's state: om3@group-D66704EFC61C:t5, leader=null, voted=om1, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c102, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-03-10 22:52:32,814 [grpc-default-executor-2] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(ELECTION, om1, group-D66704EFC61C, 6, (t:5, i:102))
om3_1    | 2023-03-10 22:52:32,836 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om3<-om2#0:OK-t6
om3_1    | 2023-03-10 22:52:32,846 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result DISCOVERED_A_NEW_TERM (term=6)
om3_1    | 2023-03-10 22:52:32,839 [grpc-default-executor-2] INFO impl.VoteContext: om3@group-D66704EFC61C-CANDIDATE: accept ELECTION from om1: our priority 0 <= candidate's priority 0
om3_1    | 2023-03-10 22:52:32,850 [grpc-default-executor-2] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from CANDIDATE to FOLLOWER at term 6 for candidate:om1
om3_1    | 2023-03-10 22:52:32,851 [grpc-default-executor-2] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-LeaderElection1
om3_1    | 2023-03-10 22:52:32,867 [grpc-default-executor-2] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
om3_1    | 2023-03-10 22:52:32,880 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-03-10 22:52:32,880 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-03-10 22:52:32,886 [grpc-default-executor-2] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to ELECTION vote request: om1<-om3#0:OK-t6. Peer's state: om3@group-D66704EFC61C:t6, leader=null, voted=om1, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c102, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-03-10 22:52:32,922 [grpc-default-executor-2] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(ELECTION, om2, group-D66704EFC61C, 6, (t:5, i:102))
om3_1    | 2023-03-10 22:52:32,925 [grpc-default-executor-2] INFO impl.VoteContext: om3@group-D66704EFC61C-FOLLOWER: reject ELECTION from om2: already has voted for om1 at current term 6
om3_1    | 2023-03-10 22:52:32,925 [grpc-default-executor-2] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to ELECTION vote request: om2<-om3#0:FAIL-t6. Peer's state: om3@group-D66704EFC61C:t6, leader=null, voted=om1, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c102, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-03-10 22:52:33,419 [om3-server-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: change Leader from null to om1 at term 6 for appendEntries, leader elected after 17417ms
om3_1    | 2023-03-10 22:52:33,446 [om3-server-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 103: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-03-10 22:52:33,469 [om3-server-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: Rolling segment log-67_102 to index:102
om3_1    | 2023-03-10 22:52:33,493 [om3@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_67 to /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_67-102
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-03-10 22:52:09,712 [IPC Server handler 41 on default port 9891] WARN ipc.Server: IPC Server handler 41 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:44848: output error
recon_1  | 2023-03-10 22:52:09,660 [IPC Server handler 39 on default port 9891] WARN ipc.Server: IPC Server handler 39 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.15:34050: output error
recon_1  | 2023-03-10 22:52:09,660 [IPC Server handler 29 on default port 9891] WARN ipc.Server: IPC Server handler 29 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.15:34038: output error
recon_1  | 2023-03-10 22:52:09,660 [IPC Server handler 25 on default port 9891] WARN ipc.Server: IPC Server handler 25 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:46298: output error
recon_1  | 2023-03-10 22:52:09,660 [IPC Server handler 24 on default port 9891] WARN ipc.Server: IPC Server handler 24 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:48158: output error
recon_1  | 2023-03-10 22:52:09,831 [IPC Server handler 24 on default port 9891] INFO ipc.Server: IPC Server handler 24 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-03-10 22:52:09,659 [IPC Server handler 28 on default port 9891] WARN ipc.Server: IPC Server handler 28 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:46304: output error
recon_1  | 2023-03-10 22:52:09,842 [IPC Server handler 28 on default port 9891] INFO ipc.Server: IPC Server handler 28 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-03-10 22:52:09,659 [IPC Server handler 30 on default port 9891] WARN ipc.Server: IPC Server handler 30 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:44852: output error
dn2_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 2023-03-10 22:52:53,355 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | 2023-03-10 22:52:53,355 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn2_1    | 2023-03-10 22:52:53,355 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn2_1    | 2023-03-10 22:52:53,356 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 2001 to close, current state is: CLOSING
dn2_1    | 2023-03-10 22:52:53,356 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn2_1    | 2023-03-10 22:52:53,356 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn2_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 2023-03-10 22:52:53,357 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn2_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 2023-03-10 22:52:53,358 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | 2023-03-10 22:52:53,358 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn2_1    | 2023-03-10 22:52:53,358 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn2_1    | 2023-03-10 22:52:53,359 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 2001 to close, current state is: CLOSING
dn2_1    | 2023-03-10 22:52:53,359 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn2_1    | 2023-03-10 22:52:53,363 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn5_1    | 2023-03-10 22:53:54,404 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-03-10 22:53:54,406 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-10 22:53:54,560 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-03-10 22:53:54,561 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-03-10 22:53:54,561 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-03-10 22:53:54,562 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-03-10 22:53:54,562 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-03-10 22:53:54,573 [pool-26-thread-1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469: start as a follower, conf=-1: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:53:54,574 [pool-26-thread-1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn5_1    | 2023-03-10 22:53:54,574 [pool-26-thread-1] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: start 178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-FollowerState
dn5_1    | 2023-03-10 22:53:54,580 [pool-26-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-CD960C190469,id=178b30e1-b74d-4f4d-a142-c930eee71455
dn5_1    | 2023-03-10 22:53:54,581 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-03-10 22:53:54,581 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-10 22:53:54,582 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-10 22:53:54,582 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-03-10 22:53:54,583 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-03-10 22:53:54,583 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-03-10 22:53:54,588 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=47893e13-e605-460c-a18a-cd960c190469
dn5_1    | 2023-03-10 22:53:54,588 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=47893e13-e605-460c-a18a-cd960c190469.
dn5_1    | 2023-03-10 22:53:54,589 [Command processor thread] INFO server.RaftServer: 178b30e1-b74d-4f4d-a142-c930eee71455: addNew group-577A15FD12B5:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] returns group-577A15FD12B5:java.util.concurrent.CompletableFuture@38f47d00[Not completed]
dn5_1    | 2023-03-10 22:53:54,593 [pool-26-thread-1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455: new RaftServerImpl for group-577A15FD12B5:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn5_1    | 2023-03-10 22:53:54,593 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-03-10 22:53:54,594 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-03-10 22:53:54,594 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-03-10 22:53:54,596 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-03-10 22:53:54,596 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-03-10 22:53:54,596 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-03-10 22:53:54,596 [pool-26-thread-1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5: ConfigurationManager, init=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-03-10 22:53:54,599 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-03-10 22:53:54,601 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-03-10 22:53:54,601 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-03-10 22:53:54,608 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-03-10 22:53:54,609 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-03-10 22:53:54,609 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-03-10 22:53:54,613 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-03-10 22:53:54,613 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-03-10 22:53:54,613 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-03-10 22:53:54,614 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-03-10 22:53:54,614 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
recon_1  | 2023-03-10 22:52:09,659 [IPC Server handler 40 on default port 9891] WARN ipc.Server: IPC Server handler 40 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:48162: output error
recon_1  | 2023-03-10 22:52:09,853 [IPC Server handler 30 on default port 9891] INFO ipc.Server: IPC Server handler 30 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-03-10 22:52:09,841 [IPC Server handler 39 on default port 9891] INFO ipc.Server: IPC Server handler 39 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-03-10 22:52:09,841 [IPC Server handler 29 on default port 9891] INFO ipc.Server: IPC Server handler 29 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
dn3_1    | 2023-03-10 22:52:15,936 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-FollowerState] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00: changes role from  FOLLOWER to CANDIDATE at term 11 for changeToCandidate
dn3_1    | 2023-03-10 22:52:15,996 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-03-10 22:52:16,009 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-FollowerState] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: start 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-LeaderElection1
dn3_1    | 2023-03-10 22:52:16,028 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-FollowerState] INFO impl.FollowerState: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5362086821ns, electionTimeout:5199ms
dn3_1    | 2023-03-10 22:52:16,096 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-FollowerState] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: shutdown 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-FollowerState
dn3_1    | 2023-03-10 22:52:16,096 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-FollowerState] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn3_1    | 2023-03-10 22:52:16,108 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-03-10 22:52:16,109 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-FollowerState] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: start 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2
dn3_1    | 2023-03-10 22:52:16,197 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-FollowerState] INFO impl.FollowerState: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5626941214ns, electionTimeout:5181ms
dn3_1    | 2023-03-10 22:52:16,198 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-FollowerState] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: shutdown 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-FollowerState
dn3_1    | 2023-03-10 22:52:16,198 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-FollowerState] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3: changes role from  FOLLOWER to CANDIDATE at term 10 for changeToCandidate
dn3_1    | 2023-03-10 22:52:16,198 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-03-10 22:52:16,198 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-FollowerState] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: start 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-LeaderElection3
dn3_1    | 2023-03-10 22:52:16,238 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-LeaderElection1] INFO impl.LeaderElection: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 11 for 11: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:16,239 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2] INFO impl.LeaderElection: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:16,315 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2] INFO impl.LeaderElection: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2 PRE_VOTE round 0: result PASSED (term=3)
dn3_1    | 2023-03-10 22:52:16,314 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-LeaderElection3] INFO impl.LeaderElection: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 10 for 22: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:16,671 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2] INFO impl.LeaderElection: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2 ELECTION round 0: submit vote requests at term 4 for 3: peers:[1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:16,672 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2] INFO impl.LeaderElection: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2 ELECTION round 0: result PASSED (term=4)
dn3_1    | 2023-03-10 22:52:16,675 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: shutdown 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2
dn3_1    | 2023-03-10 22:52:16,677 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn3_1    | 2023-03-10 22:52:16,684 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-LeaderElection3-1] INFO server.GrpcServerProtocolClient: Build channel for e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn3_1    | 2023-03-10 22:52:16,737 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-LeaderElection3-2] INFO server.GrpcServerProtocolClient: Build channel for e3e4587c-aa42-4e86-ae9a-d3e448365275
dn3_1    | 2023-03-10 22:52:16,700 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-5C511402A23E with new leaderId: 1a6d358d-6662-4447-914c-d709a67ff716
dn3_1    | 2023-03-10 22:52:16,749 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-03-10 22:52:16,785 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-10 22:52:16,729 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-03-10 22:52:16,786 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-10 22:52:16,787 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E: change Leader from null to 1a6d358d-6662-4447-914c-d709a67ff716 at term 4 for becomeLeader, leader elected after 27102ms
dn3_1    | 2023-03-10 22:52:16,739 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 178b30e1-b74d-4f4d-a142-c930eee71455
dn3_1    | 2023-03-10 22:52:16,750 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 3892a4e1-c878-42af-adb7-db66a90d61f4
dn3_1    | 2023-03-10 22:52:17,109 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn3_1    | 2023-03-10 22:52:17,232 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-03-10 22:52:17,243 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-03-10 22:52:17,337 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-03-10 22:52:17,342 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-03-10 22:52:17,342 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn3_1    | 2023-03-10 22:52:17,646 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-03-10 22:52:17,760 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn3_1    | 2023-03-10 22:52:17,984 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: start 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderStateImpl
dn3_1    | 2023-03-10 22:52:18,257 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn3_1    | 2023-03-10 22:52:18,428 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderElection2] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E: set configuration 5: peers:[1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:18,539 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/7572f4a9-8e86-407b-b624-5c511402a23e/current/log_inprogress_3 to /data/metadata/ratis/7572f4a9-8e86-407b-b624-5c511402a23e/current/log_3-4
dn3_1    | 2023-03-10 22:52:18,809 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/7572f4a9-8e86-407b-b624-5c511402a23e/current/log_inprogress_5
dn3_1    | 2023-03-10 22:52:20,467 [grpc-default-executor-0] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3: receive requestVote(PRE_VOTE, e3e4587c-aa42-4e86-ae9a-d3e448365275, group-0049FBBC23B3, 10, (t:10, i:35))
dn3_1    | 2023-03-10 22:52:20,491 [grpc-default-executor-1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3: receive requestVote(PRE_VOTE, e3e4587c-aa42-4e86-ae9a-d3e448365275, group-0049FBBC23B3, 10, (t:10, i:35))
dn3_1    | 2023-03-10 22:52:20,502 [grpc-default-executor-0] INFO impl.VoteContext: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-CANDIDATE: accept PRE_VOTE from e3e4587c-aa42-4e86-ae9a-d3e448365275: our priority 0 <= candidate's priority 1
dn3_1    | 2023-03-10 22:52:20,503 [grpc-default-executor-3] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3: receive requestVote(PRE_VOTE, e78c5ce1-46ab-4889-a0cd-5903ae46614d, group-0049FBBC23B3, 10, (t:10, i:35))
dn3_1    | 2023-03-10 22:52:20,520 [grpc-default-executor-2] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00: receive requestVote(PRE_VOTE, 178b30e1-b74d-4f4d-a142-c930eee71455, group-5B98FDC36F00, 11, (t:11, i:16))
dn3_1    | 2023-03-10 22:52:20,521 [grpc-default-executor-2] INFO impl.VoteContext: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-CANDIDATE: accept PRE_VOTE from 178b30e1-b74d-4f4d-a142-c930eee71455: our priority 0 <= candidate's priority 0
dn3_1    | 2023-03-10 22:52:20,610 [grpc-default-executor-4] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00: receive requestVote(PRE_VOTE, 178b30e1-b74d-4f4d-a142-c930eee71455, group-5B98FDC36F00, 11, (t:11, i:16))
dn3_1    | 2023-03-10 22:52:20,710 [grpc-default-executor-0] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3 replies to PRE_VOTE vote request: e3e4587c-aa42-4e86-ae9a-d3e448365275<-1a6d358d-6662-4447-914c-d709a67ff716#0:OK-t10. Peer's state: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3:t10, leader=null, voted=e3e4587c-aa42-4e86-ae9a-d3e448365275, raftlog=Memoized:1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-SegmentedRaftLog:OPENED:c35, conf=22: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:20,711 [grpc-default-executor-3] INFO impl.VoteContext: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-CANDIDATE: accept PRE_VOTE from e78c5ce1-46ab-4889-a0cd-5903ae46614d: our priority 0 <= candidate's priority 0
dn3_1    | 2023-03-10 22:52:20,711 [grpc-default-executor-3] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3 replies to PRE_VOTE vote request: e78c5ce1-46ab-4889-a0cd-5903ae46614d<-1a6d358d-6662-4447-914c-d709a67ff716#0:OK-t10. Peer's state: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3:t10, leader=null, voted=e3e4587c-aa42-4e86-ae9a-d3e448365275, raftlog=Memoized:1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-SegmentedRaftLog:OPENED:c35, conf=22: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:53:54,615 [pool-26-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/995fa1d6-41cf-4ed4-929b-577a15fd12b5 does not exist. Creating ...
dn5_1    | 2023-03-10 22:53:54,622 [pool-26-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/995fa1d6-41cf-4ed4-929b-577a15fd12b5/in_use.lock acquired by nodename 7@82dcecb82990
dn5_1    | 2023-03-10 22:53:54,630 [pool-26-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/995fa1d6-41cf-4ed4-929b-577a15fd12b5 has been successfully formatted.
dn5_1    | 2023-03-10 22:53:54,645 [pool-26-thread-1] INFO ratis.ContainerStateMachine: group-577A15FD12B5: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn5_1    | 2023-03-10 22:53:54,651 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-03-10 22:53:54,651 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-03-10 22:53:54,654 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-10 22:53:54,654 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-03-10 22:53:54,654 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-03-10 22:53:54,655 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-03-10 22:53:54,656 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-03-10 22:53:54,657 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-03-10 22:53:54,657 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: new 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/995fa1d6-41cf-4ed4-929b-577a15fd12b5
dn5_1    | 2023-03-10 22:53:54,657 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-03-10 22:53:54,658 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-03-10 22:53:54,659 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-03-10 22:53:54,659 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-03-10 22:53:54,659 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-03-10 22:53:54,660 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-03-10 22:53:54,660 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-03-10 22:53:54,660 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-03-10 22:53:54,662 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-03-10 22:53:54,663 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-03-10 22:53:54,679 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-03-10 22:53:54,681 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-03-10 22:53:54,681 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-03-10 22:53:54,681 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-03-10 22:53:54,683 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-03-10 22:53:54,683 [pool-26-thread-1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5: start as a follower, conf=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:53:54,683 [pool-26-thread-1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn5_1    | 2023-03-10 22:53:54,684 [pool-26-thread-1] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: start 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-FollowerState
dn5_1    | 2023-03-10 22:53:54,690 [pool-26-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-577A15FD12B5,id=178b30e1-b74d-4f4d-a142-c930eee71455
dn5_1    | 2023-03-10 22:53:54,690 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-03-10 22:53:54,690 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-03-10 22:53:54,690 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-03-10 22:53:54,690 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-03-10 22:53:54,697 [178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-10 22:53:54,701 [178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-10 22:53:54,703 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=995fa1d6-41cf-4ed4-929b-577a15fd12b5
dn5_1    | 2023-03-10 22:53:56,223 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-178b30e1-b74d-4f4d-a142-c930eee71455: Detected pause in JVM or host machine (eg GC): pause of approximately 572253299ns.
dn5_1    | GC pool 'ParNew' had collection(s): count=1 time=946ms
dn5_1    | 2023-03-10 22:53:56,372 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=995fa1d6-41cf-4ed4-929b-577a15fd12b5.
dn5_1    | 2023-03-10 22:53:59,069 [178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-FollowerState] INFO impl.FollowerState: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5157915273ns, electionTimeout:5135ms
dn5_1    | 2023-03-10 22:53:59,070 [178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-FollowerState] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: shutdown 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-FollowerState
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om3_1    | 2023-03-10 22:52:33,578 [om3@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_103
om3_1    | 2023-03-10 22:52:34,245 [om3@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om3_1    | [id: "om1"
om3_1    | address: "om1:9872"
om3_1    | startupRole: FOLLOWER
om3_1    | , id: "om3"
om3_1    | address: "om3:9872"
om3_1    | startupRole: FOLLOWER
om3_1    | , id: "om2"
om3_1    | address: "om2:9872"
om3_1    | startupRole: FOLLOWER
om3_1    | ]
om3_1    | 2023-03-10 22:54:06,444 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization started.
om3_1    | 2023-03-10 22:54:06,447 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: ERASURE_CODED_STORAGE_SUPPORT.
om3_1    | 2023-03-10 22:54:06,452 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature ERASURE_CODED_STORAGE_SUPPORT has been finalized.
om3_1    | 2023-03-10 22:54:06,472 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: BUCKET_LAYOUT_SUPPORT.
om3_1    | 2023-03-10 22:54:06,476 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature BUCKET_LAYOUT_SUPPORT has been finalized.
om3_1    | 2023-03-10 22:54:06,477 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: MULTITENANCY_SCHEMA.
om3_1    | 2023-03-10 22:54:06,479 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature MULTITENANCY_SCHEMA has been finalized.
om3_1    | 2023-03-10 22:54:06,480 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
om3_1    | 2023-03-10 22:54:06,480 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization is done.
om3_1    | 2023-03-10 22:54:06,504 [OMDoubleBufferFlushThread] INFO upgrade.OMFinalizeUpgradeResponse: Layout version to persist to DB : 3
om3_1    | 2023-03-10 22:55:44,793 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:new2-volume for user:hadoop
om3_1    | 2023-03-10 22:55:49,191 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: new2-volume
om3_1    | 2023-03-10 22:56:02,144 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: s3v
om3_1    | 2023-03-10 22:56:13,911 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:new2-bucket in volume:s3v
om3_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om3_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:207)
om3_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:337)
om3_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om3_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om3_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om3_1    | 2023-03-10 22:56:43,306 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:ectest-new for user:hadoop
om3_1    | 2023-03-10 22:56:47,877 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ectest of layout FILE_SYSTEM_OPTIMIZED in volume: ectest-new
om3_1    | 2023-03-10 22:56:56,820 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: testpropchange of layout FILE_SYSTEM_OPTIMIZED in volume: ectest-new
dn5_1    | 2023-03-10 22:53:59,071 [178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-FollowerState] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn5_1    | 2023-03-10 22:53:59,071 [178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-03-10 22:53:59,071 [178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-FollowerState] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: start 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-LeaderElection3
dn5_1    | 2023-03-10 22:53:59,079 [178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-LeaderElection3] INFO impl.LeaderElection: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:53:59,088 [178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-LeaderElection3-1] INFO server.GrpcServerProtocolClient: Build channel for e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn5_1    | 2023-03-10 22:53:59,091 [178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-10 22:53:59,108 [178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-10 22:53:59,160 [178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-LeaderElection3] INFO impl.LeaderElection: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-LeaderElection3: PRE_VOTE REJECTED received 1 response(s) and 0 exception(s):
dn5_1    | 2023-03-10 22:53:59,160 [178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-LeaderElection3] INFO impl.LeaderElection:   Response 0: 178b30e1-b74d-4f4d-a142-c930eee71455<-e78c5ce1-46ab-4889-a0cd-5903ae46614d#0:FAIL-t0
dn5_1    | 2023-03-10 22:53:59,160 [178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-LeaderElection3] INFO impl.LeaderElection: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-LeaderElection3 PRE_VOTE round 0: result REJECTED
dn5_1    | 2023-03-10 22:53:59,161 [178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-LeaderElection3] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
dn5_1    | 2023-03-10 22:53:59,161 [178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-LeaderElection3] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: shutdown 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-LeaderElection3
dn5_1    | 2023-03-10 22:53:59,161 [178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-LeaderElection3] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: start 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-FollowerState
dn5_1    | 2023-03-10 22:53:59,166 [178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-10 22:53:59,166 [178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-10 22:53:59,375 [grpc-default-executor-2] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88: receive requestVote(PRE_VOTE, e78c5ce1-46ab-4889-a0cd-5903ae46614d, group-6265FBDC9D88, 0, (t:0, i:0))
dn5_1    | 2023-03-10 22:53:59,377 [grpc-default-executor-2] INFO impl.VoteContext: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-FOLLOWER: accept PRE_VOTE from e78c5ce1-46ab-4889-a0cd-5903ae46614d: our priority 0 <= candidate's priority 1
dn5_1    | 2023-03-10 22:53:59,377 [grpc-default-executor-2] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88 replies to PRE_VOTE vote request: e78c5ce1-46ab-4889-a0cd-5903ae46614d<-178b30e1-b74d-4f4d-a142-c930eee71455#0:OK-t0. Peer's state: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88:t0, leader=null, voted=, raftlog=Memoized:178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:53:59,408 [grpc-default-executor-2] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88: receive requestVote(ELECTION, e78c5ce1-46ab-4889-a0cd-5903ae46614d, group-6265FBDC9D88, 1, (t:0, i:0))
dn5_1    | 2023-03-10 22:53:59,408 [grpc-default-executor-2] INFO impl.VoteContext: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-FOLLOWER: accept ELECTION from e78c5ce1-46ab-4889-a0cd-5903ae46614d: our priority 0 <= candidate's priority 1
dn5_1    | 2023-03-10 22:53:59,417 [grpc-default-executor-2] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn5_1    | 2023-03-10 22:53:59,417 [grpc-default-executor-2] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: shutdown 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-FollowerState
dn5_1    | 2023-03-10 22:53:59,419 [178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-FollowerState] INFO impl.FollowerState: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-FollowerState was interrupted
dn5_1    | 2023-03-10 22:53:59,419 [grpc-default-executor-2] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: start 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-FollowerState
dn5_1    | 2023-03-10 22:53:59,432 [178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-10 22:53:59,433 [178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 2023-03-10 22:52:53,364 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn2_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 2023-03-10 22:52:53,364 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | 2023-03-10 22:52:53,366 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn2_1    | 2023-03-10 22:52:53,366 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn2_1    | 2023-03-10 22:52:53,366 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 2001 to close, current state is: CLOSING
dn2_1    | 2023-03-10 22:52:53,367 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn2_1    | 2023-03-10 22:52:53,368 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn2_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 2023-03-10 22:52:53,369 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn2_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 2023-03-10 22:52:53,367 [ContainerOp-78b03a88-75c1-4060-9962-cbc13a60f575-0] INFO keyvalue.KeyValueContainer: Container 1002 is closed with bcsId 10.
dn2_1    | 2023-03-10 22:52:53,697 [ContainerOp-78b03a88-75c1-4060-9962-cbc13a60f575-1] INFO keyvalue.KeyValueContainer: Container 2001 is synced with bcsId 19.
dn2_1    | 2023-03-10 22:52:53,697 [ContainerOp-78b03a88-75c1-4060-9962-cbc13a60f575-1] INFO keyvalue.KeyValueContainer: Container 2001 is synced with bcsId 19.
dn2_1    | 2023-03-10 22:52:53,740 [ContainerOp-78b03a88-75c1-4060-9962-cbc13a60f575-1] INFO keyvalue.KeyValueContainer: Container 2001 is closed with bcsId 19.
dn2_1    | 2023-03-10 22:53:10,307 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn2_1    | 2023-03-10 22:53:24,230 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | 2023-03-10 22:53:24,230 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn2_1    | 2023-03-10 22:53:24,231 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
recon_1  | 2023-03-10 22:52:09,841 [IPC Server handler 25 on default port 9891] INFO ipc.Server: IPC Server handler 25 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-03-10 22:52:09,776 [IPC Server handler 31 on default port 9891] INFO ipc.Server: IPC Server handler 31 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-03-10 22:52:09,853 [IPC Server handler 40 on default port 9891] INFO ipc.Server: IPC Server handler 40 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn5_1    | 2023-03-10 22:53:59,433 [grpc-default-executor-2] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88 replies to ELECTION vote request: e78c5ce1-46ab-4889-a0cd-5903ae46614d<-178b30e1-b74d-4f4d-a142-c930eee71455#0:OK-t1. Peer's state: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88:t1, leader=null, voted=e78c5ce1-46ab-4889-a0cd-5903ae46614d, raftlog=Memoized:178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:53:59,577 [178b30e1-b74d-4f4d-a142-c930eee71455-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6265FBDC9D88 with new leaderId: e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn5_1    | 2023-03-10 22:53:59,578 [178b30e1-b74d-4f4d-a142-c930eee71455-server-thread1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88: change Leader from null to e78c5ce1-46ab-4889-a0cd-5903ae46614d at term 1 for appendEntries, leader elected after 5741ms
dn5_1    | 2023-03-10 22:53:59,579 [178b30e1-b74d-4f4d-a142-c930eee71455-server-thread1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88: set configuration 0: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:53:59,579 [178b30e1-b74d-4f4d-a142-c930eee71455-server-thread1] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 2023-03-10 22:53:59,587 [178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-6265FBDC9D88-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a8f227f9-9826-491d-a836-6265fbdc9d88/current/log_inprogress_0
dn5_1    | 2023-03-10 22:53:59,720 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-FollowerState] INFO impl.FollowerState: 178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5145888413ns, electionTimeout:5136ms
dn5_1    | 2023-03-10 22:53:59,720 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-FollowerState] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: shutdown 178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-FollowerState
dn5_1    | 2023-03-10 22:53:59,720 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-FollowerState] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn5_1    | 2023-03-10 22:53:59,720 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-03-10 22:53:59,720 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-FollowerState] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: start 178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4
dn5_1    | 2023-03-10 22:53:59,727 [178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-FollowerState] INFO impl.FollowerState: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5043019735ns, electionTimeout:5025ms
dn5_1    | 2023-03-10 22:53:59,727 [178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-FollowerState] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: shutdown 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-FollowerState
dn5_1    | 2023-03-10 22:53:59,728 [178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-FollowerState] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn5_1    | 2023-03-10 22:53:59,728 [178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-03-10 22:53:59,728 [178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-FollowerState] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: start 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-LeaderElection5
dn5_1    | 2023-03-10 22:53:59,734 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4] INFO impl.LeaderElection: 178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:53:59,734 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4] INFO impl.LeaderElection: 178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4 PRE_VOTE round 0: result PASSED (term=0)
dn5_1    | 2023-03-10 22:53:59,739 [178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-LeaderElection5] INFO impl.LeaderElection: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-LeaderElection5 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:53:59,752 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4] INFO impl.LeaderElection: 178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4 ELECTION round 0: submit vote requests at term 1 for -1: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:53:59,752 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4] INFO impl.LeaderElection: 178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4 ELECTION round 0: result PASSED (term=1)
dn5_1    | 2023-03-10 22:53:59,753 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: shutdown 178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4
dn5_1    | 2023-03-10 22:53:59,753 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn5_1    | 2023-03-10 22:53:59,754 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-CD960C190469 with new leaderId: 178b30e1-b74d-4f4d-a142-c930eee71455
dn5_1    | 2023-03-10 22:53:59,758 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469: change Leader from null to 178b30e1-b74d-4f4d-a142-c930eee71455 at term 1 for becomeLeader, leader elected after 5422ms
dn5_1    | 2023-03-10 22:53:59,765 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn4_1    | 2023-03-10 22:52:53,943 [Command processor thread] INFO utils.DatanodeStoreCache: Removed db /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-efc75ae1-5bd9-4645-9a43-bc7861eb0b3e/container.db from cache
dn4_1    | 2023-03-10 22:52:53,943 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is stopped at /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-efc75ae1-5bd9-4645-9a43-bc7861eb0b3e/container.db for volume DS-efc75ae1-5bd9-4645-9a43-bc7861eb0b3e
dn4_1    | 2023-03-10 22:52:54,012 [Command processor thread] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-efc75ae1-5bd9-4645-9a43-bc7861eb0b3e/container.db to cache
dn4_1    | 2023-03-10 22:52:54,012 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-efc75ae1-5bd9-4645-9a43-bc7861eb0b3e/container.db for volume DS-efc75ae1-5bd9-4645-9a43-bc7861eb0b3e
dn4_1    | 2023-03-10 22:52:54,016 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature DATANODE_SCHEMA_V3 has been finalized.
dn4_1    | 2023-03-10 22:52:54,016 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn4_1    | 2023-03-10 22:52:54,016 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn4_1    | 2023-03-10 22:52:54,017 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-03-10 22:52:54,017 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-03-10 22:52:54,017 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-03-10 22:52:54,017 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-03-10 22:52:54,018 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-03-10 22:52:54,018 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-03-10 22:52:54,021 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-03-10 22:52:54,021 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-03-10 22:53:11,126 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn4_1    | 2023-03-10 22:53:24,558 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: e78c5ce1-46ab-4889-a0cd-5903ae46614d: Completed APPEND_ENTRIES, lastRequest: null
dn4_1    | 2023-03-10 22:53:24,558 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: e78c5ce1-46ab-4889-a0cd-5903ae46614d: Completed APPEND_ENTRIES, lastRequest: e3e4587c-aa42-4e86-ae9a-d3e448365275->e78c5ce1-46ab-4889-a0cd-5903ae46614d#313-t11,previous=(t:11, i:40),leaderCommit=40,initializing? true,entries: size=1, first=(t:11, i:41), METADATAENTRY(c:40)
dn4_1    | 2023-03-10 22:53:24,809 [Command processor thread] INFO server.RaftServer: e78c5ce1-46ab-4889-a0cd-5903ae46614d: remove  FOLLOWER e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3:t11, leader=e3e4587c-aa42-4e86-ae9a-d3e448365275, voted=e3e4587c-aa42-4e86-ae9a-d3e448365275, raftlog=Memoized:e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-SegmentedRaftLog:OPENED:c41, conf=36: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn4_1    | 2023-03-10 22:53:24,811 [Command processor thread] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3: shutdown
dn4_1    | 2023-03-10 22:53:24,811 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-0049FBBC23B3,id=e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn4_1    | 2023-03-10 22:53:24,811 [Command processor thread] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: shutdown e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-FollowerState
dn4_1    | 2023-03-10 22:53:24,811 [Command processor thread] INFO impl.StateMachineUpdater: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-StateMachineUpdater: set stopIndex = 41
dn4_1    | 2023-03-10 22:53:24,812 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-0049FBBC23B3: Taking a snapshot at:(t:11, i:41) file /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/sm/snapshot.11_41
dn4_1    | 2023-03-10 22:53:24,812 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-FollowerState] INFO impl.FollowerState: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-FollowerState was interrupted
dn4_1    | 2023-03-10 22:53:24,817 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-0049FBBC23B3: Finished taking a snapshot at:(t:11, i:41) file:/data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/sm/snapshot.11_41 took: 5 ms
dn4_1    | 2023-03-10 22:53:24,819 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-StateMachineUpdater] INFO impl.StateMachineUpdater: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-StateMachineUpdater: Took a snapshot at index 41
dn4_1    | 2023-03-10 22:53:24,820 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-StateMachineUpdater] INFO impl.StateMachineUpdater: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-StateMachineUpdater: snapshotIndex: updateIncreasingly 35 -> 41
dn4_1    | 2023-03-10 22:53:24,826 [Command processor thread] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3: closes. applyIndex: 41
dn4_1    | 2023-03-10 22:53:24,827 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn4_1    | 2023-03-10 22:53:24,838 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3-SegmentedRaftLogWorker close()
dn4_1    | 2023-03-10 22:53:24,854 [Command processor thread] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-0049FBBC23B3: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3
dn4_1    | 2023-03-10 22:53:24,858 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=cf0cdca9-af45-4cc0-9366-0049fbbc23b3 command on datanode e78c5ce1-46ab-4889-a0cd-5903ae46614d.
dn4_1    | 2023-03-10 22:53:24,859 [Command processor thread] INFO server.RaftServer: e78c5ce1-46ab-4889-a0cd-5903ae46614d: remove    LEADER e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9:t4, leader=e78c5ce1-46ab-4889-a0cd-5903ae46614d, voted=e78c5ce1-46ab-4889-a0cd-5903ae46614d, raftlog=Memoized:e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-SegmentedRaftLog:OPENED:c6, conf=5: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn4_1    | 2023-03-10 22:53:24,859 [Command processor thread] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9: shutdown
dn4_1    | 2023-03-10 22:53:24,860 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-D98E5419B7A9,id=e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn4_1    | 2023-03-10 22:53:24,860 [Command processor thread] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: shutdown e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-LeaderStateImpl
dn4_1    | 2023-03-10 22:53:24,861 [Command processor thread] INFO impl.PendingRequests: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-PendingRequests: sendNotLeaderResponses
dn4_1    | 2023-03-10 22:53:24,869 [Command processor thread] INFO impl.StateMachineUpdater: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-StateMachineUpdater: set stopIndex = 6
dn4_1    | 2023-03-10 22:53:24,870 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-D98E5419B7A9: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/8581ae3d-c50a-4a49-9d70-d98e5419b7a9/sm/snapshot.4_6
dn4_1    | 2023-03-10 22:53:24,875 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-D98E5419B7A9: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/8581ae3d-c50a-4a49-9d70-d98e5419b7a9/sm/snapshot.4_6 took: 4 ms
dn4_1    | 2023-03-10 22:53:24,876 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-StateMachineUpdater] INFO impl.StateMachineUpdater: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-StateMachineUpdater: Took a snapshot at index 6
dn4_1    | 2023-03-10 22:53:24,876 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-StateMachineUpdater] INFO impl.StateMachineUpdater: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
dn4_1    | 2023-03-10 22:53:24,879 [Command processor thread] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9: closes. applyIndex: 6
dn4_1    | 2023-03-10 22:53:24,879 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn4_1    | 2023-03-10 22:53:24,880 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9-SegmentedRaftLogWorker close()
dn4_1    | 2023-03-10 22:53:24,886 [Command processor thread] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-D98E5419B7A9: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/8581ae3d-c50a-4a49-9d70-d98e5419b7a9
dn4_1    | 2023-03-10 22:53:24,886 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=8581ae3d-c50a-4a49-9d70-d98e5419b7a9 command on datanode e78c5ce1-46ab-4889-a0cd-5903ae46614d.
dn4_1    | 2023-03-10 22:53:24,886 [Command processor thread] INFO server.RaftServer: e78c5ce1-46ab-4889-a0cd-5903ae46614d: remove    LEADER e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575:t7, leader=e78c5ce1-46ab-4889-a0cd-5903ae46614d, voted=e78c5ce1-46ab-4889-a0cd-5903ae46614d, raftlog=Memoized:e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-SegmentedRaftLog:OPENED:c26, conf=21: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn4_1    | 2023-03-10 22:53:24,886 [Command processor thread] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575: shutdown
dn4_1    | 2023-03-10 22:53:24,887 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-CBC13A60F575,id=e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn4_1    | 2023-03-10 22:53:24,887 [Command processor thread] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: shutdown e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-LeaderStateImpl
dn2_1    | 2023-03-10 22:53:24,231 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: ERASURE_CODED_STORAGE_SUPPORT.
dn2_1    | 2023-03-10 22:53:24,234 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature ERASURE_CODED_STORAGE_SUPPORT has been finalized.
scm_1    | 2023-03-10 22:52:11,853 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
dn5_1    | 2023-03-10 22:53:59,766 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1    | 2023-03-10 22:52:11,856 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1    | 2023-03-10 22:52:12,545 [IPC Server handler 77 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/1a6d358d-6662-4447-914c-d709a67ff716
scm_1    | 2023-03-10 22:52:12,546 [IPC Server handler 77 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 1a6d358d-6662-4447-914c-d709a67ff716{ip: 10.9.0.17, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn1_1    | 2023-03-10 22:52:53,479 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn3_1    | 2023-03-10 22:52:20,722 [grpc-default-executor-1] INFO impl.VoteContext: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-CANDIDATE: accept PRE_VOTE from e3e4587c-aa42-4e86-ae9a-d3e448365275: our priority 0 <= candidate's priority 1
dn3_1    | 2023-03-10 22:52:20,753 [grpc-default-executor-1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3 replies to PRE_VOTE vote request: e3e4587c-aa42-4e86-ae9a-d3e448365275<-1a6d358d-6662-4447-914c-d709a67ff716#0:OK-t10. Peer's state: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3:t10, leader=null, voted=e3e4587c-aa42-4e86-ae9a-d3e448365275, raftlog=Memoized:1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-SegmentedRaftLog:OPENED:c35, conf=22: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:20,806 [grpc-default-executor-2] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00 replies to PRE_VOTE vote request: 178b30e1-b74d-4f4d-a142-c930eee71455<-1a6d358d-6662-4447-914c-d709a67ff716#0:OK-t11. Peer's state: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00:t11, leader=null, voted=3892a4e1-c878-42af-adb7-db66a90d61f4, raftlog=Memoized:1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-SegmentedRaftLog:OPENED:c16, conf=11: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:20,866 [grpc-default-executor-1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00: receive requestVote(ELECTION, 3892a4e1-c878-42af-adb7-db66a90d61f4, group-5B98FDC36F00, 12, (t:11, i:16))
dn1_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn2_1    | 2023-03-10 22:53:24,234 [Command processor thread] INFO upgrade.UpgradeFinalizer: Running finalization actions for layout feature: DATANODE_SCHEMA_V3
dn2_1    | 2023-03-10 22:53:24,234 [Command processor thread] INFO upgrade.DatanodeSchemaV3FinalizeAction: Upgrading Datanode volume layout for Schema V3 support.
dn2_1    | 2023-03-10 22:53:24,310 [Command processor thread] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-44bcb148-d707-4fbf-94a3-f5ad3158e5a1/container.db to cache
dn3_1    | 2023-03-10 22:52:20,870 [grpc-default-executor-1] INFO impl.VoteContext: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-CANDIDATE: accept ELECTION from 3892a4e1-c878-42af-adb7-db66a90d61f4: our priority 0 <= candidate's priority 1
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
scm_1    | 2023-03-10 22:52:12,547 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1    | 2023-03-10 22:52:12,575 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
dn2_1    | 2023-03-10 22:53:24,310 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-44bcb148-d707-4fbf-94a3-f5ad3158e5a1/container.db for volume DS-44bcb148-d707-4fbf-94a3-f5ad3158e5a1
dn4_1    | 2023-03-10 22:53:24,887 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575->e3e4587c-aa42-4e86-ae9a-d3e448365275-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575->e3e4587c-aa42-4e86-ae9a-d3e448365275-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
dn3_1    | 2023-03-10 22:52:20,871 [grpc-default-executor-1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00: changes role from CANDIDATE to FOLLOWER at term 12 for candidate:3892a4e1-c878-42af-adb7-db66a90d61f4
dn3_1    | 2023-03-10 22:52:20,874 [grpc-default-executor-1] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: shutdown 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-LeaderElection1
scm_1    | 2023-03-10 22:52:12,575 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
dn5_1    | 2023-03-10 22:53:59,767 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn2_1    | 2023-03-10 22:53:24,340 [Command processor thread] INFO utils.DatanodeStoreCache: Removed db /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-44bcb148-d707-4fbf-94a3-f5ad3158e5a1/container.db from cache
dn4_1    | 2023-03-10 22:53:24,898 [grpc-default-executor-3] INFO server.GrpcLogAppender: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575->e3e4587c-aa42-4e86-ae9a-d3e448365275-AppendLogResponseHandler: follower responses appendEntries COMPLETED
recon_1  | 2023-03-10 22:52:09,913 [IPC Server handler 41 on default port 9891] INFO ipc.Server: IPC Server handler 41 on default port 9891 caught an exception
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn3_1    | 2023-03-10 22:52:20,838 [grpc-default-executor-3] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00: receive requestVote(PRE_VOTE, 3892a4e1-c878-42af-adb7-db66a90d61f4, group-5B98FDC36F00, 11, (t:11, i:16))
scm_1    | 2023-03-10 22:52:12,576 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
dn5_1    | 2023-03-10 22:53:59,771 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn2_1    | 2023-03-10 22:53:24,341 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is stopped at /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-44bcb148-d707-4fbf-94a3-f5ad3158e5a1/container.db for volume DS-44bcb148-d707-4fbf-94a3-f5ad3158e5a1
dn4_1    | 2023-03-10 22:53:24,898 [grpc-default-executor-2] INFO server.GrpcLogAppender: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575->e3e4587c-aa42-4e86-ae9a-d3e448365275-AppendLogResponseHandler: follower responses appendEntries COMPLETED
recon_1  | java.nio.channels.ClosedChannelException
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 2023-03-10 22:52:20,883 [grpc-default-executor-1] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: start 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-FollowerState
scm_1    | 2023-03-10 22:52:12,576 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
dn2_1    | 2023-03-10 22:53:24,404 [Command processor thread] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-44bcb148-d707-4fbf-94a3-f5ad3158e5a1/container.db to cache
dn4_1    | 2023-03-10 22:53:24,902 [grpc-default-executor-2] INFO leader.FollowerInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575->e3e4587c-aa42-4e86-ae9a-d3e448365275: nextIndex: updateUnconditionally 27 -> 26
dn4_1    | 2023-03-10 22:53:24,902 [grpc-default-executor-3] INFO leader.FollowerInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575->e3e4587c-aa42-4e86-ae9a-d3e448365275: nextIndex: updateUnconditionally 26 -> 25
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn3_1    | 2023-03-10 22:52:20,958 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm_1    | 2023-03-10 22:52:12,576 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1    | 2023-03-10 22:52:12,576 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
dn2_1    | 2023-03-10 22:53:24,404 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-44bcb148-d707-4fbf-94a3-f5ad3158e5a1/container.db for volume DS-44bcb148-d707-4fbf-94a3-f5ad3158e5a1
dn4_1    | 2023-03-10 22:53:24,904 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575->3892a4e1-c878-42af-adb7-db66a90d61f4-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575->3892a4e1-c878-42af-adb7-db66a90d61f4-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn5_1    | 2023-03-10 22:53:59,772 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1    | 2023-03-10 22:52:12,576 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
dn4_1    | 2023-03-10 22:53:24,911 [grpc-default-executor-3] INFO server.GrpcLogAppender: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575->3892a4e1-c878-42af-adb7-db66a90d61f4-AppendLogResponseHandler: follower responses appendEntries COMPLETED
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn3_1    | 2023-03-10 22:52:20,977 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-10 22:53:59,772 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1    | 2023-03-10 22:52:12,608 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
dn2_1    | 2023-03-10 22:53:24,406 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature DATANODE_SCHEMA_V3 has been finalized.
dn4_1    | 2023-03-10 22:53:24,911 [grpc-default-executor-3] INFO leader.FollowerInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575->3892a4e1-c878-42af-adb7-db66a90d61f4: nextIndex: updateUnconditionally 27 -> 26
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-03-10 22:52:20,961 [grpc-default-executor-1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00 replies to ELECTION vote request: 3892a4e1-c878-42af-adb7-db66a90d61f4<-1a6d358d-6662-4447-914c-d709a67ff716#0:OK-t12. Peer's state: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00:t12, leader=null, voted=3892a4e1-c878-42af-adb7-db66a90d61f4, raftlog=Memoized:1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-SegmentedRaftLog:OPENED:c16, conf=11: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:53:59,772 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm_1    | 2023-03-10 22:52:12,613 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
dn2_1    | 2023-03-10 22:53:24,406 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn4_1    | 2023-03-10 22:53:24,912 [grpc-default-executor-2] INFO server.GrpcLogAppender: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575->3892a4e1-c878-42af-adb7-db66a90d61f4-AppendLogResponseHandler: follower responses appendEntries COMPLETED
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
dn1_1    | 2023-03-10 22:52:53,480 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn1_1    | 2023-03-10 22:52:53,480 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn5_1    | 2023-03-10 22:53:59,773 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1    | 2023-03-10 22:52:12,625 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
dn2_1    | 2023-03-10 22:53:24,406 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn4_1    | 2023-03-10 22:53:24,912 [grpc-default-executor-2] INFO leader.FollowerInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575->3892a4e1-c878-42af-adb7-db66a90d61f4: nextIndex: updateUnconditionally 26 -> 25
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
dn3_1    | 2023-03-10 22:52:20,983 [grpc-default-executor-4] INFO impl.VoteContext: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-FOLLOWER: accept PRE_VOTE from 178b30e1-b74d-4f4d-a142-c930eee71455: our priority 0 <= candidate's priority 0
dn5_1    | 2023-03-10 22:53:59,773 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: start 178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderStateImpl
scm_1    | 2023-03-10 22:52:12,734 [IPC Server handler 80 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn2_1    | 2023-03-10 22:53:24,407 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-03-10 22:53:24,914 [Command processor thread] INFO impl.PendingRequests: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-PendingRequests: sendNotLeaderResponses
dn4_1    | 2023-03-10 22:53:24,915 [Command processor thread] INFO impl.StateMachineUpdater: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-StateMachineUpdater: set stopIndex = 26
dn1_1    | 2023-03-10 22:52:53,485 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn1_1    | 2023-03-10 22:52:53,486 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 2001 to close, current state is: CLOSING
dn5_1    | 2023-03-10 22:53:59,773 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-SegmentedRaftLogWorker: Starting segment from index:0
scm_1    | 2023-03-10 22:52:12,734 [IPC Server handler 80 on default port 9861] INFO node.SCMNodeManager: Registered Data node : e78c5ce1-46ab-4889-a0cd-5903ae46614d{ip: 10.9.0.18, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn2_1    | 2023-03-10 22:53:24,407 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
dn1_1    | 2023-03-10 22:52:53,487 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn3_1    | 2023-03-10 22:52:20,984 [grpc-default-executor-4] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00 replies to PRE_VOTE vote request: 178b30e1-b74d-4f4d-a142-c930eee71455<-1a6d358d-6662-4447-914c-d709a67ff716#0:OK-t12. Peer's state: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00:t12, leader=null, voted=3892a4e1-c878-42af-adb7-db66a90d61f4, raftlog=Memoized:1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-SegmentedRaftLog:OPENED:c16, conf=11: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:53:24,918 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-CBC13A60F575: Taking a snapshot at:(t:7, i:26) file /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/sm/snapshot.7_26
dn5_1    | 2023-03-10 22:53:59,781 [178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm_1    | 2023-03-10 22:52:12,741 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
dn2_1    | 2023-03-10 22:53:24,409 [Command processor thread] INFO server.RaftServer: 3892a4e1-c878-42af-adb7-db66a90d61f4: remove    LEADER 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00:t12, leader=3892a4e1-c878-42af-adb7-db66a90d61f4, voted=3892a4e1-c878-42af-adb7-db66a90d61f4, raftlog=Memoized:3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-SegmentedRaftLog:OPENED:c20, conf=17: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
dn1_1    | 2023-03-10 22:52:53,487 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn3_1    | 2023-03-10 22:52:20,985 [grpc-default-executor-3] INFO impl.VoteContext: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-FOLLOWER: accept PRE_VOTE from 3892a4e1-c878-42af-adb7-db66a90d61f4: our priority 0 <= candidate's priority 1
scm_1    | 2023-03-10 22:52:12,855 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1.
dn4_1    | 2023-03-10 22:53:24,920 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-CBC13A60F575: Finished taking a snapshot at:(t:7, i:26) file:/data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/sm/snapshot.7_26 took: 2 ms
dn2_1    | 2023-03-10 22:53:24,411 [Command processor thread] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00: shutdown
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
dn1_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn3_1    | 2023-03-10 22:52:20,993 [grpc-default-executor-3] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00 replies to PRE_VOTE vote request: 3892a4e1-c878-42af-adb7-db66a90d61f4<-1a6d358d-6662-4447-914c-d709a67ff716#0:OK-t12. Peer's state: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00:t12, leader=null, voted=3892a4e1-c878-42af-adb7-db66a90d61f4, raftlog=Memoized:1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-SegmentedRaftLog:OPENED:c16, conf=11: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:53:59,782 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/47893e13-e605-460c-a18a-cd960c190469/current/log_inprogress_0
scm_1    | 2023-03-10 22:52:26,359 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
dn4_1    | 2023-03-10 22:53:24,921 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-StateMachineUpdater] INFO impl.StateMachineUpdater: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-StateMachineUpdater: Took a snapshot at index 26
dn2_1    | 2023-03-10 22:53:24,411 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-5B98FDC36F00,id=3892a4e1-c878-42af-adb7-db66a90d61f4
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn3_1    | 2023-03-10 22:52:21,471 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-LeaderElection1] INFO impl.LeaderElection: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-LeaderElection1: PRE_VOTE DISCOVERED_A_NEW_TERM (term=12) received 1 response(s) and 0 exception(s):
dn5_1    | 2023-03-10 22:53:59,797 [178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1    | 2023-03-10 22:52:26,369 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
dn4_1    | 2023-03-10 22:53:24,921 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-StateMachineUpdater] INFO impl.StateMachineUpdater: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-StateMachineUpdater: snapshotIndex: updateIncreasingly 20 -> 26
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn3_1    | 2023-03-10 22:52:21,476 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-LeaderElection1] INFO impl.LeaderElection:   Response 0: 1a6d358d-6662-4447-914c-d709a67ff716<-178b30e1-b74d-4f4d-a142-c930eee71455#0:OK-t12
dn5_1    | 2023-03-10 22:53:59,804 [178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469-LeaderElection4] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-CD960C190469: set configuration 0: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:53:24,922 [Command processor thread] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575: closes. applyIndex: 26
dn2_1    | 2023-03-10 22:53:24,412 [Command processor thread] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: shutdown 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-LeaderStateImpl
dn2_1    | 2023-03-10 22:53:24,413 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00->178b30e1-b74d-4f4d-a142-c930eee71455-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00->178b30e1-b74d-4f4d-a142-c930eee71455-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm_1    | 2023-03-10 22:52:42,868 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1.
dn3_1    | 2023-03-10 22:52:21,477 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-LeaderElection1] INFO impl.LeaderElection: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-LeaderElection1 PRE_VOTE round 0: result DISCOVERED_A_NEW_TERM (term=12)
dn5_1    | 2023-03-10 22:53:59,805 [178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-LeaderElection5] INFO impl.LeaderElection: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-LeaderElection5: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
dn5_1    | 2023-03-10 22:53:59,805 [178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-LeaderElection5] INFO impl.LeaderElection:   Response 0: 178b30e1-b74d-4f4d-a142-c930eee71455<-e78c5ce1-46ab-4889-a0cd-5903ae46614d#0:OK-t0
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn2_1    | 2023-03-10 22:53:24,425 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00->1a6d358d-6662-4447-914c-d709a67ff716-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00->1a6d358d-6662-4447-914c-d709a67ff716-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm_1    | 2023-03-10 22:52:45,468 [IPC Server handler 6 on default port 9860] INFO upgrade.UpgradeFinalizer: Finalization started.
dn5_1    | 2023-03-10 22:53:59,805 [178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-LeaderElection5] INFO impl.LeaderElection:   Response 1: 178b30e1-b74d-4f4d-a142-c930eee71455<-1a6d358d-6662-4447-914c-d709a67ff716#0:FAIL-t0
dn5_1    | 2023-03-10 22:53:59,806 [178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-LeaderElection5] INFO impl.LeaderElection: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-LeaderElection5 PRE_VOTE round 0: result REJECTED
dn3_1    | 2023-03-10 22:52:21,498 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-LeaderElection3] INFO impl.LeaderElection: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-LeaderElection3: PRE_VOTE DISCOVERED_A_NEW_TERM (term=11) received 1 response(s) and 0 exception(s):
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn2_1    | 2023-03-10 22:53:24,425 [Command processor thread] INFO impl.PendingRequests: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-PendingRequests: sendNotLeaderResponses
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
dn3_1    | 2023-03-10 22:52:21,517 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-LeaderElection3] INFO impl.LeaderElection:   Response 0: 1a6d358d-6662-4447-914c-d709a67ff716<-e78c5ce1-46ab-4889-a0cd-5903ae46614d#0:OK-t11
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-03-10 22:52:21,517 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-LeaderElection3] INFO impl.LeaderElection: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-LeaderElection3 PRE_VOTE round 0: result DISCOVERED_A_NEW_TERM (term=11)
scm_1    | 2023-03-10 22:52:45,470 [IPC Server handler 6 on default port 9860] INFO pipeline.BackgroundPipelineCreator: Stopping RatisPipelineUtilsThread.
scm_1    | 2023-03-10 22:52:45,471 [RatisPipelineUtilsThread - 0] WARN pipeline.BackgroundPipelineCreator: RatisPipelineUtilsThread is interrupted.
dn1_1    | 2023-03-10 22:52:53,489 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn1_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn3_1    | 2023-03-10 22:52:21,518 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-LeaderElection3] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3: changes role from CANDIDATE to FOLLOWER at term 11 for DISCOVERED_A_NEW_TERM (term=11)
dn2_1    | 2023-03-10 22:53:24,420 [grpc-default-executor-0] INFO server.GrpcLogAppender: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00->178b30e1-b74d-4f4d-a142-c930eee71455-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn2_1    | 2023-03-10 22:53:24,438 [grpc-default-executor-0] INFO leader.FollowerInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00->178b30e1-b74d-4f4d-a142-c930eee71455: nextIndex: updateUnconditionally 21 -> 20
dn2_1    | 2023-03-10 22:53:24,445 [grpc-default-executor-3] INFO server.GrpcLogAppender: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00->178b30e1-b74d-4f4d-a142-c930eee71455-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn3_1    | 2023-03-10 22:52:21,518 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-LeaderElection3] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: shutdown 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-LeaderElection3
dn2_1    | 2023-03-10 22:53:24,448 [grpc-default-executor-5] INFO server.GrpcLogAppender: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00->1a6d358d-6662-4447-914c-d709a67ff716-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn2_1    | 2023-03-10 22:53:24,446 [grpc-default-executor-3] INFO leader.FollowerInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00->178b30e1-b74d-4f4d-a142-c930eee71455: nextIndex: updateUnconditionally 20 -> 19
dn2_1    | 2023-03-10 22:53:24,448 [grpc-default-executor-5] INFO leader.FollowerInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00->1a6d358d-6662-4447-914c-d709a67ff716: nextIndex: updateUnconditionally 21 -> 20
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
scm_1    | 2023-03-10 22:52:45,472 [IPC Server handler 6 on default port 9860] INFO upgrade.UpgradeFinalizer: SCM Finalization has crossed checkpoint FINALIZATION_STARTED
scm_1    | 2023-03-10 22:52:45,483 [IPC Server handler 6 on default port 9860] INFO pipeline.PipelineManagerImpl: Container #2 closed for pipeline=PipelineID=cf0cdca9-af45-4cc0-9366-0049fbbc23b3
scm_1    | 2023-03-10 22:52:45,484 [IPC Server handler 6 on default port 9860] INFO pipeline.PipelineManagerImpl: Container #1001 closed for pipeline=PipelineID=cf0cdca9-af45-4cc0-9366-0049fbbc23b3
scm_1    | 2023-03-10 22:52:45,486 [IPC Server handler 6 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: cf0cdca9-af45-4cc0-9366-0049fbbc23b3, Nodes: e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18)e3e4587c-aa42-4e86-ae9a-d3e448365275(ha_dn1_1.ha_net/10.9.0.15)1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:e3e4587c-aa42-4e86-ae9a-d3e448365275, CreationTimestamp2023-03-10T22:51:55.100015Z[UTC]] moved to CLOSED state
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
scm_1    | 2023-03-10 22:52:45,487 [IPC Server handler 6 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 8581ae3d-c50a-4a49-9d70-d98e5419b7a9, Nodes: e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:e78c5ce1-46ab-4889-a0cd-5903ae46614d, CreationTimestamp2023-03-10T22:51:55.081200Z[UTC]] moved to CLOSED state
scm_1    | 2023-03-10 22:52:45,488 [IPC Server handler 6 on default port 9860] INFO pipeline.PipelineManagerImpl: Container #1 closed for pipeline=PipelineID=54b523a6-4d1d-4f07-a057-5b98fdc36f00
scm_1    | 2023-03-10 22:52:45,489 [IPC Server handler 6 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 54b523a6-4d1d-4f07-a057-5b98fdc36f00, Nodes: 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17)178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19)3892a4e1-c878-42af-adb7-db66a90d61f4(ha_dn2_1.ha_net/10.9.0.16), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:3892a4e1-c878-42af-adb7-db66a90d61f4, CreationTimestamp2023-03-10T22:51:55.061933Z[UTC]] moved to CLOSED state
scm_1    | 2023-03-10 22:52:45,489 [IPC Server handler 6 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d1fa3d0d-6abe-48bc-b456-d0b7aa0d159e, Nodes: e3e4587c-aa42-4e86-ae9a-d3e448365275(ha_dn1_1.ha_net/10.9.0.15), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:e3e4587c-aa42-4e86-ae9a-d3e448365275, CreationTimestamp2023-03-10T22:51:55.100810Z[UTC]] moved to CLOSED state
dn3_1    | 2023-03-10 22:52:21,518 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-LeaderElection3] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: start 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-FollowerState
dn3_1    | 2023-03-10 22:52:21,536 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-03-10 22:52:21,541 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm_1    | 2023-03-10 22:52:45,490 [IPC Server handler 6 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: b1358d3e-7575-4e13-af6d-8e287caccc68, Nodes: 178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:178b30e1-b74d-4f4d-a142-c930eee71455, CreationTimestamp2023-03-10T22:51:55.081442Z[UTC]] moved to CLOSED state
scm_1    | 2023-03-10 22:52:45,491 [IPC Server handler 6 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 2d4f1397-2cdc-4336-a562-676bab171c02, Nodes: 3892a4e1-c878-42af-adb7-db66a90d61f4(ha_dn2_1.ha_net/10.9.0.16), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:3892a4e1-c878-42af-adb7-db66a90d61f4, CreationTimestamp2023-03-10T22:51:55.010465Z[UTC]] moved to CLOSED state
scm_1    | 2023-03-10 22:52:45,492 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #2, current state: CLOSING
dn1_1    | 2023-03-10 22:52:53,490 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-03-10 22:52:21,555 [grpc-default-executor-3] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3: receive requestVote(ELECTION, e3e4587c-aa42-4e86-ae9a-d3e448365275, group-0049FBBC23B3, 11, (t:10, i:35))
dn3_1    | 2023-03-10 22:52:21,557 [grpc-default-executor-3] INFO impl.VoteContext: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-FOLLOWER: accept ELECTION from e3e4587c-aa42-4e86-ae9a-d3e448365275: our priority 0 <= candidate's priority 1
dn3_1    | 2023-03-10 22:52:21,559 [grpc-default-executor-3] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3: changes role from  FOLLOWER to FOLLOWER at term 11 for candidate:e3e4587c-aa42-4e86-ae9a-d3e448365275
scm_1    | 2023-03-10 22:52:45,493 [IPC Server handler 6 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 7572f4a9-8e86-407b-b624-5c511402a23e, Nodes: 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:1a6d358d-6662-4447-914c-d709a67ff716, CreationTimestamp2023-03-10T22:51:55.063157Z[UTC]] moved to CLOSED state
scm_1    | 2023-03-10 22:52:45,494 [IPC Server handler 6 on default port 9860] INFO pipeline.PipelineManagerImpl: Container #1002 closed for pipeline=PipelineID=78b03a88-75c1-4060-9962-cbc13a60f575
scm_1    | 2023-03-10 22:52:45,495 [IPC Server handler 6 on default port 9860] INFO pipeline.PipelineManagerImpl: Container #2001 closed for pipeline=PipelineID=78b03a88-75c1-4060-9962-cbc13a60f575
dn1_1    | 2023-03-10 22:52:53,490 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn1_1    | 2023-03-10 22:52:53,491 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn1_1    | 2023-03-10 22:52:53,491 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 2001 to close, current state is: CLOSING
scm_1    | 2023-03-10 22:52:45,498 [IPC Server handler 6 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 78b03a88-75c1-4060-9962-cbc13a60f575, Nodes: 3892a4e1-c878-42af-adb7-db66a90d61f4(ha_dn2_1.ha_net/10.9.0.16)e3e4587c-aa42-4e86-ae9a-d3e448365275(ha_dn1_1.ha_net/10.9.0.15)e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:e78c5ce1-46ab-4889-a0cd-5903ae46614d, CreationTimestamp2023-03-10T22:51:55.080894Z[UTC]] moved to CLOSED state
scm_1    | 2023-03-10 22:52:45,498 [IPC Server handler 6 on default port 9860] INFO upgrade.UpgradeFinalizer:   Existing pipelines and containers will be closed during Upgrade.
scm_1    |   New pipelines creation will remain frozen until Upgrade is finalized.
scm_1    | 2023-03-10 22:52:45,500 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #1001, current state: CLOSING
scm_1    | 2023-03-10 22:52:45,500 [IPC Server handler 6 on default port 9860] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: ERASURE_CODED_STORAGE_SUPPORT.
dn3_1    | 2023-03-10 22:52:21,559 [grpc-default-executor-3] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: shutdown 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-FollowerState
dn3_1    | 2023-03-10 22:52:21,560 [grpc-default-executor-3] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: start 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-FollowerState
dn3_1    | 2023-03-10 22:52:21,567 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-FollowerState] INFO impl.FollowerState: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-FollowerState was interrupted
dn3_1    | 2023-03-10 22:52:21,649 [grpc-default-executor-3] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3 replies to ELECTION vote request: e3e4587c-aa42-4e86-ae9a-d3e448365275<-1a6d358d-6662-4447-914c-d709a67ff716#0:OK-t11. Peer's state: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3:t11, leader=null, voted=e3e4587c-aa42-4e86-ae9a-d3e448365275, raftlog=Memoized:1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-SegmentedRaftLog:OPENED:c35, conf=22: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:21,653 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm_1    | 2023-03-10 22:52:45,502 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #1, current state: CLOSING
dn1_1    | 2023-03-10 22:52:53,492 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn3_1    | 2023-03-10 22:52:21,708 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-10 22:52:21,834 [1a6d358d-6662-4447-914c-d709a67ff716-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-0049FBBC23B3 with new leaderId: e3e4587c-aa42-4e86-ae9a-d3e448365275
dn3_1    | 2023-03-10 22:52:21,849 [1a6d358d-6662-4447-914c-d709a67ff716-server-thread1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3: change Leader from null to e3e4587c-aa42-4e86-ae9a-d3e448365275 at term 11 for appendEntries, leader elected after 32082ms
dn3_1    | 2023-03-10 22:52:21,849 [1a6d358d-6662-4447-914c-d709a67ff716-server-thread2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-5B98FDC36F00 with new leaderId: 3892a4e1-c878-42af-adb7-db66a90d61f4
dn2_1    | 2023-03-10 22:53:24,449 [Command processor thread] INFO impl.StateMachineUpdater: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-StateMachineUpdater: set stopIndex = 20
dn2_1    | 2023-03-10 22:53:24,448 [grpc-default-executor-6] INFO server.GrpcLogAppender: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00->1a6d358d-6662-4447-914c-d709a67ff716-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn3_1    | 2023-03-10 22:52:21,857 [1a6d358d-6662-4447-914c-d709a67ff716-server-thread2] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00: change Leader from null to 3892a4e1-c878-42af-adb7-db66a90d61f4 at term 12 for appendEntries, leader elected after 33484ms
dn3_1    | 2023-03-10 22:52:21,872 [1a6d358d-6662-4447-914c-d709a67ff716-server-thread1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00: set configuration 17: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:21,874 [1a6d358d-6662-4447-914c-d709a67ff716-server-thread1] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-SegmentedRaftLogWorker: Rolling segment log-11_16 to index:16
dn3_1    | 2023-03-10 22:52:21,937 [1a6d358d-6662-4447-914c-d709a67ff716-server-thread2] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3: set configuration 36: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:21,997 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/current/log_inprogress_11 to /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/current/log_11-16
scm_1    | 2023-03-10 22:52:45,503 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #1002, current state: CLOSING
dn2_1    | 2023-03-10 22:53:24,450 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-5B98FDC36F00: Taking a snapshot at:(t:12, i:20) file /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/sm/snapshot.12_20
dn2_1    | 2023-03-10 22:53:24,450 [grpc-default-executor-6] INFO leader.FollowerInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00->1a6d358d-6662-4447-914c-d709a67ff716: nextIndex: updateUnconditionally 20 -> 19
scm_1    | 2023-03-10 22:52:45,504 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #2001, current state: CLOSING
scm_1    | 2023-03-10 22:52:45,545 [IPC Server handler 6 on default port 9860] INFO upgrade.AbstractLayoutVersionManager: Layout feature ERASURE_CODED_STORAGE_SUPPORT has been finalized.
dn2_1    | 2023-03-10 22:53:24,453 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-5B98FDC36F00: Finished taking a snapshot at:(t:12, i:20) file:/data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/sm/snapshot.12_20 took: 3 ms
dn1_1    | 2023-03-10 22:52:53,492 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
scm_1    | 2023-03-10 22:52:45,546 [IPC Server handler 6 on default port 9860] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: DATANODE_SCHEMA_V3.
scm_1    | 2023-03-10 22:52:45,548 [IPC Server handler 6 on default port 9860] INFO upgrade.AbstractLayoutVersionManager: Layout feature DATANODE_SCHEMA_V3 has been finalized.
scm_1    | 2023-03-10 22:52:45,548 [IPC Server handler 6 on default port 9860] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn3_1    | 2023-03-10 22:52:22,019 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/current/log_inprogress_17
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn2_1    | 2023-03-10 22:53:24,455 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-StateMachineUpdater] INFO impl.StateMachineUpdater: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-StateMachineUpdater: Took a snapshot at index 20
dn3_1    | 2023-03-10 22:52:22,021 [1a6d358d-6662-4447-914c-d709a67ff716-server-thread2] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-SegmentedRaftLogWorker: Rolling segment log-22_35 to index:35
dn4_1    | 2023-03-10 22:53:24,922 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn4_1    | 2023-03-10 22:53:24,924 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575-SegmentedRaftLogWorker close()
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1    | 2023-03-10 22:52:45,550 [IPC Server handler 6 on default port 9860] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm_1    | 2023-03-10 22:52:45,551 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode e3e4587c-aa42-4e86-ae9a-d3e448365275(ha_dn1_1.ha_net/10.9.0.15) moved to HEALTHY READONLY state.
dn1_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-03-10 22:52:10,678 [IPC Server handler 1 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for f1622084839d
dn2_1    | 2023-03-10 22:53:24,456 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-StateMachineUpdater] INFO impl.StateMachineUpdater: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-StateMachineUpdater: snapshotIndex: updateIncreasingly 16 -> 20
dn2_1    | 2023-03-10 22:53:24,460 [Command processor thread] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00: closes. applyIndex: 20
dn2_1    | 2023-03-10 22:53:24,461 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn2_1    | 2023-03-10 22:53:24,463 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00-SegmentedRaftLogWorker close()
dn2_1    | 2023-03-10 22:53:24,473 [Command processor thread] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-5B98FDC36F00: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00
dn2_1    | 2023-03-10 22:53:24,475 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=54b523a6-4d1d-4f07-a057-5b98fdc36f00 command on datanode 3892a4e1-c878-42af-adb7-db66a90d61f4.
dn3_1    | 2023-03-10 22:52:22,211 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/current/log_inprogress_22 to /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/current/log_22-35
dn4_1    | 2023-03-10 22:53:24,926 [Command processor thread] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-CBC13A60F575: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575
dn4_1    | 2023-03-10 22:53:24,926 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=78b03a88-75c1-4060-9962-cbc13a60f575 command on datanode e78c5ce1-46ab-4889-a0cd-5903ae46614d.
dn4_1    | 2023-03-10 22:53:25,202 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@18a096b5] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[2(0)], numOfContainers=1, numOfBlocks=1
recon_1  | 2023-03-10 22:52:10,823 [IPC Server handler 24 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for 516736b05cb5
dn3_1    | 2023-03-10 22:52:22,253 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/current/log_inprogress_36
dn3_1    | 2023-03-10 22:52:47,922 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-03-10 22:52:47,936 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn3_1    | 2023-03-10 22:52:47,937 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn4_1    | 2023-03-10 22:53:54,201 [grpc-default-executor-2] INFO server.RaftServer: e78c5ce1-46ab-4889-a0cd-5903ae46614d: addNew group-6265FBDC9D88:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:0|startupRole:FOLLOWER] returns group-6265FBDC9D88:java.util.concurrent.CompletableFuture@6d33aed3[Not completed]
dn4_1    | 2023-03-10 22:53:54,202 [pool-38-thread-1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d: new RaftServerImpl for group-6265FBDC9D88:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn5_1    | 2023-03-10 22:53:59,806 [178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-LeaderElection5] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
dn4_1    | 2023-03-10 22:53:54,203 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-03-10 22:53:24,475 [Command processor thread] INFO server.RaftServer: 3892a4e1-c878-42af-adb7-db66a90d61f4: remove    LEADER 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02:t4, leader=3892a4e1-c878-42af-adb7-db66a90d61f4, voted=3892a4e1-c878-42af-adb7-db66a90d61f4, raftlog=Memoized:3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-SegmentedRaftLog:OPENED:c6, conf=5: peers:[3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
scm_1    | 2023-03-10 22:52:45,551 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=cf0cdca9-af45-4cc0-9366-0049fbbc23b3 in state CLOSED which uses HEALTHY_READONLY datanode e3e4587c-aa42-4e86-ae9a-d3e448365275. This will send close commands for its containers.
dn3_1    | 2023-03-10 22:52:47,938 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: CLOSING
dn5_1    | 2023-03-10 22:53:59,806 [178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-LeaderElection5] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: shutdown 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-LeaderElection5
dn5_1    | 2023-03-10 22:53:59,806 [178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-LeaderElection5] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: start 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-FollowerState
dn2_1    | 2023-03-10 22:53:24,475 [Command processor thread] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02: shutdown
dn2_1    | 2023-03-10 22:53:24,476 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-676BAB171C02,id=3892a4e1-c878-42af-adb7-db66a90d61f4
recon_1  | 2023-03-10 22:52:11,026 [IPC Server handler 33 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for 0de9850545d7
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | 2023-03-10 22:52:53,493 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn1_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn4_1    | 2023-03-10 22:53:54,203 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-03-10 22:53:59,816 [178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-10 22:53:59,816 [178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-03-10 22:54:00,034 [grpc-default-executor-2] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5: receive requestVote(PRE_VOTE, 1a6d358d-6662-4447-914c-d709a67ff716, group-577A15FD12B5, 0, (t:0, i:0))
dn5_1    | 2023-03-10 22:54:00,034 [grpc-default-executor-2] INFO impl.VoteContext: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-FOLLOWER: accept PRE_VOTE from 1a6d358d-6662-4447-914c-d709a67ff716: our priority 0 <= candidate's priority 1
recon_1  | 2023-03-10 22:52:11,076 [IPC Server handler 36 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for b0c55c35d5b9
dn4_1    | 2023-03-10 22:53:54,204 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-03-10 22:52:47,938 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn3_1    | 2023-03-10 22:52:47,938 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn5_1    | 2023-03-10 22:54:00,035 [grpc-default-executor-2] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5 replies to PRE_VOTE vote request: 1a6d358d-6662-4447-914c-d709a67ff716<-178b30e1-b74d-4f4d-a142-c930eee71455#0:OK-t0. Peer's state: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5:t0, leader=null, voted=, raftlog=Memoized:178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:53:24,476 [Command processor thread] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: shutdown 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-LeaderStateImpl
recon_1  | 2023-03-10 22:52:11,299 [IPC Server handler 43 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn5_1.ha_net
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn4_1    | 2023-03-10 22:53:54,204 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn5_1    | 2023-03-10 22:54:00,060 [grpc-default-executor-2] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5: receive requestVote(ELECTION, 1a6d358d-6662-4447-914c-d709a67ff716, group-577A15FD12B5, 1, (t:0, i:0))
dn2_1    | 2023-03-10 22:53:24,476 [Command processor thread] INFO impl.PendingRequests: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-PendingRequests: sendNotLeaderResponses
recon_1  | 2023-03-10 22:52:11,804 [IPC Server handler 42 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
recon_1  | 2023-03-10 22:52:12,595 [IPC Server handler 13 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
recon_1  | 2023-03-10 22:52:12,654 [IPC Server handler 1 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn5_1    | 2023-03-10 22:54:00,061 [grpc-default-executor-2] INFO impl.VoteContext: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-FOLLOWER: accept ELECTION from 1a6d358d-6662-4447-914c-d709a67ff716: our priority 0 <= candidate's priority 1
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
scm_1    | 2023-03-10 22:52:45,551 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d1fa3d0d-6abe-48bc-b456-d0b7aa0d159e in state CLOSED which uses HEALTHY_READONLY datanode e3e4587c-aa42-4e86-ae9a-d3e448365275. This will send close commands for its containers.
dn2_1    | 2023-03-10 22:53:24,478 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-676BAB171C02: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/2d4f1397-2cdc-4336-a562-676bab171c02/sm/snapshot.4_6
dn2_1    | 2023-03-10 22:53:24,479 [Command processor thread] INFO impl.StateMachineUpdater: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-StateMachineUpdater: set stopIndex = 6
dn2_1    | 2023-03-10 22:53:24,482 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-676BAB171C02: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/2d4f1397-2cdc-4336-a562-676bab171c02/sm/snapshot.4_6 took: 4 ms
dn2_1    | 2023-03-10 22:53:24,482 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-StateMachineUpdater] INFO impl.StateMachineUpdater: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-StateMachineUpdater: Took a snapshot at index 6
dn2_1    | 2023-03-10 22:53:24,483 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-StateMachineUpdater] INFO impl.StateMachineUpdater: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
recon_1  | 2023-03-10 22:52:12,834 [IPC Server handler 42 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn2_1    | 2023-03-10 22:53:24,483 [Command processor thread] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02: closes. applyIndex: 6
dn2_1    | 2023-03-10 22:53:24,484 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn2_1    | 2023-03-10 22:53:24,486 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02-SegmentedRaftLogWorker close()
dn2_1    | 2023-03-10 22:53:24,489 [Command processor thread] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-676BAB171C02: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/2d4f1397-2cdc-4336-a562-676bab171c02
dn2_1    | 2023-03-10 22:53:24,490 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=2d4f1397-2cdc-4336-a562-676bab171c02 command on datanode 3892a4e1-c878-42af-adb7-db66a90d61f4.
recon_1  | 2023-03-10 22:52:13,692 [IPC Server handler 37 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-10 22:52:13,957 [IPC Server handler 33 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-10 22:52:45,552 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=78b03a88-75c1-4060-9962-cbc13a60f575 in state CLOSED which uses HEALTHY_READONLY datanode e3e4587c-aa42-4e86-ae9a-d3e448365275. This will send close commands for its containers.
dn2_1    | 2023-03-10 22:53:24,490 [Command processor thread] INFO server.RaftServer: 3892a4e1-c878-42af-adb7-db66a90d61f4: remove  FOLLOWER 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575:t7, leader=e78c5ce1-46ab-4889-a0cd-5903ae46614d, voted=e78c5ce1-46ab-4889-a0cd-5903ae46614d, raftlog=Memoized:3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-SegmentedRaftLog:OPENED:c26, conf=21: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn2_1    | 2023-03-10 22:53:24,490 [Command processor thread] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575: shutdown
dn2_1    | 2023-03-10 22:53:24,491 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-CBC13A60F575,id=3892a4e1-c878-42af-adb7-db66a90d61f4
dn2_1    | 2023-03-10 22:53:24,491 [Command processor thread] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: shutdown 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-FollowerState
dn2_1    | 2023-03-10 22:53:24,492 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-CBC13A60F575: Taking a snapshot at:(t:7, i:26) file /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/sm/snapshot.7_26
scm_1    | 2023-03-10 22:52:45,552 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17) moved to HEALTHY READONLY state.
dn4_1    | 2023-03-10 22:53:54,204 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-03-10 22:53:24,492 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-FollowerState] INFO impl.FollowerState: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-FollowerState was interrupted
dn2_1    | 2023-03-10 22:53:24,492 [Command processor thread] INFO impl.StateMachineUpdater: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-StateMachineUpdater: set stopIndex = 26
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
scm_1    | 2023-03-10 22:52:45,552 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=cf0cdca9-af45-4cc0-9366-0049fbbc23b3 in state CLOSED which uses HEALTHY_READONLY datanode 1a6d358d-6662-4447-914c-d709a67ff716. This will send close commands for its containers.
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn4_1    | 2023-03-10 22:53:54,204 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-03-10 22:53:54,204 [pool-38-thread-1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88: ConfigurationManager, init=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-03-10 22:53:54,204 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm_1    | 2023-03-10 22:52:45,552 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=54b523a6-4d1d-4f07-a057-5b98fdc36f00 in state CLOSED which uses HEALTHY_READONLY datanode 1a6d358d-6662-4447-914c-d709a67ff716. This will send close commands for its containers.
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:52:14,446 [IPC Server handler 96 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn4_1    | 2023-03-10 22:53:54,206 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-03-10 22:53:54,206 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-03-10 22:53:54,207 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-03-10 22:53:54,207 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-03-10 22:52:53,495 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
recon_1  | 2023-03-10 22:52:14,557 [IPC Server handler 13 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn4_1    | 2023-03-10 22:53:54,207 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-03-10 22:54:00,061 [grpc-default-executor-2] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:1a6d358d-6662-4447-914c-d709a67ff716
dn5_1    | 2023-03-10 22:54:00,061 [grpc-default-executor-2] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: shutdown 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-FollowerState
dn5_1    | 2023-03-10 22:54:00,062 [grpc-default-executor-2] INFO impl.RoleInfo: 178b30e1-b74d-4f4d-a142-c930eee71455: start 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-FollowerState
dn1_1    | 2023-03-10 22:52:53,495 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
recon_1  | 2023-03-10 22:52:14,742 [IPC Server handler 27 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn4_1    | 2023-03-10 22:53:54,208 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-03-10 22:54:00,062 [178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-FollowerState] INFO impl.FollowerState: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-FollowerState was interrupted
dn3_1    | 2023-03-10 22:52:47,939 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn1_1    | 2023-03-10 22:52:53,496 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
recon_1  | 2023-03-10 22:52:15,257 [IPC Server handler 43 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn4_1    | 2023-03-10 22:53:54,208 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-03-10 22:54:00,076 [grpc-default-executor-2] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5 replies to ELECTION vote request: 1a6d358d-6662-4447-914c-d709a67ff716<-178b30e1-b74d-4f4d-a142-c930eee71455#0:OK-t1. Peer's state: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5:t1, leader=null, voted=1a6d358d-6662-4447-914c-d709a67ff716, raftlog=Memoized:178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-03-10 22:54:00,086 [178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-03-10 22:54:00,086 [178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-03-10 22:52:53,497 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 2001 to close, current state is: CLOSING
recon_1  | 2023-03-10 22:52:16,346 [IPC Server handler 50 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn4_1    | 2023-03-10 22:53:54,209 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-03-10 22:54:00,248 [178b30e1-b74d-4f4d-a142-c930eee71455-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-577A15FD12B5 with new leaderId: 1a6d358d-6662-4447-914c-d709a67ff716
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn1_1    | 2023-03-10 22:52:53,497 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
scm_1    | 2023-03-10 22:52:45,552 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=7572f4a9-8e86-407b-b624-5c511402a23e in state CLOSED which uses HEALTHY_READONLY datanode 1a6d358d-6662-4447-914c-d709a67ff716. This will send close commands for its containers.
scm_1    | 2023-03-10 22:52:45,553 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19) moved to HEALTHY READONLY state.
scm_1    | 2023-03-10 22:52:45,553 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=54b523a6-4d1d-4f07-a057-5b98fdc36f00 in state CLOSED which uses HEALTHY_READONLY datanode 178b30e1-b74d-4f4d-a142-c930eee71455. This will send close commands for its containers.
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
recon_1  | 2023-03-10 22:52:16,913 [IPC Server handler 26 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn1_1    | 2023-03-10 22:52:53,498 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn5_1    | 2023-03-10 22:54:00,249 [178b30e1-b74d-4f4d-a142-c930eee71455-server-thread1] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5: change Leader from null to 1a6d358d-6662-4447-914c-d709a67ff716 at term 1 for appendEntries, leader elected after 5644ms
dn5_1    | 2023-03-10 22:54:00,283 [178b30e1-b74d-4f4d-a142-c930eee71455-server-thread2] INFO server.RaftServer$Division: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5: set configuration 0: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
recon_1  | 2023-03-10 22:52:20,644 [IPC Server handler 0 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn1_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn5_1    | 2023-03-10 22:54:00,284 [178b30e1-b74d-4f4d-a142-c930eee71455-server-thread2] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 2023-03-10 22:54:00,288 [178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 178b30e1-b74d-4f4d-a142-c930eee71455@group-577A15FD12B5-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/995fa1d6-41cf-4ed4-929b-577a15fd12b5/current/log_inprogress_0
dn5_1    | 2023-03-10 22:54:08,887 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 2/4998 blocks from 1 candidate containers.
recon_1  | 2023-03-10 22:52:20,653 [IPC Server handler 32 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn4_1    | 2023-03-10 22:53:54,209 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn5_1    | 2023-03-10 22:54:08,926 [BlockDeletingService#0] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/current/containerDir0/1/chunks/111677748019200001.block
dn5_1    | 2023-03-10 22:54:08,928 [BlockDeletingService#0] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/current/containerDir0/1/chunks/111677748019200003.block
dn2_1    | 2023-03-10 22:53:24,496 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-CBC13A60F575: Finished taking a snapshot at:(t:7, i:26) file:/data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/sm/snapshot.7_26 took: 4 ms
recon_1  | 2023-03-10 22:52:20,703 [IPC Server handler 27 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn4_1    | 2023-03-10 22:53:54,209 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:52:37,466 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
dn4_1    | 2023-03-10 22:53:54,210 [pool-38-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/a8f227f9-9826-491d-a836-6265fbdc9d88 does not exist. Creating ...
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn2_1    | 2023-03-10 22:53:24,497 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-StateMachineUpdater] INFO impl.StateMachineUpdater: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-StateMachineUpdater: Took a snapshot at index 26
dn2_1    | 2023-03-10 22:53:24,497 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-StateMachineUpdater] INFO impl.StateMachineUpdater: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-StateMachineUpdater: snapshotIndex: updateIncreasingly 20 -> 26
dn4_1    | 2023-03-10 22:53:54,213 [pool-38-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a8f227f9-9826-491d-a836-6265fbdc9d88/in_use.lock acquired by nodename 6@b0c55c35d5b9
dn4_1    | 2023-03-10 22:53:54,216 [pool-38-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/a8f227f9-9826-491d-a836-6265fbdc9d88 has been successfully formatted.
dn3_1    | 2023-03-10 22:52:47,939 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-03-10 22:52:47,939 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn2_1    | 2023-03-10 22:53:24,500 [Command processor thread] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575: closes. applyIndex: 26
dn2_1    | 2023-03-10 22:53:24,500 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn2_1    | 2023-03-10 22:53:24,507 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575-SegmentedRaftLogWorker close()
scm_1    | 2023-03-10 22:52:45,553 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=b1358d3e-7575-4e13-af6d-8e287caccc68 in state CLOSED which uses HEALTHY_READONLY datanode 178b30e1-b74d-4f4d-a142-c930eee71455. This will send close commands for its containers.
dn3_1    | 2023-03-10 22:52:47,939 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
recon_1  | 2023-03-10 22:52:37,485 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-03-10 22:53:54,221 [pool-38-thread-1] INFO ratis.ContainerStateMachine: group-6265FBDC9D88: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
scm_1    | 2023-03-10 22:52:45,554 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18) moved to HEALTHY READONLY state.
dn3_1    | 2023-03-10 22:52:47,939 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: CLOSING
recon_1  | 2023-03-10 22:52:37,588 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 8 pipelines in house.
dn2_1    | 2023-03-10 22:53:24,513 [Command processor thread] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-CBC13A60F575: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575
dn2_1    | 2023-03-10 22:53:24,514 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=78b03a88-75c1-4060-9962-cbc13a60f575 command on datanode 3892a4e1-c878-42af-adb7-db66a90d61f4.
dn1_1    | 2023-03-10 22:52:53,498 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn1_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
scm_1    | 2023-03-10 22:52:45,554 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=cf0cdca9-af45-4cc0-9366-0049fbbc23b3 in state CLOSED which uses HEALTHY_READONLY datanode e78c5ce1-46ab-4889-a0cd-5903ae46614d. This will send close commands for its containers.
recon_1  | 2023-03-10 22:52:37,635 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 113 milliseconds.
recon_1  | 2023-03-10 22:52:38,084 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 582 milliseconds to process 4 existing database records.
dn5_1    | 2023-03-10 22:55:08,895 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 2023-03-10 22:56:08,896 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 2023-03-10 22:57:08,897 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1    | 2023-03-10 22:52:45,554 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=8581ae3d-c50a-4a49-9d70-d98e5419b7a9 in state CLOSED which uses HEALTHY_READONLY datanode e78c5ce1-46ab-4889-a0cd-5903ae46614d. This will send close commands for its containers.
scm_1    | 2023-03-10 22:52:45,554 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=78b03a88-75c1-4060-9962-cbc13a60f575 in state CLOSED which uses HEALTHY_READONLY datanode e78c5ce1-46ab-4889-a0cd-5903ae46614d. This will send close commands for its containers.
dn2_1    | 2023-03-10 22:53:24,646 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@28cd2c2] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(0)], numOfContainers=1, numOfBlocks=2
dn4_1    | 2023-03-10 22:53:54,221 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
scm_1    | 2023-03-10 22:52:45,554 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 3892a4e1-c878-42af-adb7-db66a90d61f4(ha_dn2_1.ha_net/10.9.0.16) moved to HEALTHY READONLY state.
dn3_1    | 2023-03-10 22:52:47,939 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
recon_1  | 2023-03-10 22:52:38,114 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 29 milliseconds for processing 5 containers.
dn2_1    | 2023-03-10 22:53:24,906 [grpc-default-executor-5] INFO server.GrpcServerProtocolService: 3892a4e1-c878-42af-adb7-db66a90d61f4: Completed APPEND_ENTRIES, lastRequest: null
dn4_1    | 2023-03-10 22:53:54,221 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-03-10 22:53:54,222 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1    | 2023-03-10 22:52:45,554 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=54b523a6-4d1d-4f07-a057-5b98fdc36f00 in state CLOSED which uses HEALTHY_READONLY datanode 3892a4e1-c878-42af-adb7-db66a90d61f4. This will send close commands for its containers.
dn3_1    | 2023-03-10 22:52:47,940 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
recon_1  | 2023-03-10 22:52:43,954 [IPC Server handler 33 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn2_1    | 2023-03-10 22:53:24,907 [grpc-default-executor-6] INFO server.GrpcServerProtocolService: 3892a4e1-c878-42af-adb7-db66a90d61f4: Completed APPEND_ENTRIES, lastRequest: e78c5ce1-46ab-4889-a0cd-5903ae46614d->3892a4e1-c878-42af-adb7-db66a90d61f4#314-t7,previous=(t:7, i:25),leaderCommit=25,initializing? true,entries: size=1, first=(t:7, i:26), METADATAENTRY(c:25)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
scm_1    | 2023-03-10 22:52:45,554 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=2d4f1397-2cdc-4336-a562-676bab171c02 in state CLOSED which uses HEALTHY_READONLY datanode 3892a4e1-c878-42af-adb7-db66a90d61f4. This will send close commands for its containers.
scm_1    | 2023-03-10 22:52:45,554 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=78b03a88-75c1-4060-9962-cbc13a60f575 in state CLOSED which uses HEALTHY_READONLY datanode 3892a4e1-c878-42af-adb7-db66a90d61f4. This will send close commands for its containers.
scm_1    | 2023-03-10 22:52:45,563 [IPC Server handler 6 on default port 9860] INFO upgrade.UpgradeFinalizer: SCM Finalization has crossed checkpoint MLV_EQUALS_SLV
recon_1  | 2023-03-10 22:52:46,797 [IPC Server handler 42 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn2_1    | 2023-03-10 22:53:54,740 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | 2023-03-10 22:54:10,308 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 2/4998 blocks from 1 candidate containers.
dn2_1    | 2023-03-10 22:54:10,360 [BlockDeletingService#2] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/current/containerDir0/1/chunks/111677748019200001.block
dn4_1    | 2023-03-10 22:53:54,222 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-03-10 22:53:54,222 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn2_1    | 2023-03-10 22:54:10,362 [BlockDeletingService#2] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/current/containerDir0/1/chunks/111677748019200003.block
scm_1    | 2023-03-10 22:52:45,563 [IPC Server handler 6 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm_1    | 2023-03-10 22:52:45,565 [RatisPipelineUtilsThread - 0] ERROR pipeline.PipelinePlacementPolicy: No healthy node found to allocate container.
dn4_1    | 2023-03-10 22:53:54,226 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
recon_1  | 2023-03-10 22:52:47,815 [IPC Server handler 28 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn2_1    | 2023-03-10 22:54:24,923 [pool-34-thread-1] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4: new RaftServerImpl for group-C7BE60589B89:[3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:10.9.0.16:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
scm_1    | 2023-03-10 22:52:46,797 [IPC Server handler 76 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-10 22:52:47,814 [IPC Server handler 76 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn4_1    | 2023-03-10 22:53:54,227 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-03-10 22:53:54,228 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
scm_1    | 2023-03-10 22:52:47,894 [IPC Server handler 66 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-10 22:52:47,924 [IPC Server handler 74 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn4_1    | 2023-03-10 22:53:54,228 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: new e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a8f227f9-9826-491d-a836-6265fbdc9d88
recon_1  | 2023-03-10 22:52:47,818 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: Container #2 has state OPEN, but given state is CLOSING.
dn4_1    | 2023-03-10 22:53:54,230 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-03-10 22:54:24,929 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
recon_1  | 2023-03-10 22:52:47,905 [IPC Server handler 26 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn4_1    | 2023-03-10 22:53:54,230 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-03-10 22:54:24,929 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
recon_1  | 2023-03-10 22:52:47,908 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: Container #1001 has state OPEN, but given state is CLOSING.
scm_1    | 2023-03-10 22:52:50,564 [IPC Server handler 6 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn4_1    | 2023-03-10 22:53:54,230 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-03-10 22:54:24,929 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
recon_1  | 2023-03-10 22:52:47,924 [IPC Server handler 31 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-10 22:52:50,631 [IPC Server handler 81 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn4_1    | 2023-03-10 22:53:54,231 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-03-10 22:54:24,929 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-03-10 22:54:24,929 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1  | 2023-03-10 22:52:47,927 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: Container #1 has state OPEN, but given state is CLOSING.
scm_1    | 2023-03-10 22:52:50,642 [IPC Server handler 80 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn4_1    | 2023-03-10 22:53:54,231 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-03-10 22:54:24,929 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
recon_1  | 2023-03-10 22:52:50,628 [IPC Server handler 21 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn4_1    | 2023-03-10 22:53:54,231 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-03-10 22:54:24,930 [pool-34-thread-1] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89: ConfigurationManager, init=-1: peers:[3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:10.9.0.16:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-03-10 22:54:24,930 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm_1    | 2023-03-10 22:52:50,689 [IPC Server handler 78 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-10 22:52:50,638 [IPC Server handler 18 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-10 22:52:50,681 [IPC Server handler 27 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-10 22:52:51,643 [IPC Server handler 17 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn4_1    | 2023-03-10 22:53:54,231 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-03-10 22:54:24,938 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-03-10 22:54:24,938 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1    | 2023-03-10 22:52:51,649 [IPC Server handler 80 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-10 22:52:51,711 [IPC Server handler 38 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn4_1    | 2023-03-10 22:53:54,233 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
recon_1  | 2023-03-10 22:52:51,713 [IPC Server handler 37 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-10 22:52:51,723 [IPC Server handler 76 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn4_1    | 2023-03-10 22:53:54,235 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
recon_1  | 2023-03-10 22:52:51,948 [IPC Server handler 41 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-10 22:52:51,726 [IPC Server handler 66 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-10 22:52:51,929 [IPC Server handler 74 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn4_1    | 2023-03-10 22:53:54,237 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1  | 2023-03-10 22:52:52,004 [IPC Server handler 35 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn4_1    | 2023-03-10 22:53:54,242 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
recon_1  | 2023-03-10 22:52:52,005 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Container #1002 has state OPEN, but given state is CLOSING.
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | 2023-03-10 22:52:53,499 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn1_1    | 2023-03-10 22:52:53,499 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn4_1    | 2023-03-10 22:53:54,242 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
recon_1  | 2023-03-10 22:52:52,316 [IPC Server handler 50 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:52:52,384 [IPC Server handler 54 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn2_1    | 2023-03-10 22:54:24,938 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-03-10 22:54:24,938 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-03-10 22:53:54,243 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-03-10 22:53:54,243 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-03-10 22:54:24,938 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-03-10 22:52:53,499 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
recon_1  | 2023-03-10 22:52:52,385 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode e3e4587c-aa42-4e86-ae9a-d3e448365275(ha_dn1_1.ha_net/10.9.0.15) reported CLOSED replica.
recon_1  | 2023-03-10 22:52:52,511 [IPC Server handler 13 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn3_1    | 2023-03-10 22:52:47,940 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn4_1    | 2023-03-10 22:53:54,243 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-03-10 22:54:24,940 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-03-10 22:52:53,499 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 2001 to close, current state is: CLOSING
scm_1    | 2023-03-10 22:52:52,011 [IPC Server handler 75 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-10 22:52:52,512 [FixedThreadPoolWithAffinityExecutor-8-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode 3892a4e1-c878-42af-adb7-db66a90d61f4(ha_dn2_1.ha_net/10.9.0.16) reported CLOSED replica.
recon_1  | 2023-03-10 22:52:52,697 [IPC Server handler 37 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn4_1    | 2023-03-10 22:53:54,244 [pool-38-thread-1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88: start as a follower, conf=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:54:24,940 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-03-10 22:52:53,499 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
recon_1  | 2023-03-10 22:52:52,702 [IPC Server handler 38 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-10 22:52:52,728 [IPC Server handler 42 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn2_1    | 2023-03-10 22:54:24,940 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-03-10 22:52:53,500 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn1_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
scm_1    | 2023-03-10 22:52:52,340 [IPC Server handler 93 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-10 22:52:52,374 [IPC Server handler 87 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-10 22:52:52,376 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode e3e4587c-aa42-4e86-ae9a-d3e448365275(ha_dn1_1.ha_net/10.9.0.15) reported CLOSED replica.
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | 2023-03-10 22:52:53,500 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
scm_1    | 2023-03-10 22:52:52,510 [IPC Server handler 4 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-10 22:52:52,513 [FixedThreadPoolWithAffinityExecutor-8-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode 3892a4e1-c878-42af-adb7-db66a90d61f4(ha_dn2_1.ha_net/10.9.0.16) reported CLOSED replica.
scm_1    | 2023-03-10 22:52:52,694 [IPC Server handler 78 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn1_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
scm_1    | 2023-03-10 22:52:52,705 [IPC Server handler 76 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-10 22:52:52,713 [IPC Server handler 66 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn2_1    | 2023-03-10 22:54:24,940 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-03-10 22:53:54,244 [pool-38-thread-1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
recon_1  | 2023-03-10 22:52:52,783 [IPC Server handler 28 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn2_1    | 2023-03-10 22:54:24,940 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-03-10 22:54:24,947 [Command processor thread] INFO server.RaftServer: 3892a4e1-c878-42af-adb7-db66a90d61f4: addNew group-C7BE60589B89:[3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:10.9.0.16:9858|priority:1|startupRole:FOLLOWER] returns group-C7BE60589B89:java.util.concurrent.CompletableFuture@31c4c90b[Completed normally]
scm_1    | 2023-03-10 22:52:52,778 [IPC Server handler 74 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-10 22:52:52,810 [IPC Server handler 75 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
recon_1  | 2023-03-10 22:52:52,800 [IPC Server handler 24 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn2_1    | 2023-03-10 22:54:24,949 [pool-34-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/808ced26-8679-46a5-9ee3-c7be60589b89 does not exist. Creating ...
scm_1    | 2023-03-10 22:52:53,130 [IPC Server handler 69 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-10 22:52:53,197 [IPC Server handler 72 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
scm_1    | 2023-03-10 22:52:53,198 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1002 to CLOSED state, datanode e3e4587c-aa42-4e86-ae9a-d3e448365275(ha_dn1_1.ha_net/10.9.0.15) reported CLOSED replica.
scm_1    | 2023-03-10 22:52:53,262 [IPC Server handler 27 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn2_1    | 2023-03-10 22:54:24,953 [pool-34-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/808ced26-8679-46a5-9ee3-c7be60589b89/in_use.lock acquired by nodename 7@f1622084839d
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
scm_1    | 2023-03-10 22:52:53,264 [IPC Server handler 8 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-10 22:52:53,265 [FixedThreadPoolWithAffinityExecutor-1-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to CLOSED state, datanode 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17) reported CLOSED replica.
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn2_1    | 2023-03-10 22:54:24,957 [pool-34-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/808ced26-8679-46a5-9ee3-c7be60589b89 has been successfully formatted.
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-03-10 22:53:54,245 [pool-38-thread-1] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: start e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-FollowerState
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn2_1    | 2023-03-10 22:54:24,957 [pool-34-thread-1] INFO ratis.ContainerStateMachine: group-C7BE60589B89: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-03-10 22:52:47,940 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | 2023-03-10 22:54:24,957 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-03-10 22:54:24,957 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 2023-03-10 22:54:24,957 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-03-10 22:54:24,957 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-03-10 22:52:53,501 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
recon_1  | 2023-03-10 22:52:53,125 [IPC Server handler 36 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn2_1    | 2023-03-10 22:54:24,957 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-03-10 22:52:53,501 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn3_1    | 2023-03-10 22:52:47,940 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
recon_1  | 2023-03-10 22:52:53,206 [IPC Server handler 43 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn2_1    | 2023-03-10 22:54:24,958 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-03-10 22:52:47,940 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn3_1    | 2023-03-10 22:52:47,941 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: CLOSING
recon_1  | 2023-03-10 22:52:53,207 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1002 to CLOSED state, datanode e3e4587c-aa42-4e86-ae9a-d3e448365275(ha_dn1_1.ha_net/10.9.0.15) reported CLOSED replica.
dn2_1    | 2023-03-10 22:54:24,958 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1    | 2023-03-10 22:52:53,322 [IPC Server handler 97 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-10 22:52:53,401 [IPC Server handler 90 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn3_1    | 2023-03-10 22:52:47,941 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn3_1    | 2023-03-10 22:52:47,941 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
recon_1  | 2023-03-10 22:52:53,269 [IPC Server handler 50 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn2_1    | 2023-03-10 22:54:24,958 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
recon_1  | 2023-03-10 22:52:53,270 [IPC Server handler 44 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn2_1    | 2023-03-10 22:54:24,958 [pool-34-thread-1] INFO segmented.SegmentedRaftLogWorker: new 3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/808ced26-8679-46a5-9ee3-c7be60589b89
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
recon_1  | 2023-03-10 22:52:53,272 [FixedThreadPoolWithAffinityExecutor-1-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to CLOSED state, datanode 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17) reported CLOSED replica.
dn2_1    | 2023-03-10 22:54:24,958 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
recon_1  | 2023-03-10 22:52:53,334 [IPC Server handler 54 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn2_1    | 2023-03-10 22:54:24,958 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
recon_1  | 2023-03-10 22:52:53,402 [IPC Server handler 69 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn2_1    | 2023-03-10 22:54:24,958 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-03-10 22:54:24,958 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm_1    | 2023-03-10 22:52:53,407 [IPC Server handler 89 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-10 22:52:53,437 [IPC Server handler 4 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-10 22:52:53,464 [IPC Server handler 2 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-10 22:52:53,632 [IPC Server handler 77 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn2_1    | 2023-03-10 22:54:24,958 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-03-10 22:54:24,958 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-03-10 22:54:24,958 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-03-10 22:54:24,958 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-03-10 22:53:54,251 [pool-38-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6265FBDC9D88,id=e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn4_1    | 2023-03-10 22:53:54,251 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-03-10 22:53:54,251 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-03-10 22:53:54,251 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-03-10 22:54:24,959 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-03-10 22:54:25,002 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-03-10 22:54:25,061 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-03-10 22:54:25,072 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1    | 2023-03-10 22:52:53,658 [IPC Server handler 78 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-10 22:52:53,728 [IPC Server handler 74 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-10 22:52:53,730 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #2001 to CLOSED state, datanode e3e4587c-aa42-4e86-ae9a-d3e448365275(ha_dn1_1.ha_net/10.9.0.15) reported CLOSED replica.
scm_1    | 2023-03-10 22:52:53,745 [IPC Server handler 75 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn2_1    | 2023-03-10 22:54:25,073 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn4_1    | 2023-03-10 22:53:54,252 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-03-10 22:54:25,075 [pool-34-thread-1] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-03-10 22:54:25,075 [pool-34-thread-1] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-03-10 22:54:25,084 [pool-34-thread-1] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89: start as a follower, conf=-1: peers:[3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:10.9.0.16:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:54:25,085 [pool-34-thread-1] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn2_1    | 2023-03-10 22:54:25,085 [pool-34-thread-1] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: start 3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-FollowerState
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-03-10 22:53:54,257 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-10 22:53:54,270 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-10 22:53:54,878 [pool-38-thread-1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d: new RaftServerImpl for group-577A15FD12B5:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn4_1    | 2023-03-10 22:53:54,880 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-03-10 22:54:25,092 [pool-34-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C7BE60589B89,id=3892a4e1-c878-42af-adb7-db66a90d61f4
dn3_1    | 2023-03-10 22:52:47,941 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn4_1    | 2023-03-10 22:53:54,880 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-03-10 22:53:54,880 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-03-10 22:53:54,880 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-03-10 22:54:25,101 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1    | 2023-03-10 22:52:53,839 [IPC Server handler 69 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn2_1    | 2023-03-10 22:54:25,102 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm_1    | 2023-03-10 22:52:55,478 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=cf0cdca9-af45-4cc0-9366-0049fbbc23b3 since it stays at CLOSED stage.
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn4_1    | 2023-03-10 22:53:54,881 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-03-10 22:53:54,881 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-03-10 22:52:53,502 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn2_1    | 2023-03-10 22:54:25,102 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
recon_1  | 2023-03-10 22:52:53,404 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Container #2001 has state OPEN, but given state is CLOSING.
recon_1  | 2023-03-10 22:52:53,413 [IPC Server handler 84 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn4_1    | 2023-03-10 22:53:54,881 [pool-38-thread-1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5: ConfigurationManager, init=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-03-10 22:53:54,881 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-03-10 22:54:25,102 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn4_1    | 2023-03-10 22:53:54,882 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-03-10 22:52:53,502 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 2001 to close, current state is: CLOSING
dn1_1    | 2023-03-10 22:52:53,506 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn1_1    | 2023-03-10 22:52:53,507 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn1_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
scm_1    | 2023-03-10 22:52:55,481 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=cf0cdca9-af45-4cc0-9366-0049fbbc23b3 close command to datanode e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn2_1    | 2023-03-10 22:54:25,105 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-03-10 22:54:25,105 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
scm_1    | 2023-03-10 22:52:55,481 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=cf0cdca9-af45-4cc0-9366-0049fbbc23b3 close command to datanode e3e4587c-aa42-4e86-ae9a-d3e448365275
dn4_1    | 2023-03-10 22:53:54,883 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-03-10 22:54:25,110 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=808ced26-8679-46a5-9ee3-c7be60589b89
dn2_1    | 2023-03-10 22:54:25,111 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=808ced26-8679-46a5-9ee3-c7be60589b89.
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
scm_1    | 2023-03-10 22:52:55,481 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=cf0cdca9-af45-4cc0-9366-0049fbbc23b3 close command to datanode 1a6d358d-6662-4447-914c-d709a67ff716
dn4_1    | 2023-03-10 22:53:54,883 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-03-10 22:54:30,197 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-FollowerState] INFO impl.FollowerState: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5112243971ns, electionTimeout:5092ms
dn2_1    | 2023-03-10 22:54:30,198 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-FollowerState] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: shutdown 3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-FollowerState
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-03-10 22:52:52,545 [ContainerOp-54b523a6-4d1d-4f07-a057-5b98fdc36f00-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 15.
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn4_1    | 2023-03-10 22:53:54,884 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-03-10 22:54:30,198 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-FollowerState] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn2_1    | 2023-03-10 22:54:30,198 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-03-10 22:52:52,547 [ContainerOp-54b523a6-4d1d-4f07-a057-5b98fdc36f00-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 15.
dn3_1    | 2023-03-10 22:52:52,584 [ContainerOp-cf0cdca9-af45-4cc0-9366-0049fbbc23b3-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 33.
scm_1    | 2023-03-10 22:52:55,484 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: cf0cdca9-af45-4cc0-9366-0049fbbc23b3, Nodes: e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18)e3e4587c-aa42-4e86-ae9a-d3e448365275(ha_dn1_1.ha_net/10.9.0.15)1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:e3e4587c-aa42-4e86-ae9a-d3e448365275, CreationTimestamp2023-03-10T22:51:55.100015Z[UTC]] removed.
scm_1    | 2023-03-10 22:52:55,485 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=8581ae3d-c50a-4a49-9d70-d98e5419b7a9 since it stays at CLOSED stage.
dn4_1    | 2023-03-10 22:53:54,884 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-03-10 22:54:30,198 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-FollowerState] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: start 3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4
dn2_1    | 2023-03-10 22:54:30,203 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4] INFO impl.LeaderElection: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:10.9.0.16:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:52,626 [ContainerOp-cf0cdca9-af45-4cc0-9366-0049fbbc23b3-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 33.
scm_1    | 2023-03-10 22:52:55,485 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=8581ae3d-c50a-4a49-9d70-d98e5419b7a9 close command to datanode e78c5ce1-46ab-4889-a0cd-5903ae46614d
scm_1    | 2023-03-10 22:52:55,485 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 8581ae3d-c50a-4a49-9d70-d98e5419b7a9, Nodes: e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:e78c5ce1-46ab-4889-a0cd-5903ae46614d, CreationTimestamp2023-03-10T22:51:55.081200Z[UTC]] removed.
dn4_1    | 2023-03-10 22:53:54,886 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-03-10 22:54:30,204 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4] INFO impl.LeaderElection: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4 PRE_VOTE round 0: result PASSED (term=0)
dn2_1    | 2023-03-10 22:54:30,211 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4] INFO impl.LeaderElection: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4 ELECTION round 0: submit vote requests at term 1 for -1: peers:[3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:10.9.0.16:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:52:52,654 [ContainerOp-54b523a6-4d1d-4f07-a057-5b98fdc36f00-0] INFO keyvalue.KeyValueContainer: Container 1 is closed with bcsId 15.
scm_1    | 2023-03-10 22:52:55,485 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=54b523a6-4d1d-4f07-a057-5b98fdc36f00 since it stays at CLOSED stage.
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn2_1    | 2023-03-10 22:54:30,211 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4] INFO impl.LeaderElection: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4 ELECTION round 0: result PASSED (term=1)
dn2_1    | 2023-03-10 22:54:30,211 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: shutdown 3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4
dn3_1    | 2023-03-10 22:52:52,655 [ContainerOp-cf0cdca9-af45-4cc0-9366-0049fbbc23b3-0] INFO keyvalue.KeyValueContainer: Container 2 is closed with bcsId 33.
recon_1  | 2023-03-10 22:52:53,440 [IPC Server handler 70 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-10 22:52:53,462 [IPC Server handler 72 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn3_1    | 2023-03-10 22:52:53,193 [ContainerOp-cf0cdca9-af45-4cc0-9366-0049fbbc23b3-1] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 20.
dn3_1    | 2023-03-10 22:52:53,194 [ContainerOp-cf0cdca9-af45-4cc0-9366-0049fbbc23b3-1] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 20.
recon_1  | 2023-03-10 22:52:53,632 [IPC Server handler 18 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn2_1    | 2023-03-10 22:54:30,212 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn2_1    | 2023-03-10 22:54:30,212 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-C7BE60589B89 with new leaderId: 3892a4e1-c878-42af-adb7-db66a90d61f4
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn2_1    | 2023-03-10 22:54:30,212 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89: change Leader from null to 3892a4e1-c878-42af-adb7-db66a90d61f4 at term 1 for becomeLeader, leader elected after 5273ms
dn4_1    | 2023-03-10 22:53:54,887 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | 2023-03-10 22:52:53,507 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
recon_1  | 2023-03-10 22:52:53,655 [IPC Server handler 5 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-10 22:52:55,485 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=54b523a6-4d1d-4f07-a057-5b98fdc36f00 close command to datanode 1a6d358d-6662-4447-914c-d709a67ff716
dn3_1    | 2023-03-10 22:52:53,255 [ContainerOp-cf0cdca9-af45-4cc0-9366-0049fbbc23b3-1] INFO keyvalue.KeyValueContainer: Container 1001 is closed with bcsId 20.
dn1_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn4_1    | 2023-03-10 22:53:54,887 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
recon_1  | 2023-03-10 22:52:53,708 [IPC Server handler 42 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn2_1    | 2023-03-10 22:54:30,212 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn2_1    | 2023-03-10 22:54:30,215 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
scm_1    | 2023-03-10 22:52:55,486 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=54b523a6-4d1d-4f07-a057-5b98fdc36f00 close command to datanode 178b30e1-b74d-4f4d-a142-c930eee71455
dn2_1    | 2023-03-10 22:54:30,215 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
scm_1    | 2023-03-10 22:52:55,486 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=54b523a6-4d1d-4f07-a057-5b98fdc36f00 close command to datanode 3892a4e1-c878-42af-adb7-db66a90d61f4
dn2_1    | 2023-03-10 22:54:30,216 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-03-10 22:53:11,309 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-03-10 22:53:18,914 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
recon_1  | 2023-03-10 22:52:53,709 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #2001 to CLOSED state, datanode e3e4587c-aa42-4e86-ae9a-d3e448365275(ha_dn1_1.ha_net/10.9.0.15) reported CLOSED replica.
dn2_1    | 2023-03-10 22:54:30,216 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn2_1    | 2023-03-10 22:54:30,217 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn2_1    | 2023-03-10 22:54:30,217 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-03-10 22:54:30,217 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn4_1    | 2023-03-10 22:53:54,887 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1  | 2023-03-10 22:52:53,752 [IPC Server handler 28 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-10 22:52:53,816 [IPC Server handler 29 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-10 22:53:03,931 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
scm_1    | 2023-03-10 22:52:55,486 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 54b523a6-4d1d-4f07-a057-5b98fdc36f00, Nodes: 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17)178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19)3892a4e1-c878-42af-adb7-db66a90d61f4(ha_dn2_1.ha_net/10.9.0.16), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:3892a4e1-c878-42af-adb7-db66a90d61f4, CreationTimestamp2023-03-10T22:51:55.061933Z[UTC]] removed.
scm_1    | 2023-03-10 22:52:55,486 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=d1fa3d0d-6abe-48bc-b456-d0b7aa0d159e since it stays at CLOSED stage.
dn2_1    | 2023-03-10 22:54:30,246 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4] INFO impl.RoleInfo: 3892a4e1-c878-42af-adb7-db66a90d61f4: start 3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderStateImpl
dn2_1    | 2023-03-10 22:54:30,247 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-SegmentedRaftLogWorker: Starting segment from index:0
recon_1  | 2023-03-10 22:53:03,934 [pool-27-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-03-10 22:53:03,934 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-03-10 22:53:03,935 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-03-10 22:53:03,935 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-03-10 22:53:03,935 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-03-10 22:53:03,935 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-03-10 22:53:03,935 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1  | 2023-03-10 22:53:03,935 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 102 
recon_1  | 2023-03-10 22:53:04,101 [pool-27-thread-1] WARN impl.OzoneManagerServiceProviderImpl: Unable to get and apply delta updates from OM.
dn3_1    | 2023-03-10 22:53:18,914 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn3_1    | 2023-03-10 22:53:18,914 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn3_1    | 2023-03-10 22:53:18,914 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: ERASURE_CODED_STORAGE_SUPPORT.
dn3_1    | 2023-03-10 22:53:18,917 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature ERASURE_CODED_STORAGE_SUPPORT has been finalized.
dn3_1    | 2023-03-10 22:53:18,917 [Command processor thread] INFO upgrade.UpgradeFinalizer: Running finalization actions for layout feature: DATANODE_SCHEMA_V3
dn3_1    | 2023-03-10 22:53:18,917 [Command processor thread] INFO upgrade.DatanodeSchemaV3FinalizeAction: Upgrading Datanode volume layout for Schema V3 support.
dn3_1    | 2023-03-10 22:53:18,975 [Command processor thread] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-bd09cc52-c0ff-44b7-8581-ef547d85d522/container.db to cache
dn3_1    | 2023-03-10 22:53:18,975 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-bd09cc52-c0ff-44b7-8581-ef547d85d522/container.db for volume DS-bd09cc52-c0ff-44b7-8581-ef547d85d522
dn3_1    | 2023-03-10 22:53:18,977 [Command processor thread] INFO utils.DatanodeStoreCache: Removed db /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-bd09cc52-c0ff-44b7-8581-ef547d85d522/container.db from cache
dn3_1    | 2023-03-10 22:53:18,977 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is stopped at /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-bd09cc52-c0ff-44b7-8581-ef547d85d522/container.db for volume DS-bd09cc52-c0ff-44b7-8581-ef547d85d522
dn3_1    | 2023-03-10 22:53:19,021 [Command processor thread] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-bd09cc52-c0ff-44b7-8581-ef547d85d522/container.db to cache
scm_1    | 2023-03-10 22:52:55,486 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=d1fa3d0d-6abe-48bc-b456-d0b7aa0d159e close command to datanode e3e4587c-aa42-4e86-ae9a-d3e448365275
scm_1    | 2023-03-10 22:52:55,487 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: d1fa3d0d-6abe-48bc-b456-d0b7aa0d159e, Nodes: e3e4587c-aa42-4e86-ae9a-d3e448365275(ha_dn1_1.ha_net/10.9.0.15), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:e3e4587c-aa42-4e86-ae9a-d3e448365275, CreationTimestamp2023-03-10T22:51:55.100810Z[UTC]] removed.
recon_1  | INTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: Invalid transaction log iterator when getting updates since sequence number 102
recon_1  | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:709)
recon_1  | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getDBUpdates(OzoneManagerProtocolClientSideTranslatorPB.java:1959)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:443)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
scm_1    | 2023-03-10 22:52:55,487 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=b1358d3e-7575-4e13-af6d-8e287caccc68 since it stays at CLOSED stage.
scm_1    | 2023-03-10 22:52:55,487 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=b1358d3e-7575-4e13-af6d-8e287caccc68 close command to datanode 178b30e1-b74d-4f4d-a142-c930eee71455
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:53:04,102 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1  | 2023-03-10 22:53:04,483 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1678488784103
scm_1    | 2023-03-10 22:52:55,489 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: b1358d3e-7575-4e13-af6d-8e287caccc68, Nodes: 178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:178b30e1-b74d-4f4d-a142-c930eee71455, CreationTimestamp2023-03-10T22:51:55.081442Z[UTC]] removed.
scm_1    | 2023-03-10 22:52:55,489 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=2d4f1397-2cdc-4336-a562-676bab171c02 since it stays at CLOSED stage.
recon_1  | 2023-03-10 22:53:04,484 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Cleaning up old OM snapshot db at /data/metadata/om.snapshot.db_1678488594284.
recon_1  | 2023-03-10 22:53:04,506 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-03-10 22:53:04,507 [pool-27-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 2023-03-10 22:53:19,022 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-bd09cc52-c0ff-44b7-8581-ef547d85d522/container.db for volume DS-bd09cc52-c0ff-44b7-8581-ef547d85d522
dn3_1    | 2023-03-10 22:53:19,023 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature DATANODE_SCHEMA_V3 has been finalized.
dn4_1    | 2023-03-10 22:53:54,887 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1    | 2023-03-10 22:52:55,490 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=2d4f1397-2cdc-4336-a562-676bab171c02 close command to datanode 3892a4e1-c878-42af-adb7-db66a90d61f4
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-03-10 22:53:54,888 [Command processor thread] INFO server.RaftServer: e78c5ce1-46ab-4889-a0cd-5903ae46614d: addNew group-577A15FD12B5:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] returns      null e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5:t0, leader=null, voted=null, raftlog=UNINITIALIZED, conf=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null NEW
dn4_1    | 2023-03-10 22:53:54,891 [pool-38-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/995fa1d6-41cf-4ed4-929b-577a15fd12b5 does not exist. Creating ...
dn4_1    | 2023-03-10 22:53:54,894 [pool-38-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/995fa1d6-41cf-4ed4-929b-577a15fd12b5/in_use.lock acquired by nodename 6@b0c55c35d5b9
dn1_1    | 2023-03-10 22:52:53,664 [ContainerOp-78b03a88-75c1-4060-9962-cbc13a60f575-1] INFO keyvalue.KeyValueContainer: Container 2001 is synced with bcsId 19.
dn2_1    | 2023-03-10 22:54:30,261 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-LeaderElection4] INFO server.RaftServer$Division: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89: set configuration 0: peers:[3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:10.9.0.16:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-03-10 22:54:30,262 [3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3892a4e1-c878-42af-adb7-db66a90d61f4@group-C7BE60589B89-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/808ced26-8679-46a5-9ee3-c7be60589b89/current/log_inprogress_0
scm_1    | 2023-03-10 22:52:55,490 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 2d4f1397-2cdc-4336-a562-676bab171c02, Nodes: 3892a4e1-c878-42af-adb7-db66a90d61f4(ha_dn2_1.ha_net/10.9.0.16), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:3892a4e1-c878-42af-adb7-db66a90d61f4, CreationTimestamp2023-03-10T22:51:55.010465Z[UTC]] removed.
dn4_1    | 2023-03-10 22:53:54,899 [pool-38-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/995fa1d6-41cf-4ed4-929b-577a15fd12b5 has been successfully formatted.
dn3_1    | 2023-03-10 22:53:19,023 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn3_1    | 2023-03-10 22:53:19,023 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn1_1    | 2023-03-10 22:52:53,665 [ContainerOp-78b03a88-75c1-4060-9962-cbc13a60f575-1] INFO keyvalue.KeyValueContainer: Container 2001 is synced with bcsId 19.
scm_1    | 2023-03-10 22:52:55,490 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=7572f4a9-8e86-407b-b624-5c511402a23e since it stays at CLOSED stage.
scm_1    | 2023-03-10 22:52:55,490 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=7572f4a9-8e86-407b-b624-5c511402a23e close command to datanode 1a6d358d-6662-4447-914c-d709a67ff716
dn1_1    | 2023-03-10 22:52:53,699 [ContainerOp-78b03a88-75c1-4060-9962-cbc13a60f575-1] INFO keyvalue.KeyValueContainer: Container 2001 is closed with bcsId 19.
dn1_1    | 2023-03-10 22:53:10,082 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-03-10 22:53:24,433 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn1_1    | 2023-03-10 22:53:24,433 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn1_1    | 2023-03-10 22:53:24,433 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn2_1    | 2023-03-10 22:55:10,325 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn2_1    | 2023-03-10 22:56:10,325 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn2_1    | 2023-03-10 22:57:10,326 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-03-10 22:53:24,434 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: ERASURE_CODED_STORAGE_SUPPORT.
dn3_1    | 2023-03-10 22:53:19,023 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-03-10 22:53:19,023 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-03-10 22:53:19,023 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-03-10 22:53:24,257 [Command processor thread] INFO server.RaftServer: 1a6d358d-6662-4447-914c-d709a67ff716: remove  FOLLOWER 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3:t11, leader=e3e4587c-aa42-4e86-ae9a-d3e448365275, voted=e3e4587c-aa42-4e86-ae9a-d3e448365275, raftlog=Memoized:1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-SegmentedRaftLog:OPENED:c41, conf=36: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn1_1    | 2023-03-10 22:53:24,436 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature ERASURE_CODED_STORAGE_SUPPORT has been finalized.
dn1_1    | 2023-03-10 22:53:24,436 [Command processor thread] INFO upgrade.UpgradeFinalizer: Running finalization actions for layout feature: DATANODE_SCHEMA_V3
dn3_1    | 2023-03-10 22:53:24,262 [Command processor thread] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3: shutdown
dn3_1    | 2023-03-10 22:53:24,262 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-0049FBBC23B3,id=1a6d358d-6662-4447-914c-d709a67ff716
dn3_1    | 2023-03-10 22:53:24,262 [Command processor thread] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: shutdown 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-FollowerState
dn3_1    | 2023-03-10 22:53:24,262 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-FollowerState] INFO impl.FollowerState: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-FollowerState was interrupted
dn1_1    | 2023-03-10 22:53:24,436 [Command processor thread] INFO upgrade.DatanodeSchemaV3FinalizeAction: Upgrading Datanode volume layout for Schema V3 support.
dn1_1    | 2023-03-10 22:53:24,510 [Command processor thread] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-2ff2b208-6b8d-4deb-84e0-fc20d633e954/container.db to cache
dn3_1    | 2023-03-10 22:53:24,264 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-0049FBBC23B3: Taking a snapshot at:(t:11, i:41) file /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/sm/snapshot.11_41
dn3_1    | 2023-03-10 22:53:24,267 [Command processor thread] INFO impl.StateMachineUpdater: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-StateMachineUpdater: set stopIndex = 41
scm_1    | 2023-03-10 22:52:55,491 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 7572f4a9-8e86-407b-b624-5c511402a23e, Nodes: 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:1a6d358d-6662-4447-914c-d709a67ff716, CreationTimestamp2023-03-10T22:51:55.063157Z[UTC]] removed.
scm_1    | 2023-03-10 22:52:55,491 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=78b03a88-75c1-4060-9962-cbc13a60f575 since it stays at CLOSED stage.
dn1_1    | 2023-03-10 22:53:24,510 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-2ff2b208-6b8d-4deb-84e0-fc20d633e954/container.db for volume DS-2ff2b208-6b8d-4deb-84e0-fc20d633e954
dn4_1    | 2023-03-10 22:53:54,912 [pool-38-thread-1] INFO ratis.ContainerStateMachine: group-577A15FD12B5: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn4_1    | 2023-03-10 22:53:54,913 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-03-10 22:53:54,914 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-03-10 22:53:54,916 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-03-10 22:53:24,511 [Command processor thread] INFO utils.DatanodeStoreCache: Removed db /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-2ff2b208-6b8d-4deb-84e0-fc20d633e954/container.db from cache
dn4_1    | 2023-03-10 22:53:54,916 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-03-10 22:53:54,916 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-03-10 22:53:54,917 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-03-10 22:53:54,917 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1    | 2023-03-10 22:52:55,491 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=78b03a88-75c1-4060-9962-cbc13a60f575 close command to datanode 3892a4e1-c878-42af-adb7-db66a90d61f4
scm_1    | 2023-03-10 22:52:55,492 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=78b03a88-75c1-4060-9962-cbc13a60f575 close command to datanode e3e4587c-aa42-4e86-ae9a-d3e448365275
scm_1    | 2023-03-10 22:52:55,492 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=78b03a88-75c1-4060-9962-cbc13a60f575 close command to datanode e78c5ce1-46ab-4889-a0cd-5903ae46614d
scm_1    | 2023-03-10 22:52:55,494 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 78b03a88-75c1-4060-9962-cbc13a60f575, Nodes: 3892a4e1-c878-42af-adb7-db66a90d61f4(ha_dn2_1.ha_net/10.9.0.16)e3e4587c-aa42-4e86-ae9a-d3e448365275(ha_dn1_1.ha_net/10.9.0.15)e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:e78c5ce1-46ab-4889-a0cd-5903ae46614d, CreationTimestamp2023-03-10T22:51:55.080894Z[UTC]] removed.
dn1_1    | 2023-03-10 22:53:24,511 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is stopped at /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-2ff2b208-6b8d-4deb-84e0-fc20d633e954/container.db for volume DS-2ff2b208-6b8d-4deb-84e0-fc20d633e954
scm_1    | 2023-03-10 22:52:55,564 [IPC Server handler 6 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm_1    | 2023-03-10 22:52:56,360 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm_1    | 2023-03-10 22:52:56,370 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
recon_1  | 2023-03-10 22:53:04,589 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1678488784103.
dn3_1    | 2023-03-10 22:53:24,268 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-0049FBBC23B3: Finished taking a snapshot at:(t:11, i:41) file:/data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/sm/snapshot.11_41 took: 4 ms
dn3_1    | 2023-03-10 22:53:24,271 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-StateMachineUpdater] INFO impl.StateMachineUpdater: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-StateMachineUpdater: Took a snapshot at index 41
dn3_1    | 2023-03-10 22:53:24,272 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-StateMachineUpdater] INFO impl.StateMachineUpdater: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-StateMachineUpdater: snapshotIndex: updateIncreasingly 35 -> 41
dn3_1    | 2023-03-10 22:53:24,282 [Command processor thread] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3: closes. applyIndex: 41
recon_1  | 2023-03-10 22:53:04,604 [pool-27-thread-1] ERROR impl.OzoneManagerServiceProviderImpl: Unable to update Recon's metadata with new OM DB. 
recon_1  | org.apache.hadoop.metrics2.MetricsException: Metrics source userTableCache already exists!
recon_1  | 	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:152)
dn1_1    | 2023-03-10 22:53:24,541 [Command processor thread] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-2ff2b208-6b8d-4deb-84e0-fc20d633e954/container.db to cache
recon_1  | 	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:125)
scm_1    | 2023-03-10 22:53:00,248 [SCMBlockDeletingService#0] INFO block.SCMBlockDeletingService: Totally added 9 blocks to be deleted for 5 datanodes, task elapsed time: 7ms
scm_1    | 2023-03-10 22:53:00,564 [IPC Server handler 6 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm_1    | 2023-03-10 22:53:05,565 [IPC Server handler 6 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
recon_1  | 	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:229)
recon_1  | 	at org.apache.hadoop.hdds.utils.TableCacheMetrics.create(TableCacheMetrics.java:67)
dn4_1    | 2023-03-10 22:53:54,919 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-03-10 22:53:54,919 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: new e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/995fa1d6-41cf-4ed4-929b-577a15fd12b5
dn3_1    | 2023-03-10 22:53:24,285 [1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn3_1    | 2023-03-10 22:53:24,288 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3-SegmentedRaftLogWorker close()
dn3_1    | 2023-03-10 22:53:24,302 [Command processor thread] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-0049FBBC23B3: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3
dn3_1    | 2023-03-10 22:53:24,303 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=cf0cdca9-af45-4cc0-9366-0049fbbc23b3 command on datanode 1a6d358d-6662-4447-914c-d709a67ff716.
dn1_1    | 2023-03-10 22:53:24,541 [Command processor thread] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/DS-2ff2b208-6b8d-4deb-84e0-fc20d633e954/container.db for volume DS-2ff2b208-6b8d-4deb-84e0-fc20d633e954
dn3_1    | 2023-03-10 22:53:24,304 [Command processor thread] INFO server.RaftServer: 1a6d358d-6662-4447-914c-d709a67ff716: remove  FOLLOWER 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00:t12, leader=3892a4e1-c878-42af-adb7-db66a90d61f4, voted=3892a4e1-c878-42af-adb7-db66a90d61f4, raftlog=Memoized:1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-SegmentedRaftLog:OPENED:c20, conf=17: peers:[178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn4_1    | 2023-03-10 22:53:54,920 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-03-10 22:53:54,920 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm_1    | 2023-03-10 22:53:10,565 [IPC Server handler 6 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn3_1    | 2023-03-10 22:53:24,305 [Command processor thread] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00: shutdown
dn3_1    | 2023-03-10 22:53:24,305 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-5B98FDC36F00,id=1a6d358d-6662-4447-914c-d709a67ff716
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.createCacheMetrics(TypedTable.java:328)
recon_1  | 	at org.apache.hadoop.ozone.om.OmMetadataManagerImpl.checkTableStatus(OmMetadataManagerImpl.java:418)
dn1_1    | 2023-03-10 22:53:24,543 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature DATANODE_SCHEMA_V3 has been finalized.
dn4_1    | 2023-03-10 22:53:54,920 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-03-10 22:53:54,921 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
recon_1  | 	at org.apache.hadoop.ozone.om.OmMetadataManagerImpl.initializeOmTables(OmMetadataManagerImpl.java:543)
recon_1  | 	at org.apache.hadoop.ozone.recon.recovery.ReconOmMetadataManagerImpl.initializeNewRdbStore(ReconOmMetadataManagerImpl.java:97)
dn1_1    | 2023-03-10 22:53:24,543 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn4_1    | 2023-03-10 22:53:54,921 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-03-10 22:53:54,921 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-03-10 22:53:54,921 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-03-10 22:53:54,924 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1    | 2023-03-10 22:53:15,566 [IPC Server handler 6 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn1_1    | 2023-03-10 22:53:24,543 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn3_1    | 2023-03-10 22:53:24,305 [Command processor thread] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: shutdown 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-FollowerState
scm_1    | 2023-03-10 22:53:15,571 [RatisPipelineUtilsThread - 0] ERROR pipeline.PipelinePlacementPolicy: No healthy node found to allocate container.
scm_1    | 2023-03-10 22:53:20,566 [IPC Server handler 6 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm_1    | 2023-03-10 22:53:23,267 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=cf0cdca9-af45-4cc0-9366-0049fbbc23b3 is not found
dn3_1    | 2023-03-10 22:53:24,306 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-FollowerState] INFO impl.FollowerState: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-FollowerState was interrupted
dn1_1    | 2023-03-10 22:53:24,544 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
recon_1  | 	at org.apache.hadoop.ozone.recon.recovery.ReconOmMetadataManagerImpl.updateOmDB(ReconOmMetadataManagerImpl.java:114)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.updateReconOmDBWithNewSnapshot(OzoneManagerServiceProviderImpl.java:384)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:519)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-03-10 22:53:54,924 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
scm_1    | 2023-03-10 22:53:23,268 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=54b523a6-4d1d-4f07-a057-5b98fdc36f00 is not found
dn1_1    | 2023-03-10 22:53:24,546 [Command processor thread] INFO server.RaftServer: e3e4587c-aa42-4e86-ae9a-d3e448365275: remove    LEADER e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3:t11, leader=e3e4587c-aa42-4e86-ae9a-d3e448365275, voted=e3e4587c-aa42-4e86-ae9a-d3e448365275, raftlog=Memoized:e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-SegmentedRaftLog:OPENED:c41, conf=36: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn3_1    | 2023-03-10 22:53:24,307 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-5B98FDC36F00: Taking a snapshot at:(t:12, i:20) file /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/sm/snapshot.12_20
dn3_1    | 2023-03-10 22:53:24,307 [Command processor thread] INFO impl.StateMachineUpdater: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-StateMachineUpdater: set stopIndex = 20
dn4_1    | 2023-03-10 22:53:54,929 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-10 22:53:55,442 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-03-10 22:53:24,309 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-5B98FDC36F00: Finished taking a snapshot at:(t:12, i:20) file:/data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00/sm/snapshot.12_20 took: 3 ms
dn4_1    | 2023-03-10 22:53:55,442 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-03-10 22:53:24,550 [Command processor thread] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3: shutdown
recon_1  | 2023-03-10 22:53:06,292 [pool-50-thread-1] INFO scm.ReconStorageContainerManagerFacade: Got list of containers from SCM : 5
dn3_1    | 2023-03-10 22:53:24,312 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-StateMachineUpdater] INFO impl.StateMachineUpdater: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-StateMachineUpdater: Took a snapshot at index 20
scm_1    | 2023-03-10 22:53:23,268 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=7572f4a9-8e86-407b-b624-5c511402a23e is not found
scm_1    | 2023-03-10 22:53:23,712 [IPC Server handler 76 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn3_1    | 2023-03-10 22:53:24,313 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-StateMachineUpdater] INFO impl.StateMachineUpdater: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-StateMachineUpdater: snapshotIndex: updateIncreasingly 16 -> 20
dn3_1    | 2023-03-10 22:53:24,322 [Command processor thread] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00: closes. applyIndex: 20
dn3_1    | 2023-03-10 22:53:24,325 [1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn4_1    | 2023-03-10 22:53:55,443 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-03-10 22:53:55,447 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-03-10 22:53:55,447 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-03-10 22:53:55,455 [pool-38-thread-1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5: start as a follower, conf=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 2023-03-10 22:53:23,713 [IPC Server handler 42 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
recon_1  | 2023-03-10 22:53:23,754 [IPC Server handler 28 on default port 9891] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
dn3_1    | 2023-03-10 22:53:24,327 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00-SegmentedRaftLogWorker close()
dn3_1    | 2023-03-10 22:53:24,338 [Command processor thread] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5B98FDC36F00: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/54b523a6-4d1d-4f07-a057-5b98fdc36f00
dn4_1    | 2023-03-10 22:53:55,456 [pool-38-thread-1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm_1    | 2023-03-10 22:53:23,748 [IPC Server handler 75 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 2, SCM MetadataLayoutVersion = 4
scm_1    | 2023-03-10 22:53:24,221 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17) moved to HEALTHY state.
dn3_1    | 2023-03-10 22:53:24,338 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=54b523a6-4d1d-4f07-a057-5b98fdc36f00 command on datanode 1a6d358d-6662-4447-914c-d709a67ff716.
recon_1  | 2023-03-10 22:53:53,904 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=a8f227f9-9826-491d-a836-6265fbdc9d88. Trying to get from SCM.
dn4_1    | 2023-03-10 22:53:55,456 [pool-38-thread-1] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: start e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5-FollowerState
dn1_1    | 2023-03-10 22:53:24,551 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-0049FBBC23B3,id=e3e4587c-aa42-4e86-ae9a-d3e448365275
dn3_1    | 2023-03-10 22:53:24,339 [Command processor thread] INFO server.RaftServer: 1a6d358d-6662-4447-914c-d709a67ff716: remove    LEADER 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E:t4, leader=1a6d358d-6662-4447-914c-d709a67ff716, voted=1a6d358d-6662-4447-914c-d709a67ff716, raftlog=Memoized:1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-SegmentedRaftLog:OPENED:c6, conf=5: peers:[1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn3_1    | 2023-03-10 22:53:24,339 [Command processor thread] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E: shutdown
scm_1    | 2023-03-10 22:53:24,221 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
dn4_1    | 2023-03-10 22:53:55,480 [pool-38-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-577A15FD12B5,id=e78c5ce1-46ab-4889-a0cd-5903ae46614d
recon_1  | 2023-03-10 22:53:53,930 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: a8f227f9-9826-491d-a836-6265fbdc9d88, Nodes: 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17)e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18)178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-10T22:53:24.245Z[UTC]] to Recon pipeline metadata.
recon_1  | 2023-03-10 22:53:53,932 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: a8f227f9-9826-491d-a836-6265fbdc9d88, Nodes: 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17)e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18)178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-10T22:53:24.245Z[UTC]].
scm_1    | 2023-03-10 22:53:24,224 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19) moved to HEALTHY state.
dn4_1    | 2023-03-10 22:53:55,480 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
recon_1  | 2023-03-10 22:53:53,934 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a8f227f9-9826-491d-a836-6265fbdc9d88 reported by 178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19)
dn1_1    | 2023-03-10 22:53:24,551 [Command processor thread] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: shutdown e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-LeaderStateImpl
dn1_1    | 2023-03-10 22:53:24,552 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3->1a6d358d-6662-4447-914c-d709a67ff716-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3->1a6d358d-6662-4447-914c-d709a67ff716-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
recon_1  | 2023-03-10 22:53:54,087 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a8f227f9-9826-491d-a836-6265fbdc9d88 reported by 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17)
scm_1    | 2023-03-10 22:53:24,224 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
dn4_1    | 2023-03-10 22:53:55,481 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
recon_1  | 2023-03-10 22:53:54,220 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a8f227f9-9826-491d-a836-6265fbdc9d88 reported by e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18)
recon_1  | 2023-03-10 22:53:54,376 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a8f227f9-9826-491d-a836-6265fbdc9d88 reported by 178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19)
scm_1    | 2023-03-10 22:53:24,225 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18) moved to HEALTHY state.
dn4_1    | 2023-03-10 22:53:55,481 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
recon_1  | 2023-03-10 22:53:54,376 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=47893e13-e605-460c-a18a-cd960c190469. Trying to get from SCM.
dn3_1    | 2023-03-10 22:53:24,340 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-5C511402A23E,id=1a6d358d-6662-4447-914c-d709a67ff716
dn3_1    | 2023-03-10 22:53:24,340 [Command processor thread] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: shutdown 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-LeaderStateImpl
dn1_1    | 2023-03-10 22:53:24,553 [Command processor thread] INFO impl.PendingRequests: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-PendingRequests: sendNotLeaderResponses
dn1_1    | 2023-03-10 22:53:24,552 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3->e78c5ce1-46ab-4889-a0cd-5903ae46614d-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3->e78c5ce1-46ab-4889-a0cd-5903ae46614d-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
recon_1  | 2023-03-10 22:53:54,384 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 47893e13-e605-460c-a18a-cd960c190469, Nodes: 178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:178b30e1-b74d-4f4d-a142-c930eee71455, CreationTimestamp2023-03-10T22:53:24.260Z[UTC]] to Recon pipeline metadata.
dn3_1    | 2023-03-10 22:53:24,342 [Command processor thread] INFO impl.PendingRequests: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-PendingRequests: sendNotLeaderResponses
dn1_1    | 2023-03-10 22:53:24,564 [grpc-default-executor-2] INFO server.GrpcLogAppender: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3->1a6d358d-6662-4447-914c-d709a67ff716-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn1_1    | 2023-03-10 22:53:24,569 [grpc-default-executor-0] INFO server.GrpcLogAppender: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3->e78c5ce1-46ab-4889-a0cd-5903ae46614d-AppendLogResponseHandler: follower responses appendEntries COMPLETED
recon_1  | 2023-03-10 22:53:54,385 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 47893e13-e605-460c-a18a-cd960c190469, Nodes: 178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:178b30e1-b74d-4f4d-a142-c930eee71455, CreationTimestamp2023-03-10T22:53:24.260Z[UTC]].
recon_1  | 2023-03-10 22:53:54,396 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=5b37649b-4609-4fbb-bbb4-98cd3de58f62. Trying to get from SCM.
recon_1  | 2023-03-10 22:53:54,399 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 5b37649b-4609-4fbb-bbb4-98cd3de58f62, Nodes: 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-10T22:53:24.225Z[UTC]] to Recon pipeline metadata.
dn4_1    | 2023-03-10 22:53:55,481 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-03-10 22:53:24,349 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-5C511402A23E: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/7572f4a9-8e86-407b-b624-5c511402a23e/sm/snapshot.4_6
dn3_1    | 2023-03-10 22:53:24,349 [Command processor thread] INFO impl.StateMachineUpdater: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-StateMachineUpdater: set stopIndex = 6
dn4_1    | 2023-03-10 22:53:55,490 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm_1    | 2023-03-10 22:53:24,225 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
recon_1  | 2023-03-10 22:53:54,399 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 5b37649b-4609-4fbb-bbb4-98cd3de58f62, Nodes: 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-10T22:53:24.225Z[UTC]].
dn1_1    | 2023-03-10 22:53:24,569 [grpc-default-executor-0] INFO leader.FollowerInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3->e78c5ce1-46ab-4889-a0cd-5903ae46614d: nextIndex: updateUnconditionally 42 -> 41
dn3_1    | 2023-03-10 22:53:24,352 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-5C511402A23E: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/7572f4a9-8e86-407b-b624-5c511402a23e/sm/snapshot.4_6 took: 3 ms
dn3_1    | 2023-03-10 22:53:24,353 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-StateMachineUpdater] INFO impl.StateMachineUpdater: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-StateMachineUpdater: Took a snapshot at index 6
recon_1  | 2023-03-10 22:53:54,400 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=5b37649b-4609-4fbb-bbb4-98cd3de58f62 reported by 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17)
dn3_1    | 2023-03-10 22:53:24,353 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-StateMachineUpdater] INFO impl.StateMachineUpdater: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
dn3_1    | 2023-03-10 22:53:24,355 [Command processor thread] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E: closes. applyIndex: 6
dn1_1    | 2023-03-10 22:53:24,570 [grpc-default-executor-0] INFO server.GrpcLogAppender: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3->e78c5ce1-46ab-4889-a0cd-5903ae46614d-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn1_1    | 2023-03-10 22:53:24,570 [grpc-default-executor-2] INFO leader.FollowerInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3->1a6d358d-6662-4447-914c-d709a67ff716: nextIndex: updateUnconditionally 42 -> 41
dn1_1    | 2023-03-10 22:53:24,571 [grpc-default-executor-0] INFO leader.FollowerInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3->e78c5ce1-46ab-4889-a0cd-5903ae46614d: nextIndex: updateUnconditionally 41 -> 40
recon_1  | 2023-03-10 22:53:54,400 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 5b37649b-4609-4fbb-bbb4-98cd3de58f62, Nodes: 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:1a6d358d-6662-4447-914c-d709a67ff716, CreationTimestamp2023-03-10T22:53:24.225Z[UTC]] moved to OPEN state
dn4_1    | 2023-03-10 22:53:55,498 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=995fa1d6-41cf-4ed4-929b-577a15fd12b5
dn4_1    | 2023-03-10 22:53:55,528 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-03-10 22:53:24,573 [grpc-default-executor-0] INFO server.GrpcLogAppender: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3->1a6d358d-6662-4447-914c-d709a67ff716-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn3_1    | 2023-03-10 22:53:24,355 [1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn3_1    | 2023-03-10 22:53:24,357 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E-SegmentedRaftLogWorker close()
scm_1    | 2023-03-10 22:53:24,226 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5b37649b-4609-4fbb-bbb4-98cd3de58f62 to datanode:1a6d358d-6662-4447-914c-d709a67ff716
dn4_1    | 2023-03-10 22:53:56,419 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=995fa1d6-41cf-4ed4-929b-577a15fd12b5.
dn3_1    | 2023-03-10 22:53:24,361 [Command processor thread] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-5C511402A23E: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/7572f4a9-8e86-407b-b624-5c511402a23e
dn3_1    | 2023-03-10 22:53:24,361 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=7572f4a9-8e86-407b-b624-5c511402a23e command on datanode 1a6d358d-6662-4447-914c-d709a67ff716.
scm_1    | 2023-03-10 22:53:24,229 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 5b37649b-4609-4fbb-bbb4-98cd3de58f62, Nodes: 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-10T22:53:24.225Z[UTC]].
dn4_1    | 2023-03-10 22:53:56,420 [Command processor thread] INFO server.RaftServer: e78c5ce1-46ab-4889-a0cd-5903ae46614d: addNew group-C0DB968E18E2:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] returns group-C0DB968E18E2:java.util.concurrent.CompletableFuture@cd0de9b[Not completed]
dn3_1    | 2023-03-10 22:53:24,427 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: 1a6d358d-6662-4447-914c-d709a67ff716: Completed APPEND_ENTRIES, lastRequest: null
dn3_1    | 2023-03-10 22:53:24,432 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: 1a6d358d-6662-4447-914c-d709a67ff716: Completed APPEND_ENTRIES, lastRequest: 3892a4e1-c878-42af-adb7-db66a90d61f4->1a6d358d-6662-4447-914c-d709a67ff716#48-t12,previous=(t:12, i:19),leaderCommit=19,initializing? true,entries: size=1, first=(t:12, i:20), METADATAENTRY(c:19)
scm_1    | 2023-03-10 22:53:24,245 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a8f227f9-9826-491d-a836-6265fbdc9d88 to datanode:1a6d358d-6662-4447-914c-d709a67ff716
dn4_1    | 2023-03-10 22:53:56,422 [pool-38-thread-1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d: new RaftServerImpl for group-C0DB968E18E2:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn4_1    | 2023-03-10 22:53:56,424 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm_1    | 2023-03-10 22:53:24,246 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a8f227f9-9826-491d-a836-6265fbdc9d88 to datanode:e78c5ce1-46ab-4889-a0cd-5903ae46614d
scm_1    | 2023-03-10 22:53:24,247 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a8f227f9-9826-491d-a836-6265fbdc9d88 to datanode:178b30e1-b74d-4f4d-a142-c930eee71455
dn4_1    | 2023-03-10 22:53:56,425 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-03-10 22:53:56,425 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
recon_1  | 2023-03-10 22:53:54,409 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a8f227f9-9826-491d-a836-6265fbdc9d88 reported by 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17)
dn4_1    | 2023-03-10 22:53:56,426 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-03-10 22:53:56,426 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-03-10 22:53:56,426 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-03-10 22:53:56,427 [pool-38-thread-1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2: ConfigurationManager, init=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-03-10 22:53:56,428 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-03-10 22:53:56,429 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-03-10 22:53:24,573 [grpc-default-executor-0] INFO leader.FollowerInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3->1a6d358d-6662-4447-914c-d709a67ff716: nextIndex: updateUnconditionally 41 -> 40
dn1_1    | 2023-03-10 22:53:24,574 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-0049FBBC23B3: Taking a snapshot at:(t:11, i:41) file /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/sm/snapshot.11_41
recon_1  | 2023-03-10 22:53:54,646 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a8f227f9-9826-491d-a836-6265fbdc9d88 reported by 178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19)
recon_1  | 2023-03-10 22:53:54,646 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=995fa1d6-41cf-4ed4-929b-577a15fd12b5. Trying to get from SCM.
recon_1  | 2023-03-10 22:53:54,649 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 995fa1d6-41cf-4ed4-929b-577a15fd12b5, Nodes: 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17)178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19)e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-10T22:53:24.284Z[UTC]] to Recon pipeline metadata.
dn3_1    | 2023-03-10 22:53:24,555 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: 1a6d358d-6662-4447-914c-d709a67ff716: Completed APPEND_ENTRIES, lastRequest: e3e4587c-aa42-4e86-ae9a-d3e448365275->1a6d358d-6662-4447-914c-d709a67ff716#322-t11,previous=(t:11, i:40),leaderCommit=40,initializing? true,entries: size=1, first=(t:11, i:41), METADATAENTRY(c:40)
dn3_1    | 2023-03-10 22:53:24,563 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: 1a6d358d-6662-4447-914c-d709a67ff716: Completed APPEND_ENTRIES, lastRequest: null
dn3_1    | 2023-03-10 22:53:24,829 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@733fb462] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(0),2(0)], numOfContainers=2, numOfBlocks=3
dn3_1    | 2023-03-10 22:53:54,062 [grpc-default-executor-3] INFO server.RaftServer: 1a6d358d-6662-4447-914c-d709a67ff716: addNew group-6265FBDC9D88:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:0|startupRole:FOLLOWER] returns group-6265FBDC9D88:java.util.concurrent.CompletableFuture@3f48064d[Not completed]
dn3_1    | 2023-03-10 22:53:54,064 [pool-34-thread-1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716: new RaftServerImpl for group-6265FBDC9D88:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn3_1    | 2023-03-10 22:53:54,064 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
recon_1  | 2023-03-10 22:53:54,652 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 995fa1d6-41cf-4ed4-929b-577a15fd12b5, Nodes: 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17)178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19)e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-10T22:53:24.284Z[UTC]].
recon_1  | 2023-03-10 22:53:54,652 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=995fa1d6-41cf-4ed4-929b-577a15fd12b5 reported by 178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19)
recon_1  | 2023-03-10 22:53:54,841 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a8f227f9-9826-491d-a836-6265fbdc9d88 reported by 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17)
recon_1  | 2023-03-10 22:53:54,841 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=995fa1d6-41cf-4ed4-929b-577a15fd12b5 reported by 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17)
recon_1  | 2023-03-10 22:53:54,907 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a8f227f9-9826-491d-a836-6265fbdc9d88 reported by e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18)
recon_1  | 2023-03-10 22:53:54,908 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=995fa1d6-41cf-4ed4-929b-577a15fd12b5 reported by e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18)
recon_1  | 2023-03-10 22:53:56,472 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=d8130c23-c209-44b4-868f-c0db968e18e2. Trying to get from SCM.
dn4_1    | 2023-03-10 22:53:56,429 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-03-10 22:53:56,431 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-03-10 22:53:56,432 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-03-10 22:53:54,064 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-03-10 22:53:54,065 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-03-10 22:53:54,065 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-03-10 22:53:24,577 [Command processor thread] INFO impl.StateMachineUpdater: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-StateMachineUpdater: set stopIndex = 41
dn1_1    | 2023-03-10 22:53:24,579 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-0049FBBC23B3: Finished taking a snapshot at:(t:11, i:41) file:/data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3/sm/snapshot.11_41 took: 5 ms
dn1_1    | 2023-03-10 22:53:24,580 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-StateMachineUpdater] INFO impl.StateMachineUpdater: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-StateMachineUpdater: Took a snapshot at index 41
dn1_1    | 2023-03-10 22:53:24,581 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-StateMachineUpdater] INFO impl.StateMachineUpdater: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-StateMachineUpdater: snapshotIndex: updateIncreasingly 35 -> 41
dn1_1    | 2023-03-10 22:53:24,585 [Command processor thread] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3: closes. applyIndex: 41
dn1_1    | 2023-03-10 22:53:24,586 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn1_1    | 2023-03-10 22:53:24,589 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3-SegmentedRaftLogWorker close()
dn1_1    | 2023-03-10 22:53:24,595 [Command processor thread] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-0049FBBC23B3: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/cf0cdca9-af45-4cc0-9366-0049fbbc23b3
dn4_1    | 2023-03-10 22:53:56,432 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-03-10 22:53:56,434 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-03-10 22:53:56,434 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-03-10 22:53:56,438 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-03-10 22:53:54,065 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-03-10 22:53:54,065 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-03-10 22:53:54,065 [pool-34-thread-1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88: ConfigurationManager, init=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-03-10 22:53:54,066 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-03-10 22:53:54,069 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-03-10 22:53:54,069 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
recon_1  | 2023-03-10 22:53:56,477 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: d8130c23-c209-44b4-868f-c0db968e18e2, Nodes: e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-10T22:53:24.293Z[UTC]] to Recon pipeline metadata.
recon_1  | 2023-03-10 22:53:56,478 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d8130c23-c209-44b4-868f-c0db968e18e2, Nodes: e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-10T22:53:24.293Z[UTC]].
scm_1    | 2023-03-10 22:53:24,250 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: a8f227f9-9826-491d-a836-6265fbdc9d88, Nodes: 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17)e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18)178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-10T22:53:24.245Z[UTC]].
scm_1    | 2023-03-10 22:53:24,260 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=47893e13-e605-460c-a18a-cd960c190469 to datanode:178b30e1-b74d-4f4d-a142-c930eee71455
dn3_1    | 2023-03-10 22:53:54,070 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-03-10 22:53:54,070 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-03-10 22:53:54,070 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-03-10 22:53:54,071 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-03-10 22:53:54,075 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-03-10 22:53:24,596 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=cf0cdca9-af45-4cc0-9366-0049fbbc23b3 command on datanode e3e4587c-aa42-4e86-ae9a-d3e448365275.
dn3_1    | 2023-03-10 22:53:54,075 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-03-10 22:53:54,076 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1  | 2023-03-10 22:53:56,478 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=d8130c23-c209-44b4-868f-c0db968e18e2 reported by e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18)
dn4_1    | 2023-03-10 22:53:56,439 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-03-10 22:53:56,439 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-03-10 22:53:56,439 [pool-38-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d8130c23-c209-44b4-868f-c0db968e18e2 does not exist. Creating ...
dn4_1    | 2023-03-10 22:53:56,445 [pool-38-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d8130c23-c209-44b4-868f-c0db968e18e2/in_use.lock acquired by nodename 6@b0c55c35d5b9
recon_1  | 2023-03-10 22:53:56,479 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d8130c23-c209-44b4-868f-c0db968e18e2, Nodes: e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:e78c5ce1-46ab-4889-a0cd-5903ae46614d, CreationTimestamp2023-03-10T22:53:24.293Z[UTC]] moved to OPEN state
recon_1  | 2023-03-10 22:53:56,479 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a8f227f9-9826-491d-a836-6265fbdc9d88 reported by e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18)
recon_1  | 2023-03-10 22:53:56,481 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=995fa1d6-41cf-4ed4-929b-577a15fd12b5 reported by e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18)
recon_1  | 2023-03-10 22:53:59,460 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a8f227f9-9826-491d-a836-6265fbdc9d88 reported by e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18)
recon_1  | 2023-03-10 22:53:59,461 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: a8f227f9-9826-491d-a836-6265fbdc9d88, Nodes: 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17)e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18)178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:e78c5ce1-46ab-4889-a0cd-5903ae46614d, CreationTimestamp2023-03-10T22:53:24.245Z[UTC]] moved to OPEN state
dn3_1    | 2023-03-10 22:53:54,076 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-03-10 22:53:56,450 [pool-38-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d8130c23-c209-44b4-868f-c0db968e18e2 has been successfully formatted.
recon_1  | 2023-03-10 22:53:59,463 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=995fa1d6-41cf-4ed4-929b-577a15fd12b5 reported by e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18)
recon_1  | 2023-03-10 22:53:59,794 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=995fa1d6-41cf-4ed4-929b-577a15fd12b5 reported by 178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19)
recon_1  | 2023-03-10 22:53:59,873 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=995fa1d6-41cf-4ed4-929b-577a15fd12b5 reported by 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17)
recon_1  | 2023-03-10 22:54:00,082 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=995fa1d6-41cf-4ed4-929b-577a15fd12b5 reported by 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17)
recon_1  | 2023-03-10 22:54:00,082 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 995fa1d6-41cf-4ed4-929b-577a15fd12b5, Nodes: 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17)178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19)e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:1a6d358d-6662-4447-914c-d709a67ff716, CreationTimestamp2023-03-10T22:53:24.284Z[UTC]] moved to OPEN state
recon_1  | 2023-03-10 22:54:04,606 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
dn4_1    | 2023-03-10 22:53:56,451 [pool-38-thread-1] INFO ratis.ContainerStateMachine: group-C0DB968E18E2: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn4_1    | 2023-03-10 22:53:56,452 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-03-10 22:53:56,452 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-03-10 22:53:56,452 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-10 22:53:56,453 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-03-10 22:53:56,454 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-03-10 22:53:56,454 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-03-10 22:53:56,458 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-03-10 22:53:56,459 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-03-10 22:53:56,459 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: new e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d8130c23-c209-44b4-868f-c0db968e18e2
dn4_1    | 2023-03-10 22:53:56,459 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-03-10 22:53:56,459 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-03-10 22:53:56,459 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-03-10 22:53:56,459 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
recon_1  | 2023-03-10 22:54:04,607 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1  | 2023-03-10 22:54:04,607 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 150 
recon_1  | 2023-03-10 22:54:04,621 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
dn3_1    | 2023-03-10 22:53:54,076 [pool-34-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/a8f227f9-9826-491d-a836-6265fbdc9d88 does not exist. Creating ...
dn3_1    | 2023-03-10 22:53:54,078 [pool-34-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a8f227f9-9826-491d-a836-6265fbdc9d88/in_use.lock acquired by nodename 7@0de9850545d7
dn1_1    | 2023-03-10 22:53:24,596 [Command processor thread] INFO server.RaftServer: e3e4587c-aa42-4e86-ae9a-d3e448365275: remove    LEADER e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E:t4, leader=e3e4587c-aa42-4e86-ae9a-d3e448365275, voted=e3e4587c-aa42-4e86-ae9a-d3e448365275, raftlog=Memoized:e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-SegmentedRaftLog:OPENED:c6, conf=5: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
scm_1    | 2023-03-10 22:53:24,278 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 47893e13-e605-460c-a18a-cd960c190469, Nodes: 178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-10T22:53:24.260Z[UTC]].
dn3_1    | 2023-03-10 22:53:54,080 [pool-34-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/a8f227f9-9826-491d-a836-6265fbdc9d88 has been successfully formatted.
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
scm_1    | 2023-03-10 22:53:24,284 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=995fa1d6-41cf-4ed4-929b-577a15fd12b5 to datanode:1a6d358d-6662-4447-914c-d709a67ff716
dn3_1    | 2023-03-10 22:53:54,083 [pool-34-thread-1] INFO ratis.ContainerStateMachine: group-6265FBDC9D88: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
dn4_1    | 2023-03-10 22:53:56,459 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-03-10 22:53:56,459 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-03-10 22:53:24,596 [Command processor thread] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E: shutdown
scm_1    | 2023-03-10 22:53:24,285 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=995fa1d6-41cf-4ed4-929b-577a15fd12b5 to datanode:178b30e1-b74d-4f4d-a142-c930eee71455
dn3_1    | 2023-03-10 22:53:54,083 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
dn4_1    | 2023-03-10 22:53:56,459 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-03-10 22:53:24,597 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-D0B7AA0D159E,id=e3e4587c-aa42-4e86-ae9a-d3e448365275
scm_1    | 2023-03-10 22:53:24,286 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=995fa1d6-41cf-4ed4-929b-577a15fd12b5 to datanode:e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn3_1    | 2023-03-10 22:53:54,084 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
dn4_1    | 2023-03-10 22:53:56,460 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-03-10 22:53:24,597 [Command processor thread] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: shutdown e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-LeaderStateImpl
dn3_1    | 2023-03-10 22:53:54,084 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
dn4_1    | 2023-03-10 22:53:56,460 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
scm_1    | 2023-03-10 22:53:24,288 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 995fa1d6-41cf-4ed4-929b-577a15fd12b5, Nodes: 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17)178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19)e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-10T22:53:24.284Z[UTC]].
dn1_1    | 2023-03-10 22:53:24,597 [Command processor thread] INFO impl.PendingRequests: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-PendingRequests: sendNotLeaderResponses
dn3_1    | 2023-03-10 22:53:54,084 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
dn4_1    | 2023-03-10 22:53:56,487 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1    | 2023-03-10 22:53:24,290 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=995fa1d6-41cf-4ed4-929b-577a15fd12b5 contains same datanodes as previous pipelines: PipelineID=a8f227f9-9826-491d-a836-6265fbdc9d88 nodeIds: 1a6d358d-6662-4447-914c-d709a67ff716, 178b30e1-b74d-4f4d-a142-c930eee71455, e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn1_1    | 2023-03-10 22:53:24,598 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-D0B7AA0D159E: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/d1fa3d0d-6abe-48bc-b456-d0b7aa0d159e/sm/snapshot.4_6
dn3_1    | 2023-03-10 22:53:54,084 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
dn4_1    | 2023-03-10 22:53:56,676 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1    | 2023-03-10 22:53:24,293 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d8130c23-c209-44b4-868f-c0db968e18e2 to datanode:e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn1_1    | 2023-03-10 22:53:24,599 [Command processor thread] INFO impl.StateMachineUpdater: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-StateMachineUpdater: set stopIndex = 6
dn3_1    | 2023-03-10 22:53:54,084 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
scm_1    | 2023-03-10 22:53:24,296 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d8130c23-c209-44b4-868f-c0db968e18e2, Nodes: e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-10T22:53:24.293Z[UTC]].
dn1_1    | 2023-03-10 22:53:24,600 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-D0B7AA0D159E: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/d1fa3d0d-6abe-48bc-b456-d0b7aa0d159e/sm/snapshot.4_6 took: 2 ms
dn3_1    | 2023-03-10 22:53:54,085 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
dn4_1    | 2023-03-10 22:53:56,677 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1    | 2023-03-10 22:53:24,299 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0.
scm_1    | 2023-03-10 22:53:24,302 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0.
dn3_1    | 2023-03-10 22:53:54,085 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
dn4_1    | 2023-03-10 22:53:56,682 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm_1    | 2023-03-10 22:53:25,567 [IPC Server handler 6 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn1_1    | 2023-03-10 22:53:24,601 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-StateMachineUpdater] INFO impl.StateMachineUpdater: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-StateMachineUpdater: Took a snapshot at index 6
dn3_1    | 2023-03-10 22:53:54,085 [pool-34-thread-1] INFO segmented.SegmentedRaftLogWorker: new 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a8f227f9-9826-491d-a836-6265fbdc9d88
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn4_1    | 2023-03-10 22:53:56,682 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm_1    | 2023-03-10 22:53:26,360 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
dn1_1    | 2023-03-10 22:53:24,602 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-StateMachineUpdater] INFO impl.StateMachineUpdater: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
dn3_1    | 2023-03-10 22:53:54,086 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
dn4_1    | 2023-03-10 22:53:56,682 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1    | 2023-03-10 22:53:26,370 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
dn1_1    | 2023-03-10 22:53:24,602 [Command processor thread] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E: closes. applyIndex: 6
dn3_1    | 2023-03-10 22:53:54,086 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
dn4_1    | 2023-03-10 22:53:56,685 [pool-38-thread-1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2: start as a follower, conf=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm_1    | 2023-03-10 22:53:30,567 [IPC Server handler 6 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn1_1    | 2023-03-10 22:53:24,603 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn3_1    | 2023-03-10 22:53:54,086 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 2023-03-10 22:53:56,686 [pool-38-thread-1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm_1    | 2023-03-10 22:53:35,567 [IPC Server handler 6 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm_1    | 2023-03-10 22:53:40,568 [IPC Server handler 6 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn3_1    | 2023-03-10 22:53:54,086 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-03-10 22:53:54,087 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1    | 2023-03-10 22:53:45,568 [IPC Server handler 6 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn1_1    | 2023-03-10 22:53:24,604 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E-SegmentedRaftLogWorker close()
dn3_1    | 2023-03-10 22:53:54,087 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-03-10 22:53:56,710 [pool-38-thread-1] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: start e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-FollowerState
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm_1    | 2023-03-10 22:53:50,569 [IPC Server handler 6 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn1_1    | 2023-03-10 22:53:24,606 [Command processor thread] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-D0B7AA0D159E: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/d1fa3d0d-6abe-48bc-b456-d0b7aa0d159e
dn1_1    | 2023-03-10 22:53:24,606 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=d1fa3d0d-6abe-48bc-b456-d0b7aa0d159e command on datanode e3e4587c-aa42-4e86-ae9a-d3e448365275.
dn4_1    | 2023-03-10 22:53:56,712 [pool-38-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C0DB968E18E2,id=e78c5ce1-46ab-4889-a0cd-5903ae46614d
recon_1  | 2023-03-10 22:54:04,623 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 1, SequenceNumber diff: 1, SequenceNumber Lag from OM 0.
scm_1    | 2023-03-10 22:53:54,223 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode e3e4587c-aa42-4e86-ae9a-d3e448365275(ha_dn1_1.ha_net/10.9.0.15) moved to HEALTHY state.
scm_1    | 2023-03-10 22:53:54,223 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
dn1_1    | 2023-03-10 22:53:24,607 [Command processor thread] INFO server.RaftServer: e3e4587c-aa42-4e86-ae9a-d3e448365275: remove  FOLLOWER e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575:t7, leader=e78c5ce1-46ab-4889-a0cd-5903ae46614d, voted=e78c5ce1-46ab-4889-a0cd-5903ae46614d, raftlog=Memoized:e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-SegmentedRaftLog:OPENED:c26, conf=21: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:|priority:0|startupRole:FOLLOWER, e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3892a4e1-c878-42af-adb7-db66a90d61f4|rpc:10.9.0.16:9856|admin:10.9.0.16:9857|client:10.9.0.16:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn3_1    | 2023-03-10 22:53:54,087 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-03-10 22:53:56,713 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
recon_1  | 2023-03-10 22:54:04,623 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 1 records
dn1_1    | 2023-03-10 22:53:24,607 [Command processor thread] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575: shutdown
scm_1    | 2023-03-10 22:53:54,223 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 3892a4e1-c878-42af-adb7-db66a90d61f4(ha_dn2_1.ha_net/10.9.0.16) moved to HEALTHY state.
dn3_1    | 2023-03-10 22:53:54,087 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-03-10 22:53:56,713 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
recon_1  | 2023-03-10 22:54:24,875 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=67e5bfb1-7c89-4f8f-97b9-bf3cdec9b952. Trying to get from SCM.
recon_1  | 2023-03-10 22:54:24,913 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 67e5bfb1-7c89-4f8f-97b9-bf3cdec9b952, Nodes: e3e4587c-aa42-4e86-ae9a-d3e448365275(ha_dn1_1.ha_net/10.9.0.15), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:e3e4587c-aa42-4e86-ae9a-d3e448365275, CreationTimestamp2023-03-10T22:53:54.224Z[UTC]] to Recon pipeline metadata.
dn1_1    | 2023-03-10 22:53:24,607 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-CBC13A60F575,id=e3e4587c-aa42-4e86-ae9a-d3e448365275
scm_1    | 2023-03-10 22:53:54,224 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=67e5bfb1-7c89-4f8f-97b9-bf3cdec9b952 to datanode:e3e4587c-aa42-4e86-ae9a-d3e448365275
dn3_1    | 2023-03-10 22:53:54,089 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-03-10 22:53:56,714 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
recon_1  | 2023-03-10 22:54:24,914 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 67e5bfb1-7c89-4f8f-97b9-bf3cdec9b952, Nodes: e3e4587c-aa42-4e86-ae9a-d3e448365275(ha_dn1_1.ha_net/10.9.0.15), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:e3e4587c-aa42-4e86-ae9a-d3e448365275, CreationTimestamp2023-03-10T22:53:54.224Z[UTC]].
dn1_1    | 2023-03-10 22:53:24,607 [Command processor thread] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: shutdown e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-FollowerState
scm_1    | 2023-03-10 22:53:54,226 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
dn3_1    | 2023-03-10 22:53:54,089 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-10 22:53:56,714 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
recon_1  | 2023-03-10 22:54:24,977 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=808ced26-8679-46a5-9ee3-c7be60589b89. Trying to get from SCM.
dn1_1    | 2023-03-10 22:53:24,608 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-FollowerState] INFO impl.FollowerState: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-FollowerState was interrupted
scm_1    | 2023-03-10 22:53:54,227 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 67e5bfb1-7c89-4f8f-97b9-bf3cdec9b952, Nodes: e3e4587c-aa42-4e86-ae9a-d3e448365275(ha_dn1_1.ha_net/10.9.0.15), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-10T22:53:54.224Z[UTC]].
dn3_1    | 2023-03-10 22:53:54,099 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-03-10 22:53:56,716 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=d8130c23-c209-44b4-868f-c0db968e18e2
recon_1  | 2023-03-10 22:54:24,986 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 808ced26-8679-46a5-9ee3-c7be60589b89, Nodes: 3892a4e1-c878-42af-adb7-db66a90d61f4(ha_dn2_1.ha_net/10.9.0.16), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-10T22:53:54.228Z[UTC]] to Recon pipeline metadata.
dn1_1    | 2023-03-10 22:53:24,608 [Command processor thread] INFO impl.StateMachineUpdater: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-StateMachineUpdater: set stopIndex = 26
scm_1    | 2023-03-10 22:53:54,227 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
dn3_1    | 2023-03-10 22:53:54,100 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-03-10 22:53:56,716 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=d8130c23-c209-44b4-868f-c0db968e18e2.
recon_1  | 2023-03-10 22:54:24,987 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 808ced26-8679-46a5-9ee3-c7be60589b89, Nodes: 3892a4e1-c878-42af-adb7-db66a90d61f4(ha_dn2_1.ha_net/10.9.0.16), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-10T22:53:54.228Z[UTC]].
dn1_1    | 2023-03-10 22:53:24,608 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-CBC13A60F575: Taking a snapshot at:(t:7, i:26) file /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/sm/snapshot.7_26
scm_1    | 2023-03-10 22:53:54,228 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=808ced26-8679-46a5-9ee3-c7be60589b89 to datanode:3892a4e1-c878-42af-adb7-db66a90d61f4
dn3_1    | 2023-03-10 22:53:54,100 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-03-10 22:53:56,717 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
recon_1  | 2023-03-10 22:54:24,987 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=808ced26-8679-46a5-9ee3-c7be60589b89 reported by 3892a4e1-c878-42af-adb7-db66a90d61f4(ha_dn2_1.ha_net/10.9.0.16)
dn1_1    | 2023-03-10 22:53:24,610 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-CBC13A60F575: Finished taking a snapshot at:(t:7, i:26) file:/data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575/sm/snapshot.7_26 took: 2 ms
scm_1    | 2023-03-10 22:53:54,229 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 808ced26-8679-46a5-9ee3-c7be60589b89, Nodes: 3892a4e1-c878-42af-adb7-db66a90d61f4(ha_dn2_1.ha_net/10.9.0.16), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-10T22:53:54.228Z[UTC]].
dn3_1    | 2023-03-10 22:53:54,100 [pool-34-thread-1] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm_1    | 2023-03-10 22:53:54,232 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
scm_1    | 2023-03-10 22:53:54,372 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 47893e13-e605-460c-a18a-cd960c190469, Nodes: 178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:178b30e1-b74d-4f4d-a142-c930eee71455, CreationTimestamp2023-03-10T22:53:24.260Z[UTC]] moved to OPEN state
scm_1    | 2023-03-10 22:53:54,412 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 5b37649b-4609-4fbb-bbb4-98cd3de58f62, Nodes: 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:1a6d358d-6662-4447-914c-d709a67ff716, CreationTimestamp2023-03-10T22:53:24.225Z[UTC]] moved to OPEN state
dn1_1    | 2023-03-10 22:53:24,611 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-StateMachineUpdater] INFO impl.StateMachineUpdater: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-StateMachineUpdater: Took a snapshot at index 26
dn1_1    | 2023-03-10 22:53:24,611 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-StateMachineUpdater] INFO impl.StateMachineUpdater: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-StateMachineUpdater: snapshotIndex: updateIncreasingly 20 -> 26
dn1_1    | 2023-03-10 22:53:24,612 [Command processor thread] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575: closes. applyIndex: 26
dn1_1    | 2023-03-10 22:53:24,613 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn4_1    | 2023-03-10 22:53:56,728 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-10 22:53:59,142 [grpc-default-executor-2] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88: receive requestVote(PRE_VOTE, 178b30e1-b74d-4f4d-a142-c930eee71455, group-6265FBDC9D88, 0, (t:0, i:0))
dn4_1    | 2023-03-10 22:53:59,143 [grpc-default-executor-2] INFO impl.VoteContext: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-FOLLOWER: reject PRE_VOTE from 178b30e1-b74d-4f4d-a142-c930eee71455: our priority 1 > candidate's priority 0
dn4_1    | 2023-03-10 22:53:59,144 [grpc-default-executor-2] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88 replies to PRE_VOTE vote request: 178b30e1-b74d-4f4d-a142-c930eee71455<-e78c5ce1-46ab-4889-a0cd-5903ae46614d#0:FAIL-t0. Peer's state: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88:t0, leader=null, voted=, raftlog=Memoized:e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:53:59,339 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-FollowerState] INFO impl.FollowerState: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5094269976ns, electionTimeout:5068ms
dn4_1    | 2023-03-10 22:53:59,339 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-FollowerState] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: shutdown e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-FollowerState
dn4_1    | 2023-03-10 22:53:59,340 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-FollowerState] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn4_1    | 2023-03-10 22:53:59,340 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | 2023-03-10 22:53:59,340 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-FollowerState] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: start e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4
dn4_1    | 2023-03-10 22:53:59,341 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO impl.LeaderElection: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1    | 2023-03-10 22:53:55,569 [IPC Server handler 6 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm_1    | 2023-03-10 22:53:56,371 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm_1    | 2023-03-10 22:53:56,371 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm_1    | 2023-03-10 22:53:56,491 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d8130c23-c209-44b4-868f-c0db968e18e2, Nodes: e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:e78c5ce1-46ab-4889-a0cd-5903ae46614d, CreationTimestamp2023-03-10T22:53:24.293Z[UTC]] moved to OPEN state
scm_1    | 2023-03-10 22:53:59,456 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: a8f227f9-9826-491d-a836-6265fbdc9d88, Nodes: 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17)e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18)178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:e78c5ce1-46ab-4889-a0cd-5903ae46614d, CreationTimestamp2023-03-10T22:53:24.245Z[UTC]] moved to OPEN state
scm_1    | 2023-03-10 22:54:00,078 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 995fa1d6-41cf-4ed4-929b-577a15fd12b5, Nodes: 1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17)178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19)e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:1a6d358d-6662-4447-914c-d709a67ff716, CreationTimestamp2023-03-10T22:53:24.284Z[UTC]] moved to OPEN state
dn1_1    | 2023-03-10 22:53:24,614 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575-SegmentedRaftLogWorker close()
dn1_1    | 2023-03-10 22:53:24,618 [Command processor thread] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-CBC13A60F575: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/78b03a88-75c1-4060-9962-cbc13a60f575
dn1_1    | 2023-03-10 22:53:24,618 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=78b03a88-75c1-4060-9962-cbc13a60f575 command on datanode e3e4587c-aa42-4e86-ae9a-d3e448365275.
dn1_1    | 2023-03-10 22:53:24,891 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: e3e4587c-aa42-4e86-ae9a-d3e448365275: Completed APPEND_ENTRIES, lastRequest: e78c5ce1-46ab-4889-a0cd-5903ae46614d->e3e4587c-aa42-4e86-ae9a-d3e448365275#317-t7,previous=(t:7, i:25),leaderCommit=25,initializing? true,entries: size=1, first=(t:7, i:26), METADATAENTRY(c:25)
scm_1    | 2023-03-10 22:54:00,570 [IPC Server handler 6 on default port 9860] INFO upgrade.UpgradeFinalizer: Open pipeline found after SCM finalization
dn1_1    | 2023-03-10 22:53:24,892 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: e3e4587c-aa42-4e86-ae9a-d3e448365275: Completed APPEND_ENTRIES, lastRequest: null
dn1_1    | 2023-03-10 22:53:26,620 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@459003a0] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[2(0)], numOfContainers=1, numOfBlocks=1
dn4_1    | 2023-03-10 22:53:59,342 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4-1] INFO server.GrpcServerProtocolClient: Build channel for 178b30e1-b74d-4f4d-a142-c930eee71455
dn4_1    | 2023-03-10 22:53:59,345 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm_1    | 2023-03-10 22:54:00,571 [IPC Server handler 6 on default port 9860] INFO upgrade.UpgradeFinalizer: Finalization is done.
recon_1  | 2023-03-10 22:54:24,987 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 808ced26-8679-46a5-9ee3-c7be60589b89, Nodes: 3892a4e1-c878-42af-adb7-db66a90d61f4(ha_dn2_1.ha_net/10.9.0.16), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3892a4e1-c878-42af-adb7-db66a90d61f4, CreationTimestamp2023-03-10T22:53:54.228Z[UTC]] moved to OPEN state
dn1_1    | 2023-03-10 22:53:54,700 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn1_1    | 2023-03-10 22:54:10,083 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 1/4999 blocks from 1 candidate containers.
dn3_1    | 2023-03-10 22:53:54,101 [pool-34-thread-1] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1    | 2023-03-10 22:54:24,233 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
recon_1  | 2023-03-10 22:55:04,629 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
dn4_1    | 2023-03-10 22:53:59,349 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-03-10 22:54:10,112 [BlockDeletingService#0] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/current/containerDir0/2/chunks/111677748019200002.block
dn1_1    | 2023-03-10 22:54:24,786 [pool-38-thread-1] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275: new RaftServerImpl for group-BF3CDEC9B952:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn1_1    | 2023-03-10 22:54:24,786 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-03-10 22:53:54,102 [pool-34-thread-1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88: start as a follower, conf=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:53:54,102 [pool-34-thread-1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn3_1    | 2023-03-10 22:53:54,103 [pool-34-thread-1] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: start 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88-FollowerState
dn4_1    | 2023-03-10 22:53:59,373 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO impl.LeaderElection: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn4_1    | 2023-03-10 22:53:59,373 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO impl.LeaderElection:   Response 0: e78c5ce1-46ab-4889-a0cd-5903ae46614d<-1a6d358d-6662-4447-914c-d709a67ff716#0:OK-t0
scm_1    | 2023-03-10 22:54:24,897 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 67e5bfb1-7c89-4f8f-97b9-bf3cdec9b952, Nodes: e3e4587c-aa42-4e86-ae9a-d3e448365275(ha_dn1_1.ha_net/10.9.0.15), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:e3e4587c-aa42-4e86-ae9a-d3e448365275, CreationTimestamp2023-03-10T22:53:54.224Z[UTC]] moved to OPEN state
dn3_1    | 2023-03-10 22:53:54,104 [pool-34-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6265FBDC9D88,id=1a6d358d-6662-4447-914c-d709a67ff716
dn3_1    | 2023-03-10 22:53:54,104 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-03-10 22:54:24,786 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-03-10 22:54:24,786 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-03-10 22:54:24,786 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
scm_1    | 2023-03-10 22:54:24,984 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 808ced26-8679-46a5-9ee3-c7be60589b89, Nodes: 3892a4e1-c878-42af-adb7-db66a90d61f4(ha_dn2_1.ha_net/10.9.0.16), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3892a4e1-c878-42af-adb7-db66a90d61f4, CreationTimestamp2023-03-10T22:53:54.228Z[UTC]] moved to OPEN state
dn3_1    | 2023-03-10 22:53:54,104 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-03-10 22:53:54,104 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-03-10 22:54:24,786 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1    | 2023-03-10 22:54:26,371 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
dn3_1    | 2023-03-10 22:53:54,104 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-03-10 22:53:54,105 [1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-10 22:53:59,376 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO impl.LeaderElection: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4 PRE_VOTE round 0: result PASSED
dn1_1    | 2023-03-10 22:54:24,786 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-03-10 22:53:54,114 [1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-10 22:53:54,320 [Command processor thread] INFO server.RaftServer: 1a6d358d-6662-4447-914c-d709a67ff716: addNew group-98CD3DE58F62:[1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] returns group-98CD3DE58F62:java.util.concurrent.CompletableFuture@8f2346f[Not completed]
dn4_1    | 2023-03-10 22:53:59,383 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO impl.LeaderElection: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4 ELECTION round 0: submit vote requests at term 1 for -1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:53:59,383 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-10 22:53:59,383 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-10 22:53:54,338 [pool-34-thread-1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716: new RaftServerImpl for group-98CD3DE58F62:[1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn3_1    | 2023-03-10 22:53:54,338 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-03-10 22:54:24,791 [pool-38-thread-1] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952: ConfigurationManager, init=-1: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1    | 2023-03-10 22:54:26,372 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
dn3_1    | 2023-03-10 22:53:54,340 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
recon_1  | 2023-03-10 22:55:04,630 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
dn1_1    | 2023-03-10 22:54:24,791 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm_1    | 2023-03-10 22:54:54,236 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
dn3_1    | 2023-03-10 22:53:54,342 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-03-10 22:53:54,342 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-03-10 22:53:54,342 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-03-10 22:54:24,796 [Command processor thread] INFO server.RaftServer: e3e4587c-aa42-4e86-ae9a-d3e448365275: addNew group-BF3CDEC9B952:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER] returns group-BF3CDEC9B952:java.util.concurrent.CompletableFuture@4b4f0718[Not completed]
scm_1    | 2023-03-10 22:54:56,371 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm_1    | 2023-03-10 22:54:56,372 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
recon_1  | 2023-03-10 22:55:04,630 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 151 
dn3_1    | 2023-03-10 22:53:54,344 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-03-10 22:54:24,799 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
recon_1  | 2023-03-10 22:55:04,639 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
dn3_1    | 2023-03-10 22:53:54,347 [pool-34-thread-1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62: ConfigurationManager, init=-1: peers:[1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-03-10 22:53:59,435 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO impl.LeaderElection: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn4_1    | 2023-03-10 22:53:59,435 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO impl.LeaderElection:   Response 0: e78c5ce1-46ab-4889-a0cd-5903ae46614d<-1a6d358d-6662-4447-914c-d709a67ff716#0:OK-t1
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
dn1_1    | 2023-03-10 22:54:24,799 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-03-10 22:53:54,347 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-03-10 22:53:54,351 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-03-10 22:53:54,351 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-03-10 22:53:54,352 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-03-10 22:54:24,799 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
dn3_1    | 2023-03-10 22:53:54,353 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
scm_1    | 2023-03-10 22:55:24,239 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
dn4_1    | 2023-03-10 22:53:59,435 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO impl.LeaderElection: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4 ELECTION round 0: result PASSED
dn4_1    | 2023-03-10 22:53:59,436 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: shutdown e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
dn3_1    | 2023-03-10 22:53:54,354 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1    | 2023-03-10 22:55:26,373 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm_1    | 2023-03-10 22:55:26,373 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
dn1_1    | 2023-03-10 22:54:24,799 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-03-10 22:54:24,799 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-03-10 22:53:54,359 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1    | 2023-03-10 22:55:53,596 [IPC Server handler 7 on default port 9863] WARN ha.SequenceIdGenerator: Failed to allocate a batch for containerId, expected lastId is 0, actual lastId is 3000.
scm_1    | 2023-03-10 22:55:53,597 [IPC Server handler 7 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 3000 to 4000.
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
dn1_1    | 2023-03-10 22:54:24,800 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-03-10 22:53:54,361 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-03-10 22:53:59,437 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm_1    | 2023-03-10 22:55:53,600 [IPC Server handler 7 on default port 9863] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019203000.
scm_1    | 2023-03-10 22:55:53,601 [IPC Server handler 7 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019203000 to 111677748019204000.
scm_1    | 2023-03-10 22:55:54,240 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
dn4_1    | 2023-03-10 22:53:59,438 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6265FBDC9D88 with new leaderId: e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn4_1    | 2023-03-10 22:53:59,438 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88: change Leader from null to e78c5ce1-46ab-4889-a0cd-5903ae46614d at term 1 for becomeLeader, leader elected after 5231ms
dn4_1    | 2023-03-10 22:53:59,441 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
dn4_1    | 2023-03-10 22:53:59,442 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-03-10 22:53:59,443 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn4_1    | 2023-03-10 22:53:59,443 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
dn3_1    | 2023-03-10 22:53:54,365 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-03-10 22:53:54,365 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1    | 2023-03-10 22:55:56,373 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm_1    | 2023-03-10 22:55:56,374 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:55:04,640 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
dn1_1    | 2023-03-10 22:54:24,800 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-03-10 22:53:59,444 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn4_1    | 2023-03-10 22:53:59,445 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn4_1    | 2023-03-10 22:53:59,445 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-03-10 22:53:59,445 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn4_1    | 2023-03-10 22:53:59,448 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn1_1    | 2023-03-10 22:54:24,800 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-03-10 22:54:24,800 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-03-10 22:54:24,800 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-03-10 22:54:24,800 [pool-38-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/67e5bfb1-7c89-4f8f-97b9-bf3cdec9b952 does not exist. Creating ...
dn1_1    | 2023-03-10 22:54:24,821 [pool-38-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/67e5bfb1-7c89-4f8f-97b9-bf3cdec9b952/in_use.lock acquired by nodename 7@516736b05cb5
dn1_1    | 2023-03-10 22:54:24,825 [pool-38-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/67e5bfb1-7c89-4f8f-97b9-bf3cdec9b952 has been successfully formatted.
dn1_1    | 2023-03-10 22:54:24,825 [pool-38-thread-1] INFO ratis.ContainerStateMachine: group-BF3CDEC9B952: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn1_1    | 2023-03-10 22:54:24,825 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-03-10 22:54:24,825 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-03-10 22:54:24,825 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-03-10 22:54:24,826 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-03-10 22:54:24,826 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-03-10 22:54:24,826 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-03-10 22:54:24,826 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-03-10 22:54:24,826 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-03-10 22:54:24,826 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: new e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/67e5bfb1-7c89-4f8f-97b9-bf3cdec9b952
dn1_1    | 2023-03-10 22:54:24,826 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-03-10 22:54:24,826 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-03-10 22:54:24,826 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-03-10 22:54:24,826 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-03-10 22:53:54,366 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-03-10 22:53:54,366 [pool-34-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5b37649b-4609-4fbb-bbb4-98cd3de58f62 does not exist. Creating ...
dn3_1    | 2023-03-10 22:53:54,371 [pool-34-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5b37649b-4609-4fbb-bbb4-98cd3de58f62/in_use.lock acquired by nodename 7@0de9850545d7
dn3_1    | 2023-03-10 22:53:54,391 [pool-34-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5b37649b-4609-4fbb-bbb4-98cd3de58f62 has been successfully formatted.
dn3_1    | 2023-03-10 22:53:54,392 [pool-34-thread-1] INFO ratis.ContainerStateMachine: group-98CD3DE58F62: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-03-10 22:53:54,394 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-03-10 22:53:54,401 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-03-10 22:53:54,402 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-03-10 22:53:54,402 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-03-10 22:53:54,402 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-03-10 22:53:54,402 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-03-10 22:53:54,404 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-03-10 22:53:54,404 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-03-10 22:53:54,406 [pool-34-thread-1] INFO segmented.SegmentedRaftLogWorker: new 1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5b37649b-4609-4fbb-bbb4-98cd3de58f62
dn3_1    | 2023-03-10 22:53:54,414 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-03-10 22:53:54,414 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-03-10 22:53:54,415 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-03-10 22:53:54,418 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm_1    | 2023-03-10 22:56:24,242 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
dn4_1    | 2023-03-10 22:53:59,448 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-10 22:53:59,448 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn4_1    | 2023-03-10 22:53:59,448 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn4_1    | 2023-03-10 22:53:59,455 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-03-10 22:53:59,455 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-03-10 22:53:59,455 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-03-10 22:53:59,455 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn4_1    | 2023-03-10 22:53:59,460 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn4_1    | 2023-03-10 22:53:59,460 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-03-10 22:53:59,460 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn4_1    | 2023-03-10 22:53:59,461 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
scm_1    | 2023-03-10 22:56:26,382 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm_1    | 2023-03-10 22:56:26,383 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm_1    | 2023-03-10 22:56:54,244 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
dn4_1    | 2023-03-10 22:53:59,461 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-03-10 22:53:59,461 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-03-10 22:53:59,461 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-03-10 22:53:59,461 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn4_1    | 2023-03-10 22:53:59,462 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: start e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderStateImpl
dn4_1    | 2023-03-10 22:53:59,462 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-03-10 22:53:59,467 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a8f227f9-9826-491d-a836-6265fbdc9d88/current/log_inprogress_0
dn4_1    | 2023-03-10 22:53:59,522 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88-LeaderElection4] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-6265FBDC9D88: set configuration 0: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:53:59,780 [grpc-default-executor-5] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5: receive requestVote(PRE_VOTE, 178b30e1-b74d-4f4d-a142-c930eee71455, group-577A15FD12B5, 0, (t:0, i:0))
dn4_1    | 2023-03-10 22:53:59,781 [grpc-default-executor-5] INFO impl.VoteContext: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5-FOLLOWER: accept PRE_VOTE from 178b30e1-b74d-4f4d-a142-c930eee71455: our priority 0 <= candidate's priority 0
dn4_1    | 2023-03-10 22:53:59,781 [grpc-default-executor-5] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5 replies to PRE_VOTE vote request: 178b30e1-b74d-4f4d-a142-c930eee71455<-e78c5ce1-46ab-4889-a0cd-5903ae46614d#0:OK-t0. Peer's state: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5:t0, leader=null, voted=, raftlog=Memoized:e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:54:00,024 [grpc-default-executor-5] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5: receive requestVote(PRE_VOTE, 1a6d358d-6662-4447-914c-d709a67ff716, group-577A15FD12B5, 0, (t:0, i:0))
dn4_1    | 2023-03-10 22:54:00,025 [grpc-default-executor-5] INFO impl.VoteContext: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5-FOLLOWER: accept PRE_VOTE from 1a6d358d-6662-4447-914c-d709a67ff716: our priority 0 <= candidate's priority 1
dn4_1    | 2023-03-10 22:54:00,025 [grpc-default-executor-5] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5 replies to PRE_VOTE vote request: 1a6d358d-6662-4447-914c-d709a67ff716<-e78c5ce1-46ab-4889-a0cd-5903ae46614d#0:OK-t0. Peer's state: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5:t0, leader=null, voted=, raftlog=Memoized:e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:54:00,054 [grpc-default-executor-5] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5: receive requestVote(ELECTION, 1a6d358d-6662-4447-914c-d709a67ff716, group-577A15FD12B5, 1, (t:0, i:0))
dn4_1    | 2023-03-10 22:54:00,055 [grpc-default-executor-5] INFO impl.VoteContext: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5-FOLLOWER: accept ELECTION from 1a6d358d-6662-4447-914c-d709a67ff716: our priority 0 <= candidate's priority 1
dn4_1    | 2023-03-10 22:54:00,055 [grpc-default-executor-5] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:1a6d358d-6662-4447-914c-d709a67ff716
dn4_1    | 2023-03-10 22:54:00,055 [grpc-default-executor-5] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: shutdown e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5-FollowerState
dn4_1    | 2023-03-10 22:54:00,055 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5-FollowerState] INFO impl.FollowerState: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5-FollowerState was interrupted
dn4_1    | 2023-03-10 22:54:00,056 [grpc-default-executor-5] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: start e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5-FollowerState
dn4_1    | 2023-03-10 22:54:00,057 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-03-10 22:54:00,057 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-03-10 22:54:00,061 [grpc-default-executor-5] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5 replies to ELECTION vote request: 1a6d358d-6662-4447-914c-d709a67ff716<-e78c5ce1-46ab-4889-a0cd-5903ae46614d#0:OK-t1. Peer's state: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5:t1, leader=null, voted=1a6d358d-6662-4447-914c-d709a67ff716, raftlog=Memoized:e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:54:24,826 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-03-10 22:54:24,826 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-03-10 22:54:24,826 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-03-10 22:54:24,826 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-03-10 22:54:24,827 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-03-10 22:54:24,827 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-03-10 22:54:24,841 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
scm_1    | 2023-03-10 22:56:56,383 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
dn4_1    | 2023-03-10 22:54:00,240 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-577A15FD12B5 with new leaderId: 1a6d358d-6662-4447-914c-d709a67ff716
dn4_1    | 2023-03-10 22:54:00,240 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-server-thread1] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5: change Leader from null to 1a6d358d-6662-4447-914c-d709a67ff716 at term 1 for appendEntries, leader elected after 5356ms
dn4_1    | 2023-03-10 22:54:00,278 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-server-thread2] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5: set configuration 0: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:54:24,885 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-03-10 22:54:24,885 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-03-10 22:54:24,886 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-03-10 22:54:00,279 [e78c5ce1-46ab-4889-a0cd-5903ae46614d-server-thread2] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-03-10 22:54:00,284 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-577A15FD12B5-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/995fa1d6-41cf-4ed4-929b-577a15fd12b5/current/log_inprogress_0
dn3_1    | 2023-03-10 22:53:54,419 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-03-10 22:53:54,419 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-03-10 22:54:01,784 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-FollowerState] INFO impl.FollowerState: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5073829818ns, electionTimeout:5050ms
dn4_1    | 2023-03-10 22:54:01,785 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-FollowerState] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: shutdown e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-FollowerState
dn3_1    | 2023-03-10 22:53:54,419 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-03-10 22:53:54,419 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-03-10 22:53:54,420 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-03-10 22:54:01,785 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-FollowerState] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn4_1    | 2023-03-10 22:54:01,785 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | 2023-03-10 22:54:01,786 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-FollowerState] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: start e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5
dn4_1    | 2023-03-10 22:54:01,793 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5] INFO impl.LeaderElection: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1    | 2023-03-10 22:56:56,383 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
dn1_1    | 2023-03-10 22:54:24,891 [pool-38-thread-1] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-03-10 22:54:24,922 [pool-38-thread-1] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952: start as a follower, conf=-1: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:54:01,793 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5] INFO impl.LeaderElection: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5 PRE_VOTE round 0: result PASSED (term=0)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:55:04,641 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
scm_1    | 2023-03-10 22:56:56,407 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 33 milliseconds for processing 6 containers.
scm_1    | 2023-03-10 22:57:09,697 [IPC Server handler 7 on default port 9863] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: f8a55e49-9acd-4b0a-9d97-19a5082b98df, Nodes: e3e4587c-aa42-4e86-ae9a-d3e448365275(ha_dn1_1.ha_net/10.9.0.15)178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19)e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18)3892a4e1-c878-42af-adb7-db66a90d61f4(ha_dn2_1.ha_net/10.9.0.16)1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17), ReplicationConfig: EC{rs-3-2-1048576}, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-10T22:57:09.694Z[UTC]].
scm_1    | 2023-03-10 22:57:09,699 [IPC Server handler 7 on default port 9863] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: f8a55e49-9acd-4b0a-9d97-19a5082b98df, Nodes: e3e4587c-aa42-4e86-ae9a-d3e448365275(ha_dn1_1.ha_net/10.9.0.15)178b30e1-b74d-4f4d-a142-c930eee71455(ha_dn5_1.ha_net/10.9.0.19)e78c5ce1-46ab-4889-a0cd-5903ae46614d(ha_dn4_1.ha_net/10.9.0.18)3892a4e1-c878-42af-adb7-db66a90d61f4(ha_dn2_1.ha_net/10.9.0.16)1a6d358d-6662-4447-914c-d709a67ff716(ha_dn3_1.ha_net/10.9.0.17), ReplicationConfig: EC{rs-3-2-1048576}, State:ALLOCATED, leaderId:, CreationTimestamp2023-03-10T22:57:09.694Z[UTC]] moved to OPEN state
scm_1    | 2023-03-10 22:57:24,247 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
scm_1    | 2023-03-10 22:57:26,384 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm_1    | 2023-03-10 22:57:26,385 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
dn3_1    | 2023-03-10 22:53:54,421 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-03-10 22:53:54,672 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-03-10 22:53:54,676 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-03-10 22:53:54,676 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-03-10 22:53:54,678 [pool-34-thread-1] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-03-10 22:54:24,922 [pool-38-thread-1] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn4_1    | 2023-03-10 22:54:01,797 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5] INFO impl.LeaderElection: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5 ELECTION round 0: submit vote requests at term 1 for -1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:54:01,797 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5] INFO impl.LeaderElection: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5 ELECTION round 0: result PASSED (term=1)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
dn1_1    | 2023-03-10 22:54:24,922 [pool-38-thread-1] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: start e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-FollowerState
dn1_1    | 2023-03-10 22:54:24,930 [pool-38-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-BF3CDEC9B952,id=e3e4587c-aa42-4e86-ae9a-d3e448365275
dn3_1    | 2023-03-10 22:53:54,678 [pool-34-thread-1] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-03-10 22:53:54,687 [pool-34-thread-1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62: start as a follower, conf=-1: peers:[1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:53:54,687 [pool-34-thread-1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn1_1    | 2023-03-10 22:54:24,931 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-03-10 22:54:24,931 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-03-10 22:53:54,687 [pool-34-thread-1] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: start 1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-FollowerState
dn3_1    | 2023-03-10 22:53:54,693 [pool-34-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-98CD3DE58F62,id=1a6d358d-6662-4447-914c-d709a67ff716
dn3_1    | 2023-03-10 22:53:54,693 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-03-10 22:54:24,932 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-03-10 22:54:24,933 [pool-38-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-03-10 22:53:54,694 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-03-10 22:53:54,694 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-03-10 22:53:54,694 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-03-10 22:54:24,935 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-03-10 22:54:24,936 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-10 22:53:54,695 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-03-10 22:53:54,696 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-10 22:53:54,717 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=5b37649b-4609-4fbb-bbb4-98cd3de58f62
dn1_1    | 2023-03-10 22:54:24,947 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=67e5bfb1-7c89-4f8f-97b9-bf3cdec9b952
dn1_1    | 2023-03-10 22:54:24,950 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=67e5bfb1-7c89-4f8f-97b9-bf3cdec9b952.
dn1_1    | 2023-03-10 22:54:30,096 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-FollowerState] INFO impl.FollowerState: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5173638208ns, electionTimeout:5158ms
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
dn1_1    | 2023-03-10 22:54:30,096 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-FollowerState] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: shutdown e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-FollowerState
dn1_1    | 2023-03-10 22:54:30,096 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-FollowerState] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn3_1    | 2023-03-10 22:53:54,718 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=5b37649b-4609-4fbb-bbb4-98cd3de58f62.
dn4_1    | 2023-03-10 22:54:01,797 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: shutdown e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5
dn3_1    | 2023-03-10 22:53:54,719 [Command processor thread] INFO server.RaftServer: 1a6d358d-6662-4447-914c-d709a67ff716: addNew group-577A15FD12B5:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] returns group-577A15FD12B5:java.util.concurrent.CompletableFuture@39548442[Not completed]
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
dn4_1    | 2023-03-10 22:54:01,797 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn1_1    | 2023-03-10 22:54:30,097 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-03-10 22:54:30,097 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-FollowerState] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: start e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
dn4_1    | 2023-03-10 22:54:01,798 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-C0DB968E18E2 with new leaderId: e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn4_1    | 2023-03-10 22:54:01,799 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2: change Leader from null to e78c5ce1-46ab-4889-a0cd-5903ae46614d at term 1 for becomeLeader, leader elected after 5366ms
dn4_1    | 2023-03-10 22:54:01,800 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn4_1    | 2023-03-10 22:54:01,817 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
dn4_1    | 2023-03-10 22:54:01,817 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn4_1    | 2023-03-10 22:54:01,818 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn4_1    | 2023-03-10 22:54:01,818 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn4_1    | 2023-03-10 22:54:01,818 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
dn3_1    | 2023-03-10 22:53:54,723 [pool-34-thread-1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716: new RaftServerImpl for group-577A15FD12B5:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn3_1    | 2023-03-10 22:53:54,724 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-03-10 22:53:54,729 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-03-10 22:53:54,729 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 2023-03-10 22:53:54,730 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-03-10 22:53:54,730 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-03-10 22:53:54,731 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-03-10 22:53:54,731 [pool-34-thread-1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5: ConfigurationManager, init=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-03-10 22:53:54,742 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-03-10 22:53:54,750 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-03-10 22:53:54,751 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-03-10 22:53:54,754 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-03-10 22:53:54,755 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-03-10 22:53:54,755 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-03-10 22:53:54,759 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-03-10 22:53:54,759 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-03-10 22:53:54,760 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-03-10 22:53:54,760 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-03-10 22:53:54,761 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-03-10 22:53:54,761 [pool-34-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/995fa1d6-41cf-4ed4-929b-577a15fd12b5 does not exist. Creating ...
dn3_1    | 2023-03-10 22:53:54,766 [pool-34-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/995fa1d6-41cf-4ed4-929b-577a15fd12b5/in_use.lock acquired by nodename 7@0de9850545d7
dn3_1    | 2023-03-10 22:53:54,770 [pool-34-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/995fa1d6-41cf-4ed4-929b-577a15fd12b5 has been successfully formatted.
dn3_1    | 2023-03-10 22:53:54,800 [pool-34-thread-1] INFO ratis.ContainerStateMachine: group-577A15FD12B5: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-03-10 22:53:54,801 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-03-10 22:53:54,801 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-03-10 22:53:54,804 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-03-10 22:53:54,805 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-03-10 22:53:54,805 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-03-10 22:53:54,805 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-03-10 22:53:54,806 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-03-10 22:53:54,815 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-03-10 22:53:54,815 [pool-34-thread-1] INFO segmented.SegmentedRaftLogWorker: new 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/995fa1d6-41cf-4ed4-929b-577a15fd12b5
dn3_1    | 2023-03-10 22:53:54,815 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-03-10 22:53:54,817 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-03-10 22:53:54,817 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-03-10 22:53:54,818 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-03-10 22:53:54,818 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-03-10 22:53:54,818 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-03-10 22:53:54,818 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-03-10 22:53:54,818 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-03-10 22:53:54,822 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-03-10 22:53:54,824 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-03-10 22:53:54,841 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-03-10 22:53:54,841 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-03-10 22:53:54,841 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-03-10 22:53:54,842 [pool-34-thread-1] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-03-10 22:53:54,842 [pool-34-thread-1] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-03-10 22:53:54,844 [pool-34-thread-1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5: start as a follower, conf=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:55:04,641 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 2, SequenceNumber diff: 3, SequenceNumber Lag from OM 0.
recon_1  | 2023-03-10 22:55:04,641 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 3 records
recon_1  | 2023-03-10 22:55:56,965 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #3001 got from ha_dn4_1.ha_net.
recon_1  | 2023-03-10 22:55:56,981 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Successfully added container #3001 to Recon.
recon_1  | 2023-03-10 22:56:04,647 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1  | 2023-03-10 22:56:04,648 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1  | 2023-03-10 22:56:04,649 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 154 
recon_1  | 2023-03-10 22:56:04,659 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:56:04,661 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
dn4_1    | 2023-03-10 22:54:01,819 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-03-10 22:54:01,819 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn4_1    | 2023-03-10 22:54:01,819 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5] INFO impl.RoleInfo: e78c5ce1-46ab-4889-a0cd-5903ae46614d: start e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderStateImpl
dn1_1    | 2023-03-10 22:54:30,104 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4] INFO impl.LeaderElection: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:54:30,104 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4] INFO impl.LeaderElection: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4 PRE_VOTE round 0: result PASSED (term=0)
dn1_1    | 2023-03-10 22:54:30,107 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4] INFO impl.LeaderElection: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4 ELECTION round 0: submit vote requests at term 1 for -1: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:54:30,107 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4] INFO impl.LeaderElection: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4 ELECTION round 0: result PASSED (term=1)
dn1_1    | 2023-03-10 22:54:30,107 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: shutdown e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4
dn1_1    | 2023-03-10 22:54:30,108 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn1_1    | 2023-03-10 22:54:30,108 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-BF3CDEC9B952 with new leaderId: e3e4587c-aa42-4e86-ae9a-d3e448365275
dn1_1    | 2023-03-10 22:54:30,108 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952: change Leader from null to e3e4587c-aa42-4e86-ae9a-d3e448365275 at term 1 for becomeLeader, leader elected after 5308ms
dn1_1    | 2023-03-10 22:54:30,108 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn1_1    | 2023-03-10 22:54:30,109 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-03-10 22:54:30,109 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn1_1    | 2023-03-10 22:54:30,110 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn1_1    | 2023-03-10 22:54:30,110 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn1_1    | 2023-03-10 22:54:30,110 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn1_1    | 2023-03-10 22:54:30,110 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-03-10 22:54:30,111 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn1_1    | 2023-03-10 22:54:30,112 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4] INFO impl.RoleInfo: e3e4587c-aa42-4e86-ae9a-d3e448365275: start e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderStateImpl
dn1_1    | 2023-03-10 22:54:30,112 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-SegmentedRaftLogWorker: Starting segment from index:0
dn1_1    | 2023-03-10 22:54:30,135 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-LeaderElection4] INFO server.RaftServer$Division: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952: set configuration 0: peers:[e3e4587c-aa42-4e86-ae9a-d3e448365275|rpc:10.9.0.15:9856|admin:10.9.0.15:9857|client:10.9.0.15:9858|dataStream:10.9.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-03-10 22:54:30,137 [e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e3e4587c-aa42-4e86-ae9a-d3e448365275@group-BF3CDEC9B952-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/67e5bfb1-7c89-4f8f-97b9-bf3cdec9b952/current/log_inprogress_0
dn1_1    | 2023-03-10 22:55:10,089 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-03-10 22:56:10,090 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-03-10 22:57:10,090 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn4_1    | 2023-03-10 22:54:01,819 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-03-10 22:54:01,837 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d8130c23-c209-44b4-868f-c0db968e18e2/current/log_inprogress_0
dn4_1    | 2023-03-10 22:54:01,854 [e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2-LeaderElection5] INFO server.RaftServer$Division: e78c5ce1-46ab-4889-a0cd-5903ae46614d@group-C0DB968E18E2: set configuration 0: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-03-10 22:54:11,127 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 1/4999 blocks from 1 candidate containers.
dn4_1    | 2023-03-10 22:54:11,154 [BlockDeletingService#0] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/current/containerDir0/2/chunks/111677748019200002.block
dn4_1    | 2023-03-10 22:55:11,132 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn4_1    | 2023-03-10 22:56:11,133 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn4_1    | 2023-03-10 22:57:11,133 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-03-10 22:53:54,844 [pool-34-thread-1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn3_1    | 2023-03-10 22:53:54,844 [pool-34-thread-1] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: start 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-FollowerState
dn3_1    | 2023-03-10 22:53:54,849 [pool-34-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-577A15FD12B5,id=1a6d358d-6662-4447-914c-d709a67ff716
dn3_1    | 2023-03-10 22:53:54,849 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-03-10 22:53:54,849 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-03-10 22:53:54,849 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-03-10 22:53:54,849 [pool-34-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-03-10 22:53:54,853 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-03-10 22:53:54,853 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=995fa1d6-41cf-4ed4-929b-577a15fd12b5
dn3_1    | 2023-03-10 22:53:54,870 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-10 22:53:55,809 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-1a6d358d-6662-4447-914c-d709a67ff716: Detected pause in JVM or host machine (eg GC): pause of approximately 412926868ns.
dn3_1    | GC pool 'ParNew' had collection(s): count=1 time=793ms
dn3_1    | 2023-03-10 22:53:56,731 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=995fa1d6-41cf-4ed4-929b-577a15fd12b5.
dn3_1    | 2023-03-10 22:53:59,097 [grpc-default-executor-3] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88: receive requestVote(PRE_VOTE, 178b30e1-b74d-4f4d-a142-c930eee71455, group-6265FBDC9D88, 0, (t:0, i:0))
dn3_1    | 2023-03-10 22:53:59,098 [grpc-default-executor-3] INFO impl.VoteContext: 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88-FOLLOWER: accept PRE_VOTE from 178b30e1-b74d-4f4d-a142-c930eee71455: our priority 0 <= candidate's priority 0
dn3_1    | 2023-03-10 22:53:59,098 [grpc-default-executor-3] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88 replies to PRE_VOTE vote request: 178b30e1-b74d-4f4d-a142-c930eee71455<-1a6d358d-6662-4447-914c-d709a67ff716#0:OK-t0. Peer's state: 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88:t0, leader=null, voted=, raftlog=Memoized:1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:53:59,136 [1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-03-10 22:53:59,136 [1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-10 22:53:59,353 [grpc-default-executor-3] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88: receive requestVote(PRE_VOTE, e78c5ce1-46ab-4889-a0cd-5903ae46614d, group-6265FBDC9D88, 0, (t:0, i:0))
dn3_1    | 2023-03-10 22:53:59,354 [grpc-default-executor-3] INFO impl.VoteContext: 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88-FOLLOWER: accept PRE_VOTE from e78c5ce1-46ab-4889-a0cd-5903ae46614d: our priority 0 <= candidate's priority 1
dn3_1    | 2023-03-10 22:53:59,354 [grpc-default-executor-3] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88 replies to PRE_VOTE vote request: e78c5ce1-46ab-4889-a0cd-5903ae46614d<-1a6d358d-6662-4447-914c-d709a67ff716#0:OK-t0. Peer's state: 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88:t0, leader=null, voted=, raftlog=Memoized:1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:53:59,414 [grpc-default-executor-3] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88: receive requestVote(ELECTION, e78c5ce1-46ab-4889-a0cd-5903ae46614d, group-6265FBDC9D88, 1, (t:0, i:0))
dn3_1    | 2023-03-10 22:53:59,414 [grpc-default-executor-3] INFO impl.VoteContext: 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88-FOLLOWER: accept ELECTION from e78c5ce1-46ab-4889-a0cd-5903ae46614d: our priority 0 <= candidate's priority 1
dn3_1    | 2023-03-10 22:53:59,414 [grpc-default-executor-3] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn3_1    | 2023-03-10 22:53:59,414 [grpc-default-executor-3] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: shutdown 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88-FollowerState
dn3_1    | 2023-03-10 22:53:59,415 [1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88-FollowerState] INFO impl.FollowerState: 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88-FollowerState was interrupted
dn3_1    | 2023-03-10 22:53:59,415 [grpc-default-executor-3] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: start 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88-FollowerState
dn3_1    | 2023-03-10 22:53:59,424 [1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-03-10 22:53:59,428 [grpc-default-executor-3] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88 replies to ELECTION vote request: e78c5ce1-46ab-4889-a0cd-5903ae46614d<-1a6d358d-6662-4447-914c-d709a67ff716#0:OK-t1. Peer's state: 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88:t1, leader=null, voted=e78c5ce1-46ab-4889-a0cd-5903ae46614d, raftlog=Memoized:1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:53:59,438 [1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-10 22:53:59,555 [1a6d358d-6662-4447-914c-d709a67ff716-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6265FBDC9D88 with new leaderId: e78c5ce1-46ab-4889-a0cd-5903ae46614d
dn3_1    | 2023-03-10 22:53:59,555 [1a6d358d-6662-4447-914c-d709a67ff716-server-thread1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88: change Leader from null to e78c5ce1-46ab-4889-a0cd-5903ae46614d at term 1 for appendEntries, leader elected after 5484ms
dn3_1    | 2023-03-10 22:53:59,556 [1a6d358d-6662-4447-914c-d709a67ff716-server-thread1] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88: set configuration 0: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:53:59,556 [1a6d358d-6662-4447-914c-d709a67ff716-server-thread1] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88-SegmentedRaftLogWorker: Starting segment from index:0
dn3_1    | 2023-03-10 22:53:59,558 [1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-6265FBDC9D88-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a8f227f9-9826-491d-a836-6265fbdc9d88/current/log_inprogress_0
dn3_1    | 2023-03-10 22:53:59,789 [grpc-default-executor-3] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5: receive requestVote(PRE_VOTE, 178b30e1-b74d-4f4d-a142-c930eee71455, group-577A15FD12B5, 0, (t:0, i:0))
dn3_1    | 2023-03-10 22:53:59,791 [grpc-default-executor-3] INFO impl.VoteContext: 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-FOLLOWER: reject PRE_VOTE from 178b30e1-b74d-4f4d-a142-c930eee71455: our priority 1 > candidate's priority 0
dn3_1    | 2023-03-10 22:53:59,792 [grpc-default-executor-3] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5 replies to PRE_VOTE vote request: 178b30e1-b74d-4f4d-a142-c930eee71455<-1a6d358d-6662-4447-914c-d709a67ff716#0:FAIL-t0. Peer's state: 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5:t0, leader=null, voted=, raftlog=Memoized:1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:53:59,840 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-FollowerState] INFO impl.FollowerState: 1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5153410633ns, electionTimeout:5144ms
dn3_1    | 2023-03-10 22:53:59,841 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-FollowerState] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: shutdown 1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-FollowerState
dn3_1    | 2023-03-10 22:53:59,842 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-FollowerState] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn3_1    | 2023-03-10 22:53:59,842 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-03-10 22:53:59,845 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-FollowerState] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: start 1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:56:04,681 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:56:04,683 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:56:04,684 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:56:04,685 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:56:04,690 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
dn3_1    | 2023-03-10 22:53:59,856 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4] INFO impl.LeaderElection: 1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:53:59,856 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4] INFO impl.LeaderElection: 1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4 PRE_VOTE round 0: result PASSED (term=0)
dn3_1    | 2023-03-10 22:53:59,862 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4] INFO impl.LeaderElection: 1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4 ELECTION round 0: submit vote requests at term 1 for -1: peers:[1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:53:59,862 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4] INFO impl.LeaderElection: 1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4 ELECTION round 0: result PASSED (term=1)
dn3_1    | 2023-03-10 22:53:59,862 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: shutdown 1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4
dn3_1    | 2023-03-10 22:53:59,862 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn3_1    | 2023-03-10 22:53:59,862 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-98CD3DE58F62 with new leaderId: 1a6d358d-6662-4447-914c-d709a67ff716
dn3_1    | 2023-03-10 22:53:59,863 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62: change Leader from null to 1a6d358d-6662-4447-914c-d709a67ff716 at term 1 for becomeLeader, leader elected after 5510ms
dn3_1    | 2023-03-10 22:53:59,865 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn3_1    | 2023-03-10 22:53:59,866 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-03-10 22:53:59,866 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-03-10 22:53:59,866 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-03-10 22:53:59,868 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-03-10 22:53:59,868 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn3_1    | 2023-03-10 22:53:59,868 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-03-10 22:53:59,868 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn3_1    | 2023-03-10 22:53:59,868 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: start 1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderStateImpl
dn3_1    | 2023-03-10 22:53:59,869 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-SegmentedRaftLogWorker: Starting segment from index:0
dn3_1    | 2023-03-10 22:53:59,871 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5b37649b-4609-4fbb-bbb4-98cd3de58f62/current/log_inprogress_0
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
dn3_1    | 2023-03-10 22:53:59,897 [1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62-LeaderElection4] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-98CD3DE58F62: set configuration 0: peers:[1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:54:00,016 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-FollowerState] INFO impl.FollowerState: 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5171507782ns, electionTimeout:5144ms
dn3_1    | 2023-03-10 22:54:00,017 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-FollowerState] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: shutdown 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-FollowerState
dn3_1    | 2023-03-10 22:54:00,017 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-FollowerState] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn3_1    | 2023-03-10 22:54:00,017 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-03-10 22:54:00,017 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-FollowerState] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: start 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5
dn3_1    | 2023-03-10 22:54:00,020 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO impl.LeaderElection: 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:54:00,029 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-03-10 22:54:00,041 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-10 22:54:00,041 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO impl.LeaderElection: 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn3_1    | 2023-03-10 22:54:00,041 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO impl.LeaderElection:   Response 0: 1a6d358d-6662-4447-914c-d709a67ff716<-178b30e1-b74d-4f4d-a142-c930eee71455#0:OK-t0
dn3_1    | 2023-03-10 22:54:00,041 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO impl.LeaderElection: 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5 PRE_VOTE round 0: result PASSED
dn3_1    | 2023-03-10 22:54:00,046 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO impl.LeaderElection: 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5 ELECTION round 0: submit vote requests at term 1 for -1: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:54:00,054 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-03-10 22:54:00,055 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-03-10 22:54:00,066 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO impl.LeaderElection: 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn3_1    | 2023-03-10 22:54:00,067 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO impl.LeaderElection:   Response 0: 1a6d358d-6662-4447-914c-d709a67ff716<-e78c5ce1-46ab-4889-a0cd-5903ae46614d#0:OK-t1
dn3_1    | 2023-03-10 22:54:00,068 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO impl.LeaderElection: 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5 ELECTION round 0: result PASSED
dn3_1    | 2023-03-10 22:54:00,068 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: shutdown 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5
dn3_1    | 2023-03-10 22:54:00,068 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn3_1    | 2023-03-10 22:54:00,068 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-577A15FD12B5 with new leaderId: 1a6d358d-6662-4447-914c-d709a67ff716
dn3_1    | 2023-03-10 22:54:00,069 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5: change Leader from null to 1a6d358d-6662-4447-914c-d709a67ff716 at term 1 for becomeLeader, leader elected after 5313ms
dn3_1    | 2023-03-10 22:54:00,069 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn3_1    | 2023-03-10 22:54:00,069 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-03-10 22:54:00,070 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-03-10 22:54:00,070 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-03-10 22:54:00,070 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-03-10 22:54:00,072 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn3_1    | 2023-03-10 22:54:00,075 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-03-10 22:54:00,076 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn3_1    | 2023-03-10 22:54:00,132 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn3_1    | 2023-03-10 22:54:00,133 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-03-10 22:54:00,133 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn3_1    | 2023-03-10 22:54:00,140 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn3_1    | 2023-03-10 22:54:00,143 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 2023-03-10 22:54:00,143 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-03-10 22:54:00,144 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-03-10 22:54:00,144 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn3_1    | 2023-03-10 22:54:00,155 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn3_1    | 2023-03-10 22:54:00,155 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-03-10 22:54:00,157 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn3_1    | 2023-03-10 22:54:00,157 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn3_1    | 2023-03-10 22:54:00,157 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-03-10 22:54:00,157 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-03-10 22:54:00,162 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-03-10 22:54:00,162 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn3_1    | 2023-03-10 22:54:00,173 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO impl.RoleInfo: 1a6d358d-6662-4447-914c-d709a67ff716: start 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderStateImpl
dn3_1    | 2023-03-10 22:54:00,175 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-SegmentedRaftLogWorker: Starting segment from index:0
dn3_1    | 2023-03-10 22:54:00,178 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/995fa1d6-41cf-4ed4-929b-577a15fd12b5/current/log_inprogress_0
dn3_1    | 2023-03-10 22:54:00,189 [1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5-LeaderElection5] INFO server.RaftServer$Division: 1a6d358d-6662-4447-914c-d709a67ff716@group-577A15FD12B5: set configuration 0: peers:[e78c5ce1-46ab-4889-a0cd-5903ae46614d|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 178b30e1-b74d-4f4d-a142-c930eee71455|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 1a6d358d-6662-4447-914c-d709a67ff716|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-03-10 22:54:11,311 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 3/4997 blocks from 2 candidate containers.
dn3_1    | 2023-03-10 22:54:11,366 [BlockDeletingService#0] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/current/containerDir0/1/chunks/111677748019200001.block
dn3_1    | 2023-03-10 22:54:11,366 [BlockDeletingService#2] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/current/containerDir0/2/chunks/111677748019200002.block
dn3_1    | 2023-03-10 22:54:11,368 [BlockDeletingService#0] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-d8d7fcf8-04e0-4b43-9ae2-eefeb561683b/current/containerDir0/1/chunks/111677748019200003.block
dn3_1    | 2023-03-10 22:55:11,319 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-03-10 22:56:11,319 [BlockDeletingService#4] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-03-10 22:57:11,321 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:56:04,691 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:56:04,692 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:56:04,699 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.delete(OMDBUpdatesHandler.java:77)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:56:04,701 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:56:04,703 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:56:04,705 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:56:04,707 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:56:04,729 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:56:04,733 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:56:04,736 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 6, SequenceNumber diff: 17, SequenceNumber Lag from OM 0.
recon_1  | 2023-03-10 22:56:04,736 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 17 records
recon_1  | 2023-03-10 22:56:04,753 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
recon_1  | 2023-03-10 22:56:04,753 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
recon_1  | 2023-03-10 22:56:05,276 [pool-49-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
recon_1  | 2023-03-10 22:56:05,278 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 0 OM DB update event(s).
recon_1  | 2023-03-10 22:56:05,281 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
recon_1  | 2023-03-10 22:57:05,295 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1  | 2023-03-10 22:57:05,296 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1  | 2023-03-10 22:57:05,296 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 171 
recon_1  | 2023-03-10 22:57:05,311 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,312 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,313 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,313 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.delete(OMDBUpdatesHandler.java:77)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,314 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,314 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,315 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,315 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,316 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,316 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,317 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,317 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.delete(OMDBUpdatesHandler.java:77)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,318 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,319 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,319 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,320 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,321 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,321 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,322 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,322 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,325 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,326 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,326 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,327 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,327 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,328 [pool-27-thread-1] ERROR tasks.OMDBUpdatesHandler: Exception when reading key : 
recon_1  | java.io.IOException: Rocks Database is closed
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.assertClose(RocksDatabase.java:406)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RocksDatabase.get(RocksDatabase.java:640)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:110)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.RDBTable.get(RDBTable.java:40)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTable(TypedTable.java:256)
recon_1  | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getSkipCache(TypedTable.java:196)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.processEvent(OMDBUpdatesHandler.java:128)
recon_1  | 	at org.apache.hadoop.ozone.recon.tasks.OMDBUpdatesHandler.put(OMDBUpdatesHandler.java:67)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(Native Method)
recon_1  | 	at org.rocksdb.WriteBatch.iterate(WriteBatch.java:63)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:456)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:415)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:494)
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-03-10 22:57:05,328 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 10, SequenceNumber diff: 27, SequenceNumber Lag from OM 0.
recon_1  | 2023-03-10 22:57:05,328 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 27 records
recon_1  | 2023-03-10 22:57:05,332 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
recon_1  | 2023-03-10 22:57:05,332 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
recon_1  | 2023-03-10 22:57:05,747 [pool-49-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
recon_1  | 2023-03-10 22:57:05,747 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 0 OM DB update event(s).
recon_1  | 2023-03-10 22:57:05,750 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
recon_1  | 2023-03-10 22:57:12,284 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #3002 got from ha_dn3_1.ha_net.
recon_1  | 2023-03-10 22:57:12,303 [FixedThreadPoolWithAffinityExecutor-1-0] WARN scm.ReconContainerManager: Pipeline PipelineID=f8a55e49-9acd-4b0a-9d97-19a5082b98df not found. Cannot add container #3002
recon_1  | 2023-03-10 22:57:12,305 [FixedThreadPoolWithAffinityExecutor-1-0] WARN scm.ReconIncrementalContainerReportHandler: Container 3002 not found!
recon_1  | 2023-03-10 22:57:12,309 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: New container #3002 got from ha_dn1_1.ha_net.
recon_1  | 2023-03-10 22:57:12,317 [FixedThreadPoolWithAffinityExecutor-9-0] WARN scm.ReconContainerManager: Pipeline PipelineID=f8a55e49-9acd-4b0a-9d97-19a5082b98df not found. Cannot add container #3002
recon_1  | 2023-03-10 22:57:12,318 [FixedThreadPoolWithAffinityExecutor-9-0] WARN scm.ReconIncrementalContainerReportHandler: Container 3002 not found!
recon_1  | 2023-03-10 22:57:12,339 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #3002 got from ha_dn2_1.ha_net.
recon_1  | 2023-03-10 22:57:12,348 [FixedThreadPoolWithAffinityExecutor-8-0] WARN scm.ReconContainerManager: Pipeline PipelineID=f8a55e49-9acd-4b0a-9d97-19a5082b98df not found. Cannot add container #3002
recon_1  | 2023-03-10 22:57:12,349 [FixedThreadPoolWithAffinityExecutor-8-0] WARN scm.ReconIncrementalContainerReportHandler: Container 3002 not found!
recon_1  | 2023-03-10 22:57:12,566 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #3002 got from ha_dn4_1.ha_net.
recon_1  | 2023-03-10 22:57:12,578 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: New container #3002 got from ha_dn5_1.ha_net.
recon_1  | 2023-03-10 22:57:12,594 [FixedThreadPoolWithAffinityExecutor-0-0] WARN scm.ReconContainerManager: Pipeline PipelineID=f8a55e49-9acd-4b0a-9d97-19a5082b98df not found. Cannot add container #3002
recon_1  | 2023-03-10 22:57:12,596 [FixedThreadPoolWithAffinityExecutor-0-0] WARN scm.ReconIncrementalContainerReportHandler: Container 3002 not found!
recon_1  | 2023-03-10 22:57:12,605 [FixedThreadPoolWithAffinityExecutor-9-0] WARN scm.ReconContainerManager: Pipeline PipelineID=f8a55e49-9acd-4b0a-9d97-19a5082b98df not found. Cannot add container #3002
recon_1  | 2023-03-10 22:57:12,605 [FixedThreadPoolWithAffinityExecutor-9-0] WARN scm.ReconIncrementalContainerReportHandler: Container 3002 not found!
