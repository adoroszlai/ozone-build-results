rm: cannot remove '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/*': No such file or directory
Executing test in ozone-csi
Removing network ozone-csi_default
Network ozone-csi_default not found.
Creating network "ozone-csi_default" with the default driver
Pulling datanode (apache/ozone-runner:20230104-1)...
20230104-1: Pulling from apache/ozone-runner
Digest: sha256:bc4c3ecdf9e653e2287e8b99449222807ee6011b520beae9a8874d9b8fc037f3
Status: Downloaded newer image for apache/ozone-runner:20230104-1
Creating ozone-csi_om_1 ... 
Creating ozone-csi_datanode_1 ... 
Creating ozone-csi_datanode_2 ... 
Creating ozone-csi_datanode_3 ... 
Creating ozone-csi_scm_1      ... 
Creating ozone-csi_csi_1      ... 
Creating ozone-csi_datanode_2 ... done
Creating ozone-csi_datanode_3 ... done
Creating ozone-csi_csi_1      ... done
Creating ozone-csi_scm_1      ... done
Creating ozone-csi_datanode_1 ... done
Creating ozone-csi_om_1       ... done
SECONDS: 55
com.google.protobuf.ServiceException: java.net.ConnectException: Call From f3ab2d6f4cde/172.18.0.6 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.6:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From f3ab2d6f4cde/172.18.0.6 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.6:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From f3ab2d6f4cde/172.18.0.6 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.6:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From f3ab2d6f4cde/172.18.0.6 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.6:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From f3ab2d6f4cde/172.18.0.6 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.6:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:9b6631bf-1a7e-41fb-9e0e-970b8853f737 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:197) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:64993) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976) , while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.6:9860 after 6 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:9b6631bf-1a7e-41fb-9e0e-970b8853f737 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:197) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:64993) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976) , while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.6:9860 after 7 failover attempts. Trying to failover after sleeping for 2000ms. SCM is in safe mode. validated:false, DataNodeSafeModeRule, registered datanodes (=0) >= required datanodes (=1) validated:true, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=0) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 77
SCM is out of safe mode.
Safe mode is off
No OM HA service, no need to wait
==============================================================================
Csi :: Smoketest Ozone CSI service                                            
==============================================================================
Check if CSI server is started                                        | PASS |
------------------------------------------------------------------------------
Test CSI identity service                                             | PASS |
------------------------------------------------------------------------------
Csi :: Smoketest Ozone CSI service                                    | PASS |
2 tests, 2 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozone-csi/result/robot-ozone-csi-ozone-csi-csi-csi.xml
Stopping ozone-csi_datanode_2 ... 
Stopping ozone-csi_csi_1      ... 
Stopping ozone-csi_scm_1      ... 
Stopping ozone-csi_datanode_3 ... 
Stopping ozone-csi_datanode_1 ... 
Stopping ozone-csi_om_1       ... 
Stopping ozone-csi_csi_1      ... done
Stopping ozone-csi_om_1       ... done
Stopping ozone-csi_datanode_1 ... done
Stopping ozone-csi_datanode_2 ... done
Stopping ozone-csi_datanode_3 ... done
Stopping ozone-csi_scm_1      ... done
Removing ozone-csi_datanode_2 ... 
Removing ozone-csi_csi_1      ... 
Removing ozone-csi_scm_1      ... 
Removing ozone-csi_datanode_3 ... 
Removing ozone-csi_datanode_1 ... 
Removing ozone-csi_om_1       ... 
Removing ozone-csi_csi_1      ... done
Removing ozone-csi_datanode_1 ... done
Removing ozone-csi_om_1       ... done
Removing ozone-csi_datanode_2 ... done
Removing ozone-csi_datanode_3 ... done
Removing ozone-csi_scm_1      ... done
Removing network ozone-csi_default
Log:     /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/ozone-csi/result/log.html
Report:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/ozone-csi/result/report.html
Output:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-csi.xml
removed 'ozone-csi/result/robot-ozone-csi-ozone-csi-csi-csi.xml'
removed 'ozone-csi/result/log.html'
removed 'ozone-csi/result/report.html'
renamed 'ozone-csi/result/dn-audit-1ed5ec09446c.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-csi/dn-audit-1ed5ec09446c.log'
renamed 'ozone-csi/result/dn-audit-5ba758c3875c.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-csi/dn-audit-5ba758c3875c.log'
renamed 'ozone-csi/result/dn-audit-ca45fa2feddd.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-csi/dn-audit-ca45fa2feddd.log'
renamed 'ozone-csi/result/docker-ozone-csi-ozone-csi-csi-csi.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-csi/docker-ozone-csi-ozone-csi-csi-csi.log'
renamed 'ozone-csi/result/om-audit-ed02412793ec.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-csi/om-audit-ed02412793ec.log'
renamed 'ozone-csi/result/scm-audit-f3ab2d6f4cde.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-csi/scm-audit-f3ab2d6f4cde.log'
Executing test in ozone-om-prepare
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/ozone-om-prepare/data/scm': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/ozone-om-prepare/data/dn1': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/ozone-om-prepare/data/om2': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/ozone-om-prepare/data/dn2': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/ozone-om-prepare/data/om3': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/ozone-om-prepare/data/om1': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/ozone-om-prepare/data/dn3': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/ozone-om-prepare/data': Operation not permitted
Removing network ozone-om-prepare_net
Network ozone-om-prepare_net not found.
Creating network "ozone-om-prepare_net" with driver "bridge"
Creating ozone-om-prepare_scm_1 ... 
Creating ozone-om-prepare_dn3_1 ... 
Creating ozone-om-prepare_om2_1 ... 
Creating ozone-om-prepare_dn1_1 ... 
Creating ozone-om-prepare_om1_1 ... 
Creating ozone-om-prepare_om3_1 ... 
Creating ozone-om-prepare_dn2_1 ... 
Creating ozone-om-prepare_scm_1 ... done
Creating ozone-om-prepare_dn3_1 ... done
Creating ozone-om-prepare_om2_1 ... done
Creating ozone-om-prepare_om1_1 ... done
Creating ozone-om-prepare_om3_1 ... done
Creating ozone-om-prepare_dn2_1 ... done
Creating ozone-om-prepare_dn1_1 ... done
SECONDS: 53
com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4e465f95b7e1/10.9.0.17 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4e465f95b7e1/10.9.0.17 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4e465f95b7e1/10.9.0.17 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:f267f18b-cacf-415d-8c4d-555a3215a0e4 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:197) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:64993) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976) , while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:f267f18b-cacf-415d-8c4d-555a3215a0e4 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:197) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:64993) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976) , while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms. SCM is in safe mode. validated:false, DataNodeSafeModeRule, registered datanodes (=0) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 84
SCM is out of safe mode.
Safe mode is off
Found OM leader for service omservice: om3 : LEADER (om3)
==============================================================================
Om-Prepare :: Smoke test to test preparing OMs in an OM HA cluster.           
==============================================================================
Prepare Ozone Manager                                                 | PASS |
------------------------------------------------------------------------------
Checks if the expected data is present in OM                          | PASS |
------------------------------------------------------------------------------
Test write operation fails                                            | PASS |
------------------------------------------------------------------------------
Om-Prepare :: Smoke test to test preparing OMs in an OM HA cluster.   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm.xml
==============================================================================
Om-Cancel-Prepare :: Smoke test for ozone manager cancel prepare              
==============================================================================
Cancel Ozone Manager Prepare                                          | PASS |
------------------------------------------------------------------------------
Test write operations                                                 | PASS |
------------------------------------------------------------------------------
Om-Cancel-Prepare :: Smoke test for ozone manager cancel prepare      | PASS |
2 tests, 2 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm-1.xml
==============================================================================
Om-Prepare :: Smoke test to test preparing OMs in an OM HA cluster.           
==============================================================================
Prepare Ozone Manager                                                 | PASS |
------------------------------------------------------------------------------
Checks if the expected data is present in OM                          | PASS |
------------------------------------------------------------------------------
Test write operation fails                                            | PASS |
------------------------------------------------------------------------------
Om-Prepare :: Smoke test to test preparing OMs in an OM HA cluster.   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm-2.xml
==============================================================================
Om-Prepared :: Smoke test to test that OMs are prepared in an OM HA cluster.  
==============================================================================
Test create volume fails                                              | PASS |
------------------------------------------------------------------------------
Test list volumes succeeds                                            | PASS |
------------------------------------------------------------------------------
Om-Prepared :: Smoke test to test that OMs are prepared in an OM H... | PASS |
2 tests, 2 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm-3.xml
Stopping ozone-om-prepare_om3_1 ... 
Stopping ozone-om-prepare_dn2_1 ... 
Stopping ozone-om-prepare_om1_1 ... 
Stopping ozone-om-prepare_dn1_1 ... 
Stopping ozone-om-prepare_om2_1 ... 
Stopping ozone-om-prepare_scm_1 ... 
Stopping ozone-om-prepare_dn3_1 ... 
Stopping ozone-om-prepare_om3_1 ... done
Stopping ozone-om-prepare_om1_1 ... done
Stopping ozone-om-prepare_om2_1 ... done
Stopping ozone-om-prepare_dn1_1 ... done
Stopping ozone-om-prepare_dn3_1 ... done
Stopping ozone-om-prepare_dn2_1 ... done
Stopping ozone-om-prepare_scm_1 ... done
Removing ozone-om-prepare_om3_1 ... 
Removing ozone-om-prepare_dn2_1 ... 
Removing ozone-om-prepare_om1_1 ... 
Removing ozone-om-prepare_dn1_1 ... 
Removing ozone-om-prepare_om2_1 ... 
Removing ozone-om-prepare_scm_1 ... 
Removing ozone-om-prepare_dn3_1 ... 
Removing ozone-om-prepare_dn1_1 ... done
Removing ozone-om-prepare_om2_1 ... done
Removing ozone-om-prepare_om3_1 ... done
Removing ozone-om-prepare_dn3_1 ... done
Removing ozone-om-prepare_om1_1 ... done
Removing ozone-om-prepare_scm_1 ... done
Removing ozone-om-prepare_dn2_1 ... done
Removing network ozone-om-prepare_net
Removing network ozone-om-prepare_net
Network ozone-om-prepare_net not found.
Creating network "ozone-om-prepare_net" with driver "bridge"
Creating ozone-om-prepare_dn2_1 ... 
Creating ozone-om-prepare_dn3_1 ... 
Creating ozone-om-prepare_dn1_1 ... 
Creating ozone-om-prepare_om1_1 ... 
Creating ozone-om-prepare_om3_1 ... 
Creating ozone-om-prepare_om2_1 ... 
Creating ozone-om-prepare_scm_1 ... 
Creating ozone-om-prepare_dn3_1 ... done
Creating ozone-om-prepare_dn2_1 ... done
Creating ozone-om-prepare_scm_1 ... done
Creating ozone-om-prepare_dn1_1 ... done
Creating ozone-om-prepare_om2_1 ... done
Creating ozone-om-prepare_om1_1 ... done
Creating ozone-om-prepare_om3_1 ... done
SECONDS: 50
com.google.protobuf.ServiceException: java.net.ConnectException: Call From 5c73c459d795/10.9.0.17 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:f267f18b-cacf-415d-8c4d-555a3215a0e4 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:197) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:64993) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976) , while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:f267f18b-cacf-415d-8c4d-555a3215a0e4 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:197) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:64993) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976) , while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. SCM is in safe mode. validated:false, DataNodeSafeModeRule, registered datanodes (=0) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:false, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=2)
SECONDS: 80
SCM is out of safe mode.
Safe mode is off
Found OM leader for service omservice: om3 : LEADER (om3)
==============================================================================
Om-Prepared :: Smoke test to test that OMs are prepared in an OM HA cluster.  
==============================================================================
Test create volume fails                                              | PASS |
------------------------------------------------------------------------------
Test list volumes succeeds                                            | PASS |
------------------------------------------------------------------------------
Om-Prepared :: Smoke test to test that OMs are prepared in an OM H... | PASS |
2 tests, 2 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm-4.xml
Stopping ozone-om-prepare_scm_1 ... 
Stopping ozone-om-prepare_om1_1 ... 
Stopping ozone-om-prepare_om2_1 ... 
Stopping ozone-om-prepare_om3_1 ... 
Stopping ozone-om-prepare_dn3_1 ... 
Stopping ozone-om-prepare_dn1_1 ... 
Stopping ozone-om-prepare_dn2_1 ... 
Stopping ozone-om-prepare_om1_1 ... done
Stopping ozone-om-prepare_om3_1 ... done
Stopping ozone-om-prepare_om2_1 ... done
Stopping ozone-om-prepare_dn3_1 ... done
Stopping ozone-om-prepare_dn2_1 ... done
Stopping ozone-om-prepare_dn1_1 ... done
Stopping ozone-om-prepare_scm_1 ... done
Removing ozone-om-prepare_scm_1 ... 
Removing ozone-om-prepare_om1_1 ... 
Removing ozone-om-prepare_om2_1 ... 
Removing ozone-om-prepare_om3_1 ... 
Removing ozone-om-prepare_dn3_1 ... 
Removing ozone-om-prepare_dn1_1 ... 
Removing ozone-om-prepare_dn2_1 ... 
Removing ozone-om-prepare_dn2_1 ... done
Removing ozone-om-prepare_dn1_1 ... done
Removing ozone-om-prepare_dn3_1 ... done
Removing ozone-om-prepare_scm_1 ... done
Removing ozone-om-prepare_om3_1 ... done
Removing ozone-om-prepare_om2_1 ... done
Removing ozone-om-prepare_om1_1 ... done
Removing network ozone-om-prepare_net
Removing network ozone-om-prepare_net
Network ozone-om-prepare_net not found.
Creating network "ozone-om-prepare_net" with driver "bridge"
Creating ozone-om-prepare_scm_1 ... 
Creating ozone-om-prepare_om2_1 ... 
Creating ozone-om-prepare_om1_1 ... 
Creating ozone-om-prepare_dn2_1 ... 
Creating ozone-om-prepare_dn3_1 ... 
Creating ozone-om-prepare_om3_1 ... 
Creating ozone-om-prepare_dn1_1 ... 
Creating ozone-om-prepare_om2_1 ... done
Creating ozone-om-prepare_scm_1 ... done
Creating ozone-om-prepare_om3_1 ... done
Creating ozone-om-prepare_om1_1 ... done
Creating ozone-om-prepare_dn3_1 ... done
Creating ozone-om-prepare_dn1_1 ... done
Creating ozone-om-prepare_dn2_1 ... done
SECONDS: 50
com.google.protobuf.ServiceException: java.net.ConnectException: Call From ee30a0ce2067/10.9.0.17 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:f267f18b-cacf-415d-8c4d-555a3215a0e4 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:197) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:64993) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976) , while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:f267f18b-cacf-415d-8c4d-555a3215a0e4 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:197) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:64993) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976) , while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. SCM is in safe mode. validated:false, DataNodeSafeModeRule, registered datanodes (=0) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:false, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=2)
SECONDS: 79
SCM is out of safe mode.
Safe mode is off
Found OM leader for service omservice: om1 : LEADER (om1)
==============================================================================
Loaddata :: Smoketest ozone cluster startup                                   
==============================================================================
Create a volume, bucket and key                                       | PASS |
------------------------------------------------------------------------------
Loaddata :: Smoketest ozone cluster startup                           | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm-5.xml
==============================================================================
Readdata :: Smoketest ozone cluster startup                                   
==============================================================================
Read data from previously created key                                 | PASS |
------------------------------------------------------------------------------
Readdata :: Smoketest ozone cluster startup                           | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm-6.xml
Stopping ozone-om-prepare_dn2_1 ... 
Stopping ozone-om-prepare_dn1_1 ... 
Stopping ozone-om-prepare_dn3_1 ... 
Stopping ozone-om-prepare_om3_1 ... 
Stopping ozone-om-prepare_om1_1 ... 
Stopping ozone-om-prepare_om2_1 ... 
Stopping ozone-om-prepare_scm_1 ... 
Stopping ozone-om-prepare_om3_1 ... done
Stopping ozone-om-prepare_om2_1 ... done
Stopping ozone-om-prepare_om1_1 ... done
Stopping ozone-om-prepare_dn3_1 ... done
Stopping ozone-om-prepare_dn2_1 ... done
Stopping ozone-om-prepare_dn1_1 ... done
Stopping ozone-om-prepare_scm_1 ... done
Removing ozone-om-prepare_dn2_1 ... 
Removing ozone-om-prepare_dn1_1 ... 
Removing ozone-om-prepare_dn3_1 ... 
Removing ozone-om-prepare_om3_1 ... 
Removing ozone-om-prepare_om1_1 ... 
Removing ozone-om-prepare_om2_1 ... 
Removing ozone-om-prepare_scm_1 ... 
Removing ozone-om-prepare_dn2_1 ... done
Removing ozone-om-prepare_om3_1 ... done
Removing ozone-om-prepare_dn3_1 ... done
Removing ozone-om-prepare_om1_1 ... done
Removing ozone-om-prepare_scm_1 ... done
Removing ozone-om-prepare_dn1_1 ... done
Removing ozone-om-prepare_om2_1 ... done
Removing network ozone-om-prepare_net
Log:     /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/ozone-om-prepare/result/log.html
Report:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/ozone-om-prepare/result/report.html
Output:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-om-prepare.xml
removed 'ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm-1.xml'
removed 'ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm-2.xml'
removed 'ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm-3.xml'
removed 'ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm-4.xml'
removed 'ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm-5.xml'
removed 'ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm-6.xml'
removed 'ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm.xml'
removed 'ozone-om-prepare/result/log.html'
removed 'ozone-om-prepare/result/report.html'
renamed 'ozone-om-prepare/result/dn-audit-0c1f1fcd26c3.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-om-prepare/dn-audit-0c1f1fcd26c3.log'
renamed 'ozone-om-prepare/result/dn-audit-145aedac86ac.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-om-prepare/dn-audit-145aedac86ac.log'
renamed 'ozone-om-prepare/result/dn-audit-20444e6a500b.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-om-prepare/dn-audit-20444e6a500b.log'
renamed 'ozone-om-prepare/result/dn-audit-2b3fc0fc1153.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-om-prepare/dn-audit-2b3fc0fc1153.log'
renamed 'ozone-om-prepare/result/dn-audit-3f904eb30620.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-om-prepare/dn-audit-3f904eb30620.log'
renamed 'ozone-om-prepare/result/dn-audit-5b427a26c321.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-om-prepare/dn-audit-5b427a26c321.log'
renamed 'ozone-om-prepare/result/dn-audit-967ccf8da1b6.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-om-prepare/dn-audit-967ccf8da1b6.log'
renamed 'ozone-om-prepare/result/dn-audit-d0f376f6938f.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-om-prepare/dn-audit-d0f376f6938f.log'
renamed 'ozone-om-prepare/result/dn-audit-e80ef6514d7b.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-om-prepare/dn-audit-e80ef6514d7b.log'
renamed 'ozone-om-prepare/result/docker-ozone-om-prepare-ozone-om-prepare-om-prepare-scm.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-om-prepare/docker-ozone-om-prepare-ozone-om-prepare-om-prepare-scm.log'
renamed 'ozone-om-prepare/result/om-audit-160c36b5aa6d.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-om-prepare/om-audit-160c36b5aa6d.log'
renamed 'ozone-om-prepare/result/om-audit-1c39126183f0.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-om-prepare/om-audit-1c39126183f0.log'
renamed 'ozone-om-prepare/result/om-audit-253451700b13.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-om-prepare/om-audit-253451700b13.log'
renamed 'ozone-om-prepare/result/om-audit-63684ce5300e.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-om-prepare/om-audit-63684ce5300e.log'
renamed 'ozone-om-prepare/result/om-audit-64aa3685baf7.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-om-prepare/om-audit-64aa3685baf7.log'
renamed 'ozone-om-prepare/result/om-audit-becbef50eba9.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-om-prepare/om-audit-becbef50eba9.log'
renamed 'ozone-om-prepare/result/om-audit-e7d1ffa26897.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-om-prepare/om-audit-e7d1ffa26897.log'
renamed 'ozone-om-prepare/result/om-audit-f7260f03b6d1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-om-prepare/om-audit-f7260f03b6d1.log'
renamed 'ozone-om-prepare/result/om-audit-fa0c4545717c.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-om-prepare/om-audit-fa0c4545717c.log'
renamed 'ozone-om-prepare/result/scm-audit-4e465f95b7e1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-om-prepare/scm-audit-4e465f95b7e1.log'
renamed 'ozone-om-prepare/result/scm-audit-5c73c459d795.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-om-prepare/scm-audit-5c73c459d795.log'
renamed 'ozone-om-prepare/result/scm-audit-ee30a0ce2067.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-om-prepare/scm-audit-ee30a0ce2067.log'
Executing test in ozone-topology
Removing network ozone-topology_net
Network ozone-topology_net not found.
Creating network "ozone-topology_net" with driver "bridge"
Creating ozone-topology_datanode_2_1 ... 
Creating ozone-topology_datanode_3_1 ... 
Creating ozone-topology_datanode_5_1 ... 
Creating ozone-topology_datanode_4_1 ... 
Creating ozone-topology_datanode_1_1 ... 
Creating ozone-topology_recon_1      ... 
Creating ozone-topology_scm_1        ... 
Creating ozone-topology_om_1         ... 
Creating ozone-topology_datanode_6_1 ... 
Creating ozone-topology_datanode_3_1 ... done
Creating ozone-topology_recon_1      ... done
Creating ozone-topology_om_1         ... done
Creating ozone-topology_datanode_2_1 ... done
Creating ozone-topology_datanode_6_1 ... done
Creating ozone-topology_datanode_5_1 ... done
Creating ozone-topology_datanode_4_1 ... done
Creating ozone-topology_scm_1        ... done
Creating ozone-topology_datanode_1_1 ... done
SECONDS: 94
com.google.protobuf.ServiceException: java.net.ConnectException: Call From a765ff1a6559/10.5.0.71 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.5.0.71:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From a765ff1a6559/10.5.0.71 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.5.0.71:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From a765ff1a6559/10.5.0.71 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.5.0.71:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From a765ff1a6559/10.5.0.71 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.5.0.71:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From a765ff1a6559/10.5.0.71 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.5.0.71:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From a765ff1a6559/10.5.0.71 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.5.0.71:9860 after 6 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:86774ab9-6fcc-445e-9506-73c44b5b53b7 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:197) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:64993) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976) , while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.5.0.71:9860 after 7 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:86774ab9-6fcc-445e-9506-73c44b5b53b7 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:197) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:64993) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976) , while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.5.0.71:9860 after 8 failover attempts. Trying to failover after sleeping for 2000ms. SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=4) >= required datanodes (=4) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 110
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=4) >= required datanodes (=4) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 118
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=4) >= required datanodes (=4) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 147
SCM is out of safe mode.
Safe mode is off
No OM HA service, no need to wait
==============================================================================
Basic :: Smoketest ozone cluster startup                                      
==============================================================================
Check webui static resources                                          | PASS |
------------------------------------------------------------------------------
Basic Freon smoketest                                                 | PASS |
------------------------------------------------------------------------------
Basic :: Smoketest ozone cluster startup                              | PASS |
2 tests, 2 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozone-topology/result/robot-ozone-topology-ozone-topology-basic-scm.xml
==============================================================================
Cli :: Smoketest ozone cluster startup                                        
==============================================================================
Run printTopology                                                     | PASS |
------------------------------------------------------------------------------
Run printTopology -o                                                  | PASS |
------------------------------------------------------------------------------
Run printTopology --operational-state IN_SERVICE                      | PASS |
------------------------------------------------------------------------------
Run printTopology --node-state HEALTHY                                | PASS |
------------------------------------------------------------------------------
Cli :: Smoketest ozone cluster startup                                | PASS |
4 tests, 4 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozone-topology/result/robot-ozone-topology-ozone-topology-basic-scm-1.xml
==============================================================================
Recon                                                                         
==============================================================================
Recon.Recon-Api :: Smoke test to start cluster with docker-compose environm...
==============================================================================
Check if Recon picks up OM data                                       | PASS |
------------------------------------------------------------------------------
Check if Recon picks up DN heartbeats                                 | PASS |
------------------------------------------------------------------------------
Check if Recon Web UI is up                                           | PASS |
------------------------------------------------------------------------------
Check web UI access                                                   | PASS |
------------------------------------------------------------------------------
Check admin only api access                                           | PASS |
------------------------------------------------------------------------------
Check unhealthy, (admin) api access                                   | PASS |
------------------------------------------------------------------------------
Check normal api access                                               | PASS |
------------------------------------------------------------------------------
Recon.Recon-Api :: Smoke test to start cluster with docker-compose... | PASS |
7 tests, 7 passed, 0 failed
==============================================================================
Recon.Recon-Nssummary :: Smoke test for Recon Namespace Summary Endpoint fo...
==============================================================================
Check volume creation                                                 | PASS |
------------------------------------------------------------------------------
Check bucket creation                                                 | PASS |
------------------------------------------------------------------------------
Check keys creation                                                   | PASS |
------------------------------------------------------------------------------
Check Summary api access                                              | PASS |
------------------------------------------------------------------------------
Check Disk Usage api access                                           | PASS |
------------------------------------------------------------------------------
Check Quota Usage api access                                          | PASS |
------------------------------------------------------------------------------
Check File Size Distribution api access                               | PASS |
------------------------------------------------------------------------------
Check Recon Namespace Summary Root                                    | PASS |
------------------------------------------------------------------------------
Check Recon Namespace Summary Volume                                  | PASS |
------------------------------------------------------------------------------
Check Recon Namespace Summary Bucket                                  | PASS |
------------------------------------------------------------------------------
Check Recon Namespace Summary Key                                     | PASS |
------------------------------------------------------------------------------
Check Recon Namespace Summary Directory                               | PASS |
------------------------------------------------------------------------------
Check Recon Namespace Disk Usage                                      | PASS |
------------------------------------------------------------------------------
Check Recon Namespace Volume Quota Usage                              | PASS |
------------------------------------------------------------------------------
Check Recon Namespace Bucket Quota Usage                              | PASS |
------------------------------------------------------------------------------
Check Recon Namespace File Size Distribution Root                     | PASS |
------------------------------------------------------------------------------
Recon.Recon-Nssummary :: Smoke test for Recon Namespace Summary En... | PASS |
16 tests, 16 passed, 0 failed
==============================================================================
Recon                                                                 | PASS |
23 tests, 23 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozone-topology/result/robot-ozone-topology-ozone-topology-basic-scm-2.xml
==============================================================================
Loaddata :: Smoketest ozone cluster startup                                   
==============================================================================
Create a volume, bucket and key                                       | PASS |
------------------------------------------------------------------------------
Loaddata :: Smoketest ozone cluster startup                           | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozone-topology/result/robot-ozone-topology-ozone-topology-basic-scm-3.xml
Stopping ozone-topology_datanode_1_1 ... 
Stopping ozone-topology_datanode_3_1 ... 
Stopping ozone-topology_datanode_2_1 ... 
Stopping ozone-topology_datanode_2_1 ... done
Stopping ozone-topology_datanode_1_1 ... done
Stopping ozone-topology_datanode_3_1 ... done
==============================================================================
readdata-first-half :: Smoketest ozone cluster startup                        
==============================================================================
Read data from previously created key                                 | PASS |
------------------------------------------------------------------------------
readdata-first-half :: Smoketest ozone cluster startup                | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozone-topology/result/robot-ozone-topology-ozone-topology-basic-scm-4.xml
Starting datanode_1 ... 
Starting datanode_2 ... 
Starting datanode_3 ... 
Starting datanode_3 ... done
Starting datanode_1 ... done
Starting datanode_2 ... done
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Port 9858 is not available on datanode_1 yet
Timed out waiting on datanode_1 9858 to become available
ERROR: Test execution of ozone-topology is FAILED!!!!
Output:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-topology.xml
removed 'ozone-topology/result/robot-ozone-topology-ozone-topology-basic-scm-1.xml'
removed 'ozone-topology/result/robot-ozone-topology-ozone-topology-basic-scm-2.xml'
removed 'ozone-topology/result/robot-ozone-topology-ozone-topology-basic-scm-3.xml'
removed 'ozone-topology/result/robot-ozone-topology-ozone-topology-basic-scm-4.xml'
removed 'ozone-topology/result/robot-ozone-topology-ozone-topology-basic-scm.xml'
renamed 'ozone-topology/result/dn-audit-4c261a3912f7.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-topology/dn-audit-4c261a3912f7.log'
renamed 'ozone-topology/result/dn-audit-60267cbdcae1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-topology/dn-audit-60267cbdcae1.log'
renamed 'ozone-topology/result/dn-audit-8d6eed8d1d8c.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-topology/dn-audit-8d6eed8d1d8c.log'
renamed 'ozone-topology/result/dn-audit-b81a2fe3082e.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-topology/dn-audit-b81a2fe3082e.log'
renamed 'ozone-topology/result/dn-audit-f9c50c2fa192.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-topology/dn-audit-f9c50c2fa192.log'
renamed 'ozone-topology/result/dn-audit-fc1e2cf2064c.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-topology/dn-audit-fc1e2cf2064c.log'
renamed 'ozone-topology/result/om-audit-e612b261f405.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-topology/om-audit-e612b261f405.log'
renamed 'ozone-topology/result/scm-audit-a765ff1a6559.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozone-topology/scm-audit-a765ff1a6559.log'
Executing test in ozones3-haproxy
Removing network ozones3-haproxy_default
Network ozones3-haproxy_default not found.
Creating network "ozones3-haproxy_default" with the default driver
Pulling s3g (haproxy:latest)...
latest: Pulling from library/haproxy
Digest: sha256:bf3b3664610476a2b9d4c1e64438f5d6b0bbffcc0eca5c866d6aeff4bc9dbd7c
Status: Downloaded newer image for haproxy:latest
Creating ozones3-haproxy_s3g3_1 ... 
Creating ozones3-haproxy_datanode_1 ... 
Creating ozones3-haproxy_datanode_2 ... 
Creating ozones3-haproxy_datanode_3 ... 
Creating ozones3-haproxy_s3g_1      ... 
Creating ozones3-haproxy_s3g2_1     ... 
Creating ozones3-haproxy_s3g1_1     ... 
Creating ozones3-haproxy_om_1       ... 
Creating ozones3-haproxy_scm_1      ... 
Host is already in use by another container
Creating ozones3-haproxy_om_1       ... error

ERROR: for ozones3-haproxy_om_1  Cannot start service om: driver failed programming external connectivity on endpoint ozones3-haproxy_om_1 (05cfbf66ade6a3496ec995c23629e57605de2817489b11f36f60b868819145a0): Bind for 0.0.0.0:9874 failed: port is already allocated
Host is already in use by another container
Creating ozones3-haproxy_scm_1      ... error

ERROR: for ozones3-haproxy_scm_1  Cannot start service scm: driver failed programming external connectivity on endpoint ozones3-haproxy_scm_1 (dba53098859a25989549d2e83e7a7d029dca404a8b52d76bb42cfa1c3857d3e9): Bind for 0.0.0.0:9876 failed: port is already allocated
Creating ozones3-haproxy_s3g3_1     ... done
Creating ozones3-haproxy_datanode_3 ... done
Creating ozones3-haproxy_datanode_2 ... done
Creating ozones3-haproxy_s3g_1      ... done
Creating ozones3-haproxy_s3g2_1     ... done
Creating ozones3-haproxy_datanode_1 ... done
Creating ozones3-haproxy_s3g1_1     ... done

ERROR: for om  Cannot start service om: driver failed programming external connectivity on endpoint ozones3-haproxy_om_1 (05cfbf66ade6a3496ec995c23629e57605de2817489b11f36f60b868819145a0): Bind for 0.0.0.0:9874 failed: port is already allocated

ERROR: for scm  Cannot start service scm: driver failed programming external connectivity on endpoint ozones3-haproxy_scm_1 (dba53098859a25989549d2e83e7a7d029dca404a8b52d76bb42cfa1c3857d3e9): Bind for 0.0.0.0:9876 failed: port is already allocated
Encountered errors while bringing up the project.
Stopping ozones3-haproxy_s3g1_1     ... 
Stopping ozones3-haproxy_s3g2_1     ... 
Stopping ozones3-haproxy_datanode_1 ... 
Stopping ozones3-haproxy_s3g_1      ... 
Stopping ozones3-haproxy_datanode_2 ... 
Stopping ozones3-haproxy_datanode_3 ... 
Stopping ozones3-haproxy_s3g3_1     ... 
Stopping ozones3-haproxy_datanode_2 ... done
Stopping ozones3-haproxy_s3g1_1     ... done
Stopping ozones3-haproxy_s3g3_1     ... done
Stopping ozones3-haproxy_s3g_1      ... done
Stopping ozones3-haproxy_s3g2_1     ... done
Stopping ozones3-haproxy_datanode_1 ... done
Stopping ozones3-haproxy_datanode_3 ... done
Removing ozones3-haproxy_scm_1      ... 
Removing ozones3-haproxy_s3g1_1     ... 
Removing ozones3-haproxy_om_1       ... 
Removing ozones3-haproxy_s3g2_1     ... 
Removing ozones3-haproxy_datanode_1 ... 
Removing ozones3-haproxy_s3g_1      ... 
Removing ozones3-haproxy_datanode_2 ... 
Removing ozones3-haproxy_datanode_3 ... 
Removing ozones3-haproxy_s3g3_1     ... 
Removing ozones3-haproxy_om_1       ... done
Removing ozones3-haproxy_s3g1_1     ... done
Removing ozones3-haproxy_scm_1      ... done
Removing ozones3-haproxy_datanode_1 ... done
Removing ozones3-haproxy_s3g_1      ... done
Removing ozones3-haproxy_s3g3_1     ... done
Removing ozones3-haproxy_datanode_2 ... done
Removing ozones3-haproxy_s3g2_1     ... done
Removing ozones3-haproxy_datanode_3 ... done
Removing network ozones3-haproxy_default
ERROR: Test execution of ozones3-haproxy is FAILED!!!!
renamed 'ozones3-haproxy/result/docker-ozones3-haproxy.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozones3-haproxy/docker-ozones3-haproxy.log'
Executing test in ozonescripts
Removing network ozonescripts_default
Network ozonescripts_default not found.
Creating network "ozonescripts_default" with the default driver
Building datanode
Sending build context to Docker daemon  30.21kB
Step 1/17 : ARG OZONE_RUNNER_IMAGE
Step 2/17 : ARG OZONE_RUNNER_VERSION
Step 3/17 : FROM ${OZONE_RUNNER_IMAGE}:${OZONE_RUNNER_VERSION}
 ---> 34ffed355199
Step 4/17 : RUN sudo yum install -y openssh-clients openssh-server
 ---> Running in 3554afbd4018
Loaded plugins: fastestmirror, ovl
Determining fastest mirrors
 * base: mirror.grid.uchicago.edu
 * epel: mirror.sfo12.us.leaseweb.net
 * extras: mirror.keystealth.org
 * updates: centos.mirror.shastacoe.net
Resolving Dependencies
--> Running transaction check
---> Package openssh-clients.x86_64 0:7.4p1-22.el7_9 will be installed
--> Processing Dependency: openssh = 7.4p1-22.el7_9 for package: openssh-clients-7.4p1-22.el7_9.x86_64
--> Processing Dependency: fipscheck-lib(x86-64) >= 1.3.0 for package: openssh-clients-7.4p1-22.el7_9.x86_64
--> Processing Dependency: libfipscheck.so.1()(64bit) for package: openssh-clients-7.4p1-22.el7_9.x86_64
--> Processing Dependency: libedit.so.0()(64bit) for package: openssh-clients-7.4p1-22.el7_9.x86_64
---> Package openssh-server.x86_64 0:7.4p1-22.el7_9 will be installed
--> Processing Dependency: libwrap.so.0()(64bit) for package: openssh-server-7.4p1-22.el7_9.x86_64
--> Running transaction check
---> Package fipscheck-lib.x86_64 0:1.4.1-6.el7 will be installed
--> Processing Dependency: /usr/bin/fipscheck for package: fipscheck-lib-1.4.1-6.el7.x86_64
---> Package libedit.x86_64 0:3.0-12.20121213cvs.el7 will be installed
---> Package openssh.x86_64 0:7.4p1-22.el7_9 will be installed
---> Package tcp_wrappers-libs.x86_64 0:7.6-77.el7 will be installed
--> Running transaction check
---> Package fipscheck.x86_64 0:1.4.1-6.el7 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

================================================================================
 Package               Arch       Version                     Repository   Size
================================================================================
Installing:
 openssh-clients       x86_64     7.4p1-22.el7_9              updates     655 k
 openssh-server        x86_64     7.4p1-22.el7_9              updates     459 k
Installing for dependencies:
 fipscheck             x86_64     1.4.1-6.el7                 base         21 k
 fipscheck-lib         x86_64     1.4.1-6.el7                 base         11 k
 libedit               x86_64     3.0-12.20121213cvs.el7      base         92 k
 openssh               x86_64     7.4p1-22.el7_9              updates     510 k
 tcp_wrappers-libs     x86_64     7.6-77.el7                  base         66 k

Transaction Summary
================================================================================
Install  2 Packages (+5 Dependent packages)

Total download size: 1.8 M
Installed size: 5.8 M
Downloading packages:
--------------------------------------------------------------------------------
Total                                              1.9 MB/s | 1.8 MB  00:00     
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : fipscheck-1.4.1-6.el7.x86_64                                 1/7 
  Installing : fipscheck-lib-1.4.1-6.el7.x86_64                             2/7 
  Installing : openssh-7.4p1-22.el7_9.x86_64                                3/7 
  Installing : tcp_wrappers-libs-7.6-77.el7.x86_64                          4/7 
  Installing : libedit-3.0-12.20121213cvs.el7.x86_64                        5/7 
  Installing : openssh-clients-7.4p1-22.el7_9.x86_64                        6/7 
  Installing : openssh-server-7.4p1-22.el7_9.x86_64                         7/7 
  Verifying  : fipscheck-lib-1.4.1-6.el7.x86_64                             1/7 
  Verifying  : openssh-server-7.4p1-22.el7_9.x86_64                         2/7 
  Verifying  : fipscheck-1.4.1-6.el7.x86_64                                 3/7 
  Verifying  : libedit-3.0-12.20121213cvs.el7.x86_64                        4/7 
  Verifying  : openssh-clients-7.4p1-22.el7_9.x86_64                        5/7 
  Verifying  : tcp_wrappers-libs-7.6-77.el7.x86_64                          6/7 
  Verifying  : openssh-7.4p1-22.el7_9.x86_64                                7/7 

Installed:
  openssh-clients.x86_64 0:7.4p1-22.el7_9                                       
  openssh-server.x86_64 0:7.4p1-22.el7_9                                        

Dependency Installed:
  fipscheck.x86_64 0:1.4.1-6.el7            fipscheck-lib.x86_64 0:1.4.1-6.el7  
  libedit.x86_64 0:3.0-12.20121213cvs.el7   openssh.x86_64 0:7.4p1-22.el7_9     
  tcp_wrappers-libs.x86_64 0:7.6-77.el7    

Complete!
Removing intermediate container 3554afbd4018
 ---> 3800c6b5d105
Step 5/17 : RUN sudo ssh-keygen -A
 ---> Running in 9fc00bae9fb2
ssh-keygen: generating new host keys: RSA1 RSA DSA ECDSA ED25519 
Removing intermediate container 9fc00bae9fb2
 ---> 6c17377d2a9b
Step 6/17 : RUN sudo mkdir -p /run/sshd
 ---> Running in 03f434d9ee44
Removing intermediate container 03f434d9ee44
 ---> 58a5d689c6d7
Step 7/17 : RUN sudo sed -i "s/.*UsePrivilegeSeparation.*/UsePrivilegeSeparation no/g" /etc/ssh/sshd_config
 ---> Running in 38b355000440
Removing intermediate container 38b355000440
 ---> c1bed6811628
Step 8/17 : RUN sudo sed -i "s/.*PermitUserEnvironment.*/PermitUserEnvironment yes/g" /etc/ssh/sshd_config
 ---> Running in 96d93ecd7c2f
Removing intermediate container 96d93ecd7c2f
 ---> 5c71e36af610
Step 9/17 : RUN sudo sed 's@session\s*required\s*pam_loginuid.so@session optional pam_loginuid.so@g' -i /etc/pam.d/sshd
 ---> Running in 6bb2f6188417
Removing intermediate container 6bb2f6188417
 ---> e251fc3bf893
Step 10/17 : RUN sudo usermod -d /opt hadoop
 ---> Running in e20b3460971b
Removing intermediate container e20b3460971b
 ---> afcd531b3628
Step 11/17 : ADD .ssh /opt/.ssh
 ---> dcdb6bcfa92e
Step 12/17 : RUN sudo chown -R hadoop /opt/.ssh
 ---> Running in 2d2da5124232
Removing intermediate container 2d2da5124232
 ---> c6bb1067bf8d
Step 13/17 : RUN sudo chown hadoop /opt
 ---> Running in 370401ff31e0
Removing intermediate container 370401ff31e0
 ---> 528b06a0b467
Step 14/17 : RUN sudo chmod 600 /opt/.ssh/*
 ---> Running in 72ced53a669f
Removing intermediate container 72ced53a669f
 ---> 678303e1c158
Step 15/17 : RUN sudo chmod 700 /opt/.ssh
 ---> Running in 1b1082e753de
Removing intermediate container 1b1082e753de
 ---> 8f989b3b250f
Step 16/17 : RUN sudo sh -c 'echo "export JAVA_HOME=/usr/lib/jvm/jre/" >> /etc/profile'
 ---> Running in 62cd74fc0fad
Removing intermediate container 62cd74fc0fad
 ---> 7c7d83f0da59
Step 17/17 : CMD ["sudo","/usr/sbin/sshd","-D"]
 ---> Running in 27c9aef99011
Removing intermediate container 27c9aef99011
 ---> 2932add053d0
Successfully built 2932add053d0
Successfully tagged ozone-runner-scripts:20230104-1
Image for service datanode was built because it did not already exist. To rebuild this image you must use `docker-compose build` or `docker-compose up --build`.
Creating ozonescripts_scm_1 ... 
Creating ozonescripts_om_1  ... 
Creating ozonescripts_datanode_1 ... 
Host is already in use by another container
Creating ozonescripts_scm_1      ... error

ERROR: for ozonescripts_scm_1  Cannot start service scm: driver failed programming external connectivity on endpoint ozonescripts_scm_1 (e96bad98538d80464eaf5ba61ba64bf74e6556432af0d4e51cc5a3d2f1cf2332): Bind for 0.0.0.0:9876 failed: port is already allocated
Host is already in use by another container
Creating ozonescripts_om_1       ... error

ERROR: for ozonescripts_om_1  Cannot start service om: driver failed programming external connectivity on endpoint ozonescripts_om_1 (cefb6bfd584dc858b04aed293577d366ca09ca41570577b90121aa7eb135019b): Bind for 0.0.0.0:9874 failed: port is already allocated
Creating ozonescripts_datanode_1 ... done

ERROR: for scm  Cannot start service scm: driver failed programming external connectivity on endpoint ozonescripts_scm_1 (e96bad98538d80464eaf5ba61ba64bf74e6556432af0d4e51cc5a3d2f1cf2332): Bind for 0.0.0.0:9876 failed: port is already allocated

ERROR: for om  Cannot start service om: driver failed programming external connectivity on endpoint ozonescripts_om_1 (cefb6bfd584dc858b04aed293577d366ca09ca41570577b90121aa7eb135019b): Bind for 0.0.0.0:9874 failed: port is already allocated
Encountered errors while bringing up the project.
Stopping ozonescripts_datanode_1 ... 
Stopping ozonescripts_datanode_1 ... done
Removing ozonescripts_datanode_1 ... 
Removing ozonescripts_om_1       ... 
Removing ozonescripts_scm_1      ... 
Removing ozonescripts_scm_1      ... done
Removing ozonescripts_om_1       ... done
Removing ozonescripts_datanode_1 ... done
Removing network ozonescripts_default
ERROR: Test execution of ozonescripts is FAILED!!!!
renamed 'ozonescripts/result/docker-ozonescripts.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/ozonescripts/docker-ozonescripts.log'
Executing test in restart
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/restart/data/scm': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/restart/data/dn1': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/restart/data/om': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/restart/data/recon': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/restart/data/s3g': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/restart/data/dn2': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/restart/data/dn3': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/restart/data': Operation not permitted
Removing network restart_net
Network restart_net not found.
Creating network "restart_net" with driver "bridge"
Creating restart_recon_1 ... 
Creating restart_om_1    ... 
Creating restart_dn3_1   ... 
Creating restart_scm_1   ... 
Creating restart_dn1_1   ... 
Creating restart_s3g_1   ... 
Creating restart_dn2_1   ... 
Host is already in use by another container
Creating restart_om_1    ... error

ERROR: for restart_om_1  Cannot start service om: driver failed programming external connectivity on endpoint restart_om_1 (6d74deea5b7450b3611783a8455f26dcef6430acc09f9caa6840fe5d42a920e0): Bind for 0.0.0.0:9874 failed: port is already allocated
Host is already in use by another container
Creating restart_recon_1 ... error

ERROR: for restart_recon_1  Cannot start service recon: driver failed programming external connectivity on endpoint restart_recon_1 (1fa664ff7e18edd010f191a64fc8001d108c3ae15cda6156ade35c20777449b1): Bind for 0.0.0.0:9888 failed: port is already allocated
Host is already in use by another container
Creating restart_scm_1   ... error

ERROR: for restart_scm_1  Cannot start service scm: driver failed programming external connectivity on endpoint restart_scm_1 (d738b126e972773aed994ee8651bbad75477fb66f050c51687c38c930898590f): Bind for 0.0.0.0:9876 failed: port is already allocated
Creating restart_s3g_1   ... done
Creating restart_dn2_1   ... done
Creating restart_dn1_1   ... done
Creating restart_dn3_1   ... done

ERROR: for om  Cannot start service om: driver failed programming external connectivity on endpoint restart_om_1 (6d74deea5b7450b3611783a8455f26dcef6430acc09f9caa6840fe5d42a920e0): Bind for 0.0.0.0:9874 failed: port is already allocated

ERROR: for recon  Cannot start service recon: driver failed programming external connectivity on endpoint restart_recon_1 (1fa664ff7e18edd010f191a64fc8001d108c3ae15cda6156ade35c20777449b1): Bind for 0.0.0.0:9888 failed: port is already allocated

ERROR: for scm  Cannot start service scm: driver failed programming external connectivity on endpoint restart_scm_1 (d738b126e972773aed994ee8651bbad75477fb66f050c51687c38c930898590f): Bind for 0.0.0.0:9876 failed: port is already allocated
Encountered errors while bringing up the project.
Stopping restart_dn2_1 ... 
Stopping restart_dn1_1 ... 
Stopping restart_s3g_1 ... 
Stopping restart_dn3_1 ... 
Stopping restart_dn3_1 ... done
Stopping restart_dn1_1 ... done
Stopping restart_s3g_1 ... done
Stopping restart_dn2_1 ... done
Removing restart_dn2_1   ... 
Removing restart_dn1_1   ... 
Removing restart_s3g_1   ... 
Removing restart_scm_1   ... 
Removing restart_om_1    ... 
Removing restart_dn3_1   ... 
Removing restart_recon_1 ... 
Removing restart_s3g_1   ... done
Removing restart_om_1    ... done
Removing restart_recon_1 ... done
Removing restart_dn2_1   ... done
Removing restart_dn3_1   ... done
Removing restart_scm_1   ... done
Removing restart_dn1_1   ... done
Removing network restart_net
ERROR: Test execution of restart is FAILED!!!!
renamed 'restart/result/docker-restart.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/restart/docker-restart.log'
Exception in thread "main" java.net.SocketException: Socket closed
	at java.base/java.net.PlainSocketImpl.socketAccept(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)
	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)
	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)
	at org.apache.hadoop.test.JacocoServer.main(JacocoServer.java:60)
Log:     /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/log.html
Report:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/report.html
