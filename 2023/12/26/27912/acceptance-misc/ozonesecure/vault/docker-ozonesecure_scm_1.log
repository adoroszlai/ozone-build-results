No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
2023-12-26 12:07:34,435 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting StorageContainerManager
STARTUP_MSG:   host = scm/172.20.0.6
STARTUP_MSG:   args = [--init]
STARTUP_MSG:   version = 1.5.0-SNAPSHOT
STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jgraphx-2.0.0.1.jar:/opt/hadoop/share/ozone/lib/commons-net-3.10.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/okhttp-4.12.0.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.3.2.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.9.21.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.25.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.9.21.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.6.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.2.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.6.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/okio-3.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-3.0.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-3.0.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-3.0.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.9.21.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.12.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/jgraph-5.13.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/antlr4-runtime-4.5.3.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/picocli-4.7.5.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.16.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.9.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/jgrapht-ext-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-1.2.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.9.21.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jgrapht-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.14.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-dropwizard3-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-3.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-3.0.0.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.58.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.9.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-3.0.0.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.5.0-SNAPSHOT.jar
STARTUP_MSG:   build = https://github.com/apache/ozone/41f284af94206f1c4196a957ddbb9421407782b6 ; compiled by 'runner' on 2023-12-26T11:28Z
STARTUP_MSG:   java = 11.0.19
STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=4MB, dfs.container.ratis.segment.size=64MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.command.worker.interval=2s, hdds.datanode.block.delete.max.lock.wait.timeout=100ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.max.lock.holding.time=1s, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=19864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.close.threads.max=3, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.file.size=100B, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/dn.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/dn@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.queue.limit=4096, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=100MB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.log.appender.wait-time.min=0us, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/scm.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=5s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=5s, hdds.scm.replication.under.replicated.interval=5s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=30s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.expired.certificate.check.interval=P1D, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=1MB, ozone.chunk.read.mapped.buffer.threshold=32KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.grpc.write.timeout=30s, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.ec.grpc.zerocopy.enabled=true, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/om.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.max.buckets=100000, ozone.om.multitenancy.enabled=true, ozone.om.multitenancy.ranger.sync.interval=30s, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.open.mpu.cleanup.service.interval=24h, ozone.om.open.mpu.cleanup.service.timeout=300s, ozone.om.open.mpu.expire.threshold=30d, ozone.om.open.mpu.parts.cleanup.limit.per.task=1000, ozone.om.ranger.https-address=https://ranger:6182, ozone.om.ranger.https.admin.api.passwd=Passwd1, ozone.om.ranger.https.admin.api.user=admin, ozone.om.ranger.service=cm_ozone, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.checkpoint.dir.creation.poll.timeout=20s, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.load.native.lib=true, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.tenant.dev.skip.ranger=true, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.Hadoop3OmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=6000, ozone.recon.address=recon:9891, ozone.recon.administrators=testuser2, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3.administrators=testuser,s3g, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/s3g.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.list-keys.shallow.enabled=true, ozone.s3g.secret.http.auth.type=kerberos, ozone.s3g.secret.http.enabled=true, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.admin.monitor.logging.limit=1000, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=1, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=45s, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.raft.server.log.appender.wait-time.min=0ms, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=scm:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=30s, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
************************************************************/
2023-12-26 12:07:34,520 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
2023-12-26 12:07:34,918 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-12-26 12:07:36,395 [main] INFO reflections.Reflections: Reflections took 1127 ms to scan 3 urls, producing 134 keys and 291 values 
2023-12-26 12:07:37,120 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
2023-12-26 12:07:37,176 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
2023-12-26 12:07:37,588 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
2023-12-26 12:07:43,759 [main] INFO client.SCMCertificateClient: Certificate serial ID set to null
2023-12-26 12:07:43,765 [main] ERROR client.SCMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
2023-12-26 12:07:43,765 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
2023-12-26 12:07:43,786 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
2023-12-26 12:07:45,959 [main] INFO client.SCMCertificateClient: Init response: GETCERT
2023-12-26 12:07:49,499 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.20.0.6,host:scm
2023-12-26 12:07:49,499 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
2023-12-26 12:07:49,502 [main] ERROR utils.SelfSignedCertificate: Invalid domain scm
2023-12-26 12:07:49,690 [main] INFO utils.SelfSignedCertificate: Certificate 1 is issued by CN=scm@scm,OU=665fb5eb-c818-4f06-9452-123bbd014da5,O=CID-47725a8b-331b-4bd3-8386-19b33bfc4b4d,SERIALNUMBER=1 to CN=scm@scm,OU=665fb5eb-c818-4f06-9452-123bbd014da5,O=CID-47725a8b-331b-4bd3-8386-19b33bfc4b4d,SERIALNUMBER=1, valid from Tue Dec 26 12:07:49 UTC 2023 to Fri Feb 02 12:07:49 UTC 2029
2023-12-26 12:07:49,748 [main] INFO utils.CertificateCodec: Save certificate to /data/metadata/scm/ca/certs/certificate.crt
2023-12-26 12:07:49,749 [main] INFO utils.CertificateCodec: Certificate -----BEGIN CERTIFICATE-----
MIIDsTCCApmgAwIBAgIBATANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkNjY1ZmI1ZWItYzgxOC00ZjA2LTk0NTItMTIzYmJkMDE0
ZGE1MTEwLwYDVQQKDChDSUQtNDc3MjVhOGItMzMxYi00YmQzLTgzODYtMTliMzNi
ZmM0YjRkMQowCAYDVQQFEwExMB4XDTIzMTIyNjEyMDc0OVoXDTI5MDIwMjEyMDc0
OVowgYAxEDAOBgNVBAMMB3NjbUBzY20xLTArBgNVBAsMJDY2NWZiNWViLWM4MTgt
NGYwNi05NDUyLTEyM2JiZDAxNGRhNTExMC8GA1UECgwoQ0lELTQ3NzI1YThiLTMz
MWItNGJkMy04Mzg2LTE5YjMzYmZjNGI0ZDEKMAgGA1UEBRMBMTCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBAMmDO4+2px8BT0UCHWfmmU66e4LXQPYvf2YS
mvtIzjg8O9Cs/qsGXuQaw6qazwpsqhWLEWMNoQEqZlS9u3WEqRoA3oSg56T365V7
yly+5yOSHzX5tXJwAvGqi+2m+om9OVkA8wZLgK6hxagabow7ZW+Fc1vIJctxeZV5
BmCR3EQMbl5cLaFZm5yy1MwzMpaI4zxGIfXY3gG/FUz6l1H7Uy5ljRdOa9slzPoz
rmKWhsCmvyMT2QrbVDJaE9gvBCMyPmSP0gDCH/bO2NswPq1sFZ/nQZYQiF2Wjsy4
wTLoIFlNod8SgINGwK6bHJ0qdvreEPMFNgUSNzwnG3dyVZXDrfsCAwEAAaM0MDIw
DwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0RBAgwBocErBQA
BjANBgkqhkiG9w0BAQsFAAOCAQEAptDKHNEkJDUwUny+NoVszELKkSRE051Y1tAJ
g6pan7zqhhOoTZ8avN66k2B57J/BkFBrfKluRNn0RA2nHF0LdsewwjJQZaKxjZVt
D0WLPM5i60H1WUNKNTI+DU6pGZ940HRNVHpZ0P/vEiNy7z6XbN9cELUO7Ako/b5P
ztuM2YS9dC03ERM4VV2d51CLfGS2o/Wqn5a35335MeujQlaRKtJ86iArCFReb1qO
SkY//UFM3WGES+sHtqHitnXBnruU6MXcBBoIvdBck3k8QjytgxmZl/lHdXuQN53e
yu++iYKoUNjTAjtnKYvpf056egdr4fqu3NvjlhH3GnGMo8/5Vw==
-----END CERTIFICATE-----

2023-12-26 12:07:49,897 [main] INFO client.SCMCertificateClient: Creating csr for SCM->hostName:scm,scmId:665fb5eb-c818-4f06-9452-123bbd014da5,clusterId:CID-47725a8b-331b-4bd3-8386-19b33bfc4b4d,subject:scm-sub@scm
2023-12-26 12:07:49,902 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.20.0.6,host:scm
2023-12-26 12:07:49,902 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
2023-12-26 12:07:49,902 [main] ERROR utils.CertificateSignRequest: Invalid domain scm
2023-12-26 12:07:50,007 [main] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.19, 2.5.29.15, 2.5.29.17
2023-12-26 12:07:50,008 [main] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-12-26 12:07:50,070 [main] INFO utils.CertificateCodec: Save certificate to /data/metadata/scm/sub-ca/certs/CA-1.crt
2023-12-26 12:07:50,073 [main] INFO utils.CertificateCodec: Certificate -----BEGIN CERTIFICATE-----
MIIDsTCCApmgAwIBAgIBATANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkNjY1ZmI1ZWItYzgxOC00ZjA2LTk0NTItMTIzYmJkMDE0
ZGE1MTEwLwYDVQQKDChDSUQtNDc3MjVhOGItMzMxYi00YmQzLTgzODYtMTliMzNi
ZmM0YjRkMQowCAYDVQQFEwExMB4XDTIzMTIyNjEyMDc0OVoXDTI5MDIwMjEyMDc0
OVowgYAxEDAOBgNVBAMMB3NjbUBzY20xLTArBgNVBAsMJDY2NWZiNWViLWM4MTgt
NGYwNi05NDUyLTEyM2JiZDAxNGRhNTExMC8GA1UECgwoQ0lELTQ3NzI1YThiLTMz
MWItNGJkMy04Mzg2LTE5YjMzYmZjNGI0ZDEKMAgGA1UEBRMBMTCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBAMmDO4+2px8BT0UCHWfmmU66e4LXQPYvf2YS
mvtIzjg8O9Cs/qsGXuQaw6qazwpsqhWLEWMNoQEqZlS9u3WEqRoA3oSg56T365V7
yly+5yOSHzX5tXJwAvGqi+2m+om9OVkA8wZLgK6hxagabow7ZW+Fc1vIJctxeZV5
BmCR3EQMbl5cLaFZm5yy1MwzMpaI4zxGIfXY3gG/FUz6l1H7Uy5ljRdOa9slzPoz
rmKWhsCmvyMT2QrbVDJaE9gvBCMyPmSP0gDCH/bO2NswPq1sFZ/nQZYQiF2Wjsy4
wTLoIFlNod8SgINGwK6bHJ0qdvreEPMFNgUSNzwnG3dyVZXDrfsCAwEAAaM0MDIw
DwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0RBAgwBocErBQA
BjANBgkqhkiG9w0BAQsFAAOCAQEAptDKHNEkJDUwUny+NoVszELKkSRE051Y1tAJ
g6pan7zqhhOoTZ8avN66k2B57J/BkFBrfKluRNn0RA2nHF0LdsewwjJQZaKxjZVt
D0WLPM5i60H1WUNKNTI+DU6pGZ940HRNVHpZ0P/vEiNy7z6XbN9cELUO7Ako/b5P
ztuM2YS9dC03ERM4VV2d51CLfGS2o/Wqn5a35335MeujQlaRKtJ86iArCFReb1qO
SkY//UFM3WGES+sHtqHitnXBnruU6MXcBBoIvdBck3k8QjytgxmZl/lHdXuQN53e
yu++iYKoUNjTAjtnKYvpf056egdr4fqu3NvjlhH3GnGMo8/5Vw==
-----END CERTIFICATE-----

2023-12-26 12:07:50,100 [main] INFO utils.CertificateCodec: Save certificate to /data/metadata/scm/sub-ca/certs/2.crt
2023-12-26 12:07:50,100 [main] INFO utils.CertificateCodec: Certificate -----BEGIN CERTIFICATE-----
MIIDtTCCAp2gAwIBAgIBAjANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkNjY1ZmI1ZWItYzgxOC00ZjA2LTk0NTItMTIzYmJkMDE0
ZGE1MTEwLwYDVQQKDChDSUQtNDc3MjVhOGItMzMxYi00YmQzLTgzODYtMTliMzNi
ZmM0YjRkMQowCAYDVQQFEwExMB4XDTIzMTIyNjEyMDc0OVoXDTI5MDIwMjEyMDc0
OVowgYQxFDASBgNVBAMMC3NjbS1zdWJAc2NtMS0wKwYDVQQLDCQ2NjVmYjVlYi1j
ODE4LTRmMDYtOTQ1Mi0xMjNiYmQwMTRkYTUxMTAvBgNVBAoMKENJRC00NzcyNWE4
Yi0zMzFiLTRiZDMtODM4Ni0xOWIzM2JmYzRiNGQxCjAIBgNVBAUTATIwggEiMA0G
CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDpJTZgutthf2yvBkQ+0IFW9az26rv0
RHB05E7wwCB61eTuRKUZDos96Eqto3WIttNn9wFhngBys2LyNsuZ11PYmIRVRGzu
PeEK/c4B9SdKuTL0oREmIyqtl51doX4BRIj9O1ygeUXrPuSfmglsEg1Gafcb/ifT
aVt42CUqqLB/EmkHFP02Rrhi5eA/9EqIAtrOyLDfqpY1OREF4gbac/gZn1UayxS5
TvLvvIQGOJGO8nEC0oiXeUqASrefUGAXAlIrvLXDWWnzspZhuEDiFCYO6b5taFQ6
FMwRvaREDDKhad2H1eQ/oVQtIn5DceEkgiHFwpctvBwypVhpPKoNc1ljAgMBAAGj
NDAyMA8GA1UdEQQIMAaHBKwUAAYwDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8E
BAMCAb4wDQYJKoZIhvcNAQELBQADggEBAGSst631mAVfANA3nN/x/QWK7LhTYVpF
R1uEhwvOJvn0RUikg1I4DPg7E8u+v4gHOIBtdMsA82w/n+om1AySvROgNVPv/y0T
4qZozGNwGnQrH1pR2XHvBAXUe9jlDBzzeOW3wKiPkt+seP00O0fu2aYsni9U1t4O
0OX3esvIK3Oq7hfAgB0oZ8dy4dSu1pFO2/KFRTAXYnOZdVLppq3f9JbfEHkv/LBy
/f7ZdGZBahyJGm70KoDAb+a9Few1/cFHs0H1DA+4lzPqb8H8+QD74GCzqn9QHlrN
/2FQmKgLFfdt8ZZOB31IEC7V6il7C6DPRAkGh5KwKkB3/g/2wxCjAPg=
-----END CERTIFICATE-----

-----BEGIN CERTIFICATE-----
MIIDsTCCApmgAwIBAgIBATANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkNjY1ZmI1ZWItYzgxOC00ZjA2LTk0NTItMTIzYmJkMDE0
ZGE1MTEwLwYDVQQKDChDSUQtNDc3MjVhOGItMzMxYi00YmQzLTgzODYtMTliMzNi
ZmM0YjRkMQowCAYDVQQFEwExMB4XDTIzMTIyNjEyMDc0OVoXDTI5MDIwMjEyMDc0
OVowgYAxEDAOBgNVBAMMB3NjbUBzY20xLTArBgNVBAsMJDY2NWZiNWViLWM4MTgt
NGYwNi05NDUyLTEyM2JiZDAxNGRhNTExMC8GA1UECgwoQ0lELTQ3NzI1YThiLTMz
MWItNGJkMy04Mzg2LTE5YjMzYmZjNGI0ZDEKMAgGA1UEBRMBMTCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBAMmDO4+2px8BT0UCHWfmmU66e4LXQPYvf2YS
mvtIzjg8O9Cs/qsGXuQaw6qazwpsqhWLEWMNoQEqZlS9u3WEqRoA3oSg56T365V7
yly+5yOSHzX5tXJwAvGqi+2m+om9OVkA8wZLgK6hxagabow7ZW+Fc1vIJctxeZV5
BmCR3EQMbl5cLaFZm5yy1MwzMpaI4zxGIfXY3gG/FUz6l1H7Uy5ljRdOa9slzPoz
rmKWhsCmvyMT2QrbVDJaE9gvBCMyPmSP0gDCH/bO2NswPq1sFZ/nQZYQiF2Wjsy4
wTLoIFlNod8SgINGwK6bHJ0qdvreEPMFNgUSNzwnG3dyVZXDrfsCAwEAAaM0MDIw
DwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0RBAgwBocErBQA
BjANBgkqhkiG9w0BAQsFAAOCAQEAptDKHNEkJDUwUny+NoVszELKkSRE051Y1tAJ
g6pan7zqhhOoTZ8avN66k2B57J/BkFBrfKluRNn0RA2nHF0LdsewwjJQZaKxjZVt
D0WLPM5i60H1WUNKNTI+DU6pGZ940HRNVHpZ0P/vEiNy7z6XbN9cELUO7Ako/b5P
ztuM2YS9dC03ERM4VV2d51CLfGS2o/Wqn5a35335MeujQlaRKtJ86iArCFReb1qO
SkY//UFM3WGES+sHtqHitnXBnruU6MXcBBoIvdBck3k8QjytgxmZl/lHdXuQN53e
yu++iYKoUNjTAjtnKYvpf056egdr4fqu3NvjlhH3GnGMo8/5Vw==
-----END CERTIFICATE-----

2023-12-26 12:07:50,110 [main] INFO utils.CertificateCodec: Save certificate to /data/metadata/scm/sub-ca/certs/certificate.crt
2023-12-26 12:07:50,111 [main] INFO utils.CertificateCodec: Certificate -----BEGIN CERTIFICATE-----
MIIDtTCCAp2gAwIBAgIBAjANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkNjY1ZmI1ZWItYzgxOC00ZjA2LTk0NTItMTIzYmJkMDE0
ZGE1MTEwLwYDVQQKDChDSUQtNDc3MjVhOGItMzMxYi00YmQzLTgzODYtMTliMzNi
ZmM0YjRkMQowCAYDVQQFEwExMB4XDTIzMTIyNjEyMDc0OVoXDTI5MDIwMjEyMDc0
OVowgYQxFDASBgNVBAMMC3NjbS1zdWJAc2NtMS0wKwYDVQQLDCQ2NjVmYjVlYi1j
ODE4LTRmMDYtOTQ1Mi0xMjNiYmQwMTRkYTUxMTAvBgNVBAoMKENJRC00NzcyNWE4
Yi0zMzFiLTRiZDMtODM4Ni0xOWIzM2JmYzRiNGQxCjAIBgNVBAUTATIwggEiMA0G
CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDpJTZgutthf2yvBkQ+0IFW9az26rv0
RHB05E7wwCB61eTuRKUZDos96Eqto3WIttNn9wFhngBys2LyNsuZ11PYmIRVRGzu
PeEK/c4B9SdKuTL0oREmIyqtl51doX4BRIj9O1ygeUXrPuSfmglsEg1Gafcb/ifT
aVt42CUqqLB/EmkHFP02Rrhi5eA/9EqIAtrOyLDfqpY1OREF4gbac/gZn1UayxS5
TvLvvIQGOJGO8nEC0oiXeUqASrefUGAXAlIrvLXDWWnzspZhuEDiFCYO6b5taFQ6
FMwRvaREDDKhad2H1eQ/oVQtIn5DceEkgiHFwpctvBwypVhpPKoNc1ljAgMBAAGj
NDAyMA8GA1UdEQQIMAaHBKwUAAYwDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8E
BAMCAb4wDQYJKoZIhvcNAQELBQADggEBAGSst631mAVfANA3nN/x/QWK7LhTYVpF
R1uEhwvOJvn0RUikg1I4DPg7E8u+v4gHOIBtdMsA82w/n+om1AySvROgNVPv/y0T
4qZozGNwGnQrH1pR2XHvBAXUe9jlDBzzeOW3wKiPkt+seP00O0fu2aYsni9U1t4O
0OX3esvIK3Oq7hfAgB0oZ8dy4dSu1pFO2/KFRTAXYnOZdVLppq3f9JbfEHkv/LBy
/f7ZdGZBahyJGm70KoDAb+a9Few1/cFHs0H1DA+4lzPqb8H8+QD74GCzqn9QHlrN
/2FQmKgLFfdt8ZZOB31IEC7V6il7C6DPRAkGh5KwKkB3/g/2wxCjAPg=
-----END CERTIFICATE-----

-----BEGIN CERTIFICATE-----
MIIDsTCCApmgAwIBAgIBATANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkNjY1ZmI1ZWItYzgxOC00ZjA2LTk0NTItMTIzYmJkMDE0
ZGE1MTEwLwYDVQQKDChDSUQtNDc3MjVhOGItMzMxYi00YmQzLTgzODYtMTliMzNi
ZmM0YjRkMQowCAYDVQQFEwExMB4XDTIzMTIyNjEyMDc0OVoXDTI5MDIwMjEyMDc0
OVowgYAxEDAOBgNVBAMMB3NjbUBzY20xLTArBgNVBAsMJDY2NWZiNWViLWM4MTgt
NGYwNi05NDUyLTEyM2JiZDAxNGRhNTExMC8GA1UECgwoQ0lELTQ3NzI1YThiLTMz
MWItNGJkMy04Mzg2LTE5YjMzYmZjNGI0ZDEKMAgGA1UEBRMBMTCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBAMmDO4+2px8BT0UCHWfmmU66e4LXQPYvf2YS
mvtIzjg8O9Cs/qsGXuQaw6qazwpsqhWLEWMNoQEqZlS9u3WEqRoA3oSg56T365V7
yly+5yOSHzX5tXJwAvGqi+2m+om9OVkA8wZLgK6hxagabow7ZW+Fc1vIJctxeZV5
BmCR3EQMbl5cLaFZm5yy1MwzMpaI4zxGIfXY3gG/FUz6l1H7Uy5ljRdOa9slzPoz
rmKWhsCmvyMT2QrbVDJaE9gvBCMyPmSP0gDCH/bO2NswPq1sFZ/nQZYQiF2Wjsy4
wTLoIFlNod8SgINGwK6bHJ0qdvreEPMFNgUSNzwnG3dyVZXDrfsCAwEAAaM0MDIw
DwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0RBAgwBocErBQA
BjANBgkqhkiG9w0BAQsFAAOCAQEAptDKHNEkJDUwUny+NoVszELKkSRE051Y1tAJ
g6pan7zqhhOoTZ8avN66k2B57J/BkFBrfKluRNn0RA2nHF0LdsewwjJQZaKxjZVt
D0WLPM5i60H1WUNKNTI+DU6pGZ940HRNVHpZ0P/vEiNy7z6XbN9cELUO7Ako/b5P
ztuM2YS9dC03ERM4VV2d51CLfGS2o/Wqn5a35335MeujQlaRKtJ86iArCFReb1qO
SkY//UFM3WGES+sHtqHitnXBnruU6MXcBBoIvdBck3k8QjytgxmZl/lHdXuQN53e
yu++iYKoUNjTAjtnKYvpf056egdr4fqu3NvjlhH3GnGMo8/5Vw==
-----END CERTIFICATE-----

2023-12-26 12:07:50,112 [main] INFO client.SCMCertificateClient: Successfully stored SCM signed certificate.
2023-12-26 12:07:50,448 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
2023-12-26 12:07:50,616 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2023-12-26 12:07:50,617 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
2023-12-26 12:07:50,618 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2023-12-26 12:07:50,618 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
2023-12-26 12:07:50,618 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
2023-12-26 12:07:50,618 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
2023-12-26 12:07:50,619 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
2023-12-26 12:07:50,620 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-12-26 12:07:50,621 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
2023-12-26 12:07:50,624 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
2023-12-26 12:07:50,644 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
2023-12-26 12:07:50,647 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
2023-12-26 12:07:50,648 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
2023-12-26 12:07:51,105 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
2023-12-26 12:07:51,108 [main] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
2023-12-26 12:07:51,108 [main] INFO server.RaftServerConfigKeys: raft.server.close.threshold = 60s (default)
2023-12-26 12:07:51,108 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-12-26 12:07:51,133 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
2023-12-26 12:07:51,134 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
2023-12-26 12:07:51,134 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
2023-12-26 12:07:51,159 [main] INFO server.RaftServer: 665fb5eb-c818-4f06-9452-123bbd014da5: addNew group-19B33BFC4B4D:[665fb5eb-c818-4f06-9452-123bbd014da5|scm:9894] returns group-19B33BFC4B4D:java.util.concurrent.CompletableFuture@4cd7e993[Not completed]
2023-12-26 12:07:51,196 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServer$Division: 665fb5eb-c818-4f06-9452-123bbd014da5: new RaftServerImpl for group-19B33BFC4B4D:[665fb5eb-c818-4f06-9452-123bbd014da5|scm:9894] with SCMStateMachine:uninitialized
2023-12-26 12:07:51,199 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
2023-12-26 12:07:51,204 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
2023-12-26 12:07:51,204 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
2023-12-26 12:07:51,204 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
2023-12-26 12:07:51,205 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-12-26 12:07:51,206 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.member.majority-add = false (default)
2023-12-26 12:07:51,206 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
2023-12-26 12:07:51,216 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServer$Division: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D: ConfigurationManager, init=-1: peers:[665fb5eb-c818-4f06-9452-123bbd014da5|scm:9894]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-12-26 12:07:51,245 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
2023-12-26 12:07:51,250 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
2023-12-26 12:07:51,255 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
2023-12-26 12:07:51,255 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100ms (default)
2023-12-26 12:07:51,259 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
2023-12-26 12:07:51,260 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.read-after-write-consistent.write-index-cache.expiry-time = 60s (default)
2023-12-26 12:07:51,272 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.dropwizard3.Dm3MetricRegistriesImpl
2023-12-26 12:07:51,463 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
2023-12-26 12:07:51,465 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-12-26 12:07:51,465 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
2023-12-26 12:07:51,466 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
2023-12-26 12:07:51,466 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
2023-12-26 12:07:51,466 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
2023-12-26 12:07:51,468 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
2023-12-26 12:07:51,468 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
2023-12-26 12:07:51,468 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
2023-12-26 12:07:51,475 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/47725a8b-331b-4bd3-8386-19b33bfc4b4d does not exist. Creating ...
2023-12-26 12:07:51,488 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/47725a8b-331b-4bd3-8386-19b33bfc4b4d/in_use.lock acquired by nodename 12@scm
2023-12-26 12:07:51,514 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/47725a8b-331b-4bd3-8386-19b33bfc4b4d has been successfully formatted.
2023-12-26 12:07:51,531 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
2023-12-26 12:07:51,544 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
2023-12-26 12:07:51,557 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-12-26 12:07:51,559 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-12-26 12:07:51,562 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
2023-12-26 12:07:51,570 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
2023-12-26 12:07:51,575 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
2023-12-26 12:07:51,576 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-12-26 12:07:51,576 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-12-26 12:07:51,578 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO util.AwaitToRun: Thread[665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-cacheEviction-AwaitToRun,5,main] started
2023-12-26 12:07:51,582 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/47725a8b-331b-4bd3-8386-19b33bfc4b4d
2023-12-26 12:07:51,583 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
2023-12-26 12:07:51,583 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
2023-12-26 12:07:51,585 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
2023-12-26 12:07:51,586 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
2023-12-26 12:07:51,586 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
2023-12-26 12:07:51,587 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
2023-12-26 12:07:51,587 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-12-26 12:07:51,588 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-12-26 12:07:51,600 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 33554440 (custom)
2023-12-26 12:07:51,609 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-12-26 12:07:51,612 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
2023-12-26 12:07:51,612 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
2023-12-26 12:07:51,615 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
2023-12-26 12:07:51,622 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-12-26 12:07:51,622 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-12-26 12:07:51,624 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServer$Division: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D: start as a follower, conf=-1: peers:[665fb5eb-c818-4f06-9452-123bbd014da5|scm:9894]|listeners:[], old=null
2023-12-26 12:07:51,625 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServer$Division: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-12-26 12:07:51,627 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO impl.RoleInfo: 665fb5eb-c818-4f06-9452-123bbd014da5: start 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-FollowerState
2023-12-26 12:07:51,635 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
2023-12-26 12:07:51,636 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-12-26 12:07:51,643 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-19B33BFC4B4D,id=665fb5eb-c818-4f06-9452-123bbd014da5
2023-12-26 12:07:51,645 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.trigger-when-stop.enabled = true (default)
2023-12-26 12:07:51,645 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-12-26 12:07:51,645 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
2023-12-26 12:07:51,646 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
2023-12-26 12:07:51,646 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
2023-12-26 12:07:51,650 [main] INFO server.RaftServer: 665fb5eb-c818-4f06-9452-123bbd014da5: start RPC server
2023-12-26 12:07:51,762 [main] INFO server.GrpcService: 665fb5eb-c818-4f06-9452-123bbd014da5: GrpcService started, listening on 9894
2023-12-26 12:07:51,769 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-665fb5eb-c818-4f06-9452-123bbd014da5: Started
2023-12-26 12:07:56,717 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-FollowerState] INFO impl.FollowerState: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5090231986ns, electionTimeout:5080ms
2023-12-26 12:07:56,717 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-FollowerState] INFO impl.RoleInfo: 665fb5eb-c818-4f06-9452-123bbd014da5: shutdown 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-FollowerState
2023-12-26 12:07:56,717 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-FollowerState] INFO server.RaftServer$Division: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-12-26 12:07:56,719 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
2023-12-26 12:07:56,719 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-FollowerState] INFO impl.RoleInfo: 665fb5eb-c818-4f06-9452-123bbd014da5: start 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1
2023-12-26 12:07:56,722 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO impl.LeaderElection: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[665fb5eb-c818-4f06-9452-123bbd014da5|scm:9894]|listeners:[], old=null
2023-12-26 12:07:56,723 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO impl.LeaderElection: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
2023-12-26 12:07:56,725 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO impl.LeaderElection: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[665fb5eb-c818-4f06-9452-123bbd014da5|scm:9894]|listeners:[], old=null
2023-12-26 12:07:56,725 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO impl.LeaderElection: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1 ELECTION round 0: result PASSED (term=1)
2023-12-26 12:07:56,725 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO impl.RoleInfo: 665fb5eb-c818-4f06-9452-123bbd014da5: shutdown 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1
2023-12-26 12:07:56,726 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServer$Division: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-12-26 12:07:56,729 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
2023-12-26 12:07:56,732 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
2023-12-26 12:07:56,732 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
2023-12-26 12:07:56,735 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
2023-12-26 12:07:56,735 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
2023-12-26 12:07:56,736 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
2023-12-26 12:07:56,741 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.read.leader.lease.enabled = false (default)
2023-12-26 12:07:56,742 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.read.leader.lease.timeout.ratio = 0.9 (default)
2023-12-26 12:07:56,743 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
2023-12-26 12:07:56,743 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
2023-12-26 12:07:56,743 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-12-26 12:07:56,744 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO impl.RoleInfo: 665fb5eb-c818-4f06-9452-123bbd014da5: start 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderStateImpl
2023-12-26 12:07:56,744 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServer$Division: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D: set firstElectionSinceStartup to false for becomeLeader
2023-12-26 12:07:56,744 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServer$Division: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D: change Leader from null to 665fb5eb-c818-4f06-9452-123bbd014da5 at term 1 for becomeLeader, leader elected after 5499ms
2023-12-26 12:07:56,795 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-SegmentedRaftLogWorker: Starting segment from index:0
2023-12-26 12:07:56,814 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServer$Division: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D: set configuration 0: peers:[665fb5eb-c818-4f06-9452-123bbd014da5|scm:9894]|listeners:[], old=null
2023-12-26 12:07:56,849 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/47725a8b-331b-4bd3-8386-19b33bfc4b4d/current/log_inprogress_0
2023-12-26 12:07:56,860 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO server.RaftServer$Division: leader is ready since appliedIndex == 0 >= startIndex == 0
2023-12-26 12:07:57,767 [main] INFO server.RaftServer: 665fb5eb-c818-4f06-9452-123bbd014da5: close
2023-12-26 12:07:57,768 [main] INFO server.GrpcService: 665fb5eb-c818-4f06-9452-123bbd014da5: shutdown server GrpcServerProtocolService now
2023-12-26 12:07:57,768 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServer$Division: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D: shutdown
2023-12-26 12:07:57,768 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-19B33BFC4B4D,id=665fb5eb-c818-4f06-9452-123bbd014da5
2023-12-26 12:07:57,768 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO impl.RoleInfo: 665fb5eb-c818-4f06-9452-123bbd014da5: shutdown 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderStateImpl
2023-12-26 12:07:57,772 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO impl.PendingRequests: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-PendingRequests: sendNotLeaderResponses
2023-12-26 12:07:57,774 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO impl.StateMachineUpdater: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater: set stopIndex = 0
2023-12-26 12:07:57,774 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO impl.StateMachineUpdater: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater: Took a snapshot at index 0
2023-12-26 12:07:57,775 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO impl.StateMachineUpdater: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2023-12-26 12:07:57,775 [main] INFO server.GrpcService: 665fb5eb-c818-4f06-9452-123bbd014da5: shutdown server GrpcServerProtocolService successfully
2023-12-26 12:07:57,778 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServer$Division: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D: applyIndex: 0
2023-12-26 12:07:57,778 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-cacheEviction-AwaitToRun] INFO util.AwaitToRun: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-cacheEviction-AwaitToRun-AwaitForSignal is interrupted
2023-12-26 12:07:57,854 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-SegmentedRaftLogWorker close()
2023-12-26 12:07:57,855 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-665fb5eb-c818-4f06-9452-123bbd014da5: Stopped
2023-12-26 12:07:57,855 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-12-26 12:07:57,857 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-47725a8b-331b-4bd3-8386-19b33bfc4b4d; layoutVersion=7; scmId=665fb5eb-c818-4f06-9452-123bbd014da5
2023-12-26 12:07:57,859 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down StorageContainerManager at scm/172.20.0.6
************************************************************/
No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
2023-12-26 12:07:59,454 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting StorageContainerManager
STARTUP_MSG:   host = scm/172.20.0.6
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 1.5.0-SNAPSHOT
STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jgraphx-2.0.0.1.jar:/opt/hadoop/share/ozone/lib/commons-net-3.10.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/okhttp-4.12.0.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.3.2.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.9.21.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.25.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.9.21.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.6.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.2.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.6.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/okio-3.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-3.0.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-3.0.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-3.0.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.9.21.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.12.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/jgraph-5.13.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/antlr4-runtime-4.5.3.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/picocli-4.7.5.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.16.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.9.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/jgrapht-ext-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-1.2.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.9.21.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jgrapht-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.14.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-dropwizard3-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-3.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-3.0.0.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.58.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.9.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-3.0.0.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.5.0-SNAPSHOT.jar
STARTUP_MSG:   build = https://github.com/apache/ozone/41f284af94206f1c4196a957ddbb9421407782b6 ; compiled by 'runner' on 2023-12-26T11:28Z
STARTUP_MSG:   java = 11.0.19
STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=4MB, dfs.container.ratis.segment.size=64MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.command.worker.interval=2s, hdds.datanode.block.delete.max.lock.wait.timeout=100ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.max.lock.holding.time=1s, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=19864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.close.threads.max=3, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.file.size=100B, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/dn.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/dn@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.queue.limit=4096, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=100MB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.log.appender.wait-time.min=0us, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/scm.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=5s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=5s, hdds.scm.replication.under.replicated.interval=5s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=30s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.expired.certificate.check.interval=P1D, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=1MB, ozone.chunk.read.mapped.buffer.threshold=32KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.grpc.write.timeout=30s, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.ec.grpc.zerocopy.enabled=true, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/om.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.max.buckets=100000, ozone.om.multitenancy.enabled=true, ozone.om.multitenancy.ranger.sync.interval=30s, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.open.mpu.cleanup.service.interval=24h, ozone.om.open.mpu.cleanup.service.timeout=300s, ozone.om.open.mpu.expire.threshold=30d, ozone.om.open.mpu.parts.cleanup.limit.per.task=1000, ozone.om.ranger.https-address=https://ranger:6182, ozone.om.ranger.https.admin.api.passwd=Passwd1, ozone.om.ranger.https.admin.api.user=admin, ozone.om.ranger.service=cm_ozone, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.checkpoint.dir.creation.poll.timeout=20s, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.load.native.lib=true, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.tenant.dev.skip.ranger=true, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.Hadoop3OmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=6000, ozone.recon.address=recon:9891, ozone.recon.administrators=testuser2, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3.administrators=testuser,s3g, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/s3g.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.list-keys.shallow.enabled=true, ozone.s3g.secret.http.auth.type=kerberos, ozone.s3g.secret.http.enabled=true, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.admin.monitor.logging.limit=1000, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=1, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=45s, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.raft.server.log.appender.wait-time.min=0ms, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=scm:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=30s, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
************************************************************/
2023-12-26 12:07:59,471 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
2023-12-26 12:07:59,527 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-12-26 12:07:59,651 [main] INFO reflections.Reflections: Reflections took 93 ms to scan 3 urls, producing 134 keys and 291 values 
2023-12-26 12:07:59,724 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
2023-12-26 12:07:59,731 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
2023-12-26 12:07:59,884 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
2023-12-26 12:07:59,884 [main] INFO server.StorageContainerManager: SCM login successful.
2023-12-26 12:08:00,337 [main] INFO client.SCMCertificateClient: Certificate serial ID set to 2
2023-12-26 12:08:00,449 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
         SerialNumber: 2
             IssuerDN: CN=scm@scm,OU=665fb5eb-c818-4f06-9452-123bbd014da5,O=CID-47725a8b-331b-4bd3-8386-19b33bfc4b4d,SERIALNUMBER=1
           Start Date: Tue Dec 26 12:07:49 UTC 2023
           Final Date: Fri Feb 02 12:07:49 UTC 2029
            SubjectDN: CN=scm-sub@scm,OU=665fb5eb-c818-4f06-9452-123bbd014da5,O=CID-47725a8b-331b-4bd3-8386-19b33bfc4b4d,SERIALNUMBER=2
           Public Key: RSA Public Key [dc:bf:9c:bb:73:34:f4:63:53:90:06:8a:47:f3:e6:a1:18:ad:69:d5],[56:66:d1:a4]
        modulus: e9253660badb617f6caf06443ed08156f5acf6eabbf4447074e44ef0c0207ad5e4ee44a5190e8b3de84aada37588b6d367f701619e0072b362f236cb99d753d8988455446cee3de10afdce01f5274ab932f4a11126232aad979d5da17e014488fd3b5ca07945eb3ee49f9a096c120d4669f71bfe27d3695b78d8252aa8b07f12690714fd3646b862e5e03ff44a8802dacec8b0dfaa9635391105e206da73f8199f551acb14b94ef2efbc840638918ef27102d28897794a804ab79f50601702522bbcb5c35969f3b29661b840e214260ee9be6d68543a14cc11bda4440c32a169dd87d5e43fa1542d227e4371e1248221c5c2972dbc1c32a558693caa0d735963
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 64acb7adf598055f00d0379cdff1fd058aecb853
                       615a45475b84870bce26f9f44548a48352380cf8
                       3b13cbbebf880738806d74cb00f36c3f9fea26d4
                       0c92bd13a03553efff2d13e2a668cc63701a742b
                       1f5a51d971ef0405d47bd8e50c1cf378e5b7c0a8
                       8f92dfac78fd343b47eed9a62c9e2f54d6de0ed0
                       e5f77acbc82b73aaee17c0801d2867c772e1d4ae
                       d6914edbf2854530176273997552e9a6addff496
                       df10792ffcb072fdfed97466416a1c891a6ef42a
                       80c06fe6bd15ec35fdc147b341f50c0fb89733ea
                       6fc1fcf900fbe060b3aa7f501e5acdff615098a8
                       0b15f76df1964e077d48102ed5ea297b0ba0cf44
                       09068792b02a4077fe0ff6c310a300f8
       Extensions: 
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [7] IMPLICIT 
        DER Octet String[4] 

                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0xbe
 from file: /data/metadata/scm/sub-ca/certs/2.crt.
2023-12-26 12:08:00,454 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
         SerialNumber: 1
             IssuerDN: CN=scm@scm,OU=665fb5eb-c818-4f06-9452-123bbd014da5,O=CID-47725a8b-331b-4bd3-8386-19b33bfc4b4d,SERIALNUMBER=1
           Start Date: Tue Dec 26 12:07:49 UTC 2023
           Final Date: Fri Feb 02 12:07:49 UTC 2029
            SubjectDN: CN=scm@scm,OU=665fb5eb-c818-4f06-9452-123bbd014da5,O=CID-47725a8b-331b-4bd3-8386-19b33bfc4b4d,SERIALNUMBER=1
           Public Key: RSA Public Key [d5:ca:d8:e0:dd:70:e8:fc:91:87:b9:7c:94:0a:4b:7c:5e:bc:c1:e3],[56:66:d1:a4]
        modulus: c9833b8fb6a71f014f45021d67e6994eba7b82d740f62f7f66129afb48ce383c3bd0acfeab065ee41ac3aa9acf0a6caa158b11630da1012a6654bdbb7584a91a00de84a0e7a4f7eb957bca5cbee723921f35f9b5727002f1aa8beda6fa89bd395900f3064b80aea1c5a81a6e8c3b656f85735bc825cb71799579066091dc440c6e5e5c2da1599b9cb2d4cc33329688e33c4621f5d8de01bf154cfa9751fb532e658d174e6bdb25ccfa33ae629686c0a6bf2313d90adb54325a13d82f0423323e648fd200c21ff6ced8db303ead6c159fe7419610885d968eccb8c132e820594da1df12808346c0ae9b1c9d2a76fade10f305360512373c271b77725595c3adfb
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: a6d0ca1cd124243530527cbe36856ccc42ca9124
                       44d39d58d6d00983aa5a9fbcea8613a84d9f1abc
                       deba936079ec9fc190506b7ca96e44d9f4440da7
                       1c5d0b76c7b0c2325065a2b18d956d0f458b3cce
                       62eb41f559434a35323e0d4ea9199f78d0744d54
                       7a59d0ffef122372ef3e976cdf5c10b50eec0928
                       fdbe4fcedb8cd984bd742d37111338555d9de750
                       8b7c64b6a3f5aa9f96b7e77df931eba34256912a
                       d27cea202b08545e6f5a8e4a463ffd414cdd6184
                       4beb07b6a1e2b675c19ebb94e8c5dc041a08bdd0
                       5c93793c423cad83199997f947757b90379ddeca
                       efbe8982a850d8d3023b67298be97f4e7a7a076b
                       e1faaedcdbe39611f71a718ca3cff957
       Extensions: 
                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0x6
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [7] IMPLICIT 
        DER Octet String[4] 

 from file: /data/metadata/scm/sub-ca/certs/CA-1.crt.
2023-12-26 12:08:00,458 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
         SerialNumber: 2
             IssuerDN: CN=scm@scm,OU=665fb5eb-c818-4f06-9452-123bbd014da5,O=CID-47725a8b-331b-4bd3-8386-19b33bfc4b4d,SERIALNUMBER=1
           Start Date: Tue Dec 26 12:07:49 UTC 2023
           Final Date: Fri Feb 02 12:07:49 UTC 2029
            SubjectDN: CN=scm-sub@scm,OU=665fb5eb-c818-4f06-9452-123bbd014da5,O=CID-47725a8b-331b-4bd3-8386-19b33bfc4b4d,SERIALNUMBER=2
           Public Key: RSA Public Key [dc:bf:9c:bb:73:34:f4:63:53:90:06:8a:47:f3:e6:a1:18:ad:69:d5],[56:66:d1:a4]
        modulus: e9253660badb617f6caf06443ed08156f5acf6eabbf4447074e44ef0c0207ad5e4ee44a5190e8b3de84aada37588b6d367f701619e0072b362f236cb99d753d8988455446cee3de10afdce01f5274ab932f4a11126232aad979d5da17e014488fd3b5ca07945eb3ee49f9a096c120d4669f71bfe27d3695b78d8252aa8b07f12690714fd3646b862e5e03ff44a8802dacec8b0dfaa9635391105e206da73f8199f551acb14b94ef2efbc840638918ef27102d28897794a804ab79f50601702522bbcb5c35969f3b29661b840e214260ee9be6d68543a14cc11bda4440c32a169dd87d5e43fa1542d227e4371e1248221c5c2972dbc1c32a558693caa0d735963
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 64acb7adf598055f00d0379cdff1fd058aecb853
                       615a45475b84870bce26f9f44548a48352380cf8
                       3b13cbbebf880738806d74cb00f36c3f9fea26d4
                       0c92bd13a03553efff2d13e2a668cc63701a742b
                       1f5a51d971ef0405d47bd8e50c1cf378e5b7c0a8
                       8f92dfac78fd343b47eed9a62c9e2f54d6de0ed0
                       e5f77acbc82b73aaee17c0801d2867c772e1d4ae
                       d6914edbf2854530176273997552e9a6addff496
                       df10792ffcb072fdfed97466416a1c891a6ef42a
                       80c06fe6bd15ec35fdc147b341f50c0fb89733ea
                       6fc1fcf900fbe060b3aa7f501e5acdff615098a8
                       0b15f76df1964e077d48102ed5ea297b0ba0cf44
                       09068792b02a4077fe0ff6c310a300f8
       Extensions: 
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [7] IMPLICIT 
        DER Octet String[4] 

                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0xbe
 from file: /data/metadata/scm/sub-ca/certs/certificate.crt.
2023-12-26 12:08:00,458 [main] INFO client.SCMCertificateClient: CertificateRenewerService and root ca rotation polling is disabled for scm/sub-ca
2023-12-26 12:08:00,562 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-12-26 12:08:00,686 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-12-26 12:08:00,935 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.5.0-SNAPSHOT.jar!/network-topology-default.xml]
2023-12-26 12:08:00,936 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
2023-12-26 12:08:01,008 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.dropwizard3.Dm3MetricRegistriesImpl
2023-12-26 12:08:01,166 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:665fb5eb-c818-4f06-9452-123bbd014da5
2023-12-26 12:08:01,186 [main] INFO ssl.ReloadingX509KeyManager: Key manager is loaded with certificate chain
2023-12-26 12:08:01,188 [main] INFO ssl.ReloadingX509KeyManager:   [0]         Version: 3
         SerialNumber: 2
             IssuerDN: CN=scm@scm,OU=665fb5eb-c818-4f06-9452-123bbd014da5,O=CID-47725a8b-331b-4bd3-8386-19b33bfc4b4d,SERIALNUMBER=1
           Start Date: Tue Dec 26 12:07:49 UTC 2023
           Final Date: Fri Feb 02 12:07:49 UTC 2029
            SubjectDN: CN=scm-sub@scm,OU=665fb5eb-c818-4f06-9452-123bbd014da5,O=CID-47725a8b-331b-4bd3-8386-19b33bfc4b4d,SERIALNUMBER=2
           Public Key: RSA Public Key [dc:bf:9c:bb:73:34:f4:63:53:90:06:8a:47:f3:e6:a1:18:ad:69:d5],[56:66:d1:a4]
        modulus: e9253660badb617f6caf06443ed08156f5acf6eabbf4447074e44ef0c0207ad5e4ee44a5190e8b3de84aada37588b6d367f701619e0072b362f236cb99d753d8988455446cee3de10afdce01f5274ab932f4a11126232aad979d5da17e014488fd3b5ca07945eb3ee49f9a096c120d4669f71bfe27d3695b78d8252aa8b07f12690714fd3646b862e5e03ff44a8802dacec8b0dfaa9635391105e206da73f8199f551acb14b94ef2efbc840638918ef27102d28897794a804ab79f50601702522bbcb5c35969f3b29661b840e214260ee9be6d68543a14cc11bda4440c32a169dd87d5e43fa1542d227e4371e1248221c5c2972dbc1c32a558693caa0d735963
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 64acb7adf598055f00d0379cdff1fd058aecb853
                       615a45475b84870bce26f9f44548a48352380cf8
                       3b13cbbebf880738806d74cb00f36c3f9fea26d4
                       0c92bd13a03553efff2d13e2a668cc63701a742b
                       1f5a51d971ef0405d47bd8e50c1cf378e5b7c0a8
                       8f92dfac78fd343b47eed9a62c9e2f54d6de0ed0
                       e5f77acbc82b73aaee17c0801d2867c772e1d4ae
                       d6914edbf2854530176273997552e9a6addff496
                       df10792ffcb072fdfed97466416a1c891a6ef42a
                       80c06fe6bd15ec35fdc147b341f50c0fb89733ea
                       6fc1fcf900fbe060b3aa7f501e5acdff615098a8
                       0b15f76df1964e077d48102ed5ea297b0ba0cf44
                       09068792b02a4077fe0ff6c310a300f8
       Extensions: 
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [7] IMPLICIT 
        DER Octet String[4] 

                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0xbe

2023-12-26 12:08:01,190 [main] INFO ssl.ReloadingX509KeyManager:   [0]         Version: 3
         SerialNumber: 1
             IssuerDN: CN=scm@scm,OU=665fb5eb-c818-4f06-9452-123bbd014da5,O=CID-47725a8b-331b-4bd3-8386-19b33bfc4b4d,SERIALNUMBER=1
           Start Date: Tue Dec 26 12:07:49 UTC 2023
           Final Date: Fri Feb 02 12:07:49 UTC 2029
            SubjectDN: CN=scm@scm,OU=665fb5eb-c818-4f06-9452-123bbd014da5,O=CID-47725a8b-331b-4bd3-8386-19b33bfc4b4d,SERIALNUMBER=1
           Public Key: RSA Public Key [d5:ca:d8:e0:dd:70:e8:fc:91:87:b9:7c:94:0a:4b:7c:5e:bc:c1:e3],[56:66:d1:a4]
        modulus: c9833b8fb6a71f014f45021d67e6994eba7b82d740f62f7f66129afb48ce383c3bd0acfeab065ee41ac3aa9acf0a6caa158b11630da1012a6654bdbb7584a91a00de84a0e7a4f7eb957bca5cbee723921f35f9b5727002f1aa8beda6fa89bd395900f3064b80aea1c5a81a6e8c3b656f85735bc825cb71799579066091dc440c6e5e5c2da1599b9cb2d4cc33329688e33c4621f5d8de01bf154cfa9751fb532e658d174e6bdb25ccfa33ae629686c0a6bf2313d90adb54325a13d82f0423323e648fd200c21ff6ced8db303ead6c159fe7419610885d968eccb8c132e820594da1df12808346c0ae9b1c9d2a76fade10f305360512373c271b77725595c3adfb
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: a6d0ca1cd124243530527cbe36856ccc42ca9124
                       44d39d58d6d00983aa5a9fbcea8613a84d9f1abc
                       deba936079ec9fc190506b7ca96e44d9f4440da7
                       1c5d0b76c7b0c2325065a2b18d956d0f458b3cce
                       62eb41f559434a35323e0d4ea9199f78d0744d54
                       7a59d0ffef122372ef3e976cdf5c10b50eec0928
                       fdbe4fcedb8cd984bd742d37111338555d9de750
                       8b7c64b6a3f5aa9f96b7e77df931eba34256912a
                       d27cea202b08545e6f5a8e4a463ffd414cdd6184
                       4beb07b6a1e2b675c19ebb94e8c5dc041a08bdd0
                       5c93793c423cad83199997f947757b90379ddeca
                       efbe8982a850d8d3023b67298be97f4e7a7a076b
                       e1faaedcdbe39611f71a718ca3cff957
       Extensions: 
                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0x6
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [7] IMPLICIT 
        DER Octet String[4] 


2023-12-26 12:08:01,194 [main] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-12-26 12:08:01,194 [main] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-12-26 12:08:01,195 [main] INFO ssl.ReloadingX509TrustManager: Trust manager is loaded with certificates
2023-12-26 12:08:01,198 [main] INFO ssl.ReloadingX509TrustManager:   [0]         Version: 3
         SerialNumber: 1
             IssuerDN: CN=scm@scm,OU=665fb5eb-c818-4f06-9452-123bbd014da5,O=CID-47725a8b-331b-4bd3-8386-19b33bfc4b4d,SERIALNUMBER=1
           Start Date: Tue Dec 26 12:07:49 UTC 2023
           Final Date: Fri Feb 02 12:07:49 UTC 2029
            SubjectDN: CN=scm@scm,OU=665fb5eb-c818-4f06-9452-123bbd014da5,O=CID-47725a8b-331b-4bd3-8386-19b33bfc4b4d,SERIALNUMBER=1
           Public Key: RSA Public Key [d5:ca:d8:e0:dd:70:e8:fc:91:87:b9:7c:94:0a:4b:7c:5e:bc:c1:e3],[56:66:d1:a4]
        modulus: c9833b8fb6a71f014f45021d67e6994eba7b82d740f62f7f66129afb48ce383c3bd0acfeab065ee41ac3aa9acf0a6caa158b11630da1012a6654bdbb7584a91a00de84a0e7a4f7eb957bca5cbee723921f35f9b5727002f1aa8beda6fa89bd395900f3064b80aea1c5a81a6e8c3b656f85735bc825cb71799579066091dc440c6e5e5c2da1599b9cb2d4cc33329688e33c4621f5d8de01bf154cfa9751fb532e658d174e6bdb25ccfa33ae629686c0a6bf2313d90adb54325a13d82f0423323e648fd200c21ff6ced8db303ead6c159fe7419610885d968eccb8c132e820594da1df12808346c0ae9b1c9d2a76fade10f305360512373c271b77725595c3adfb
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: a6d0ca1cd124243530527cbe36856ccc42ca9124
                       44d39d58d6d00983aa5a9fbcea8613a84d9f1abc
                       deba936079ec9fc190506b7ca96e44d9f4440da7
                       1c5d0b76c7b0c2325065a2b18d956d0f458b3cce
                       62eb41f559434a35323e0d4ea9199f78d0744d54
                       7a59d0ffef122372ef3e976cdf5c10b50eec0928
                       fdbe4fcedb8cd984bd742d37111338555d9de750
                       8b7c64b6a3f5aa9f96b7e77df931eba34256912a
                       d27cea202b08545e6f5a8e4a463ffd414cdd6184
                       4beb07b6a1e2b675c19ebb94e8c5dc041a08bdd0
                       5c93793c423cad83199997f947757b90379ddeca
                       efbe8982a850d8d3023b67298be97f4e7a7a076b
                       e1faaedcdbe39611f71a718ca3cff957
       Extensions: 
                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0x6
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [7] IMPLICIT 
        DER Octet String[4] 


2023-12-26 12:08:01,211 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-12-26 12:08:01,215 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-12-26 12:08:01,271 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
2023-12-26 12:08:01,281 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2023-12-26 12:08:01,293 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
2023-12-26 12:08:01,294 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2023-12-26 12:08:01,294 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
2023-12-26 12:08:01,294 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
2023-12-26 12:08:01,295 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
2023-12-26 12:08:01,295 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
2023-12-26 12:08:01,330 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-12-26 12:08:01,330 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
2023-12-26 12:08:01,331 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
2023-12-26 12:08:01,341 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
2023-12-26 12:08:01,345 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
2023-12-26 12:08:01,345 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
2023-12-26 12:08:01,726 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
2023-12-26 12:08:01,727 [main] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
2023-12-26 12:08:01,727 [main] INFO server.RaftServerConfigKeys: raft.server.close.threshold = 60s (default)
2023-12-26 12:08:01,727 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-12-26 12:08:01,731 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
2023-12-26 12:08:01,732 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
2023-12-26 12:08:01,732 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
2023-12-26 12:08:01,734 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServer: 665fb5eb-c818-4f06-9452-123bbd014da5: found a subdirectory /data/metadata/scm-ha/47725a8b-331b-4bd3-8386-19b33bfc4b4d
2023-12-26 12:08:01,739 [main] INFO server.RaftServer: 665fb5eb-c818-4f06-9452-123bbd014da5: addNew group-19B33BFC4B4D:[] returns group-19B33BFC4B4D:java.util.concurrent.CompletableFuture@467cd4b9[Not completed]
2023-12-26 12:08:01,754 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServer$Division: 665fb5eb-c818-4f06-9452-123bbd014da5: new RaftServerImpl for group-19B33BFC4B4D:[] with SCMStateMachine:uninitialized
2023-12-26 12:08:01,757 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
2023-12-26 12:08:01,757 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
2023-12-26 12:08:01,757 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
2023-12-26 12:08:01,757 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
2023-12-26 12:08:01,757 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-12-26 12:08:01,758 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.member.majority-add = false (default)
2023-12-26 12:08:01,758 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
2023-12-26 12:08:01,763 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServer$Division: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-12-26 12:08:01,767 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
2023-12-26 12:08:01,769 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
2023-12-26 12:08:01,772 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
2023-12-26 12:08:01,772 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100ms (default)
2023-12-26 12:08:01,775 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
2023-12-26 12:08:01,775 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.read-after-write-consistent.write-index-cache.expiry-time = 60s (default)
2023-12-26 12:08:01,857 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
2023-12-26 12:08:01,859 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-12-26 12:08:01,859 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
2023-12-26 12:08:01,860 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
2023-12-26 12:08:01,860 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
2023-12-26 12:08:01,860 [665fb5eb-c818-4f06-9452-123bbd014da5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
2023-12-26 12:08:01,862 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
2023-12-26 12:08:01,862 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
2023-12-26 12:08:01,862 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
2023-12-26 12:08:01,885 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
2023-12-26 12:08:01,914 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
2023-12-26 12:08:01,914 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
2023-12-26 12:08:01,918 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
2023-12-26 12:08:01,920 [main] INFO ha.SequenceIdGenerator: upgrade CertificateId to 2
2023-12-26 12:08:01,922 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
2023-12-26 12:08:01,981 [main] WARN server.ServerUtils: ozone.scm.stale.node.interval value = 30000 is smaller than min = 90000 based on the key value of hdds.heartbeat.interval, reset to the min value 90000.
2023-12-26 12:08:01,981 [main] WARN server.ServerUtils: ozone.scm.stale.node.interval value = 30000 is smaller than min = 90000 based on the key value of hdds.heartbeat.interval, reset to the min value 90000.
2023-12-26 12:08:01,981 [main] WARN server.ServerUtils: ozone.scm.dead.node.interval value = 45000 is smaller than min = 180000 based on the key value of ozone.scm.stale.node.interval, reset to the min value 180000.
2023-12-26 12:08:01,986 [main] INFO node.SCMNodeManager: Entering startup safe mode.
2023-12-26 12:08:01,998 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
2023-12-26 12:08:02,001 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
2023-12-26 12:08:02,007 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
2023-12-26 12:08:02,020 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
2023-12-26 12:08:02,020 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
2023-12-26 12:08:02,025 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
2023-12-26 12:08:02,025 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
2023-12-26 12:08:02,027 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
2023-12-26 12:08:02,027 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
2023-12-26 12:08:02,032 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
2023-12-26 12:08:02,032 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
2023-12-26 12:08:02,052 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
2023-12-26 12:08:02,052 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
2023-12-26 12:08:02,069 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
2023-12-26 12:08:02,127 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
2023-12-26 12:08:02,130 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-12-26 12:08:02,132 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
2023-12-26 12:08:02,140 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
2023-12-26 12:08:02,144 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-26 12:08:02,145 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
2023-12-26 12:08:02,281 [main] INFO security.SecretKeyManagerService: Scheduling rotation checker with interval PT10M
2023-12-26 12:08:02,281 [main] INFO ha.SCMServiceManager: Registering service SecretKeyManagerService.
2023-12-26 12:08:02,298 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
2023-12-26 12:08:02,300 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
2023-12-26 12:08:02,301 [main] INFO server.StorageContainerManager: Storing sub-ca certificate serialId 2 on primary SCM
2023-12-26 12:08:02,310 [main] INFO server.SCMCertStore: Scm certificate 2 for CN=scm-sub@scm,OU=665fb5eb-c818-4f06-9452-123bbd014da5,O=CID-47725a8b-331b-4bd3-8386-19b33bfc4b4d,SERIALNUMBER=2 is stored
2023-12-26 12:08:02,311 [main] INFO server.StorageContainerManager: Storing root certificate serialId 1
2023-12-26 12:08:02,313 [main] INFO server.SCMCertStore: Scm certificate 1 for CN=scm@scm,OU=665fb5eb-c818-4f06-9452-123bbd014da5,O=CID-47725a8b-331b-4bd3-8386-19b33bfc4b4d,SERIALNUMBER=1 is stored
2023-12-26 12:08:02,317 [main] INFO ha.SequenceIdGenerator: upgrade CertificateId to 2
2023-12-26 12:08:02,332 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2023-12-26 12:08:02,353 [main] INFO ipc.Server: Listener at 0.0.0.0:9961
2023-12-26 12:08:02,354 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
2023-12-26 12:08:02,381 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [testuser, recon, om, scm]
2023-12-26 12:08:02,645 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
2023-12-26 12:08:02,650 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2023-12-26 12:08:02,651 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
2023-12-26 12:08:02,651 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
2023-12-26 12:08:02,678 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
2023-12-26 12:08:02,682 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2023-12-26 12:08:02,683 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
2023-12-26 12:08:02,683 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
2023-12-26 12:08:02,843 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
2023-12-26 12:08:02,870 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2023-12-26 12:08:02,872 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
2023-12-26 12:08:02,886 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
2023-12-26 12:08:03,000 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
2023-12-26 12:08:03,001 [main] INFO server.StorageContainerManager: 
Container Balancer status:
Key                            Value
Running                        false
Container Balancer Configuration values:
Key                                                Value
Threshold                                          10
Max Datanodes to Involve per Iteration(percent)    20
Max Size to Move per Iteration                     500GB
Max Size Entering Target per Iteration             26GB
Max Size Leaving Source per Iteration              26GB
Number of Iterations                               10
Time Limit for Single Container's Movement         65min
Time Limit for Single Container's Replication      50min
Interval between each Iteration                    70min
Whether to Enable Network Topology                 false
Whether to Trigger Refresh Datanode Usage Info     false
Container IDs to Exclude from Balancing            None
Datanodes Specified to be Balanced                 None
Datanodes Excluded from Balancing                  None

2023-12-26 12:08:03,001 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
2023-12-26 12:08:03,018 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
2023-12-26 12:08:03,024 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
2023-12-26 12:08:03,027 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
2023-12-26 12:08:03,028 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
2023-12-26 12:08:03,028 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
2023-12-26 12:08:03,043 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/47725a8b-331b-4bd3-8386-19b33bfc4b4d/in_use.lock acquired by nodename 6@scm
2023-12-26 12:08:03,047 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=665fb5eb-c818-4f06-9452-123bbd014da5} from /data/metadata/scm-ha/47725a8b-331b-4bd3-8386-19b33bfc4b4d/current/raft-meta
2023-12-26 12:08:03,070 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServer$Division: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D: set configuration 0: peers:[665fb5eb-c818-4f06-9452-123bbd014da5|scm:9894]|listeners:[], old=null
2023-12-26 12:08:03,073 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
2023-12-26 12:08:03,080 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
2023-12-26 12:08:03,080 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-12-26 12:08:03,082 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-12-26 12:08:03,082 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
2023-12-26 12:08:03,086 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
2023-12-26 12:08:03,092 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
2023-12-26 12:08:03,093 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-12-26 12:08:03,093 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-12-26 12:08:03,094 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO util.AwaitToRun: Thread[665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-cacheEviction-AwaitToRun,5,main] started
2023-12-26 12:08:03,102 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/47725a8b-331b-4bd3-8386-19b33bfc4b4d
2023-12-26 12:08:03,103 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
2023-12-26 12:08:03,103 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
2023-12-26 12:08:03,105 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
2023-12-26 12:08:03,105 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
2023-12-26 12:08:03,106 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
2023-12-26 12:08:03,108 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
2023-12-26 12:08:03,108 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-12-26 12:08:03,108 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-12-26 12:08:03,111 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 33554440 (custom)
2023-12-26 12:08:03,124 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-12-26 12:08:03,130 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
2023-12-26 12:08:03,130 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
2023-12-26 12:08:03,130 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
2023-12-26 12:08:03,163 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServer$Division: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D: set configuration 0: peers:[665fb5eb-c818-4f06-9452-123bbd014da5|scm:9894]|listeners:[], old=null
2023-12-26 12:08:03,164 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/47725a8b-331b-4bd3-8386-19b33bfc4b4d/current/log_inprogress_0
2023-12-26 12:08:03,165 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-12-26 12:08:03,239 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServer$Division: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D: start as a follower, conf=0: peers:[665fb5eb-c818-4f06-9452-123bbd014da5|scm:9894]|listeners:[], old=null
2023-12-26 12:08:03,239 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServer$Division: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D: changes role from      null to FOLLOWER at term 1 for startAsFollower
2023-12-26 12:08:03,240 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO impl.RoleInfo: 665fb5eb-c818-4f06-9452-123bbd014da5: start 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-FollowerState
2023-12-26 12:08:03,243 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
2023-12-26 12:08:03,243 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-12-26 12:08:03,244 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-19B33BFC4B4D,id=665fb5eb-c818-4f06-9452-123bbd014da5
2023-12-26 12:08:03,246 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.trigger-when-stop.enabled = true (default)
2023-12-26 12:08:03,246 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-12-26 12:08:03,246 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
2023-12-26 12:08:03,247 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
2023-12-26 12:08:03,247 [665fb5eb-c818-4f06-9452-123bbd014da5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
2023-12-26 12:08:03,252 [main] INFO server.RaftServer: 665fb5eb-c818-4f06-9452-123bbd014da5: start RPC server
2023-12-26 12:08:03,332 [main] INFO server.GrpcService: 665fb5eb-c818-4f06-9452-123bbd014da5: GrpcService started, listening on 9894
2023-12-26 12:08:03,338 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-665fb5eb-c818-4f06-9452-123bbd014da5: Started
2023-12-26 12:08:03,345 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [665fb5eb-c818-4f06-9452-123bbd014da5|scm:9894]
2023-12-26 12:08:03,346 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
2023-12-26 12:08:03,354 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
2023-12-26 12:08:03,360 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
2023-12-26 12:08:03,361 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
2023-12-26 12:08:03,437 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2023-12-26 12:08:03,451 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2023-12-26 12:08:03,451 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
2023-12-26 12:08:03,551 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
2023-12-26 12:08:03,551 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
2023-12-26 12:08:03,552 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
2023-12-26 12:08:03,581 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
2023-12-26 12:08:03,582 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
2023-12-26 12:08:03,582 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
2023-12-26 12:08:03,588 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
2023-12-26 12:08:03,605 [main] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
2023-12-26 12:08:03,607 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
2023-12-26 12:08:03,607 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
2023-12-26 12:08:03,625 [main] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
2023-12-26 12:08:03,703 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
2023-12-26 12:08:03,703 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
2023-12-26 12:08:03,704 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
2023-12-26 12:08:03,764 [main] INFO util.log: Logging initialized @5398ms to org.eclipse.jetty.util.log.Slf4jLog
2023-12-26 12:08:04,077 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:36209 / 172.20.0.4:36209
2023-12-26 12:08:04,109 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:37555 / 172.20.0.8:37555
2023-12-26 12:08:04,152 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-12-26 12:08:04,158 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-12-26 12:08:04,191 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:33773 / 172.20.0.5:33773
2023-12-26 12:08:04,194 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:45181 / 172.20.0.3:45181
2023-12-26 12:08:04,215 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
2023-12-26 12:08:04,224 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-12-26 12:08:04,231 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-12-26 12:08:04,276 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
2023-12-26 12:08:04,291 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
2023-12-26 12:08:04,291 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
2023-12-26 12:08:04,291 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
2023-12-26 12:08:04,305 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
2023-12-26 12:08:04,400 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:40901 / 172.20.0.9:40901
2023-12-26 12:08:04,432 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-12-26 12:08:04,493 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
2023-12-26 12:08:04,504 [main] INFO http.HttpServer2: Jetty bound to port 9876
2023-12-26 12:08:04,520 [main] INFO server.Server: jetty-9.4.53.v20231009; built: 2023-10-09T12:29:09.265Z; git: 27bde00a0b95a1d5bbee0eae7984f891d2d0f8c9; jvm 11.0.19+7-LTS
2023-12-26 12:08:04,541 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#6 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_recon_1.ozonesecure_default:37555 / 172.20.0.8:37555
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:665fb5eb-c818-4f06-9452-123bbd014da5 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-12-26 12:08:04,545 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#7 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_1.ozonesecure_default:33773 / 172.20.0.5:33773
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:665fb5eb-c818-4f06-9452-123bbd014da5 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-12-26 12:08:04,561 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#8 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_3.ozonesecure_default:45181 / 172.20.0.3:45181
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:665fb5eb-c818-4f06-9452-123bbd014da5 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-12-26 12:08:04,564 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#7 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_2.ozonesecure_default:36209 / 172.20.0.4:36209
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:665fb5eb-c818-4f06-9452-123bbd014da5 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-12-26 12:08:04,686 [main] INFO server.session: DefaultSessionIdManager workerName=node0
2023-12-26 12:08:04,687 [main] INFO server.session: No SessionScavenger set, using defaults
2023-12-26 12:08:04,688 [main] INFO server.session: node0 Scavenging every 600000ms
2023-12-26 12:08:04,704 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/scm.keytab, for principal HTTP/scm@EXAMPLE.COM
2023-12-26 12:08:04,721 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1bb3d045{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
2023-12-26 12:08:04,722 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5ec5a449{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2023-12-26 12:08:04,830 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/scm.keytab, for principal HTTP/scm@EXAMPLE.COM
2023-12-26 12:08:04,838 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@108d88b{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_5_0-SNAPSHOT_jar-_-any-8878112110593332862/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.5.0-SNAPSHOT.jar!/webapps/scm}
2023-12-26 12:08:04,845 [main] INFO server.AbstractConnector: Started ServerConnector@3a237f14{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
2023-12-26 12:08:04,846 [main] INFO server.Server: Started @6480ms
2023-12-26 12:08:04,847 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
2023-12-26 12:08:04,847 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
2023-12-26 12:08:04,848 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
2023-12-26 12:08:04,947 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm:40331 / 172.20.0.6:40331
2023-12-26 12:08:04,962 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-12-26 12:08:04,965 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#0 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from scm:40331 / 172.20.0.6:40331
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:665fb5eb-c818-4f06-9452-123bbd014da5 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-12-26 12:08:06,554 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#7 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_recon_1.ozonesecure_default:37555 / 172.20.0.8:37555
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:665fb5eb-c818-4f06-9452-123bbd014da5 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-12-26 12:08:06,559 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#8 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_1.ozonesecure_default:33773 / 172.20.0.5:33773
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:665fb5eb-c818-4f06-9452-123bbd014da5 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-12-26 12:08:06,578 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#9 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_3.ozonesecure_default:45181 / 172.20.0.3:45181
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:665fb5eb-c818-4f06-9452-123bbd014da5 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-12-26 12:08:06,580 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#8 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_2.ozonesecure_default:36209 / 172.20.0.4:36209
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:665fb5eb-c818-4f06-9452-123bbd014da5 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-12-26 12:08:06,978 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#1 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from scm:40331 / 172.20.0.6:40331
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:665fb5eb-c818-4f06-9452-123bbd014da5 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-12-26 12:08:06,980 [665fb5eb-c818-4f06-9452-123bbd014da5-scm/sub-ca-refreshCACertificates] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:665fb5eb-c818-4f06-9452-123bbd014da5 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
, while invoking $Proxy15.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.20.0.6:9961 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
2023-12-26 12:08:07,132 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-12-26 12:08:07,650 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from scm:44019 / 172.20.0.6:44019
2023-12-26 12:08:07,665 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-12-26 12:08:08,398 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-FollowerState] INFO impl.FollowerState: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5158361134ns, electionTimeout:5154ms
2023-12-26 12:08:08,398 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-FollowerState] INFO impl.RoleInfo: 665fb5eb-c818-4f06-9452-123bbd014da5: shutdown 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-FollowerState
2023-12-26 12:08:08,399 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-FollowerState] INFO server.RaftServer$Division: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
2023-12-26 12:08:08,401 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
2023-12-26 12:08:08,401 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-FollowerState] INFO impl.RoleInfo: 665fb5eb-c818-4f06-9452-123bbd014da5: start 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1
2023-12-26 12:08:08,403 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO impl.LeaderElection: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[665fb5eb-c818-4f06-9452-123bbd014da5|scm:9894]|listeners:[], old=null
2023-12-26 12:08:08,403 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO impl.LeaderElection: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1 PRE_VOTE round 0: result PASSED (term=1)
2023-12-26 12:08:08,407 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO impl.LeaderElection: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[665fb5eb-c818-4f06-9452-123bbd014da5|scm:9894]|listeners:[], old=null
2023-12-26 12:08:08,407 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO impl.LeaderElection: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1 ELECTION round 0: result PASSED (term=2)
2023-12-26 12:08:08,407 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO impl.RoleInfo: 665fb5eb-c818-4f06-9452-123bbd014da5: shutdown 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1
2023-12-26 12:08:08,407 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServer$Division: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
2023-12-26 12:08:08,411 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
2023-12-26 12:08:08,414 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
2023-12-26 12:08:08,415 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
2023-12-26 12:08:08,417 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
2023-12-26 12:08:08,417 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
2023-12-26 12:08:08,418 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
2023-12-26 12:08:08,422 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.read.leader.lease.enabled = false (default)
2023-12-26 12:08:08,423 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.read.leader.lease.timeout.ratio = 0.9 (default)
2023-12-26 12:08:08,423 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
2023-12-26 12:08:08,423 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
2023-12-26 12:08:08,423 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-12-26 12:08:08,424 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO impl.RoleInfo: 665fb5eb-c818-4f06-9452-123bbd014da5: start 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderStateImpl
2023-12-26 12:08:08,424 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServer$Division: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D: set firstElectionSinceStartup to false for becomeLeader
2023-12-26 12:08:08,424 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
2023-12-26 12:08:08,424 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
2023-12-26 12:08:08,429 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServer$Division: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D: change Leader from null to 665fb5eb-c818-4f06-9452-123bbd014da5 at term 2 for becomeLeader, leader elected after 6657ms
2023-12-26 12:08:08,433 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
2023-12-26 12:08:08,435 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/47725a8b-331b-4bd3-8386-19b33bfc4b4d/current/log_inprogress_0 to /data/metadata/scm-ha/47725a8b-331b-4bd3-8386-19b33bfc4b4d/current/log_0-0
2023-12-26 12:08:08,436 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-LeaderElection1] INFO server.RaftServer$Division: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D: set configuration 1: peers:[665fb5eb-c818-4f06-9452-123bbd014da5|scm:9894]|listeners:[], old=null
2023-12-26 12:08:08,447 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/47725a8b-331b-4bd3-8386-19b33bfc4b4d/current/log_inprogress_1
2023-12-26 12:08:08,452 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO server.RaftServer$Division: leader is ready since appliedIndex == 1 >= startIndex == 1
2023-12-26 12:08:08,452 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
2023-12-26 12:08:08,457 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
2023-12-26 12:08:08,457 [SecretKeyManagerService] INFO symmetric.SecretKeyManager: Initializing SecretKeys.
2023-12-26 12:08:08,458 [SecretKeyManagerService] INFO symmetric.SecretKeyManager: No valid key has been loaded. A new key is generated: SecretKey(id = 6377783b-39f5-43c7-a374-8925db86c2ff, creation at: 2023-12-26T12:08:08.458128Z, expire at: 2024-01-02T12:08:08.458128Z)
2023-12-26 12:08:08,461 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-26 12:08:08,463 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
2023-12-26 12:08:08,465 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
2023-12-26 12:08:08,465 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
2023-12-26 12:08:08,468 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
2023-12-26 12:08:08,468 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
2023-12-26 12:08:08,538 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Updating keys with [SecretKey(id = 6377783b-39f5-43c7-a374-8925db86c2ff, creation at: 2023-12-26T12:08:08.458Z, expire at: 2024-01-02T12:08:08.458Z)]
2023-12-26 12:08:08,538 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Current key updated SecretKey(id = 6377783b-39f5-43c7-a374-8925db86c2ff, creation at: 2023-12-26T12:08:08.458Z, expire at: 2024-01-02T12:08:08.458Z)
2023-12-26 12:08:08,606 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn dn, UUID: f610a764-996e-4be7-946a-c428c01d76b5
2023-12-26 12:08:08,606 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for RECON recon, UUID: 73a8b069-90ae-4fb9-b503-219efcc02218
2023-12-26 12:08:08,612 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO symmetric.LocalSecretKeyStore: Saved [SecretKey(id = 6377783b-39f5-43c7-a374-8925db86c2ff, creation at: 2023-12-26T12:08:08.458Z, expire at: 2024-01-02T12:08:08.458Z)] to file /data/metadata/scm/keys/secret_keys.json
2023-12-26 12:08:08,614 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-26 12:08:08,614 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
2023-12-26 12:08:08,614 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
2023-12-26 12:08:08,623 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for CertificateId, expected lastId is 0, actual lastId is 2.
2023-12-26 12:08:08,626 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-26 12:08:08,629 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-26 12:08:08,629 [IPC Server handler 1 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 2 to 3.
2023-12-26 12:08:08,673 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-12-26 12:08:08,673 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-12-26 12:08:08,728 [IPC Server handler 1 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-12-26 12:08:09,195 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:42013 / 172.20.0.9:42013
2023-12-26 12:08:09,198 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-12-26 12:08:09,240 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-26 12:08:09,264 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-26 12:08:09,264 [IPC Server handler 0 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 3 to 4.
2023-12-26 12:08:09,269 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-12-26 12:08:09,272 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-12-26 12:08:09,278 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-12-26 12:08:09,319 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-26 12:08:09,329 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn dn, UUID: e4062e5a-91f2-4fcd-b8f9-a573c2315923
2023-12-26 12:08:09,332 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-26 12:08:09,333 [IPC Server handler 1 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 4 to 5.
2023-12-26 12:08:09,336 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn dn, UUID: 812d23e8-21d8-4c12-a0ef-119c57e1ab94
2023-12-26 12:08:09,339 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-12-26 12:08:09,345 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-12-26 12:08:09,372 [IPC Server handler 1 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-12-26 12:08:09,456 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-26 12:08:09,483 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-26 12:08:09,484 [IPC Server handler 0 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 5 to 6.
2023-12-26 12:08:09,487 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-12-26 12:08:09,492 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-12-26 12:08:09,498 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-12-26 12:08:09,536 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-26 12:08:09,549 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-12-26 12:08:09,549 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-12-26 12:08:09,550 [665fb5eb-c818-4f06-9452-123bbd014da5-scm/sub-ca-refreshCACertificates] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-12-26 12:08:09,550 [665fb5eb-c818-4f06-9452-123bbd014da5-scm/sub-ca-refreshCACertificates] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-12-26 12:08:09,558 [665fb5eb-c818-4f06-9452-123bbd014da5-scm/sub-ca-refreshCACertificates] INFO client.SCMCertificateClient: CA certificates are not changed.
2023-12-26 12:08:09,561 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-12-26 12:08:09,561 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-12-26 12:08:09,562 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-12-26 12:08:09,562 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-12-26 12:08:09,676 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om, UUID: 3f26ba7c-1399-4107-8336-a8681063ca0c
2023-12-26 12:08:09,681 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-26 12:08:09,682 [IPC Server handler 0 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 6 to 7.
2023-12-26 12:08:09,695 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-12-26 12:08:09,701 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-12-26 12:08:09,727 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-12-26 12:08:09,818 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-26 12:08:09,845 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-12-26 12:08:09,846 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-12-26 12:08:09,847 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-12-26 12:08:09,847 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-12-26 12:08:09,998 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-12-26 12:08:09,999 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-12-26 12:08:10,218 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:41219 / 172.20.0.5:41219
2023-12-26 12:08:10,239 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-12-26 12:08:10,478 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:39405 / 172.20.0.4:39405
2023-12-26 12:08:10,490 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-12-26 12:08:10,557 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:44309 / 172.20.0.3:44309
2023-12-26 12:08:10,589 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-12-26 12:08:12,132 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-12-26 12:08:17,132 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-12-26 12:08:21,052 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:37629 / 172.20.0.9:37629
2023-12-26 12:08:21,074 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-12-26 12:08:21,659 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:44513 / 172.20.0.5:44513
2023-12-26 12:08:21,666 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-26 12:08:21,956 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:42695 / 172.20.0.3:42695
2023-12-26 12:08:21,986 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-26 12:08:22,133 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-12-26 12:08:22,224 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:45179 / 172.20.0.4:45179
2023-12-26 12:08:22,238 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-26 12:08:23,725 [IPC Server handler 4 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f610a764-996e-4be7-946a-c428c01d76b5
2023-12-26 12:08:23,732 [IPC Server handler 4 on default port 9861] INFO node.SCMNodeManager: Registered datanode: f610a764-996e-4be7-946a-c428c01d76b5{ip: 172.20.0.5, host: ozonesecure_datanode_1.ozonesecure_default, ports: [HTTP=9882, CLIENT_RPC=19864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 3, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-12-26 12:08:23,738 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
2023-12-26 12:08:23,770 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=eb41d0da-3910-4d30-84f6-c048d8d71840 to datanode:f610a764-996e-4be7-946a-c428c01d76b5
2023-12-26 12:08:23,782 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
2023-12-26 12:08:23,817 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-26 12:08:23,820 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: eb41d0da-3910-4d30-84f6-c048d8d71840, Nodes: f610a764-996e-4be7-946a-c428c01d76b5(ozonesecure_datanode_1.ozonesecure_default/172.20.0.5), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-12-26T12:08:23.769731Z[UTC]]
2023-12-26 12:08:23,956 [IPC Server handler 6 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/e4062e5a-91f2-4fcd-b8f9-a573c2315923
2023-12-26 12:08:23,959 [IPC Server handler 6 on default port 9861] INFO node.SCMNodeManager: Registered datanode: e4062e5a-91f2-4fcd-b8f9-a573c2315923{ip: 172.20.0.3, host: ozonesecure_datanode_3.ozonesecure_default, ports: [HTTP=9882, CLIENT_RPC=19864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 5, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-12-26 12:08:23,961 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
2023-12-26 12:08:23,961 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
2023-12-26 12:08:23,965 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=cafbb816-3623-4c52-87df-03240afae98b to datanode:e4062e5a-91f2-4fcd-b8f9-a573c2315923
2023-12-26 12:08:23,970 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-26 12:08:23,971 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: cafbb816-3623-4c52-87df-03240afae98b, Nodes: e4062e5a-91f2-4fcd-b8f9-a573c2315923(ozonesecure_datanode_3.ozonesecure_default/172.20.0.3), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-12-26T12:08:23.965726Z[UTC]]
2023-12-26 12:08:24,207 [IPC Server handler 8 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/812d23e8-21d8-4c12-a0ef-119c57e1ab94
2023-12-26 12:08:24,207 [IPC Server handler 8 on default port 9861] INFO node.SCMNodeManager: Registered datanode: 812d23e8-21d8-4c12-a0ef-119c57e1ab94{ip: 172.20.0.4, host: ozonesecure_datanode_2.ozonesecure_default, ports: [HTTP=9882, CLIENT_RPC=19864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 6, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-12-26 12:08:24,208 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
2023-12-26 12:08:24,209 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
2023-12-26 12:08:24,209 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
2023-12-26 12:08:24,209 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
2023-12-26 12:08:24,209 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
2023-12-26 12:08:24,209 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
2023-12-26 12:08:24,209 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=b2e94c9e-eed2-4c62-9c4c-4db60fe219a8 to datanode:812d23e8-21d8-4c12-a0ef-119c57e1ab94
2023-12-26 12:08:24,212 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-26 12:08:24,213 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: b2e94c9e-eed2-4c62-9c4c-4db60fe219a8, Nodes: 812d23e8-21d8-4c12-a0ef-119c57e1ab94(ozonesecure_datanode_2.ozonesecure_default/172.20.0.4), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-12-26T12:08:24.209456Z[UTC]]
2023-12-26 12:08:24,221 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=df27857a-10ac-4404-84bc-c26fbf9415cb to datanode:f610a764-996e-4be7-946a-c428c01d76b5
2023-12-26 12:08:24,222 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=df27857a-10ac-4404-84bc-c26fbf9415cb to datanode:e4062e5a-91f2-4fcd-b8f9-a573c2315923
2023-12-26 12:08:24,222 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=df27857a-10ac-4404-84bc-c26fbf9415cb to datanode:812d23e8-21d8-4c12-a0ef-119c57e1ab94
2023-12-26 12:08:24,232 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-26 12:08:24,239 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: df27857a-10ac-4404-84bc-c26fbf9415cb, Nodes: f610a764-996e-4be7-946a-c428c01d76b5(ozonesecure_datanode_1.ozonesecure_default/172.20.0.5)e4062e5a-91f2-4fcd-b8f9-a573c2315923(ozonesecure_datanode_3.ozonesecure_default/172.20.0.3)812d23e8-21d8-4c12-a0ef-119c57e1ab94(ozonesecure_datanode_2.ozonesecure_default/172.20.0.4), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-12-26T12:08:24.221745Z[UTC]]
2023-12-26 12:08:24,658 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:34017 / 172.20.0.8:34017
2023-12-26 12:08:24,677 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-12-26 12:08:25,410 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:36089 / 172.20.0.9:36089
2023-12-26 12:08:25,415 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolOm
2023-12-26 12:08:26,777 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-26 12:08:26,778 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=eb41d0da-3910-4d30-84f6-c048d8d71840
2023-12-26 12:08:26,779 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-12-26 12:08:26,867 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-12-26 12:08:27,122 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-26 12:08:27,124 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=cafbb816-3623-4c52-87df-03240afae98b
2023-12-26 12:08:27,124 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-12-26 12:08:27,133 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-12-26 12:08:27,293 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-12-26 12:08:27,457 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-26 12:08:27,457 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=b2e94c9e-eed2-4c62-9c4c-4db60fe219a8
2023-12-26 12:08:27,458 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-12-26 12:08:27,585 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-12-26 12:08:31,987 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-12-26 12:08:32,133 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-12-26 12:08:32,199 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-12-26 12:08:32,200 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
2023-12-26 12:08:32,201 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=df27857a-10ac-4404-84bc-c26fbf9415cb
2023-12-26 12:08:32,202 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
2023-12-26 12:08:32,202 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
2023-12-26 12:08:32,202 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
2023-12-26 12:08:32,202 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
2023-12-26 12:08:32,202 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
2023-12-26 12:08:32,202 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
2023-12-26 12:08:32,202 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
2023-12-26 12:08:32,202 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
2023-12-26 12:08:32,208 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
2023-12-26 12:08:32,208 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO SCMHATransactionMonitor: Service SCMHATransactionMonitor transitions to RUNNING.
2023-12-26 12:08:37,136 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-12-26 12:08:42,139 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-12-26 12:08:44,123 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:45449 / 172.20.0.9:45449
2023-12-26 12:08:44,126 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-12-26 12:08:44,133 [IPC Server handler 47 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
2023-12-26 12:08:44,142 [665fb5eb-c818-4f06-9452-123bbd014da5@group-19B33BFC4B4D-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
2023-12-26 12:08:44,143 [IPC Server handler 47 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
2023-12-26 12:08:45,596 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:33787 / 172.20.0.5:33787
2023-12-26 12:08:45,616 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-12-26 12:08:45,866 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:40357 / 172.20.0.5:40357
2023-12-26 12:08:45,912 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-26 12:08:45,956 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:38093 / 172.20.0.8:38093
2023-12-26 12:08:45,978 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-12-26 12:08:45,996 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:40161 / 172.20.0.4:40161
2023-12-26 12:08:46,017 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:44829 / 172.20.0.3:44829
2023-12-26 12:08:46,046 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-26 12:08:46,087 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-26 12:08:47,139 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-12-26 12:08:47,756 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:45553 / 172.20.0.9:45553
2023-12-26 12:08:47,759 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-12-26 12:08:50,511 [IPC Server handler 34 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:08:50,518 [IPC Server handler 99 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:08:51,416 [IPC Server handler 34 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:08:51,435 [IPC Server handler 99 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:08:52,133 [IPC Server handler 47 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:08:52,139 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-12-26 12:08:52,411 [IPC Server handler 34 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:08:53,061 [IPC Server handler 47 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:08:57,140 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-12-26 12:09:02,140 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-12-26 12:09:07,143 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 3 milliseconds for processing 1 containers.
2023-12-26 12:09:12,144 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 1 containers.
2023-12-26 12:09:15,864 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:46101 / 172.20.0.5:46101
2023-12-26 12:09:15,893 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-26 12:09:15,992 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:40571 / 172.20.0.4:40571
2023-12-26 12:09:16,012 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:45179 / 172.20.0.3:45179
2023-12-26 12:09:16,025 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-26 12:09:16,026 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-26 12:09:16,648 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:43837 / 172.20.0.9:43837
2023-12-26 12:09:16,653 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-12-26 12:09:16,653 [IPC Server handler 65 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:17,145 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 1 containers.
2023-12-26 12:09:17,353 [IPC Server handler 54 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:18,370 [IPC Server handler 34 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:18,434 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:39311 / 172.20.0.3:39311
2023-12-26 12:09:18,438 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-12-26 12:09:20,225 [IPC Server handler 50 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:20,976 [IPC Server handler 97 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:22,146 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 1 containers.
2023-12-26 12:09:22,157 [IPC Server handler 63 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:22,806 [IPC Server handler 97 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:23,803 [IPC Server handler 97 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:24,405 [IPC Server handler 99 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:24,452 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:39323 / 172.20.0.4:39323
2023-12-26 12:09:24,454 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-12-26 12:09:25,037 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:43857 / 172.20.0.8:43857
2023-12-26 12:09:25,040 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-12-26 12:09:25,111 [IPC Server handler 63 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:25,454 [IPC Server handler 60 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for delTxnId, change lastId from 0 to 1000.
2023-12-26 12:09:26,112 [IPC Server handler 63 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:26,647 [IPC Server handler 65 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:27,146 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 1 containers.
2023-12-26 12:09:29,073 [IPC Server handler 57 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:29,706 [IPC Server handler 25 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:30,313 [IPC Server handler 54 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:32,147 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 1 containers.
2023-12-26 12:09:32,636 [IPC Server handler 65 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:33,232 [IPC Server handler 54 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:33,781 [IPC Server handler 97 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:37,066 [IPC Server handler 47 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:37,148 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 1 containers.
2023-12-26 12:09:37,723 [IPC Server handler 72 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:40,365 [IPC Server handler 34 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:40,379 [IPC Server handler 99 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:40,383 [IPC Server handler 60 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:41,511 [IPC Server handler 91 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:41,511 [IPC Server handler 65 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:41,512 [IPC Server handler 62 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:41,994 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:33441 / 172.20.0.9:33441
2023-12-26 12:09:41,998 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-12-26 12:09:42,002 [IPC Server handler 47 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:42,071 [IPC Server handler 57 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:42,138 [IPC Server handler 50 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:42,148 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 1 containers.
2023-12-26 12:09:42,437 [IPC Server handler 65 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:42,494 [IPC Server handler 91 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:42,560 [IPC Server handler 77 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:42,617 [IPC Server handler 75 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:42,999 [IPC Server handler 47 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:43,055 [IPC Server handler 57 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:43,113 [IPC Server handler 63 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:43,157 [IPC Server handler 54 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:45,859 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:39741 / 172.20.0.5:39741
2023-12-26 12:09:45,869 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-26 12:09:45,981 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:33439 / 172.20.0.4:33439
2023-12-26 12:09:45,983 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-26 12:09:46,014 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:44257 / 172.20.0.3:44257
2023-12-26 12:09:46,026 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-26 12:09:46,332 [IPC Server handler 34 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:46,390 [IPC Server handler 65 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:46,438 [IPC Server handler 91 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:46,499 [IPC Server handler 62 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:47,149 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 1 containers.
2023-12-26 12:09:51,887 [IPC Server handler 47 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:51,937 [IPC Server handler 57 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:51,997 [IPC Server handler 63 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:52,066 [IPC Server handler 50 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:52,149 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 1 containers.
2023-12-26 12:09:52,568 [IPC Server handler 75 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:53,660 [IPC Server handler 43 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:54,813 [IPC Server handler 47 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:55,431 [IPC Server handler 91 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:56,729 [IPC Server handler 28 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:57,150 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 1 containers.
2023-12-26 12:09:57,665 [IPC Server handler 37 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:58,656 [IPC Server handler 43 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:09:59,342 [IPC Server handler 99 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:02,150 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 1 containers.
2023-12-26 12:10:02,414 [IPC Server handler 91 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:02,863 [IPC Server handler 57 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:03,311 [IPC Server handler 34 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:04,127 [IPC Server handler 54 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:04,664 [IPC Server handler 37 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:06,044 [IPC Server handler 50 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:07,151 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 1 containers.
2023-12-26 12:10:09,478 [IPC Server handler 62 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:09,482 [IPC Server handler 77 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:09,551 [IPC Server handler 43 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:10,461 [IPC Server handler 62 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:10,464 [IPC Server handler 77 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:10,509 [IPC Server handler 43 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:12,152 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 1 containers.
2023-12-26 12:10:15,862 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:33765 / 172.20.0.5:33765
2023-12-26 12:10:15,866 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-26 12:10:15,992 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:39283 / 172.20.0.4:39283
2023-12-26 12:10:16,009 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-26 12:10:16,030 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:40331 / 172.20.0.3:40331
2023-12-26 12:10:16,058 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-26 12:10:16,958 [IPC Server handler 63 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:16,962 [IPC Server handler 50 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:17,029 [IPC Server handler 34 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:17,152 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 1 containers.
2023-12-26 12:10:22,152 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 1 containers.
2023-12-26 12:10:27,153 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 1 containers.
2023-12-26 12:10:32,153 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 1 containers.
2023-12-26 12:10:37,153 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 1 containers.
2023-12-26 12:10:42,154 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 1 containers.
2023-12-26 12:10:43,166 [IPC Server handler 60 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:43,659 [IPC Server handler 38 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:44,138 [IPC Server handler 60 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:44,596 [IPC Server handler 38 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:45,057 [IPC Server handler 99 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:45,506 [IPC Server handler 43 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:45,864 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:41765 / 172.20.0.5:41765
2023-12-26 12:10:45,873 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-26 12:10:45,984 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:43195 / 172.20.0.4:43195
2023-12-26 12:10:45,993 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-26 12:10:46,011 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:42013 / 172.20.0.3:42013
2023-12-26 12:10:46,014 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-26 12:10:46,029 [IPC Server handler 34 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:46,494 [IPC Server handler 43 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:46,948 [IPC Server handler 50 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:47,154 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 1 containers.
2023-12-26 12:10:47,465 [IPC Server handler 77 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:47,922 [IPC Server handler 50 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:48,376 [IPC Server handler 91 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:48,842 [IPC Server handler 57 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:10:52,155 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 1 containers.
2023-12-26 12:10:57,155 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 1 containers.
2023-12-26 12:10:58,300 [IPC Server handler 91 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.6
2023-12-26 12:11:00,987 [IPC Server handler 34 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:11:02,155 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 1 containers.
2023-12-26 12:11:02,416 [IPC Server handler 62 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:11:03,392 [IPC Server handler 62 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:11:03,421 [IPC Server handler 77 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:11:04,066 [IPC Server handler 65 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:11:04,066 [IPC Server handler 60 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-12-26 12:11:07,156 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 1 containers.
2023-12-26 12:11:12,156 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 1 containers.
2023-12-26 12:11:15,868 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:38731 / 172.20.0.5:38731
2023-12-26 12:11:15,880 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-26 12:11:15,989 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:35969 / 172.20.0.4:35969
2023-12-26 12:11:15,994 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-26 12:11:16,011 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:33237 / 172.20.0.3:33237
2023-12-26 12:11:16,017 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-26 12:11:17,157 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 1 containers.
2023-12-26 12:11:22,157 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 1 containers.
