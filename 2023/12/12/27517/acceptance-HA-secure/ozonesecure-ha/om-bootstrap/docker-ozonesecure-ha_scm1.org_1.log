No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
2023-12-12 10:31:40,580 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting StorageContainerManager
STARTUP_MSG:   host = scm1.org/172.25.0.116
STARTUP_MSG:   args = [--init]
STARTUP_MSG:   version = 1.4.0-SNAPSHOT
STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jgraphx-2.0.0.1.jar:/opt/hadoop/share/ozone/lib/commons-net-3.10.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/okhttp-4.12.0.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.3.2.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.9.21.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.9.21.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.6.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.6.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/okio-3.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-3.0.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-3.0.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.5.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.9.21.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/jgraph-5.13.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/antlr4-runtime-4.5.3.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/picocli-4.7.5.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.16.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/jgrapht-ext-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.9.21.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jgrapht-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-dropwizard3-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-3.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-3.0.0.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.58.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-3.0.0.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
STARTUP_MSG:   build = https://github.com/apache/ozone/a93015a85b8ebe6a3e9423372f3337ed9b029704 ; compiled by 'runner' on 2023-12-12T10:10Z
STARTUP_MSG:   java = 11.0.19
STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=4MB, dfs.container.ratis.segment.size=64MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.command.worker.interval=2s, hdds.datanode.block.delete.max.lock.wait.timeout=100ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.max.lock.holding.time=1s, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=19864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.close.threads.max=3, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.file.size=100B, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.queue.limit=4096, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=100MB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.log.appender.wait-time.min=0us, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.expired.certificate.check.interval=P1D, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=1MB, ozone.chunk.read.mapped.buffer.threshold=32KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.grpc.write.timeout=30s, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.ec.grpc.zerocopy.enabled=true, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.max.buckets=100000, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.open.mpu.cleanup.service.interval=24h, ozone.om.open.mpu.cleanup.service.timeout=300s, ozone.om.open.mpu.expire.threshold=30d, ozone.om.open.mpu.parts.cleanup.limit.per.task=1000, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=50, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=16KB, ozone.om.ratis.segment.size=16KB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.auto.trigger.threshold=500, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.checkpoint.dir.creation.poll.timeout=20s, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.Hadoop3OmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=6000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3.administrators=testuser,s3g, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.list-keys.shallow.enabled=true, ozone.s3g.secret.http.auth.type=kerberos, ozone.s3g.secret.http.enabled=true, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.admin.monitor.logging.limit=1000, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.raft.server.log.appender.wait-time.min=0ms, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
************************************************************/
2023-12-12 10:31:40,628 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
2023-12-12 10:31:40,811 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-12-12 10:31:41,554 [main] INFO reflections.Reflections: Reflections took 585 ms to scan 3 urls, producing 134 keys and 291 values 
2023-12-12 10:31:41,972 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
2023-12-12 10:31:42,027 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
2023-12-12 10:31:42,155 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1.org:9894 and Ratis port: 9894
2023-12-12 10:31:42,155 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1.org
2023-12-12 10:31:42,165 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
2023-12-12 10:31:45,400 [main] INFO client.SCMCertificateClient: Certificate serial ID set to null
2023-12-12 10:31:45,400 [main] ERROR client.SCMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
2023-12-12 10:31:45,400 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
2023-12-12 10:31:45,405 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
2023-12-12 10:31:47,520 [main] INFO client.SCMCertificateClient: Init response: GETCERT
2023-12-12 10:31:48,376 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.116,host:scm1.org
2023-12-12 10:31:48,377 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
2023-12-12 10:31:48,504 [main] INFO utils.SelfSignedCertificate: Certificate 1 is issued by CN=scm@scm1.org,OU=e5fdb72c-5c5c-4ba8-951b-04fd10563c04,O=CID-5914c539-8ad3-44d3-8e5d-2cc8202ea8d6,SERIALNUMBER=1 to CN=scm@scm1.org,OU=e5fdb72c-5c5c-4ba8-951b-04fd10563c04,O=CID-5914c539-8ad3-44d3-8e5d-2cc8202ea8d6,SERIALNUMBER=1, valid from Tue Dec 12 10:31:48 UTC 2023 to Fri Jan 19 10:31:48 UTC 2029
2023-12-12 10:31:48,529 [main] INFO utils.CertificateCodec: Save certificate to /data/metadata/scm/ca/certs/certificate.crt
2023-12-12 10:31:48,529 [main] INFO utils.CertificateCodec: Certificate -----BEGIN CERTIFICATE-----
MIIDxTCCAq2gAwIBAgIBATANBgkqhkiG9w0BAQsFADCBhTEVMBMGA1UEAwwMc2Nt
QHNjbTEub3JnMS0wKwYDVQQLDCRlNWZkYjcyYy01YzVjLTRiYTgtOTUxYi0wNGZk
MTA1NjNjMDQxMTAvBgNVBAoMKENJRC01OTE0YzUzOS04YWQzLTQ0ZDMtOGU1ZC0y
Y2M4MjAyZWE4ZDYxCjAIBgNVBAUTATEwHhcNMjMxMjEyMTAzMTQ4WhcNMjkwMTE5
MTAzMTQ4WjCBhTEVMBMGA1UEAwwMc2NtQHNjbTEub3JnMS0wKwYDVQQLDCRlNWZk
YjcyYy01YzVjLTRiYTgtOTUxYi0wNGZkMTA1NjNjMDQxMTAvBgNVBAoMKENJRC01
OTE0YzUzOS04YWQzLTQ0ZDMtOGU1ZC0yY2M4MjAyZWE4ZDYxCjAIBgNVBAUTATEw
ggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCEJZ90Lk5yGMTQWpjpWP3N
Q5QgxA3GoqWKZAKdUinFmsc4Br6FVHOT80AliopleAW07L0fSZ/ZXL9fPRGNsf36
scFTmUgvVqu01uXpV3eMLp0Hf9G3F+gzuN4rYECOLuLvrIvnu0VyQMlUkFrELa5k
oMrpLVcJTGlKhpHkEm4aChsdr2z6hTHwfDVCBHQ69+y3MGW2apu+LYkWlqM7PaC3
jsJCMiacUXBe/hEgJkRC/RG3BVGRnJB8z80AJBzxc90cAgB2+q3RLOA0+xlgJ5XP
pxbLGyMUVYJIO6VlW7SalTpIkf10PhFYAGssi9btOmkl/76MRRjlOL6IwS4T7ssJ
AgMBAAGjPjA8MA8GA1UdEwEB/wQFMAMBAf8wDgYDVR0PAQH/BAQDAgEGMBkGA1Ud
EQQSMBCHBKwZAHSCCHNjbTEub3JnMA0GCSqGSIb3DQEBCwUAA4IBAQBQaNzUn/Dj
6Pco61E1BA2PCds2yOCHs9lokPI5pa5zenwKU4HffIc1Al6CSKpoBmjI3frJmLFr
oeTFrhEiY/GhC9JrWFXiHsOj0H0eRVJZBWoXqZnOr+GKWWvb4BbFHbvC3OSm0KMx
2mtEij3sh+Lb3vg6VHdo3dFa2QcXyA/vprDvVVdP3jzUfzoQ82Pz1+dibXPE9WzU
xdaJ2cEEDLBIWXsLgCCConz6WdBVne0At2LO1jXKEht52kCKw04Euciigns2/5JN
AoJAzpeXXHgRmYAJh0Dvnwt4z5TitGg9hIYFt590zkAPQOKVqOne2WhDAHt6u61m
dFMVkghZ2wzQ
-----END CERTIFICATE-----

2023-12-12 10:31:48,567 [main] INFO client.SCMCertificateClient: Creating csr for SCM->hostName:scm1.org,scmId:e5fdb72c-5c5c-4ba8-951b-04fd10563c04,clusterId:CID-5914c539-8ad3-44d3-8e5d-2cc8202ea8d6,subject:scm-sub@scm1.org
2023-12-12 10:31:48,576 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.116,host:scm1.org
2023-12-12 10:31:48,577 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
2023-12-12 10:31:48,663 [main] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.19, 2.5.29.15, 2.5.29.17
2023-12-12 10:31:48,664 [main] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-12-12 10:31:48,695 [main] INFO utils.CertificateCodec: Save certificate to /data/metadata/scm/sub-ca/certs/CA-1.crt
2023-12-12 10:31:48,695 [main] INFO utils.CertificateCodec: Certificate -----BEGIN CERTIFICATE-----
MIIDxTCCAq2gAwIBAgIBATANBgkqhkiG9w0BAQsFADCBhTEVMBMGA1UEAwwMc2Nt
QHNjbTEub3JnMS0wKwYDVQQLDCRlNWZkYjcyYy01YzVjLTRiYTgtOTUxYi0wNGZk
MTA1NjNjMDQxMTAvBgNVBAoMKENJRC01OTE0YzUzOS04YWQzLTQ0ZDMtOGU1ZC0y
Y2M4MjAyZWE4ZDYxCjAIBgNVBAUTATEwHhcNMjMxMjEyMTAzMTQ4WhcNMjkwMTE5
MTAzMTQ4WjCBhTEVMBMGA1UEAwwMc2NtQHNjbTEub3JnMS0wKwYDVQQLDCRlNWZk
YjcyYy01YzVjLTRiYTgtOTUxYi0wNGZkMTA1NjNjMDQxMTAvBgNVBAoMKENJRC01
OTE0YzUzOS04YWQzLTQ0ZDMtOGU1ZC0yY2M4MjAyZWE4ZDYxCjAIBgNVBAUTATEw
ggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCEJZ90Lk5yGMTQWpjpWP3N
Q5QgxA3GoqWKZAKdUinFmsc4Br6FVHOT80AliopleAW07L0fSZ/ZXL9fPRGNsf36
scFTmUgvVqu01uXpV3eMLp0Hf9G3F+gzuN4rYECOLuLvrIvnu0VyQMlUkFrELa5k
oMrpLVcJTGlKhpHkEm4aChsdr2z6hTHwfDVCBHQ69+y3MGW2apu+LYkWlqM7PaC3
jsJCMiacUXBe/hEgJkRC/RG3BVGRnJB8z80AJBzxc90cAgB2+q3RLOA0+xlgJ5XP
pxbLGyMUVYJIO6VlW7SalTpIkf10PhFYAGssi9btOmkl/76MRRjlOL6IwS4T7ssJ
AgMBAAGjPjA8MA8GA1UdEwEB/wQFMAMBAf8wDgYDVR0PAQH/BAQDAgEGMBkGA1Ud
EQQSMBCHBKwZAHSCCHNjbTEub3JnMA0GCSqGSIb3DQEBCwUAA4IBAQBQaNzUn/Dj
6Pco61E1BA2PCds2yOCHs9lokPI5pa5zenwKU4HffIc1Al6CSKpoBmjI3frJmLFr
oeTFrhEiY/GhC9JrWFXiHsOj0H0eRVJZBWoXqZnOr+GKWWvb4BbFHbvC3OSm0KMx
2mtEij3sh+Lb3vg6VHdo3dFa2QcXyA/vprDvVVdP3jzUfzoQ82Pz1+dibXPE9WzU
xdaJ2cEEDLBIWXsLgCCConz6WdBVne0At2LO1jXKEht52kCKw04Euciigns2/5JN
AoJAzpeXXHgRmYAJh0Dvnwt4z5TitGg9hIYFt590zkAPQOKVqOne2WhDAHt6u61m
dFMVkghZ2wzQ
-----END CERTIFICATE-----

2023-12-12 10:31:48,710 [main] INFO utils.CertificateCodec: Save certificate to /data/metadata/scm/sub-ca/certs/2.crt
2023-12-12 10:31:48,711 [main] INFO utils.CertificateCodec: Certificate -----BEGIN CERTIFICATE-----
MIIDyTCCArGgAwIBAgIBAjANBgkqhkiG9w0BAQsFADCBhTEVMBMGA1UEAwwMc2Nt
QHNjbTEub3JnMS0wKwYDVQQLDCRlNWZkYjcyYy01YzVjLTRiYTgtOTUxYi0wNGZk
MTA1NjNjMDQxMTAvBgNVBAoMKENJRC01OTE0YzUzOS04YWQzLTQ0ZDMtOGU1ZC0y
Y2M4MjAyZWE4ZDYxCjAIBgNVBAUTATEwHhcNMjMxMjEyMTAzMTQ4WhcNMjkwMTE5
MTAzMTQ4WjCBiTEZMBcGA1UEAwwQc2NtLXN1YkBzY20xLm9yZzEtMCsGA1UECwwk
ZTVmZGI3MmMtNWM1Yy00YmE4LTk1MWItMDRmZDEwNTYzYzA0MTEwLwYDVQQKDChD
SUQtNTkxNGM1MzktOGFkMy00NGQzLThlNWQtMmNjODIwMmVhOGQ2MQowCAYDVQQF
EwEyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAvvDKWhxocGFMkMSb
I/JyENK3u1G0ZglQZm7XY8axHykeJa08/ZZ7AKgrXPaHQpgITFKhb9QU4deS+pmp
9shyDAsrBfEAnPkiltQci2JcrOlJ/d9ygcOdXkJCkFRK6NK8bklRSRd5a/Q4tWcB
meamQiHsr8Ao4zajomEwpNI834/fVhztHzjI4GqiMznPS4y/qxoS23B92Xgby846
kgY9DCgud54N9jTujiIyMj6za0D/ZkJhRE/e1jY8MK5MN/lyanMlBDlVjSi65MSb
Rj72k8J1yVBYRy6w4iV4gbp/ducF3GY7ffdgy65iX7/WnlTwPsfkWtSrsIZY7NCW
akzGhQIDAQABoz4wPDAZBgNVHREEEjAQhwSsGQB0gghzY20xLm9yZzAPBgNVHRMB
Af8EBTADAQH/MA4GA1UdDwEB/wQEAwIBvjANBgkqhkiG9w0BAQsFAAOCAQEAdzDP
AScJjQe2nIpNH4bXz8KtEXSC3cATPKBvFEUlmK5J/OnyOYYu2vjRKoxUf/iuZGPd
KGF8bZRaiphJ5GBB3/pfdNPY2Y92b+EWtbFlglRrSINDQIXkH63cj59dHpJ0ymhH
r3v6Rt+1T5n7Ku/ggGPnEzWnLJ+7a+H9gX59xnXtXRiakP0WhZm6kpjMhBl0B8qB
lBeE/4nP6sfA1Ra+3yv2whYE0ffob9UHH+Vc9o9BjxJ0NyMQo+A/khmSaYPIj5Z6
CFwbtOcKMBe3yG2Xr3oJsXkSexvf+fTI3kB/RTxbmKVTyzKQ78aC0JVlIq5TUDhc
NvaELnL39bzu/EsoSg==
-----END CERTIFICATE-----

-----BEGIN CERTIFICATE-----
MIIDxTCCAq2gAwIBAgIBATANBgkqhkiG9w0BAQsFADCBhTEVMBMGA1UEAwwMc2Nt
QHNjbTEub3JnMS0wKwYDVQQLDCRlNWZkYjcyYy01YzVjLTRiYTgtOTUxYi0wNGZk
MTA1NjNjMDQxMTAvBgNVBAoMKENJRC01OTE0YzUzOS04YWQzLTQ0ZDMtOGU1ZC0y
Y2M4MjAyZWE4ZDYxCjAIBgNVBAUTATEwHhcNMjMxMjEyMTAzMTQ4WhcNMjkwMTE5
MTAzMTQ4WjCBhTEVMBMGA1UEAwwMc2NtQHNjbTEub3JnMS0wKwYDVQQLDCRlNWZk
YjcyYy01YzVjLTRiYTgtOTUxYi0wNGZkMTA1NjNjMDQxMTAvBgNVBAoMKENJRC01
OTE0YzUzOS04YWQzLTQ0ZDMtOGU1ZC0yY2M4MjAyZWE4ZDYxCjAIBgNVBAUTATEw
ggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCEJZ90Lk5yGMTQWpjpWP3N
Q5QgxA3GoqWKZAKdUinFmsc4Br6FVHOT80AliopleAW07L0fSZ/ZXL9fPRGNsf36
scFTmUgvVqu01uXpV3eMLp0Hf9G3F+gzuN4rYECOLuLvrIvnu0VyQMlUkFrELa5k
oMrpLVcJTGlKhpHkEm4aChsdr2z6hTHwfDVCBHQ69+y3MGW2apu+LYkWlqM7PaC3
jsJCMiacUXBe/hEgJkRC/RG3BVGRnJB8z80AJBzxc90cAgB2+q3RLOA0+xlgJ5XP
pxbLGyMUVYJIO6VlW7SalTpIkf10PhFYAGssi9btOmkl/76MRRjlOL6IwS4T7ssJ
AgMBAAGjPjA8MA8GA1UdEwEB/wQFMAMBAf8wDgYDVR0PAQH/BAQDAgEGMBkGA1Ud
EQQSMBCHBKwZAHSCCHNjbTEub3JnMA0GCSqGSIb3DQEBCwUAA4IBAQBQaNzUn/Dj
6Pco61E1BA2PCds2yOCHs9lokPI5pa5zenwKU4HffIc1Al6CSKpoBmjI3frJmLFr
oeTFrhEiY/GhC9JrWFXiHsOj0H0eRVJZBWoXqZnOr+GKWWvb4BbFHbvC3OSm0KMx
2mtEij3sh+Lb3vg6VHdo3dFa2QcXyA/vprDvVVdP3jzUfzoQ82Pz1+dibXPE9WzU
xdaJ2cEEDLBIWXsLgCCConz6WdBVne0At2LO1jXKEht52kCKw04Euciigns2/5JN
AoJAzpeXXHgRmYAJh0Dvnwt4z5TitGg9hIYFt590zkAPQOKVqOne2WhDAHt6u61m
dFMVkghZ2wzQ
-----END CERTIFICATE-----

2023-12-12 10:31:48,712 [main] INFO utils.CertificateCodec: Save certificate to /data/metadata/scm/sub-ca/certs/certificate.crt
2023-12-12 10:31:48,712 [main] INFO utils.CertificateCodec: Certificate -----BEGIN CERTIFICATE-----
MIIDyTCCArGgAwIBAgIBAjANBgkqhkiG9w0BAQsFADCBhTEVMBMGA1UEAwwMc2Nt
QHNjbTEub3JnMS0wKwYDVQQLDCRlNWZkYjcyYy01YzVjLTRiYTgtOTUxYi0wNGZk
MTA1NjNjMDQxMTAvBgNVBAoMKENJRC01OTE0YzUzOS04YWQzLTQ0ZDMtOGU1ZC0y
Y2M4MjAyZWE4ZDYxCjAIBgNVBAUTATEwHhcNMjMxMjEyMTAzMTQ4WhcNMjkwMTE5
MTAzMTQ4WjCBiTEZMBcGA1UEAwwQc2NtLXN1YkBzY20xLm9yZzEtMCsGA1UECwwk
ZTVmZGI3MmMtNWM1Yy00YmE4LTk1MWItMDRmZDEwNTYzYzA0MTEwLwYDVQQKDChD
SUQtNTkxNGM1MzktOGFkMy00NGQzLThlNWQtMmNjODIwMmVhOGQ2MQowCAYDVQQF
EwEyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAvvDKWhxocGFMkMSb
I/JyENK3u1G0ZglQZm7XY8axHykeJa08/ZZ7AKgrXPaHQpgITFKhb9QU4deS+pmp
9shyDAsrBfEAnPkiltQci2JcrOlJ/d9ygcOdXkJCkFRK6NK8bklRSRd5a/Q4tWcB
meamQiHsr8Ao4zajomEwpNI834/fVhztHzjI4GqiMznPS4y/qxoS23B92Xgby846
kgY9DCgud54N9jTujiIyMj6za0D/ZkJhRE/e1jY8MK5MN/lyanMlBDlVjSi65MSb
Rj72k8J1yVBYRy6w4iV4gbp/ducF3GY7ffdgy65iX7/WnlTwPsfkWtSrsIZY7NCW
akzGhQIDAQABoz4wPDAZBgNVHREEEjAQhwSsGQB0gghzY20xLm9yZzAPBgNVHRMB
Af8EBTADAQH/MA4GA1UdDwEB/wQEAwIBvjANBgkqhkiG9w0BAQsFAAOCAQEAdzDP
AScJjQe2nIpNH4bXz8KtEXSC3cATPKBvFEUlmK5J/OnyOYYu2vjRKoxUf/iuZGPd
KGF8bZRaiphJ5GBB3/pfdNPY2Y92b+EWtbFlglRrSINDQIXkH63cj59dHpJ0ymhH
r3v6Rt+1T5n7Ku/ggGPnEzWnLJ+7a+H9gX59xnXtXRiakP0WhZm6kpjMhBl0B8qB
lBeE/4nP6sfA1Ra+3yv2whYE0ffob9UHH+Vc9o9BjxJ0NyMQo+A/khmSaYPIj5Z6
CFwbtOcKMBe3yG2Xr3oJsXkSexvf+fTI3kB/RTxbmKVTyzKQ78aC0JVlIq5TUDhc
NvaELnL39bzu/EsoSg==
-----END CERTIFICATE-----

-----BEGIN CERTIFICATE-----
MIIDxTCCAq2gAwIBAgIBATANBgkqhkiG9w0BAQsFADCBhTEVMBMGA1UEAwwMc2Nt
QHNjbTEub3JnMS0wKwYDVQQLDCRlNWZkYjcyYy01YzVjLTRiYTgtOTUxYi0wNGZk
MTA1NjNjMDQxMTAvBgNVBAoMKENJRC01OTE0YzUzOS04YWQzLTQ0ZDMtOGU1ZC0y
Y2M4MjAyZWE4ZDYxCjAIBgNVBAUTATEwHhcNMjMxMjEyMTAzMTQ4WhcNMjkwMTE5
MTAzMTQ4WjCBhTEVMBMGA1UEAwwMc2NtQHNjbTEub3JnMS0wKwYDVQQLDCRlNWZk
YjcyYy01YzVjLTRiYTgtOTUxYi0wNGZkMTA1NjNjMDQxMTAvBgNVBAoMKENJRC01
OTE0YzUzOS04YWQzLTQ0ZDMtOGU1ZC0yY2M4MjAyZWE4ZDYxCjAIBgNVBAUTATEw
ggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCEJZ90Lk5yGMTQWpjpWP3N
Q5QgxA3GoqWKZAKdUinFmsc4Br6FVHOT80AliopleAW07L0fSZ/ZXL9fPRGNsf36
scFTmUgvVqu01uXpV3eMLp0Hf9G3F+gzuN4rYECOLuLvrIvnu0VyQMlUkFrELa5k
oMrpLVcJTGlKhpHkEm4aChsdr2z6hTHwfDVCBHQ69+y3MGW2apu+LYkWlqM7PaC3
jsJCMiacUXBe/hEgJkRC/RG3BVGRnJB8z80AJBzxc90cAgB2+q3RLOA0+xlgJ5XP
pxbLGyMUVYJIO6VlW7SalTpIkf10PhFYAGssi9btOmkl/76MRRjlOL6IwS4T7ssJ
AgMBAAGjPjA8MA8GA1UdEwEB/wQFMAMBAf8wDgYDVR0PAQH/BAQDAgEGMBkGA1Ud
EQQSMBCHBKwZAHSCCHNjbTEub3JnMA0GCSqGSIb3DQEBCwUAA4IBAQBQaNzUn/Dj
6Pco61E1BA2PCds2yOCHs9lokPI5pa5zenwKU4HffIc1Al6CSKpoBmjI3frJmLFr
oeTFrhEiY/GhC9JrWFXiHsOj0H0eRVJZBWoXqZnOr+GKWWvb4BbFHbvC3OSm0KMx
2mtEij3sh+Lb3vg6VHdo3dFa2QcXyA/vprDvVVdP3jzUfzoQ82Pz1+dibXPE9WzU
xdaJ2cEEDLBIWXsLgCCConz6WdBVne0At2LO1jXKEht52kCKw04Euciigns2/5JN
AoJAzpeXXHgRmYAJh0Dvnwt4z5TitGg9hIYFt590zkAPQOKVqOne2WhDAHt6u61m
dFMVkghZ2wzQ
-----END CERTIFICATE-----

2023-12-12 10:31:48,715 [main] INFO client.SCMCertificateClient: Successfully stored SCM signed certificate.
2023-12-12 10:31:48,897 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
2023-12-12 10:31:49,035 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2023-12-12 10:31:49,036 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
2023-12-12 10:31:49,039 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2023-12-12 10:31:49,040 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
2023-12-12 10:31:49,040 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
2023-12-12 10:31:49,040 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
2023-12-12 10:31:49,041 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
2023-12-12 10:31:49,043 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-12-12 10:31:49,044 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
2023-12-12 10:31:49,044 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
2023-12-12 10:31:49,063 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
2023-12-12 10:31:49,067 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
2023-12-12 10:31:49,074 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
2023-12-12 10:31:49,443 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
2023-12-12 10:31:49,445 [main] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
2023-12-12 10:31:49,454 [main] INFO server.RaftServerConfigKeys: raft.server.close.threshold = 60s (default)
2023-12-12 10:31:49,470 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-12-12 10:31:49,474 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
2023-12-12 10:31:49,484 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
2023-12-12 10:31:49,494 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
2023-12-12 10:31:49,552 [main] INFO server.RaftServer: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: addNew group-2CC8202EA8D6:[e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894] returns group-2CC8202EA8D6:java.util.concurrent.CompletableFuture@7417ef4f[Not completed]
2023-12-12 10:31:49,574 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: new RaftServerImpl for group-2CC8202EA8D6:[e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894] with SCMStateMachine:uninitialized
2023-12-12 10:31:49,578 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
2023-12-12 10:31:49,579 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
2023-12-12 10:31:49,579 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
2023-12-12 10:31:49,579 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
2023-12-12 10:31:49,580 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-12-12 10:31:49,580 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.member.majority-add = false (default)
2023-12-12 10:31:49,581 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
2023-12-12 10:31:49,609 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: ConfigurationManager, init=-1: peers:[e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-12-12 10:31:49,623 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
2023-12-12 10:31:49,633 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
2023-12-12 10:31:49,687 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
2023-12-12 10:31:49,688 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100ms (default)
2023-12-12 10:31:49,697 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
2023-12-12 10:31:49,697 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.read-after-write-consistent.write-index-cache.expiry-time = 60s (default)
2023-12-12 10:31:49,795 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.dropwizard3.Dm3MetricRegistriesImpl
2023-12-12 10:31:49,986 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
2023-12-12 10:31:49,990 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-12-12 10:31:49,995 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
2023-12-12 10:31:49,995 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
2023-12-12 10:31:49,996 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
2023-12-12 10:31:49,996 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
2023-12-12 10:31:49,998 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
2023-12-12 10:31:49,998 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
2023-12-12 10:31:49,998 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
2023-12-12 10:31:50,022 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/5914c539-8ad3-44d3-8e5d-2cc8202ea8d6 does not exist. Creating ...
2023-12-12 10:31:50,035 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/5914c539-8ad3-44d3-8e5d-2cc8202ea8d6/in_use.lock acquired by nodename 13@scm1.org
2023-12-12 10:31:50,047 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/5914c539-8ad3-44d3-8e5d-2cc8202ea8d6 has been successfully formatted.
2023-12-12 10:31:50,050 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
2023-12-12 10:31:50,066 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
2023-12-12 10:31:50,066 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-12-12 10:31:50,071 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-12-12 10:31:50,071 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
2023-12-12 10:31:50,075 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
2023-12-12 10:31:50,089 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
2023-12-12 10:31:50,089 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-12-12 10:31:50,089 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-12-12 10:31:50,102 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO util.AwaitToRun: Thread[e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-cacheEviction-AwaitToRun,5,main] started
2023-12-12 10:31:50,114 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/5914c539-8ad3-44d3-8e5d-2cc8202ea8d6
2023-12-12 10:31:50,115 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
2023-12-12 10:31:50,115 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
2023-12-12 10:31:50,117 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
2023-12-12 10:31:50,118 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
2023-12-12 10:31:50,118 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
2023-12-12 10:31:50,119 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
2023-12-12 10:31:50,119 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-12-12 10:31:50,119 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-12-12 10:31:50,122 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 33554440 (custom)
2023-12-12 10:31:50,137 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-12-12 10:31:50,140 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
2023-12-12 10:31:50,145 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
2023-12-12 10:31:50,146 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
2023-12-12 10:31:50,164 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-12-12 10:31:50,165 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-12-12 10:31:50,167 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: start as a follower, conf=-1: peers:[e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894]|listeners:[], old=null
2023-12-12 10:31:50,174 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-12-12 10:31:50,175 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO impl.RoleInfo: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: start e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-FollowerState
2023-12-12 10:31:50,188 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
2023-12-12 10:31:50,188 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-12-12 10:31:50,208 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-2CC8202EA8D6,id=e5fdb72c-5c5c-4ba8-951b-04fd10563c04
2023-12-12 10:31:50,215 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.trigger-when-stop.enabled = true (default)
2023-12-12 10:31:50,215 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-12-12 10:31:50,215 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
2023-12-12 10:31:50,216 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
2023-12-12 10:31:50,216 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
2023-12-12 10:31:50,227 [main] INFO server.RaftServer: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: start RPC server
2023-12-12 10:31:50,305 [main] INFO server.GrpcService: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: GrpcService started, listening on 9894
2023-12-12 10:31:50,315 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-e5fdb72c-5c5c-4ba8-951b-04fd10563c04: Started
2023-12-12 10:31:55,375 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-FollowerState] INFO impl.FollowerState: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5200173299ns, electionTimeout:5186ms
2023-12-12 10:31:55,375 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-FollowerState] INFO impl.RoleInfo: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: shutdown e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-FollowerState
2023-12-12 10:31:55,375 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-FollowerState] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-12-12 10:31:55,377 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
2023-12-12 10:31:55,378 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-FollowerState] INFO impl.RoleInfo: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: start e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1
2023-12-12 10:31:55,381 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO impl.LeaderElection: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894]|listeners:[], old=null
2023-12-12 10:31:55,382 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO impl.LeaderElection: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
2023-12-12 10:31:55,385 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO impl.LeaderElection: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894]|listeners:[], old=null
2023-12-12 10:31:55,385 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO impl.LeaderElection: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1 ELECTION round 0: result PASSED (term=1)
2023-12-12 10:31:55,385 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO impl.RoleInfo: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: shutdown e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1
2023-12-12 10:31:55,385 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-12-12 10:31:55,390 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
2023-12-12 10:31:55,394 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
2023-12-12 10:31:55,395 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
2023-12-12 10:31:55,398 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
2023-12-12 10:31:55,398 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
2023-12-12 10:31:55,398 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
2023-12-12 10:31:55,403 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.read.leader.lease.enabled = false (default)
2023-12-12 10:31:55,405 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.read.leader.lease.timeout.ratio = 0.9 (default)
2023-12-12 10:31:55,405 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
2023-12-12 10:31:55,405 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
2023-12-12 10:31:55,405 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-12-12 10:31:55,407 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO impl.RoleInfo: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: start e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderStateImpl
2023-12-12 10:31:55,407 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: set firstElectionSinceStartup to false for becomeLeader
2023-12-12 10:31:55,407 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: change Leader from null to e5fdb72c-5c5c-4ba8-951b-04fd10563c04 at term 1 for becomeLeader, leader elected after 5783ms
2023-12-12 10:31:55,426 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-SegmentedRaftLogWorker: Starting segment from index:0
2023-12-12 10:31:55,443 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: set configuration 0: peers:[e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894]|listeners:[], old=null
2023-12-12 10:31:55,491 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/5914c539-8ad3-44d3-8e5d-2cc8202ea8d6/current/log_inprogress_0
2023-12-12 10:31:55,498 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO server.RaftServer$Division: leader is ready since appliedIndex == 0 >= startIndex == 0
2023-12-12 10:31:56,316 [main] INFO server.RaftServer: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: close
2023-12-12 10:31:56,316 [main] INFO server.GrpcService: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: shutdown server GrpcServerProtocolService now
2023-12-12 10:31:56,316 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: shutdown
2023-12-12 10:31:56,317 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-2CC8202EA8D6,id=e5fdb72c-5c5c-4ba8-951b-04fd10563c04
2023-12-12 10:31:56,317 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO impl.RoleInfo: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: shutdown e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderStateImpl
2023-12-12 10:31:56,321 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO impl.PendingRequests: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-PendingRequests: sendNotLeaderResponses
2023-12-12 10:31:56,325 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO impl.StateMachineUpdater: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater: set stopIndex = 0
2023-12-12 10:31:56,326 [main] INFO server.GrpcService: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: shutdown server GrpcServerProtocolService successfully
2023-12-12 10:31:56,326 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO impl.StateMachineUpdater: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater: Took a snapshot at index 0
2023-12-12 10:31:56,326 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO impl.StateMachineUpdater: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2023-12-12 10:31:56,330 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: applyIndex: 0
2023-12-12 10:31:56,330 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-cacheEviction-AwaitToRun] INFO util.AwaitToRun: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-cacheEviction-AwaitToRun-AwaitForSignal is interrupted
2023-12-12 10:31:56,497 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-SegmentedRaftLogWorker close()
2023-12-12 10:31:56,498 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-e5fdb72c-5c5c-4ba8-951b-04fd10563c04: Stopped
2023-12-12 10:31:56,498 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-12-12 10:31:56,500 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-5914c539-8ad3-44d3-8e5d-2cc8202ea8d6; layoutVersion=7; scmId=e5fdb72c-5c5c-4ba8-951b-04fd10563c04
2023-12-12 10:31:56,501 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down StorageContainerManager at scm1.org/172.25.0.116
************************************************************/
No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
2023-12-12 10:31:58,039 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting StorageContainerManager
STARTUP_MSG:   host = scm1.org/172.25.0.116
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 1.4.0-SNAPSHOT
STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jgraphx-2.0.0.1.jar:/opt/hadoop/share/ozone/lib/commons-net-3.10.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/okhttp-4.12.0.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.3.2.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.9.21.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.9.21.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.6.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.6.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/okio-3.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-3.0.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-3.0.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.5.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.9.21.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/jgraph-5.13.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/antlr4-runtime-4.5.3.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/picocli-4.7.5.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.16.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/jgrapht-ext-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.9.21.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jgrapht-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-dropwizard3-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-3.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-3.0.0.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.58.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-3.0.0.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
STARTUP_MSG:   build = https://github.com/apache/ozone/a93015a85b8ebe6a3e9423372f3337ed9b029704 ; compiled by 'runner' on 2023-12-12T10:10Z
STARTUP_MSG:   java = 11.0.19
STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=4MB, dfs.container.ratis.segment.size=64MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.command.worker.interval=2s, hdds.datanode.block.delete.max.lock.wait.timeout=100ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.max.lock.holding.time=1s, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=19864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.close.threads.max=3, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.file.size=100B, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/db@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.queue.limit=4096, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=100MB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.log.appender.wait-time.min=0us, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=1h, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=1m, hdds.secret.key.rotate.duration=5m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.expired.certificate.check.interval=P1D, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=1MB, ozone.chunk.read.mapped.buffer.threshold=32KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.grpc.write.timeout=30s, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.ec.grpc.zerocopy.enabled=true, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-address.omservice.om1=om1, ozone.om.http-address.omservice.om2=om2, ozone.om.http-address.omservice.om3=om3, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.internal.service.id=omservice, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.max.buckets=100000, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.open.mpu.cleanup.service.interval=24h, ozone.om.open.mpu.cleanup.service.timeout=300s, ozone.om.open.mpu.expire.threshold=30d, ozone.om.open.mpu.parts.cleanup.limit.per.task=1000, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=50, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=16KB, ozone.om.ratis.segment.size=16KB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.auto.trigger.threshold=500, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.checkpoint.dir.creation.poll.timeout=20s, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.Hadoop3OmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=false, ozone.path.deleting.limit.per.task=6000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3.administrators=testuser,s3g, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.list-keys.shallow.enabled=true, ozone.s3g.secret.http.auth.type=kerberos, ozone.s3g.secret.http.enabled=true, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1.org, ozone.scm.address.scmservice.scm2=scm2.org, ozone.scm.address.scmservice.scm3=scm3.org, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.admin.monitor.logging.limit=1000, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.raft.server.log.appender.wait-time.min=0ms, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
************************************************************/
2023-12-12 10:31:58,052 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
2023-12-12 10:31:58,129 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-12-12 10:31:58,269 [main] INFO reflections.Reflections: Reflections took 104 ms to scan 3 urls, producing 134 keys and 291 values 
2023-12-12 10:31:58,361 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
2023-12-12 10:31:58,372 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
2023-12-12 10:31:58,397 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1.org:9894 and Ratis port: 9894
2023-12-12 10:31:58,397 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1.org
2023-12-12 10:31:58,620 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
2023-12-12 10:31:58,620 [main] INFO server.StorageContainerManager: SCM login successful.
2023-12-12 10:31:59,203 [main] INFO client.SCMCertificateClient: Certificate serial ID set to 2
2023-12-12 10:31:59,349 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
         SerialNumber: 2
             IssuerDN: CN=scm@scm1.org,OU=e5fdb72c-5c5c-4ba8-951b-04fd10563c04,O=CID-5914c539-8ad3-44d3-8e5d-2cc8202ea8d6,SERIALNUMBER=1
           Start Date: Tue Dec 12 10:31:48 UTC 2023
           Final Date: Fri Jan 19 10:31:48 UTC 2029
            SubjectDN: CN=scm-sub@scm1.org,OU=e5fdb72c-5c5c-4ba8-951b-04fd10563c04,O=CID-5914c539-8ad3-44d3-8e5d-2cc8202ea8d6,SERIALNUMBER=2
           Public Key: RSA Public Key [2d:f6:08:96:e1:4d:b8:28:e7:16:4d:9c:c9:1c:76:5c:e0:c5:f5:36],[56:66:d1:a4]
        modulus: bef0ca5a1c6870614c90c49b23f27210d2b7bb51b4660950666ed763c6b11f291e25ad3cfd967b00a82b5cf6874298084c52a16fd414e1d792fa99a9f6c8720c0b2b05f1009cf92296d41c8b625cace949fddf7281c39d5e424290544ae8d2bc6e49514917796bf438b5670199e6a64221ecafc028e336a3a26130a4d23cdf8fdf561ced1f38c8e06aa23339cf4b8cbfab1a12db707dd9781bcbce3a92063d0c282e779e0df634ee8e2232323eb36b40ff664261444fded6363c30ae4c37f9726a73250439558d28bae4c49b463ef693c275c95058472eb0e2257881ba7f76e705dc663b7df760cbae625fbfd69e54f03ec7e45ad4abb08658ecd0966a4cc685
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 7730cf0127098d07b69c8a4d1f86d7cfc2ad1174
                       82ddc0133ca06f14452598ae49fce9f239862eda
                       f8d12a8c547ff8ae6463dd28617c6d945a8a9849
                       e46041dffa5f74d3d8d98f766fe116b5b1658254
                       6b4883434085e41faddc8f9f5d1e9274ca6847af
                       7bfa46dfb54f99fb2aefe08063e71335a72c9fbb
                       6be1fd817e7dc675ed5d189a90fd168599ba9298
                       cc84197407ca81941784ff89cfeac7c0d516bedf
                       2bf6c21604d1f7e86fd5071fe55cf68f418f1274
                       372310a3e03f9219926983c88f967a085c1bb4e7
                       0a3017b7c86d97af7a09b179127b1bdff9f4c8de
                       407f453c5b98a553cb3290efc682d0956522ae53
                       50385c36f6842e72f7f5bceefc4b284a
       Extensions: 
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [7] IMPLICIT 
        DER Octet String[4] 
    Tagged [2] IMPLICIT 
        DER Octet String[8] 

                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0xbe
 from file: /data/metadata/scm/sub-ca/certs/certificate.crt.
2023-12-12 10:31:59,359 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
         SerialNumber: 1
             IssuerDN: CN=scm@scm1.org,OU=e5fdb72c-5c5c-4ba8-951b-04fd10563c04,O=CID-5914c539-8ad3-44d3-8e5d-2cc8202ea8d6,SERIALNUMBER=1
           Start Date: Tue Dec 12 10:31:48 UTC 2023
           Final Date: Fri Jan 19 10:31:48 UTC 2029
            SubjectDN: CN=scm@scm1.org,OU=e5fdb72c-5c5c-4ba8-951b-04fd10563c04,O=CID-5914c539-8ad3-44d3-8e5d-2cc8202ea8d6,SERIALNUMBER=1
           Public Key: RSA Public Key [2b:58:e9:c5:fa:a5:c7:c7:fd:be:32:86:63:76:80:c4:ae:17:1b:e9],[56:66:d1:a4]
        modulus: 84259f742e4e7218c4d05a98e958fdcd439420c40dc6a2a58a64029d5229c59ac73806be85547393f340258a8a657805b4ecbd1f499fd95cbf5f3d118db1fdfab1c15399482f56abb4d6e5e957778c2e9d077fd1b717e833b8de2b60408e2ee2efac8be7bb457240c954905ac42dae64a0cae92d57094c694a8691e4126e1a0a1b1daf6cfa8531f07c354204743af7ecb73065b66a9bbe2d891696a33b3da0b78ec24232269c51705efe1120264442fd11b70551919c907ccfcd00241cf173dd1c020076faadd12ce034fb19602795cfa716cb1b23145582483ba5655bb49a953a4891fd743e1158006b2c8bd6ed3a6925ffbe8c4518e538be88c12e13eecb09
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 5068dcd49ff0e3e8f728eb5135040d8f09db36c8
                       e087b3d96890f239a5ae737a7c0a5381df7c8735
                       025e8248aa680668c8ddfac998b16ba1e4c5ae11
                       2263f1a10bd26b5855e21ec3a3d07d1e45525905
                       6a17a999ceafe18a596bdbe016c51dbbc2dce4a6
                       d0a331da6b448a3dec87e2dbdef83a547768ddd1
                       5ad90717c80fefa6b0ef55574fde3cd47f3a10f3
                       63f3d7e7626d73c4f56cd4c5d689d9c1040cb048
                       597b0b802082a27cfa59d0559ded00b762ced635
                       ca121b79da408ac34e04b9c8a2827b36ff924d02
                       8240ce97975c78119980098740ef9f0b78cf94e2
                       b4683d848605b79f74ce400f40e295a8e9ded968
                       43007b7abbad66745315920859db0cd0
       Extensions: 
                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0x6
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [7] IMPLICIT 
        DER Octet String[4] 
    Tagged [2] IMPLICIT 
        DER Octet String[8] 

 from file: /data/metadata/scm/sub-ca/certs/CA-1.crt.
2023-12-12 10:31:59,371 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
         SerialNumber: 2
             IssuerDN: CN=scm@scm1.org,OU=e5fdb72c-5c5c-4ba8-951b-04fd10563c04,O=CID-5914c539-8ad3-44d3-8e5d-2cc8202ea8d6,SERIALNUMBER=1
           Start Date: Tue Dec 12 10:31:48 UTC 2023
           Final Date: Fri Jan 19 10:31:48 UTC 2029
            SubjectDN: CN=scm-sub@scm1.org,OU=e5fdb72c-5c5c-4ba8-951b-04fd10563c04,O=CID-5914c539-8ad3-44d3-8e5d-2cc8202ea8d6,SERIALNUMBER=2
           Public Key: RSA Public Key [2d:f6:08:96:e1:4d:b8:28:e7:16:4d:9c:c9:1c:76:5c:e0:c5:f5:36],[56:66:d1:a4]
        modulus: bef0ca5a1c6870614c90c49b23f27210d2b7bb51b4660950666ed763c6b11f291e25ad3cfd967b00a82b5cf6874298084c52a16fd414e1d792fa99a9f6c8720c0b2b05f1009cf92296d41c8b625cace949fddf7281c39d5e424290544ae8d2bc6e49514917796bf438b5670199e6a64221ecafc028e336a3a26130a4d23cdf8fdf561ced1f38c8e06aa23339cf4b8cbfab1a12db707dd9781bcbce3a92063d0c282e779e0df634ee8e2232323eb36b40ff664261444fded6363c30ae4c37f9726a73250439558d28bae4c49b463ef693c275c95058472eb0e2257881ba7f76e705dc663b7df760cbae625fbfd69e54f03ec7e45ad4abb08658ecd0966a4cc685
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 7730cf0127098d07b69c8a4d1f86d7cfc2ad1174
                       82ddc0133ca06f14452598ae49fce9f239862eda
                       f8d12a8c547ff8ae6463dd28617c6d945a8a9849
                       e46041dffa5f74d3d8d98f766fe116b5b1658254
                       6b4883434085e41faddc8f9f5d1e9274ca6847af
                       7bfa46dfb54f99fb2aefe08063e71335a72c9fbb
                       6be1fd817e7dc675ed5d189a90fd168599ba9298
                       cc84197407ca81941784ff89cfeac7c0d516bedf
                       2bf6c21604d1f7e86fd5071fe55cf68f418f1274
                       372310a3e03f9219926983c88f967a085c1bb4e7
                       0a3017b7c86d97af7a09b179127b1bdff9f4c8de
                       407f453c5b98a553cb3290efc682d0956522ae53
                       50385c36f6842e72f7f5bceefc4b284a
       Extensions: 
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [7] IMPLICIT 
        DER Octet String[4] 
    Tagged [2] IMPLICIT 
        DER Octet String[8] 

                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0xbe
 from file: /data/metadata/scm/sub-ca/certs/2.crt.
2023-12-12 10:31:59,372 [main] INFO client.SCMCertificateClient: CertificateRenewerService and root ca rotation polling is disabled for scm/sub-ca
2023-12-12 10:31:59,464 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-12-12 10:31:59,625 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-12-12 10:31:59,826 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
2023-12-12 10:31:59,827 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
2023-12-12 10:31:59,871 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.dropwizard3.Dm3MetricRegistriesImpl
2023-12-12 10:32:00,036 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:e5fdb72c-5c5c-4ba8-951b-04fd10563c04
2023-12-12 10:32:00,054 [main] INFO ssl.ReloadingX509KeyManager: Key manager is loaded with certificate chain
2023-12-12 10:32:00,056 [main] INFO ssl.ReloadingX509KeyManager:   [0]         Version: 3
         SerialNumber: 2
             IssuerDN: CN=scm@scm1.org,OU=e5fdb72c-5c5c-4ba8-951b-04fd10563c04,O=CID-5914c539-8ad3-44d3-8e5d-2cc8202ea8d6,SERIALNUMBER=1
           Start Date: Tue Dec 12 10:31:48 UTC 2023
           Final Date: Fri Jan 19 10:31:48 UTC 2029
            SubjectDN: CN=scm-sub@scm1.org,OU=e5fdb72c-5c5c-4ba8-951b-04fd10563c04,O=CID-5914c539-8ad3-44d3-8e5d-2cc8202ea8d6,SERIALNUMBER=2
           Public Key: RSA Public Key [2d:f6:08:96:e1:4d:b8:28:e7:16:4d:9c:c9:1c:76:5c:e0:c5:f5:36],[56:66:d1:a4]
        modulus: bef0ca5a1c6870614c90c49b23f27210d2b7bb51b4660950666ed763c6b11f291e25ad3cfd967b00a82b5cf6874298084c52a16fd414e1d792fa99a9f6c8720c0b2b05f1009cf92296d41c8b625cace949fddf7281c39d5e424290544ae8d2bc6e49514917796bf438b5670199e6a64221ecafc028e336a3a26130a4d23cdf8fdf561ced1f38c8e06aa23339cf4b8cbfab1a12db707dd9781bcbce3a92063d0c282e779e0df634ee8e2232323eb36b40ff664261444fded6363c30ae4c37f9726a73250439558d28bae4c49b463ef693c275c95058472eb0e2257881ba7f76e705dc663b7df760cbae625fbfd69e54f03ec7e45ad4abb08658ecd0966a4cc685
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 7730cf0127098d07b69c8a4d1f86d7cfc2ad1174
                       82ddc0133ca06f14452598ae49fce9f239862eda
                       f8d12a8c547ff8ae6463dd28617c6d945a8a9849
                       e46041dffa5f74d3d8d98f766fe116b5b1658254
                       6b4883434085e41faddc8f9f5d1e9274ca6847af
                       7bfa46dfb54f99fb2aefe08063e71335a72c9fbb
                       6be1fd817e7dc675ed5d189a90fd168599ba9298
                       cc84197407ca81941784ff89cfeac7c0d516bedf
                       2bf6c21604d1f7e86fd5071fe55cf68f418f1274
                       372310a3e03f9219926983c88f967a085c1bb4e7
                       0a3017b7c86d97af7a09b179127b1bdff9f4c8de
                       407f453c5b98a553cb3290efc682d0956522ae53
                       50385c36f6842e72f7f5bceefc4b284a
       Extensions: 
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [7] IMPLICIT 
        DER Octet String[4] 
    Tagged [2] IMPLICIT 
        DER Octet String[8] 

                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0xbe

2023-12-12 10:32:00,058 [main] INFO ssl.ReloadingX509KeyManager:   [0]         Version: 3
         SerialNumber: 1
             IssuerDN: CN=scm@scm1.org,OU=e5fdb72c-5c5c-4ba8-951b-04fd10563c04,O=CID-5914c539-8ad3-44d3-8e5d-2cc8202ea8d6,SERIALNUMBER=1
           Start Date: Tue Dec 12 10:31:48 UTC 2023
           Final Date: Fri Jan 19 10:31:48 UTC 2029
            SubjectDN: CN=scm@scm1.org,OU=e5fdb72c-5c5c-4ba8-951b-04fd10563c04,O=CID-5914c539-8ad3-44d3-8e5d-2cc8202ea8d6,SERIALNUMBER=1
           Public Key: RSA Public Key [2b:58:e9:c5:fa:a5:c7:c7:fd:be:32:86:63:76:80:c4:ae:17:1b:e9],[56:66:d1:a4]
        modulus: 84259f742e4e7218c4d05a98e958fdcd439420c40dc6a2a58a64029d5229c59ac73806be85547393f340258a8a657805b4ecbd1f499fd95cbf5f3d118db1fdfab1c15399482f56abb4d6e5e957778c2e9d077fd1b717e833b8de2b60408e2ee2efac8be7bb457240c954905ac42dae64a0cae92d57094c694a8691e4126e1a0a1b1daf6cfa8531f07c354204743af7ecb73065b66a9bbe2d891696a33b3da0b78ec24232269c51705efe1120264442fd11b70551919c907ccfcd00241cf173dd1c020076faadd12ce034fb19602795cfa716cb1b23145582483ba5655bb49a953a4891fd743e1158006b2c8bd6ed3a6925ffbe8c4518e538be88c12e13eecb09
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 5068dcd49ff0e3e8f728eb5135040d8f09db36c8
                       e087b3d96890f239a5ae737a7c0a5381df7c8735
                       025e8248aa680668c8ddfac998b16ba1e4c5ae11
                       2263f1a10bd26b5855e21ec3a3d07d1e45525905
                       6a17a999ceafe18a596bdbe016c51dbbc2dce4a6
                       d0a331da6b448a3dec87e2dbdef83a547768ddd1
                       5ad90717c80fefa6b0ef55574fde3cd47f3a10f3
                       63f3d7e7626d73c4f56cd4c5d689d9c1040cb048
                       597b0b802082a27cfa59d0559ded00b762ced635
                       ca121b79da408ac34e04b9c8a2827b36ff924d02
                       8240ce97975c78119980098740ef9f0b78cf94e2
                       b4683d848605b79f74ce400f40e295a8e9ded968
                       43007b7abbad66745315920859db0cd0
       Extensions: 
                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0x6
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [7] IMPLICIT 
        DER Octet String[4] 
    Tagged [2] IMPLICIT 
        DER Octet String[8] 


2023-12-12 10:32:00,060 [main] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-12-12 10:32:00,061 [main] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-12-12 10:32:00,061 [main] INFO ssl.ReloadingX509TrustManager: Trust manager is loaded with certificates
2023-12-12 10:32:00,062 [main] INFO ssl.ReloadingX509TrustManager:   [0]         Version: 3
         SerialNumber: 1
             IssuerDN: CN=scm@scm1.org,OU=e5fdb72c-5c5c-4ba8-951b-04fd10563c04,O=CID-5914c539-8ad3-44d3-8e5d-2cc8202ea8d6,SERIALNUMBER=1
           Start Date: Tue Dec 12 10:31:48 UTC 2023
           Final Date: Fri Jan 19 10:31:48 UTC 2029
            SubjectDN: CN=scm@scm1.org,OU=e5fdb72c-5c5c-4ba8-951b-04fd10563c04,O=CID-5914c539-8ad3-44d3-8e5d-2cc8202ea8d6,SERIALNUMBER=1
           Public Key: RSA Public Key [2b:58:e9:c5:fa:a5:c7:c7:fd:be:32:86:63:76:80:c4:ae:17:1b:e9],[56:66:d1:a4]
        modulus: 84259f742e4e7218c4d05a98e958fdcd439420c40dc6a2a58a64029d5229c59ac73806be85547393f340258a8a657805b4ecbd1f499fd95cbf5f3d118db1fdfab1c15399482f56abb4d6e5e957778c2e9d077fd1b717e833b8de2b60408e2ee2efac8be7bb457240c954905ac42dae64a0cae92d57094c694a8691e4126e1a0a1b1daf6cfa8531f07c354204743af7ecb73065b66a9bbe2d891696a33b3da0b78ec24232269c51705efe1120264442fd11b70551919c907ccfcd00241cf173dd1c020076faadd12ce034fb19602795cfa716cb1b23145582483ba5655bb49a953a4891fd743e1158006b2c8bd6ed3a6925ffbe8c4518e538be88c12e13eecb09
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 5068dcd49ff0e3e8f728eb5135040d8f09db36c8
                       e087b3d96890f239a5ae737a7c0a5381df7c8735
                       025e8248aa680668c8ddfac998b16ba1e4c5ae11
                       2263f1a10bd26b5855e21ec3a3d07d1e45525905
                       6a17a999ceafe18a596bdbe016c51dbbc2dce4a6
                       d0a331da6b448a3dec87e2dbdef83a547768ddd1
                       5ad90717c80fefa6b0ef55574fde3cd47f3a10f3
                       63f3d7e7626d73c4f56cd4c5d689d9c1040cb048
                       597b0b802082a27cfa59d0559ded00b762ced635
                       ca121b79da408ac34e04b9c8a2827b36ff924d02
                       8240ce97975c78119980098740ef9f0b78cf94e2
                       b4683d848605b79f74ce400f40e295a8e9ded968
                       43007b7abbad66745315920859db0cd0
       Extensions: 
                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0x6
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [7] IMPLICIT 
        DER Octet String[4] 
    Tagged [2] IMPLICIT 
        DER Octet String[8] 


2023-12-12 10:32:00,074 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-12-12 10:32:00,077 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-12-12 10:32:00,124 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
2023-12-12 10:32:00,135 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2023-12-12 10:32:00,136 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
2023-12-12 10:32:00,136 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2023-12-12 10:32:00,138 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
2023-12-12 10:32:00,138 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
2023-12-12 10:32:00,138 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
2023-12-12 10:32:00,138 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
2023-12-12 10:32:00,139 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-12-12 10:32:00,140 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
2023-12-12 10:32:00,140 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
2023-12-12 10:32:00,148 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
2023-12-12 10:32:00,150 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
2023-12-12 10:32:00,151 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
2023-12-12 10:32:00,443 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
2023-12-12 10:32:00,444 [main] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
2023-12-12 10:32:00,445 [main] INFO server.RaftServerConfigKeys: raft.server.close.threshold = 60s (default)
2023-12-12 10:32:00,445 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-12-12 10:32:00,448 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
2023-12-12 10:32:00,449 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
2023-12-12 10:32:00,450 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
2023-12-12 10:32:00,452 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServer: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: found a subdirectory /data/metadata/scm-ha/5914c539-8ad3-44d3-8e5d-2cc8202ea8d6
2023-12-12 10:32:00,457 [main] INFO server.RaftServer: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: addNew group-2CC8202EA8D6:[] returns group-2CC8202EA8D6:java.util.concurrent.CompletableFuture@776a3365[Not completed]
2023-12-12 10:32:00,472 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: new RaftServerImpl for group-2CC8202EA8D6:[] with SCMStateMachine:uninitialized
2023-12-12 10:32:00,473 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
2023-12-12 10:32:00,473 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
2023-12-12 10:32:00,473 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
2023-12-12 10:32:00,473 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
2023-12-12 10:32:00,474 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-12-12 10:32:00,474 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.member.majority-add = false (default)
2023-12-12 10:32:00,474 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
2023-12-12 10:32:00,479 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-12-12 10:32:00,484 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
2023-12-12 10:32:00,487 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
2023-12-12 10:32:00,490 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
2023-12-12 10:32:00,490 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100ms (default)
2023-12-12 10:32:00,494 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
2023-12-12 10:32:00,495 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.read-after-write-consistent.write-index-cache.expiry-time = 60s (default)
2023-12-12 10:32:00,598 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
2023-12-12 10:32:00,600 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-12-12 10:32:00,601 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
2023-12-12 10:32:00,601 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
2023-12-12 10:32:00,601 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
2023-12-12 10:32:00,601 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
2023-12-12 10:32:00,603 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
2023-12-12 10:32:00,603 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
2023-12-12 10:32:00,604 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
2023-12-12 10:32:00,638 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
2023-12-12 10:32:00,672 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
2023-12-12 10:32:00,672 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
2023-12-12 10:32:00,677 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
2023-12-12 10:32:00,679 [main] INFO ha.SequenceIdGenerator: upgrade CertificateId to 2
2023-12-12 10:32:00,681 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
2023-12-12 10:32:00,745 [main] INFO node.SCMNodeManager: Entering startup safe mode.
2023-12-12 10:32:00,766 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
2023-12-12 10:32:00,769 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
2023-12-12 10:32:00,781 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
2023-12-12 10:32:00,797 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
2023-12-12 10:32:00,797 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
2023-12-12 10:32:00,803 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
2023-12-12 10:32:00,803 [main] INFO pipeline.BackgroundPipelineCreator: Starting scm1-RatisPipelineUtilsThread.
2023-12-12 10:32:00,807 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
2023-12-12 10:32:00,808 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
2023-12-12 10:32:00,814 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
2023-12-12 10:32:00,815 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
2023-12-12 10:32:00,841 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
2023-12-12 10:32:00,843 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
2023-12-12 10:32:00,869 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
2023-12-12 10:32:00,950 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
2023-12-12 10:32:00,951 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
2023-12-12 10:32:00,954 [scm1-ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
2023-12-12 10:32:00,970 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
2023-12-12 10:32:00,975 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:32:00,979 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
2023-12-12 10:32:01,164 [main] INFO security.SecretKeyManagerService: Scheduling rotation checker with interval PT1M
2023-12-12 10:32:01,166 [main] INFO ha.SCMServiceManager: Registering service SecretKeyManagerService.
2023-12-12 10:32:01,196 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
2023-12-12 10:32:01,202 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
2023-12-12 10:32:01,203 [main] INFO server.StorageContainerManager: Storing sub-ca certificate serialId 2 on primary SCM
2023-12-12 10:32:01,218 [main] INFO server.SCMCertStore: Scm certificate 2 for CN=scm-sub@scm1.org,OU=e5fdb72c-5c5c-4ba8-951b-04fd10563c04,O=CID-5914c539-8ad3-44d3-8e5d-2cc8202ea8d6,SERIALNUMBER=2 is stored
2023-12-12 10:32:01,219 [main] INFO server.StorageContainerManager: Storing root certificate serialId 1
2023-12-12 10:32:01,226 [main] INFO server.SCMCertStore: Scm certificate 1 for CN=scm@scm1.org,OU=e5fdb72c-5c5c-4ba8-951b-04fd10563c04,O=CID-5914c539-8ad3-44d3-8e5d-2cc8202ea8d6,SERIALNUMBER=1 is stored
2023-12-12 10:32:01,233 [main] INFO ha.SequenceIdGenerator: upgrade CertificateId to 2
2023-12-12 10:32:01,253 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2023-12-12 10:32:01,284 [main] INFO ipc.Server: Listener at 0.0.0.0:9961
2023-12-12 10:32:01,291 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
2023-12-12 10:32:01,323 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [testuser, recon, om, scm]
2023-12-12 10:32:01,799 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
2023-12-12 10:32:01,814 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2023-12-12 10:32:01,815 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
2023-12-12 10:32:01,826 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
2023-12-12 10:32:01,880 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
2023-12-12 10:32:01,886 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2023-12-12 10:32:01,886 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
2023-12-12 10:32:01,887 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
2023-12-12 10:32:01,955 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
2023-12-12 10:32:01,965 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2023-12-12 10:32:01,975 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
2023-12-12 10:32:01,977 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
2023-12-12 10:32:02,069 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
2023-12-12 10:32:02,070 [main] INFO server.StorageContainerManager: 
Container Balancer status:
Key                            Value
Running                        false
Container Balancer Configuration values:
Key                                                Value
Threshold                                          10
Max Datanodes to Involve per Iteration(percent)    20
Max Size to Move per Iteration                     500GB
Max Size Entering Target per Iteration             26GB
Max Size Leaving Source per Iteration              26GB
Number of Iterations                               10
Time Limit for Single Container's Movement         65min
Time Limit for Single Container's Replication      50min
Interval between each Iteration                    70min
Whether to Enable Network Topology                 false
Whether to Trigger Refresh Datanode Usage Info     false
Container IDs to Exclude from Balancing            None
Datanodes Specified to be Balanced                 None
Datanodes Excluded from Balancing                  None

2023-12-12 10:32:02,070 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
2023-12-12 10:32:02,081 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
2023-12-12 10:32:02,084 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
2023-12-12 10:32:02,086 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
2023-12-12 10:32:02,086 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
2023-12-12 10:32:02,086 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
2023-12-12 10:32:02,102 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/5914c539-8ad3-44d3-8e5d-2cc8202ea8d6/in_use.lock acquired by nodename 7@scm1.org
2023-12-12 10:32:02,113 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=e5fdb72c-5c5c-4ba8-951b-04fd10563c04} from /data/metadata/scm-ha/5914c539-8ad3-44d3-8e5d-2cc8202ea8d6/current/raft-meta
2023-12-12 10:32:02,165 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: set configuration 0: peers:[e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894]|listeners:[], old=null
2023-12-12 10:32:02,171 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
2023-12-12 10:32:02,184 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
2023-12-12 10:32:02,185 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-12-12 10:32:02,186 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-12-12 10:32:02,187 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
2023-12-12 10:32:02,197 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
2023-12-12 10:32:02,203 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
2023-12-12 10:32:02,204 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-12-12 10:32:02,204 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-12-12 10:32:02,214 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO util.AwaitToRun: Thread[e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-cacheEviction-AwaitToRun,5,main] started
2023-12-12 10:32:02,218 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/5914c539-8ad3-44d3-8e5d-2cc8202ea8d6
2023-12-12 10:32:02,218 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
2023-12-12 10:32:02,218 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
2023-12-12 10:32:02,220 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
2023-12-12 10:32:02,221 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
2023-12-12 10:32:02,222 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
2023-12-12 10:32:02,222 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
2023-12-12 10:32:02,223 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-12-12 10:32:02,223 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-12-12 10:32:02,224 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 33554440 (custom)
2023-12-12 10:32:02,230 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-12-12 10:32:02,261 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
2023-12-12 10:32:02,261 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
2023-12-12 10:32:02,261 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
2023-12-12 10:32:02,304 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: set configuration 0: peers:[e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894]|listeners:[], old=null
2023-12-12 10:32:02,304 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/5914c539-8ad3-44d3-8e5d-2cc8202ea8d6/current/log_inprogress_0
2023-12-12 10:32:02,306 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-12-12 10:32:02,340 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: start as a follower, conf=0: peers:[e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894]|listeners:[], old=null
2023-12-12 10:32:02,340 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: changes role from      null to FOLLOWER at term 1 for startAsFollower
2023-12-12 10:32:02,341 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO impl.RoleInfo: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: start e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-FollowerState
2023-12-12 10:32:02,342 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
2023-12-12 10:32:02,342 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-12-12 10:32:02,343 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-2CC8202EA8D6,id=e5fdb72c-5c5c-4ba8-951b-04fd10563c04
2023-12-12 10:32:02,347 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.trigger-when-stop.enabled = true (default)
2023-12-12 10:32:02,347 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-12-12 10:32:02,347 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
2023-12-12 10:32:02,348 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
2023-12-12 10:32:02,348 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
2023-12-12 10:32:02,354 [main] INFO server.RaftServer: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: start RPC server
2023-12-12 10:32:02,391 [main] INFO server.GrpcService: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: GrpcService started, listening on 9894
2023-12-12 10:32:02,395 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-e5fdb72c-5c5c-4ba8-951b-04fd10563c04: Started
2023-12-12 10:32:02,402 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894]
2023-12-12 10:32:02,403 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
2023-12-12 10:32:02,405 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
2023-12-12 10:32:02,405 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
2023-12-12 10:32:02,405 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
2023-12-12 10:32:02,482 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2023-12-12 10:32:02,506 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2023-12-12 10:32:02,507 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
2023-12-12 10:32:02,618 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
2023-12-12 10:32:02,619 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
2023-12-12 10:32:02,619 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
2023-12-12 10:32:02,655 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
2023-12-12 10:32:02,656 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
2023-12-12 10:32:02,656 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
2023-12-12 10:32:02,657 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
2023-12-12 10:32:02,666 [main] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
2023-12-12 10:32:02,671 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
2023-12-12 10:32:02,671 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
2023-12-12 10:32:02,671 [main] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
2023-12-12 10:32:02,714 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
2023-12-12 10:32:02,714 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
2023-12-12 10:32:02,715 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
2023-12-12 10:32:02,739 [main] INFO util.log: Logging initialized @5745ms to org.eclipse.jetty.util.log.Slf4jLog
2023-12-12 10:32:02,829 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
2023-12-12 10:32:02,843 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
2023-12-12 10:32:02,845 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
2023-12-12 10:32:02,845 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
2023-12-12 10:32:02,846 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
2023-12-12 10:32:02,850 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
2023-12-12 10:32:02,898 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
2023-12-12 10:32:02,900 [main] INFO http.HttpServer2: Jetty bound to port 9876
2023-12-12 10:32:02,901 [main] INFO server.Server: jetty-9.4.53.v20231009; built: 2023-10-09T12:29:09.265Z; git: 27bde00a0b95a1d5bbee0eae7984f891d2d0f8c9; jvm 11.0.19+7-LTS
2023-12-12 10:32:02,934 [main] INFO server.session: DefaultSessionIdManager workerName=node0
2023-12-12 10:32:02,935 [main] INFO server.session: No SessionScavenger set, using defaults
2023-12-12 10:32:02,936 [main] INFO server.session: node0 Scavenging every 600000ms
2023-12-12 10:32:02,949 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
2023-12-12 10:32:02,955 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@13acb4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
2023-12-12 10:32:02,955 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7790a6fb{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2023-12-12 10:32:03,039 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
2023-12-12 10:32:03,048 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@22271065{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-3276245499107906508/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
2023-12-12 10:32:03,055 [main] INFO server.AbstractConnector: Started ServerConnector@579c24c2{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
2023-12-12 10:32:03,055 [main] INFO server.Server: Started @6060ms
2023-12-12 10:32:03,056 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
2023-12-12 10:32:03,056 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
2023-12-12 10:32:03,058 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
2023-12-12 10:32:03,210 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm1.org:43346 / 172.25.0.116:43346
2023-12-12 10:32:03,225 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-12-12 10:32:03,317 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#0 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from scm1.org:43346 / 172.25.0.116:43346
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:e5fdb72c-5c5c-4ba8-951b-04fd10563c04 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-12-12 10:32:04,971 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm2.org:47676 / 172.25.0.117:47676
2023-12-12 10:32:04,987 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-12-12 10:32:07,329 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_recon_1.ozonesecure-ha_ozone_net:34389 / 172.25.0.115:34389
2023-12-12 10:32:07,340 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-12-12 10:32:07,341 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#9 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure-ha_recon_1.ozonesecure-ha_ozone_net:34389 / 172.25.0.115:34389
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:e5fdb72c-5c5c-4ba8-951b-04fd10563c04 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-12-12 10:32:07,421 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-FollowerState] INFO impl.FollowerState: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5079806357ns, electionTimeout:5077ms
2023-12-12 10:32:07,421 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-FollowerState] INFO impl.RoleInfo: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: shutdown e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-FollowerState
2023-12-12 10:32:07,422 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-FollowerState] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
2023-12-12 10:32:07,425 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
2023-12-12 10:32:07,425 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-FollowerState] INFO impl.RoleInfo: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: start e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1
2023-12-12 10:32:07,438 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO impl.LeaderElection: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894]|listeners:[], old=null
2023-12-12 10:32:07,439 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO impl.LeaderElection: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1 PRE_VOTE round 0: result PASSED (term=1)
2023-12-12 10:32:07,447 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO impl.LeaderElection: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894]|listeners:[], old=null
2023-12-12 10:32:07,447 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO impl.LeaderElection: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1 ELECTION round 0: result PASSED (term=2)
2023-12-12 10:32:07,448 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO impl.RoleInfo: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: shutdown e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1
2023-12-12 10:32:07,448 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
2023-12-12 10:32:07,461 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
2023-12-12 10:32:07,469 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
2023-12-12 10:32:07,471 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
2023-12-12 10:32:07,477 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
2023-12-12 10:32:07,477 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
2023-12-12 10:32:07,479 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
2023-12-12 10:32:07,491 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.read.leader.lease.enabled = false (default)
2023-12-12 10:32:07,493 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.read.leader.lease.timeout.ratio = 0.9 (default)
2023-12-12 10:32:07,494 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
2023-12-12 10:32:07,495 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
2023-12-12 10:32:07,495 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-12-12 10:32:07,499 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO impl.RoleInfo: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: start e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderStateImpl
2023-12-12 10:32:07,499 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: set firstElectionSinceStartup to false for becomeLeader
2023-12-12 10:32:07,499 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
2023-12-12 10:32:07,504 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
2023-12-12 10:32:07,509 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: change Leader from null to e5fdb72c-5c5c-4ba8-951b-04fd10563c04 at term 2 for becomeLeader, leader elected after 7015ms
2023-12-12 10:32:07,523 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from scm1.org:40538 / 172.25.0.116:40538
2023-12-12 10:32:07,525 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
2023-12-12 10:32:07,534 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/5914c539-8ad3-44d3-8e5d-2cc8202ea8d6/current/log_inprogress_0 to /data/metadata/scm-ha/5914c539-8ad3-44d3-8e5d-2cc8202ea8d6/current/log_0-0
2023-12-12 10:32:07,541 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderElection1] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: set configuration 1: peers:[e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894]|listeners:[], old=null
2023-12-12 10:32:07,562 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-12-12 10:32:07,574 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/5914c539-8ad3-44d3-8e5d-2cc8202ea8d6/current/log_inprogress_1
2023-12-12 10:32:07,592 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO server.RaftServer$Division: leader is ready since appliedIndex == 1 >= startIndex == 1
2023-12-12 10:32:07,593 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
2023-12-12 10:32:07,593 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
2023-12-12 10:32:07,595 [scm1-SecretKeyManagerService] INFO symmetric.SecretKeyManager: Initializing SecretKeys.
2023-12-12 10:32:07,596 [scm1-SecretKeyManagerService] INFO symmetric.SecretKeyManager: No valid key has been loaded. A new key is generated: SecretKey(id = 937b23b2-3aef-4a5d-ad1c-4d999bfe49ab, creation at: 2023-12-12T10:32:07.595581Z, expire at: 2023-12-12T11:32:07.595581Z)
2023-12-12 10:32:07,625 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:32:07,647 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
2023-12-12 10:32:07,649 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
2023-12-12 10:32:07,649 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
2023-12-12 10:32:07,662 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
2023-12-12 10:32:07,695 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
2023-12-12 10:32:07,827 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Updating keys with [SecretKey(id = 937b23b2-3aef-4a5d-ad1c-4d999bfe49ab, creation at: 2023-12-12T10:32:07.595Z, expire at: 2023-12-12T11:32:07.595Z)]
2023-12-12 10:32:07,828 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Current key updated SecretKey(id = 937b23b2-3aef-4a5d-ad1c-4d999bfe49ab, creation at: 2023-12-12T10:32:07.595Z, expire at: 2023-12-12T11:32:07.595Z)
2023-12-12 10:32:07,896 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO symmetric.LocalSecretKeyStore: Saved [SecretKey(id = 937b23b2-3aef-4a5d-ad1c-4d999bfe49ab, creation at: 2023-12-12T10:32:07.595Z, expire at: 2023-12-12T11:32:07.595Z)] to file /data/metadata/scm/keys/secret_keys.json
2023-12-12 10:32:07,900 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:32:07,900 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
2023-12-12 10:32:07,900 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
2023-12-12 10:32:09,372 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-12-12 10:32:09,372 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-12-12 10:32:09,377 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-scm/sub-ca-refreshCACertificates] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-12-12 10:32:09,377 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-scm/sub-ca-refreshCACertificates] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-12-12 10:32:09,379 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04-scm/sub-ca-refreshCACertificates] INFO client.SCMCertificateClient: CA certificates are not changed.
2023-12-12 10:32:10,176 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm2.org:40646 / 172.25.0.117:40646
2023-12-12 10:32:10,182 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-12-12 10:32:10,183 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for scm scm2.org, nodeId: 707b32b6-be48-4439-b655-31ff544e1299
2023-12-12 10:32:10,196 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for CertificateId, expected lastId is 0, actual lastId is 2.
2023-12-12 10:32:10,199 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:32:10,202 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:32:10,202 [IPC Server handler 1 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 2 to 3.
2023-12-12 10:32:10,238 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.19, 2.5.29.15, 2.5.29.17
2023-12-12 10:32:10,238 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-12-12 10:32:10,278 [IPC Server handler 1 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-12-12 10:32:10,591 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO server.SCMCertStore: Scm certificate 3 for CN=scm-sub@scm2.org,OU=707b32b6-be48-4439-b655-31ff544e1299,O=CID-5914c539-8ad3-44d3-8e5d-2cc8202ea8d6,SERIALNUMBER=3 is stored
2023-12-12 10:32:10,594 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:32:13,369 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for RECON recon, UUID: c6cd4e58-88b6-41cb-b9e7-f99f46d7775b
2023-12-12 10:32:13,373 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:32:13,374 [IPC Server handler 0 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 3 to 4.
2023-12-12 10:32:13,380 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-12-12 10:32:13,380 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-12-12 10:32:13,404 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-12-12 10:32:13,491 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:32:13,627 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-12-12 10:32:13,627 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-12-12 10:32:17,330 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm2.org:38540 / 172.25.0.117:38540
2023-12-12 10:32:17,348 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-12-12 10:32:17,350 [IPC Server handler 30 on default port 9863] INFO ha.SCMRatisServerImpl: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: Submitting SetConfiguration request to Ratis server with new SCM peers list: [e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894, 707b32b6-be48-4439-b655-31ff544e1299|scm2.org:9894]
2023-12-12 10:32:17,353 [IPC Server handler 30 on default port 9863] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: receive setConfiguration SetConfigurationRequest:client-FEDEB90ABD39->e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6, cid=4, seq=null, RW, null, SET_UNCONDITIONALLY, servers:[e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894, 707b32b6-be48-4439-b655-31ff544e1299|scm2.org:9894], listeners:[]
2023-12-12 10:32:17,354 [IPC Server handler 30 on default port 9863] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-FEDEB90ABD39->e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6, cid=4, seq=null, RW, null, SET_UNCONDITIONALLY, servers:[e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894, 707b32b6-be48-4439-b655-31ff544e1299|scm2.org:9894], listeners:[]
2023-12-12 10:32:17,367 [IPC Server handler 30 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
2023-12-12 10:32:17,367 [IPC Server handler 30 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-12-12 10:32:17,367 [IPC Server handler 30 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
2023-12-12 10:32:17,376 [IPC Server handler 30 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 0ms (custom)
2023-12-12 10:32:17,377 [IPC Server handler 30 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 8 (default)
2023-12-12 10:32:17,377 [IPC Server handler 30 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
2023-12-12 10:32:17,377 [IPC Server handler 30 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
2023-12-12 10:32:17,377 [IPC Server handler 30 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
2023-12-12 10:32:17,378 [IPC Server handler 30 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-12-12 10:32:17,379 [IPC Server handler 30 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
2023-12-12 10:32:17,387 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->707b32b6-be48-4439-b655-31ff544e1299-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->707b32b6-be48-4439-b655-31ff544e1299-GrpcLogAppender: notifyInstallSnapshot with firstAvailable=(t:1, i:0), followerNextIndex=0
2023-12-12 10:32:17,397 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->707b32b6-be48-4439-b655-31ff544e1299-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->707b32b6-be48-4439-b655-31ff544e1299-GrpcLogAppender: send e5fdb72c-5c5c-4ba8-951b-04fd10563c04->707b32b6-be48-4439-b655-31ff544e1299#0-t2,notify:(t:1, i:0)
2023-12-12 10:32:17,398 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->707b32b6-be48-4439-b655-31ff544e1299-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcServerProtocolClient: Build channel for 707b32b6-be48-4439-b655-31ff544e1299
2023-12-12 10:32:18,053 [grpc-default-executor-1] INFO server.GrpcLogAppender: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->707b32b6-be48-4439-b655-31ff544e1299-InstallSnapshotResponseHandler: received the first reply e5fdb72c-5c5c-4ba8-951b-04fd10563c04<-707b32b6-be48-4439-b655-31ff544e1299#0:OK-t0,ALREADY_INSTALLED,snapshotIndex=0
2023-12-12 10:32:18,055 [grpc-default-executor-1] INFO server.GrpcLogAppender: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->707b32b6-be48-4439-b655-31ff544e1299-InstallSnapshotResponseHandler: Follower snapshot is already at index 0.
2023-12-12 10:32:18,056 [grpc-default-executor-1] INFO leader.FollowerInfo: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->707b32b6-be48-4439-b655-31ff544e1299: matchIndex: setUnconditionally -1 -> 0
2023-12-12 10:32:18,061 [grpc-default-executor-1] INFO leader.FollowerInfo: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->707b32b6-be48-4439-b655-31ff544e1299: nextIndex: setUnconditionally 0 -> 1
2023-12-12 10:32:18,062 [grpc-default-executor-1] INFO leader.FollowerInfo: Follower e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->707b32b6-be48-4439-b655-31ff544e1299 acknowledged installing snapshot
2023-12-12 10:32:18,067 [grpc-default-executor-1] INFO server.GrpcLogAppender: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->707b32b6-be48-4439-b655-31ff544e1299-GrpcLogAppender: updateNextIndex 1 for ALREADY_INSTALLED
2023-12-12 10:32:18,206 [grpc-default-executor-2] WARN server.GrpcLogAppender: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->707b32b6-be48-4439-b655-31ff544e1299-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 0, request=AppendEntriesRequest:cid=0,entriesCount=0
2023-12-12 10:32:18,207 [grpc-default-executor-2] INFO leader.FollowerInfo: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->707b32b6-be48-4439-b655-31ff544e1299: setNextIndex nextIndex: updateUnconditionally 15 -> 1
2023-12-12 10:32:18,208 [grpc-default-executor-1] WARN server.GrpcLogAppender: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->707b32b6-be48-4439-b655-31ff544e1299-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 0, request=AppendEntriesRequest:cid=1,entriesCount=14,entries=(t:2, i:1)...(t:2, i:14)
2023-12-12 10:32:18,208 [grpc-default-executor-1] INFO leader.FollowerInfo: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->707b32b6-be48-4439-b655-31ff544e1299: setNextIndex nextIndex: updateUnconditionally 1 -> 0
2023-12-12 10:32:18,405 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderStateImpl] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: set configuration 15: peers:[e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894, 707b32b6-be48-4439-b655-31ff544e1299|scm2.org:9894]|listeners:[], old=peers:[e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894]|listeners:[]
2023-12-12 10:32:18,476 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderStateImpl] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: set configuration 17: peers:[e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894, 707b32b6-be48-4439-b655-31ff544e1299|scm2.org:9894]|listeners:[], old=null
2023-12-12 10:32:18,525 [IPC Server handler 30 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: 707b32b6-be48-4439-b655-31ff544e1299.
2023-12-12 10:32:19,399 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm2.org:46140 / 172.25.0.117:46140
2023-12-12 10:32:19,408 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-12-12 10:32:20,181 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-12-12 10:32:20,181 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-12-12 10:32:21,431 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm3.org:44368 / 172.25.0.118:44368
2023-12-12 10:32:21,448 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-12-12 10:32:22,813 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_recon_1.ozonesecure-ha_ozone_net:42597 / 172.25.0.115:42597
2023-12-12 10:32:22,816 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-12-12 10:32:22,841 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm3.org:40682 / 172.25.0.118:40682
2023-12-12 10:32:22,862 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-12-12 10:32:22,862 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for scm scm3.org, nodeId: b01ee407-6004-400b-bafa-19d15b52321b
2023-12-12 10:32:22,872 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:32:22,872 [IPC Server handler 1 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 4 to 5.
2023-12-12 10:32:22,880 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.19, 2.5.29.15, 2.5.29.17
2023-12-12 10:32:22,880 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-12-12 10:32:22,904 [IPC Server handler 1 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-12-12 10:32:22,971 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO server.SCMCertStore: Scm certificate 5 for CN=scm-sub@scm3.org,OU=b01ee407-6004-400b-bafa-19d15b52321b,O=CID-5914c539-8ad3-44d3-8e5d-2cc8202ea8d6,SERIALNUMBER=5 is stored
2023-12-12 10:32:22,973 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:32:28,269 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm3.org:53434 / 172.25.0.118:53434
2023-12-12 10:32:28,278 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-12-12 10:32:28,279 [IPC Server handler 30 on default port 9863] INFO ha.SCMRatisServerImpl: e5fdb72c-5c5c-4ba8-951b-04fd10563c04: Submitting SetConfiguration request to Ratis server with new SCM peers list: [e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894, 707b32b6-be48-4439-b655-31ff544e1299|scm2.org:9894, b01ee407-6004-400b-bafa-19d15b52321b|scm3.org:9894]
2023-12-12 10:32:28,279 [IPC Server handler 30 on default port 9863] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: receive setConfiguration SetConfigurationRequest:client-FEDEB90ABD39->e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6, cid=6, seq=null, RW, null, SET_UNCONDITIONALLY, servers:[e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894, 707b32b6-be48-4439-b655-31ff544e1299|scm2.org:9894, b01ee407-6004-400b-bafa-19d15b52321b|scm3.org:9894], listeners:[]
2023-12-12 10:32:28,279 [IPC Server handler 30 on default port 9863] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-FEDEB90ABD39->e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6, cid=6, seq=null, RW, null, SET_UNCONDITIONALLY, servers:[e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894, 707b32b6-be48-4439-b655-31ff544e1299|scm2.org:9894, b01ee407-6004-400b-bafa-19d15b52321b|scm3.org:9894], listeners:[]
2023-12-12 10:32:28,280 [IPC Server handler 30 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
2023-12-12 10:32:28,280 [IPC Server handler 30 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-12-12 10:32:28,280 [IPC Server handler 30 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
2023-12-12 10:32:28,281 [IPC Server handler 30 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 0ms (custom)
2023-12-12 10:32:28,281 [IPC Server handler 30 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 8 (default)
2023-12-12 10:32:28,281 [IPC Server handler 30 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
2023-12-12 10:32:28,281 [IPC Server handler 30 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
2023-12-12 10:32:28,281 [IPC Server handler 30 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
2023-12-12 10:32:28,281 [IPC Server handler 30 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-12-12 10:32:28,281 [IPC Server handler 30 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
2023-12-12 10:32:28,282 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->b01ee407-6004-400b-bafa-19d15b52321b-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->b01ee407-6004-400b-bafa-19d15b52321b-GrpcLogAppender: notifyInstallSnapshot with firstAvailable=(t:1, i:0), followerNextIndex=0
2023-12-12 10:32:28,282 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->b01ee407-6004-400b-bafa-19d15b52321b-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->b01ee407-6004-400b-bafa-19d15b52321b-GrpcLogAppender: send e5fdb72c-5c5c-4ba8-951b-04fd10563c04->b01ee407-6004-400b-bafa-19d15b52321b#0-t2,notify:(t:1, i:0)
2023-12-12 10:32:28,283 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->b01ee407-6004-400b-bafa-19d15b52321b-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcServerProtocolClient: Build channel for b01ee407-6004-400b-bafa-19d15b52321b
2023-12-12 10:32:28,634 [grpc-default-executor-1] INFO server.GrpcLogAppender: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->b01ee407-6004-400b-bafa-19d15b52321b-InstallSnapshotResponseHandler: received the first reply e5fdb72c-5c5c-4ba8-951b-04fd10563c04<-b01ee407-6004-400b-bafa-19d15b52321b#0:OK-t0,ALREADY_INSTALLED,snapshotIndex=0
2023-12-12 10:32:28,634 [grpc-default-executor-1] INFO server.GrpcLogAppender: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->b01ee407-6004-400b-bafa-19d15b52321b-InstallSnapshotResponseHandler: Follower snapshot is already at index 0.
2023-12-12 10:32:28,634 [grpc-default-executor-1] INFO leader.FollowerInfo: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->b01ee407-6004-400b-bafa-19d15b52321b: matchIndex: setUnconditionally -1 -> 0
2023-12-12 10:32:28,634 [grpc-default-executor-1] INFO leader.FollowerInfo: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->b01ee407-6004-400b-bafa-19d15b52321b: nextIndex: setUnconditionally 0 -> 1
2023-12-12 10:32:28,634 [grpc-default-executor-1] INFO leader.FollowerInfo: Follower e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->b01ee407-6004-400b-bafa-19d15b52321b acknowledged installing snapshot
2023-12-12 10:32:28,634 [grpc-default-executor-1] INFO server.GrpcLogAppender: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->b01ee407-6004-400b-bafa-19d15b52321b-GrpcLogAppender: updateNextIndex 1 for ALREADY_INSTALLED
2023-12-12 10:32:28,688 [grpc-default-executor-2] WARN server.GrpcLogAppender: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->b01ee407-6004-400b-bafa-19d15b52321b-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 0, request=AppendEntriesRequest:cid=0,entriesCount=0
2023-12-12 10:32:28,689 [grpc-default-executor-2] INFO leader.FollowerInfo: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->b01ee407-6004-400b-bafa-19d15b52321b: setNextIndex nextIndex: updateUnconditionally 23 -> 1
2023-12-12 10:32:28,689 [grpc-default-executor-1] WARN server.GrpcLogAppender: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->b01ee407-6004-400b-bafa-19d15b52321b-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 0, request=AppendEntriesRequest:cid=1,entriesCount=22,entries=(t:2, i:1)...(t:2, i:22)
2023-12-12 10:32:28,689 [grpc-default-executor-1] INFO leader.FollowerInfo: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6->b01ee407-6004-400b-bafa-19d15b52321b: setNextIndex nextIndex: updateUnconditionally 1 -> 0
2023-12-12 10:32:28,776 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderStateImpl] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: set configuration 23: peers:[b01ee407-6004-400b-bafa-19d15b52321b|scm3.org:9894, e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894, 707b32b6-be48-4439-b655-31ff544e1299|scm2.org:9894]|listeners:[], old=peers:[e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894, 707b32b6-be48-4439-b655-31ff544e1299|scm2.org:9894]|listeners:[]
2023-12-12 10:32:28,781 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-LeaderStateImpl] INFO server.RaftServer$Division: e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6: set configuration 25: peers:[b01ee407-6004-400b-bafa-19d15b52321b|scm3.org:9894, e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894, 707b32b6-be48-4439-b655-31ff544e1299|scm2.org:9894]|listeners:[], old=null
2023-12-12 10:32:28,785 [IPC Server handler 30 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: b01ee407-6004-400b-bafa-19d15b52321b.
2023-12-12 10:32:30,925 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm3.org:42474 / 172.25.0.118:42474
2023-12-12 10:32:30,946 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-12-12 10:32:32,795 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-12-12 10:32:32,796 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-12-12 10:32:42,583 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om2:45556 / 172.25.0.112:45556
2023-12-12 10:32:42,669 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-12-12 10:32:42,800 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om1:38654 / 172.25.0.111:38654
2023-12-12 10:32:42,846 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-12-12 10:32:44,811 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:34922 / 172.25.0.102:34922
2023-12-12 10:32:44,845 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:47550 / 172.25.0.103:47550
2023-12-12 10:32:44,909 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-12-12 10:32:44,910 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn b403753ef1c2, UUID: f7747e3f-639e-4a66-bee7-99ddbfccd962
2023-12-12 10:32:44,919 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:32:44,920 [IPC Server handler 0 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 5 to 6.
2023-12-12 10:32:44,931 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-12-12 10:32:44,932 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn 491a9e18362f, UUID: dcb40b12-80f2-4e81-b564-c9993480f099
2023-12-12 10:32:44,958 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-12-12 10:32:44,961 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-12-12 10:32:45,006 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-12-12 10:32:45,127 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:32:45,215 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:53228 / 172.25.0.104:53228
2023-12-12 10:32:45,223 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:32:45,230 [IPC Server handler 1 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 6 to 7.
2023-12-12 10:32:45,270 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-12-12 10:32:45,270 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-12-12 10:32:45,292 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-12-12 10:32:45,306 [IPC Server handler 1 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-12-12 10:32:45,462 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:32:45,572 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn 87768e074f8f, UUID: c093f2ea-1bd4-4965-ba81-73d5fe2850ac
2023-12-12 10:32:45,584 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:32:45,590 [IPC Server handler 0 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 7 to 8.
2023-12-12 10:32:45,613 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-12-12 10:32:45,618 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-12-12 10:32:45,662 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-12-12 10:32:45,806 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:32:45,895 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-12-12 10:32:45,896 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-12-12 10:32:45,897 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-12-12 10:32:45,898 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-12-12 10:32:45,914 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om1:56540 / 172.25.0.111:56540
2023-12-12 10:32:45,948 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om2:44540 / 172.25.0.112:44540
2023-12-12 10:32:45,949 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-12-12 10:32:45,953 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om1, UUID: a7c7c5c9-6ef0-4524-960d-2971ef741ba9
2023-12-12 10:32:45,980 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:32:45,985 [IPC Server handler 1 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 8 to 9.
2023-12-12 10:32:45,996 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-12-12 10:32:45,997 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-12-12 10:32:46,001 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-12-12 10:32:46,001 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om2, UUID: 8841bedd-1feb-4e2e-bc74-b34555b558ce
2023-12-12 10:32:46,048 [IPC Server handler 1 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-12-12 10:32:46,149 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:32:46,222 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:32:46,223 [IPC Server handler 0 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 9 to 10.
2023-12-12 10:32:46,248 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-12-12 10:32:46,248 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-12-12 10:32:46,271 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-12-12 10:32:46,428 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:32:46,512 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:47558 / 172.25.0.103:47558
2023-12-12 10:32:46,514 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-12-12 10:32:46,514 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-12-12 10:32:46,533 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-12-12 10:32:46,672 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:34938 / 172.25.0.102:34938
2023-12-12 10:32:46,697 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-12-12 10:32:46,754 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-12-12 10:32:46,754 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-12-12 10:32:46,878 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-12-12 10:32:46,878 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-12-12 10:32:47,392 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:53230 / 172.25.0.104:53230
2023-12-12 10:32:47,403 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-12-12 10:32:58,640 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:47400 / 172.25.0.102:47400
2023-12-12 10:32:58,697 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-12 10:32:59,606 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:57666 / 172.25.0.103:57666
2023-12-12 10:32:59,751 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-12 10:32:59,902 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:48784 / 172.25.0.104:48784
2023-12-12 10:33:00,001 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-12 10:33:00,650 [IPC Server handler 5 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f7747e3f-639e-4a66-bee7-99ddbfccd962
2023-12-12 10:33:00,662 [IPC Server handler 5 on default port 9861] INFO node.SCMNodeManager: Registered datanode: f7747e3f-639e-4a66-bee7-99ddbfccd962{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=19864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 6, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-12-12 10:33:00,667 [scm1-EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on scm1-RatisPipelineUtilsThread.
2023-12-12 10:33:00,677 [scm1-RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=3571a92d-000e-40fa-bd24-91a95fbde0ad to datanode:f7747e3f-639e-4a66-bee7-99ddbfccd962
2023-12-12 10:33:00,682 [scm1-EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
2023-12-12 10:33:00,734 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:33:00,771 [scm1-RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 3571a92d-000e-40fa-bd24-91a95fbde0ad, Nodes: f7747e3f-639e-4a66-bee7-99ddbfccd962(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-12-12T10:33:00.671576Z[UTC]]
2023-12-12 10:33:00,844 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om2:46302 / 172.25.0.112:46302
2023-12-12 10:33:00,888 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-12-12 10:33:00,986 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om1:54380 / 172.25.0.111:54380
2023-12-12 10:33:01,011 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-12-12 10:33:01,420 [IPC Server handler 3 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/dcb40b12-80f2-4e81-b564-c9993480f099
2023-12-12 10:33:01,421 [IPC Server handler 3 on default port 9861] INFO node.SCMNodeManager: Registered datanode: dcb40b12-80f2-4e81-b564-c9993480f099{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=19864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 7, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-12-12 10:33:01,423 [scm1-EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on scm1-RatisPipelineUtilsThread.
2023-12-12 10:33:01,424 [scm1-EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
2023-12-12 10:33:01,425 [scm1-RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d87811d4-e23b-4171-950d-b836b13374cf to datanode:dcb40b12-80f2-4e81-b564-c9993480f099
2023-12-12 10:33:01,437 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:33:01,444 [scm1-RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: d87811d4-e23b-4171-950d-b836b13374cf, Nodes: dcb40b12-80f2-4e81-b564-c9993480f099(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-12-12T10:33:01.425065Z[UTC]]
2023-12-12 10:33:01,711 [IPC Server handler 11 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/c093f2ea-1bd4-4965-ba81-73d5fe2850ac
2023-12-12 10:33:01,713 [IPC Server handler 11 on default port 9861] INFO node.SCMNodeManager: Registered datanode: c093f2ea-1bd4-4965-ba81-73d5fe2850ac{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [HTTP=9882, CLIENT_RPC=19864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 8, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-12-12 10:33:01,713 [scm1-EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
2023-12-12 10:33:01,713 [scm1-EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
2023-12-12 10:33:01,714 [scm1-EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
2023-12-12 10:33:01,714 [scm1-EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
2023-12-12 10:33:01,714 [scm1-EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on scm1-RatisPipelineUtilsThread.
2023-12-12 10:33:01,717 [scm1-EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on scm1-RatisPipelineUtilsThread.
2023-12-12 10:33:01,722 [scm1-RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=365a8c0f-832c-4cec-ac33-a5340c0bd651 to datanode:c093f2ea-1bd4-4965-ba81-73d5fe2850ac
2023-12-12 10:33:01,754 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:33:01,756 [scm1-RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 365a8c0f-832c-4cec-ac33-a5340c0bd651, Nodes: c093f2ea-1bd4-4965-ba81-73d5fe2850ac(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-12-12T10:33:01.721943Z[UTC]]
2023-12-12 10:33:01,769 [scm1-RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d9688f3b-f54c-42fc-a754-0daf51d84bad to datanode:dcb40b12-80f2-4e81-b564-c9993480f099
2023-12-12 10:33:01,769 [scm1-RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d9688f3b-f54c-42fc-a754-0daf51d84bad to datanode:c093f2ea-1bd4-4965-ba81-73d5fe2850ac
2023-12-12 10:33:01,769 [scm1-RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d9688f3b-f54c-42fc-a754-0daf51d84bad to datanode:f7747e3f-639e-4a66-bee7-99ddbfccd962
2023-12-12 10:33:01,805 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:33:01,809 [scm1-RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: d9688f3b-f54c-42fc-a754-0daf51d84bad, Nodes: dcb40b12-80f2-4e81-b564-c9993480f099(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)c093f2ea-1bd4-4965-ba81-73d5fe2850ac(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)f7747e3f-639e-4a66-bee7-99ddbfccd962(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-12-12T10:33:01.769546Z[UTC]]
2023-12-12 10:33:01,812 [scm1-RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=cb84d9f1-5828-4335-9cdd-38bcf3fe9947 to datanode:f7747e3f-639e-4a66-bee7-99ddbfccd962
2023-12-12 10:33:01,813 [scm1-RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=cb84d9f1-5828-4335-9cdd-38bcf3fe9947 to datanode:c093f2ea-1bd4-4965-ba81-73d5fe2850ac
2023-12-12 10:33:01,813 [scm1-RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=cb84d9f1-5828-4335-9cdd-38bcf3fe9947 to datanode:dcb40b12-80f2-4e81-b564-c9993480f099
2023-12-12 10:33:01,847 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:33:01,849 [scm1-RatisPipelineUtilsThread-0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=cb84d9f1-5828-4335-9cdd-38bcf3fe9947 contains same datanodes as previous pipelines: PipelineID=d9688f3b-f54c-42fc-a754-0daf51d84bad nodeIds: f7747e3f-639e-4a66-bee7-99ddbfccd962, c093f2ea-1bd4-4965-ba81-73d5fe2850ac, dcb40b12-80f2-4e81-b564-c9993480f099
2023-12-12 10:33:01,849 [scm1-RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: cb84d9f1-5828-4335-9cdd-38bcf3fe9947, Nodes: f7747e3f-639e-4a66-bee7-99ddbfccd962(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)c093f2ea-1bd4-4965-ba81-73d5fe2850ac(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)dcb40b12-80f2-4e81-b564-c9993480f099(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-12-12T10:33:01.812810Z[UTC]]
2023-12-12 10:33:03,936 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:33:03,940 [scm1-EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=3571a92d-000e-40fa-bd24-91a95fbde0ad
2023-12-12 10:33:03,949 [scm1-EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-12-12 10:33:04,167 [scm1-EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-12-12 10:33:04,228 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_recon_1.ozonesecure-ha_ozone_net:34987 / 172.25.0.115:34987
2023-12-12 10:33:04,242 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-12-12 10:33:04,867 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:33:04,867 [scm1-EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=d87811d4-e23b-4171-950d-b836b13374cf
2023-12-12 10:33:04,868 [scm1-EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-12-12 10:33:05,237 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-12-12 10:33:05,238 [scm1-EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=365a8c0f-832c-4cec-ac33-a5340c0bd651
2023-12-12 10:33:05,238 [scm1-EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-12-12 10:33:05,410 [scm1-EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-12-12 10:33:05,438 [scm1-EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-12-12 10:33:07,370 [scm1-EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-12-12 10:33:07,734 [scm1-EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-12-12 10:33:08,068 [scm1-EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-12-12 10:33:09,347 [scm1-EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-12-12 10:33:10,435 [scm1-EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-12-12 10:33:10,442 [scm1-EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-12-12 10:33:10,563 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om1:45320 / 172.25.0.111:45320
2023-12-12 10:33:10,568 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolOm
2023-12-12 10:33:10,873 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om2:53498 / 172.25.0.112:53498
2023-12-12 10:33:10,878 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolOm
2023-12-12 10:33:11,040 [scm1-EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-12-12 10:33:11,063 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
2023-12-12 10:33:11,063 [scm1-EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=d9688f3b-f54c-42fc-a754-0daf51d84bad
2023-12-12 10:33:11,063 [scm1-EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
2023-12-12 10:33:11,063 [scm1-EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
2023-12-12 10:33:11,063 [scm1-EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
2023-12-12 10:33:11,063 [scm1-EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
2023-12-12 10:33:11,064 [scm1-EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
2023-12-12 10:33:11,064 [scm1-EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
2023-12-12 10:33:11,064 [scm1-EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
2023-12-12 10:33:11,064 [scm1-EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
2023-12-12 10:33:11,098 [scm1-EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
2023-12-12 10:33:11,106 [scm1-EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO SCMHATransactionMonitor: Service SCMHATransactionMonitor transitions to RUNNING.
2023-12-12 10:33:12,693 [scm1-EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=cb84d9f1-5828-4335-9cdd-38bcf3fe9947
2023-12-12 10:33:23,023 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_recon_1.ozonesecure-ha_ozone_net:37107 / 172.25.0.115:37107
2023-12-12 10:33:23,066 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-12-12 10:33:40,481 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:33400 / 172.25.0.104:33400
2023-12-12 10:33:40,495 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-12 10:33:41,060 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:53486 / 172.25.0.103:53486
2023-12-12 10:33:41,085 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-12 10:33:42,715 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:44140 / 172.25.0.102:44140
2023-12-12 10:33:42,744 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-12 10:33:53,140 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om1:42210 / 172.25.0.111:42210
2023-12-12 10:33:53,159 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-12-12 10:33:53,181 [IPC Server handler 36 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
2023-12-12 10:33:53,218 [e5fdb72c-5c5c-4ba8-951b-04fd10563c04@group-2CC8202EA8D6-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
2023-12-12 10:33:53,229 [IPC Server handler 3 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
2023-12-12 10:33:53,232 [IPC Server handler 27 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,232 [IPC Server handler 36 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,232 [IPC Server handler 11 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,232 [IPC Server handler 29 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,233 [IPC Server handler 54 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,233 [IPC Server handler 2 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,233 [IPC Server handler 26 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,233 [IPC Server handler 20 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,233 [IPC Server handler 59 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,240 [IPC Server handler 3 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,632 [IPC Server handler 63 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,634 [IPC Server handler 35 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,634 [IPC Server handler 56 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,659 [IPC Server handler 75 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,698 [IPC Server handler 51 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,731 [IPC Server handler 95 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,735 [IPC Server handler 94 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,761 [IPC Server handler 98 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,793 [IPC Server handler 87 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,823 [IPC Server handler 0 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,942 [IPC Server handler 31 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,963 [IPC Server handler 26 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,979 [IPC Server handler 59 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,992 [IPC Server handler 29 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,998 [IPC Server handler 3 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:53,998 [IPC Server handler 54 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,018 [IPC Server handler 11 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,024 [IPC Server handler 36 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,028 [IPC Server handler 2 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,040 [IPC Server handler 20 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,127 [IPC Server handler 27 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,165 [IPC Server handler 63 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,172 [IPC Server handler 56 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,203 [IPC Server handler 35 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,233 [IPC Server handler 75 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,235 [IPC Server handler 14 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,252 [IPC Server handler 65 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,269 [IPC Server handler 37 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,287 [IPC Server handler 39 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,324 [IPC Server handler 23 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,384 [IPC Server handler 47 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,392 [IPC Server handler 64 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,402 [IPC Server handler 62 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,421 [IPC Server handler 55 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,427 [IPC Server handler 19 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,440 [IPC Server handler 69 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,447 [IPC Server handler 48 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,469 [IPC Server handler 33 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,471 [IPC Server handler 45 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,500 [IPC Server handler 68 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,547 [IPC Server handler 42 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,581 [IPC Server handler 70 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,597 [IPC Server handler 41 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,601 [IPC Server handler 28 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,611 [IPC Server handler 10 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,618 [IPC Server handler 58 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,623 [IPC Server handler 17 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,630 [IPC Server handler 43 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,663 [IPC Server handler 25 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,664 [IPC Server handler 76 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,723 [IPC Server handler 52 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,734 [IPC Server handler 94 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,739 [IPC Server handler 97 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,762 [IPC Server handler 82 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,790 [IPC Server handler 87 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,805 [IPC Server handler 18 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,806 [IPC Server handler 30 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,817 [IPC Server handler 0 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,846 [IPC Server handler 38 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,860 [IPC Server handler 31 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,896 [IPC Server handler 26 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,920 [IPC Server handler 59 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,948 [IPC Server handler 29 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,968 [IPC Server handler 54 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:54,970 [IPC Server handler 3 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,035 [IPC Server handler 20 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,101 [IPC Server handler 27 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,102 [IPC Server handler 63 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,103 [IPC Server handler 56 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,168 [IPC Server handler 35 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,174 [IPC Server handler 75 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,178 [IPC Server handler 14 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,189 [IPC Server handler 65 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,190 [IPC Server handler 37 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,207 [IPC Server handler 39 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,224 [IPC Server handler 23 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,243 [IPC Server handler 47 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,269 [IPC Server handler 64 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,284 [IPC Server handler 62 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,405 [IPC Server handler 55 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,432 [IPC Server handler 69 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,438 [IPC Server handler 48 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,441 [IPC Server handler 33 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,448 [IPC Server handler 45 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,466 [IPC Server handler 68 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,478 [IPC Server handler 42 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,494 [IPC Server handler 70 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,523 [IPC Server handler 41 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,524 [IPC Server handler 28 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:33:55,605 [IPC Server handler 10 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:34:04,555 [IPC Server handler 10 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:34:06,070 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:41924 / 172.25.0.102:41924
2023-12-12 10:34:06,082 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-12-12 10:34:06,345 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:33720 / 172.25.0.102:33720
2023-12-12 10:34:06,444 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:33162 / 172.25.0.103:33162
2023-12-12 10:34:06,454 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-12 10:34:06,504 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_recon_1.ozonesecure-ha_ozone_net:40707 / 172.25.0.115:40707
2023-12-12 10:34:06,514 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-12-12 10:34:06,544 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-12 10:34:06,627 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:36222 / 172.25.0.104:36222
2023-12-12 10:34:06,757 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-12 10:34:11,053 [IPC Server handler 11 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for delTxnId, change lastId from 0 to 1000.
2023-12-12 10:34:11,923 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om1:57630 / 172.25.0.111:57630
2023-12-12 10:34:11,932 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-12-12 10:34:12,075 [IPC Server handler 56 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.25.0.111
2023-12-12 10:34:17,683 [IPC Server handler 40 on default port 9863] WARN node.SCMNodeManager: address is null
2023-12-12 10:34:19,069 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:54748 / 172.25.0.103:54748
2023-12-12 10:34:19,083 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-12-12 10:34:19,174 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:57250 / 172.25.0.102:57250
2023-12-12 10:34:19,179 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:40476 / 172.25.0.103:40476
2023-12-12 10:34:19,179 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:48796 / 172.25.0.104:48796
2023-12-12 10:34:19,188 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-12 10:34:19,249 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_recon_1.ozonesecure-ha_ozone_net:45453 / 172.25.0.115:45453
2023-12-12 10:34:19,270 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-12-12 10:34:19,272 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-12 10:34:19,300 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-12 10:34:24,039 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om1:48742 / 172.25.0.111:48742
2023-12-12 10:34:24,042 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-12-12 10:34:24,047 [IPC Server handler 56 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.25.0.111
2023-12-12 10:34:39,306 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om3:43804 / 172.25.0.113:43804
2023-12-12 10:34:39,321 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-12-12 10:34:40,450 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om3:40530 / 172.25.0.113:40530
2023-12-12 10:34:40,459 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-12-12 10:34:40,459 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om3, UUID: f29a0008-f179-430f-8ca6-f5beaa4a63f9
2023-12-12 10:34:40,466 [IPC Server handler 0 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 10 to 11.
2023-12-12 10:34:40,470 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-12-12 10:34:40,470 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-12-12 10:34:40,484 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-12-12 10:34:40,551 [grpc-default-executor-3] ERROR impl.OrderedAsync: client-783A40D41128: Failed* RaftClientRequest:client-783A40D41128->b01ee407-6004-400b-bafa-19d15b52321b@group-2CC8202EA8D6, cid=17, seq=1*, Watch(0), null
java.util.concurrent.CompletionException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server b01ee407-6004-400b-bafa-19d15b52321b@group-2CC8202EA8D6 is not the leader e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:632)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers.completeReplyExceptionally(GrpcClientProtocolClient.java:394)
	at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers.access$000(GrpcClientProtocolClient.java:300)
	at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers$1.onNext(GrpcClientProtocolClient.java:314)
	at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers$1.onNext(GrpcClientProtocolClient.java:305)
	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onMessage(ClientCalls.java:468)
	at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onMessage(ForwardingClientCallListener.java:33)
	at org.apache.ratis.thirdparty.io.grpc.internal.DelayedClientCall$DelayedListener.onMessage(DelayedClientCall.java:473)
	at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1MessagesAvailable.runInternal(ClientCallImpl.java:660)
	at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1MessagesAvailable.runInContext(ClientCallImpl.java:647)
	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server b01ee407-6004-400b-bafa-19d15b52321b@group-2CC8202EA8D6 is not the leader e5fdb72c-5c5c-4ba8-951b-04fd10563c04|scm1.org:9894
	at org.apache.ratis.client.impl.ClientProtoUtils.toRaftClientReply(ClientProtoUtils.java:397)
	at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers$1.onNext(GrpcClientProtocolClient.java:310)
	... 11 more
2023-12-12 10:34:40,700 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-12-12 10:34:40,700 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-12-12 10:34:46,826 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om3:57764 / 172.25.0.113:57764
2023-12-12 10:34:46,846 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-12-12 10:34:49,148 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:34166 / 172.25.0.103:34166
2023-12-12 10:34:49,150 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:42588 / 172.25.0.102:42588
2023-12-12 10:34:49,199 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:34342 / 172.25.0.104:34342
2023-12-12 10:34:49,209 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-12 10:34:49,221 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-12 10:34:49,250 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-12 10:34:51,541 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om3:41568 / 172.25.0.113:41568
2023-12-12 10:34:51,546 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolOm
2023-12-12 10:35:19,142 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:35288 / 172.25.0.103:35288
2023-12-12 10:35:19,232 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-12 10:35:19,232 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:33616 / 172.25.0.104:33616
2023-12-12 10:35:19,237 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:41490 / 172.25.0.102:41490
2023-12-12 10:35:19,251 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-12 10:35:19,276 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-12 10:35:43,331 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om3:35356 / 172.25.0.113:35356
2023-12-12 10:35:43,335 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-12-12 10:35:43,489 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from om3:58792 / 172.25.0.113:58792
2023-12-12 10:35:43,493 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-12-12 10:35:43,493 [IPC Server handler 70 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.25.0.113
2023-12-12 10:35:44,797 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:39756 / 172.25.0.104:39756
2023-12-12 10:35:44,802 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-12-12 10:35:49,211 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net:35860 / 172.25.0.104:35860
2023-12-12 10:35:49,225 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net:36098 / 172.25.0.103:36098
2023-12-12 10:35:49,226 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net:57638 / 172.25.0.102:57638
2023-12-12 10:35:49,253 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-12 10:35:49,253 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-12 10:35:49,274 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-12-12 10:35:49,660 [IPC Server handler 76 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.25.0.113
