Attaching to ha_scm3_1, ha_s3g_1, ha_recon_1, ha_scm2_1, ha_dn1_1, ha_dn5_1, ha_om1_1, ha_dn4_1, ha_dn2_1, ha_om2_1, ha_scm1_1, ha_om3_1, ha_dn3_1
dn2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn2_1    | 2023-05-18 20:09:27,390 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn2_1    | /************************************************************
dn2_1    | STARTUP_MSG: Starting HddsDatanodeService
dn2_1    | STARTUP_MSG:   host = e715d0592117/10.9.0.18
dn2_1    | STARTUP_MSG:   args = []
dn2_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/0f84f49736c9dcc7cfaf75d16080f1fd578598c2 ; compiled by 'runner' on 2023-05-18T19:28Z
dn2_1    | STARTUP_MSG:   java = 11.0.14.1
dn2_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn2_1    | ************************************************************/
dn2_1    | 2023-05-18 20:09:27,482 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn2_1    | 2023-05-18 20:09:27,891 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn2_1    | 2023-05-18 20:09:28,560 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn2_1    | 2023-05-18 20:09:29,516 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn2_1    | 2023-05-18 20:09:29,517 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn2_1    | 2023-05-18 20:09:30,923 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:e715d0592117 ip:10.9.0.18
dn2_1    | 2023-05-18 20:09:32,320 [main] INFO reflections.Reflections: Reflections took 1087 ms to scan 2 urls, producing 104 keys and 226 values 
dn2_1    | 2023-05-18 20:09:35,273 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
dn2_1    | 2023-05-18 20:09:35,653 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn2_1    | 2023-05-18 20:09:36,772 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 8192 at 2023-05-18T20:08:59.919Z
dn2_1    | 2023-05-18 20:09:36,901 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn2_1    | 2023-05-18 20:09:36,903 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn2_1    | 2023-05-18 20:09:36,934 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn2_1    | 2023-05-18 20:09:37,100 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn2_1    | 2023-05-18 20:09:37,147 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-05-18 20:09:37,174 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-05-18T20:08:59.914Z
dn2_1    | 2023-05-18 20:09:37,175 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn2_1    | 2023-05-18 20:09:37,175 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn2_1    | 2023-05-18 20:09:37,175 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn2_1    | 2023-05-18 20:09:37,839 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/DS-e9f1f129-4c18-407c-acc2-43d522cfa8b2/container.db to cache
dn2_1    | 2023-05-18 20:09:37,839 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/DS-e9f1f129-4c18-407c-acc2-43d522cfa8b2/container.db for volume DS-e9f1f129-4c18-407c-acc2-43d522cfa8b2
dn2_1    | 2023-05-18 20:09:38,009 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn2_1    | 2023-05-18 20:09:38,012 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn2_1    | 2023-05-18 20:09:38,012 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn2_1    | 2023-05-18 20:09:49,585 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn2_1    | 2023-05-18 20:09:50,284 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-05-18 20:09:50,739 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn2_1    | 2023-05-18 20:09:51,821 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-05-18 20:09:51,839 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn2_1    | 2023-05-18 20:09:51,840 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-05-18 20:09:51,845 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn2_1    | 2023-05-18 20:09:51,854 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn2_1    | 2023-05-18 20:09:51,856 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn2_1    | 2023-05-18 20:09:51,856 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn2_1    | 2023-05-18 20:09:51,862 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-05-18 20:09:51,897 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn2_1    | 2023-05-18 20:09:51,937 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-05-18 20:09:51,988 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-05-18 20:09:52,040 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn2_1    | 2023-05-18 20:09:52,040 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn2_1    | 2023-05-18 20:09:53,869 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn2_1    | 2023-05-18 20:09:53,872 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn2_1    | 2023-05-18 20:09:53,898 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn2_1    | 2023-05-18 20:09:53,905 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-05-18 20:09:53,909 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-05-18 20:09:53,918 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-05-18 20:09:53,961 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServer: 15e7697f-640b-4664-b91f-bd9ea47c523c: found a subdirectory /data/metadata/ratis/f0622044-e126-43df-b394-8158f807c8f5
dn2_1    | 2023-05-18 20:09:54,118 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServer: 15e7697f-640b-4664-b91f-bd9ea47c523c: addNew group-8158F807C8F5:[] returns group-8158F807C8F5:java.util.concurrent.CompletableFuture@709daea8[Not completed]
dn2_1    | 2023-05-18 20:09:54,379 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn2_1    | 2023-05-18 20:09:54,986 [15e7697f-640b-4664-b91f-bd9ea47c523c-groupManagement] INFO server.RaftServer$Division: 15e7697f-640b-4664-b91f-bd9ea47c523c: new RaftServerImpl for group-8158F807C8F5:[] with ContainerStateMachine:uninitialized
dn2_1    | 2023-05-18 20:09:55,003 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn2_1    | 2023-05-18 20:09:55,081 [15e7697f-640b-4664-b91f-bd9ea47c523c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-05-18 20:09:55,096 [15e7697f-640b-4664-b91f-bd9ea47c523c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-05-18 20:09:55,098 [15e7697f-640b-4664-b91f-bd9ea47c523c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-05-18 20:09:55,099 [15e7697f-640b-4664-b91f-bd9ea47c523c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-05-18 20:09:55,100 [15e7697f-640b-4664-b91f-bd9ea47c523c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-05-18 20:09:55,100 [15e7697f-640b-4664-b91f-bd9ea47c523c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-05-18 20:09:55,379 [15e7697f-640b-4664-b91f-bd9ea47c523c-groupManagement] INFO server.RaftServer$Division: 15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-05-18 20:09:55,379 [15e7697f-640b-4664-b91f-bd9ea47c523c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-05-18 20:09:55,438 [15e7697f-640b-4664-b91f-bd9ea47c523c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-05-18 20:09:55,469 [15e7697f-640b-4664-b91f-bd9ea47c523c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-05-18 20:09:55,706 [15e7697f-640b-4664-b91f-bd9ea47c523c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-05-18 20:09:55,781 [15e7697f-640b-4664-b91f-bd9ea47c523c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn2_1    | 2023-05-18 20:09:55,826 [15e7697f-640b-4664-b91f-bd9ea47c523c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-05-18 20:09:55,827 [15e7697f-640b-4664-b91f-bd9ea47c523c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-05-18 20:09:56,196 [15e7697f-640b-4664-b91f-bd9ea47c523c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn2_1    | 2023-05-18 20:09:56,440 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn2_1    | 2023-05-18 20:09:56,660 [15e7697f-640b-4664-b91f-bd9ea47c523c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-05-18 20:09:56,688 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn2_1    | 2023-05-18 20:09:56,712 [15e7697f-640b-4664-b91f-bd9ea47c523c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-05-18 20:09:56,757 [15e7697f-640b-4664-b91f-bd9ea47c523c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-05-18 20:09:56,758 [15e7697f-640b-4664-b91f-bd9ea47c523c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-05-18 20:09:56,758 [15e7697f-640b-4664-b91f-bd9ea47c523c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-05-18 20:09:56,765 [15e7697f-640b-4664-b91f-bd9ea47c523c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-05-18 20:09:56,916 [main] INFO util.log: Logging initialized @40422ms to org.eclipse.jetty.util.log.Slf4jLog
dn2_1    | 2023-05-18 20:09:57,690 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn2_1    | 2023-05-18 20:09:57,791 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn2_1    | 2023-05-18 20:09:57,896 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn1_1    | 2023-05-18 20:09:28,301 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn1_1    | /************************************************************
dn1_1    | STARTUP_MSG: Starting HddsDatanodeService
dn1_1    | STARTUP_MSG:   host = a1dc68dc7bff/10.9.0.17
dn1_1    | STARTUP_MSG:   args = []
dn1_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn2_1    | 2023-05-18 20:09:57,900 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn2_1    | 2023-05-18 20:09:57,913 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn2_1    | 2023-05-18 20:09:57,913 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn2_1    | 2023-05-18 20:09:58,121 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn2_1    | 2023-05-18 20:09:58,139 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn2_1    | 2023-05-18 20:09:58,154 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
dn2_1    | 2023-05-18 20:09:58,612 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn2_1    | 2023-05-18 20:09:58,620 [main] INFO server.session: No SessionScavenger set, using defaults
dn2_1    | 2023-05-18 20:09:58,625 [main] INFO server.session: node0 Scavenging every 660000ms
dn2_1    | 2023-05-18 20:09:58,773 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5a4d4f9c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn2_1    | 2023-05-18 20:09:58,785 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@51ba952e{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn2_1    | 2023-05-18 20:09:59,607 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@72c4a3aa{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-3522195831827333250/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn2_1    | 2023-05-18 20:09:59,643 [main] INFO server.AbstractConnector: Started ServerConnector@1788cb61{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn2_1    | 2023-05-18 20:09:59,658 [main] INFO server.Server: Started @43171ms
dn2_1    | 2023-05-18 20:09:59,687 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn2_1    | 2023-05-18 20:09:59,688 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn2_1    | 2023-05-18 20:09:59,690 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn2_1    | 2023-05-18 20:09:59,719 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn2_1    | 2023-05-18 20:09:59,813 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn2_1    | 2023-05-18 20:09:59,815 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn2_1    | 2023-05-18 20:09:59,827 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@201261a9] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn2_1    | 2023-05-18 20:10:00,119 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn2_1    | 2023-05-18 20:10:00,205 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn2_1    | 2023-05-18 20:10:03,064 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:03,064 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:03,064 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:04,066 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:04,066 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:04,066 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:05,068 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:05,068 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:05,069 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:06,069 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:06,070 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:07,070 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:07,071 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:07,188 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From e715d0592117/10.9.0.18 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:52118 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/0f84f49736c9dcc7cfaf75d16080f1fd578598c2 ; compiled by 'runner' on 2023-05-18T19:28Z
dn1_1    | STARTUP_MSG:   java = 11.0.14.1
dn1_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn1_1    | ************************************************************/
dn1_1    | 2023-05-18 20:09:28,377 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn1_1    | 2023-05-18 20:09:28,691 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn1_1    | 2023-05-18 20:09:29,390 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn1_1    | 2023-05-18 20:09:30,561 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn1_1    | 2023-05-18 20:09:30,562 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn1_1    | 2023-05-18 20:09:31,811 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:a1dc68dc7bff ip:10.9.0.17
dn1_1    | 2023-05-18 20:09:33,066 [main] INFO reflections.Reflections: Reflections took 1012 ms to scan 2 urls, producing 104 keys and 226 values 
dn1_1    | 2023-05-18 20:09:35,552 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
dn1_1    | 2023-05-18 20:09:35,945 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn1_1    | 2023-05-18 20:09:37,080 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 8303 at 2023-05-18T20:08:59.933Z
dn1_1    | 2023-05-18 20:09:37,178 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn1_1    | 2023-05-18 20:09:37,180 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn1_1    | 2023-05-18 20:09:37,197 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn1_1    | 2023-05-18 20:09:37,311 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn1_1    | 2023-05-18 20:09:37,447 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-05-18 20:09:37,468 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-05-18T20:08:59.923Z
dn1_1    | 2023-05-18 20:09:37,485 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn1_1    | 2023-05-18 20:09:37,489 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn1_1    | 2023-05-18 20:09:37,490 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn1_1    | 2023-05-18 20:09:40,458 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/DS-5d0f0a20-20da-493b-b5a4-e2ebfa8adafc/container.db to cache
dn1_1    | 2023-05-18 20:09:40,463 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/DS-5d0f0a20-20da-493b-b5a4-e2ebfa8adafc/container.db for volume DS-5d0f0a20-20da-493b-b5a4-e2ebfa8adafc
dn1_1    | 2023-05-18 20:09:40,613 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn1_1    | 2023-05-18 20:09:40,939 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn1_1    | 2023-05-18 20:09:40,988 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn1_1    | 2023-05-18 20:09:49,480 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn1_1    | 2023-05-18 20:09:49,882 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-05-18 20:09:50,326 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn1_1    | 2023-05-18 20:09:51,422 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-05-18 20:09:51,432 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn1_1    | 2023-05-18 20:09:51,447 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-05-18 20:09:51,451 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn1_1    | 2023-05-18 20:09:51,461 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn1_1    | 2023-05-18 20:09:51,462 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn1_1    | 2023-05-18 20:09:51,462 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn1_1    | 2023-05-18 20:09:51,469 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-05-18 20:09:51,480 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn1_1    | 2023-05-18 20:09:51,482 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-05-18 20:09:51,608 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 2023-05-18 20:09:51,619 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn1_1    | 2023-05-18 20:09:51,620 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn1_1    | 2023-05-18 20:09:53,187 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn1_1    | 2023-05-18 20:09:53,205 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn1_1    | 2023-05-18 20:09:53,208 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn1_1    | 2023-05-18 20:09:53,208 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-05-18 20:09:53,209 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-05-18 20:09:53,257 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-05-18 20:09:53,277 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServer: 76248598-2f32-4c49-8f2a-b1fddb9f8151: found a subdirectory /data/metadata/ratis/467c8493-9e44-42c8-850d-8edf1c67e99f
dn1_1    | 2023-05-18 20:09:53,292 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServer: 76248598-2f32-4c49-8f2a-b1fddb9f8151: addNew group-8EDF1C67E99F:[] returns group-8EDF1C67E99F:java.util.concurrent.CompletableFuture@530e4c7d[Not completed]
dn1_1    | 2023-05-18 20:09:53,292 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServer: 76248598-2f32-4c49-8f2a-b1fddb9f8151: found a subdirectory /data/metadata/ratis/78d664ca-7989-4249-9a90-24fe3dd84618
dn1_1    | 2023-05-18 20:09:53,293 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServer: 76248598-2f32-4c49-8f2a-b1fddb9f8151: addNew group-24FE3DD84618:[] returns group-24FE3DD84618:java.util.concurrent.CompletableFuture@2ee44ae1[Not completed]
dn1_1    | 2023-05-18 20:09:53,293 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServer: 76248598-2f32-4c49-8f2a-b1fddb9f8151: found a subdirectory /data/metadata/ratis/a2756a42-16da-435f-a15a-f31dfb92d20c
dn1_1    | 2023-05-18 20:09:53,293 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServer: 76248598-2f32-4c49-8f2a-b1fddb9f8151: addNew group-F31DFB92D20C:[] returns group-F31DFB92D20C:java.util.concurrent.CompletableFuture@ecdc917[Not completed]
dn1_1    | 2023-05-18 20:09:53,441 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn1_1    | 2023-05-18 20:09:53,852 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151: new RaftServerImpl for group-8EDF1C67E99F:[] with ContainerStateMachine:uninitialized
dn1_1    | 2023-05-18 20:09:53,892 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-05-18 20:09:53,906 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-05-18 20:09:53,906 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-05-18 20:09:53,906 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-05-18 20:09:53,906 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-05-18 20:09:53,915 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-05-18 20:09:54,083 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-05-18 20:09:54,186 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-05-18 20:09:54,210 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn1_1    | 2023-05-18 20:09:54,229 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-05-18 20:09:54,229 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-05-18 20:09:54,378 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-05-18 20:09:54,394 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn1_1    | 2023-05-18 20:09:54,467 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-05-18 20:09:54,507 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-05-18 20:09:54,904 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn1_1    | 2023-05-18 20:09:55,315 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-05-18 20:09:55,319 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-05-18 20:09:55,377 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn1_1    | 2023-05-18 20:09:55,407 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-05-18 20:09:55,407 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-05-18 20:09:55,407 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-05-18 20:09:55,413 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-05-18 20:09:55,505 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151: new RaftServerImpl for group-24FE3DD84618:[] with ContainerStateMachine:uninitialized
dn1_1    | 2023-05-18 20:09:55,506 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-05-18 20:09:55,506 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-05-18 20:09:55,509 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-05-18 20:09:55,521 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-05-18 20:09:55,521 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-05-18 20:09:55,525 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-05-18 20:09:55,525 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-05-18 20:09:55,537 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-05-18 20:09:55,537 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-05-18 20:09:55,537 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-05-18 20:09:55,538 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-05-18 20:09:55,538 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn1_1    | 2023-05-18 20:09:55,576 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-05-18 20:09:55,576 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-05-18 20:09:55,577 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn1_1    | 2023-05-18 20:09:55,578 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-05-18 20:09:55,592 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-05-18 20:09:55,592 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-05-18 20:09:55,592 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-05-18 20:09:55,593 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-05-18 20:09:55,593 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-05-18 20:09:55,595 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151: new RaftServerImpl for group-F31DFB92D20C:[] with ContainerStateMachine:uninitialized
dn1_1    | 2023-05-18 20:09:55,603 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn1_1    | 2023-05-18 20:09:55,609 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-05-18 20:09:55,609 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-05-18 20:09:55,609 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-05-18 20:09:55,609 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-05-18 20:09:55,609 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-05-18 20:09:55,613 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-05-18 20:09:55,633 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-05-18 20:09:55,692 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-05-18 20:09:55,701 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-05-18 20:09:55,702 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-05-18 20:09:55,706 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-05-18 20:09:55,706 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn1_1    | 2023-05-18 20:09:55,706 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-05-18 20:09:55,707 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-05-18 20:09:55,708 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn1_1    | 2023-05-18 20:09:55,713 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn2_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:52118 remote=recon/10.9.0.22:9891]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn2_1    | 2023-05-18 20:10:08,071 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:08,072 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:09,072 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:09,073 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:10,073 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:10,073 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:10,076 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From e715d0592117/10.9.0.18 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:51658 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn2_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:51658 remote=scm1/10.9.0.14:9861]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn3_1    | 2023-05-18 20:09:26,599 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn3_1    | /************************************************************
dn3_1    | STARTUP_MSG: Starting HddsDatanodeService
dn3_1    | STARTUP_MSG:   host = 54c4478f5de9/10.9.0.19
dn3_1    | STARTUP_MSG:   args = []
dn3_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/0f84f49736c9dcc7cfaf75d16080f1fd578598c2 ; compiled by 'runner' on 2023-05-18T19:28Z
dn3_1    | STARTUP_MSG:   java = 11.0.14.1
dn3_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn3_1    | ************************************************************/
dn3_1    | 2023-05-18 20:09:26,660 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn3_1    | 2023-05-18 20:09:26,967 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn3_1    | 2023-05-18 20:09:27,567 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn3_1    | 2023-05-18 20:09:28,858 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn3_1    | 2023-05-18 20:09:28,861 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn3_1    | 2023-05-18 20:09:30,231 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:54c4478f5de9 ip:10.9.0.19
dn3_1    | 2023-05-18 20:09:31,600 [main] INFO reflections.Reflections: Reflections took 1082 ms to scan 2 urls, producing 104 keys and 226 values 
dn3_1    | 2023-05-18 20:09:34,217 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
dn3_1    | 2023-05-18 20:09:34,645 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn3_1    | 2023-05-18 20:09:35,882 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 4207 at 2023-05-18T20:08:59.792Z
dn3_1    | 2023-05-18 20:09:35,960 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn3_1    | 2023-05-18 20:09:36,020 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn3_1    | 2023-05-18 20:09:36,038 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn3_1    | 2023-05-18 20:09:36,303 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn3_1    | 2023-05-18 20:09:36,410 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-05-18 20:09:36,455 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-05-18T20:08:59.790Z
dn3_1    | 2023-05-18 20:09:36,457 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn3_1    | 2023-05-18 20:09:36,468 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn3_1    | 2023-05-18 20:09:36,497 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn3_1    | 2023-05-18 20:09:39,732 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/DS-ec54d1f1-b612-47df-bc76-6de4422e0c94/container.db to cache
dn3_1    | 2023-05-18 20:09:39,732 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/DS-ec54d1f1-b612-47df-bc76-6de4422e0c94/container.db for volume DS-ec54d1f1-b612-47df-bc76-6de4422e0c94
dn3_1    | 2023-05-18 20:09:39,874 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn3_1    | 2023-05-18 20:09:40,335 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn3_1    | 2023-05-18 20:09:40,367 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn3_1    | 2023-05-18 20:09:48,986 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn3_1    | 2023-05-18 20:09:49,609 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-05-18 20:09:50,375 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn3_1    | 2023-05-18 20:09:50,842 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-05-18 20:09:50,882 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn3_1    | 2023-05-18 20:09:50,927 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-05-18 20:09:50,934 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn3_1    | 2023-05-18 20:09:50,934 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn3_1    | 2023-05-18 20:09:50,934 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn3_1    | 2023-05-18 20:09:50,935 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn3_1    | 2023-05-18 20:09:50,941 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-05-18 20:09:50,942 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn3_1    | 2023-05-18 20:09:50,951 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-05-18 20:09:51,012 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-05-18 20:09:51,064 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn3_1    | 2023-05-18 20:09:51,067 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn3_1    | 2023-05-18 20:09:52,657 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn3_1    | 2023-05-18 20:09:52,660 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn3_1    | 2023-05-18 20:09:52,671 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn3_1    | 2023-05-18 20:09:52,671 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-05-18 20:09:52,678 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-05-18 20:09:52,691 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-05-18 20:09:52,723 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServer: caabb77a-6190-403e-a139-2e30ca474b23: found a subdirectory /data/metadata/ratis/78d664ca-7989-4249-9a90-24fe3dd84618
dn3_1    | 2023-05-18 20:09:52,772 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServer: caabb77a-6190-403e-a139-2e30ca474b23: addNew group-24FE3DD84618:[] returns group-24FE3DD84618:java.util.concurrent.CompletableFuture@7a4fef92[Not completed]
dn3_1    | 2023-05-18 20:09:52,772 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServer: caabb77a-6190-403e-a139-2e30ca474b23: found a subdirectory /data/metadata/ratis/a2756a42-16da-435f-a15a-f31dfb92d20c
dn3_1    | 2023-05-18 20:09:52,772 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServer: caabb77a-6190-403e-a139-2e30ca474b23: addNew group-F31DFB92D20C:[] returns group-F31DFB92D20C:java.util.concurrent.CompletableFuture@5d90ed2f[Not completed]
dn3_1    | 2023-05-18 20:09:52,772 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServer: caabb77a-6190-403e-a139-2e30ca474b23: found a subdirectory /data/metadata/ratis/9693fb8d-2a36-45cd-aee5-9db787f5d8f4
dn3_1    | 2023-05-18 20:09:52,787 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServer: caabb77a-6190-403e-a139-2e30ca474b23: addNew group-9DB787F5D8F4:[] returns group-9DB787F5D8F4:java.util.concurrent.CompletableFuture@28901d5a[Not completed]
dn3_1    | 2023-05-18 20:09:53,072 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn3_1    | 2023-05-18 20:09:53,351 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23: new RaftServerImpl for group-24FE3DD84618:[] with ContainerStateMachine:uninitialized
dn3_1    | 2023-05-18 20:09:53,405 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-05-18 20:09:53,453 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-05-18 20:09:53,453 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-05-18 20:09:53,453 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-05-18 20:09:53,454 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-05-18 20:09:53,454 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-05-18 20:09:53,519 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-05-18 20:09:53,519 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-05-18 20:09:53,613 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-05-18 20:09:53,614 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-05-18 20:09:53,662 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn3_1    | 2023-05-18 20:09:53,727 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-05-18 20:09:53,780 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn3_1    | 2023-05-18 20:09:53,830 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-05-18 20:09:53,834 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-05-18 20:09:54,271 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn3_1    | 2023-05-18 20:09:54,729 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn3_1    | 2023-05-18 20:09:54,787 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-05-18 20:09:54,807 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-05-18 20:09:54,881 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-05-18 20:09:54,873 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn3_1    | 2023-05-18 20:09:54,897 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn2_1    | 2023-05-18 20:10:11,076 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:11,077 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:12,077 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:12,078 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:13,079 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:13,079 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:14,080 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:15,080 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:16,081 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:17,082 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:17,083 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn2_1    | java.net.ConnectException: Call From e715d0592117/10.9.0.18 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn2_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.ConnectException: Connection refused
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:790)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:363)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
dn2_1    | 	... 12 more
dn2_1    | 2023-05-18 20:10:18,084 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:18,087 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From e715d0592117/10.9.0.18 to scm2:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:39490 remote=scm2/10.9.0.15:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn2_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:39490 remote=scm2/10.9.0.15:9861]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn2_1    | 2023-05-18 20:10:18,213 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn2_1    | 2023-05-18 20:10:18,227 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn2_1    | 2023-05-18 20:10:18,603 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn2_1    | 2023-05-18 20:10:18,603 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 15e7697f-640b-4664-b91f-bd9ea47c523c
dn2_1    | 2023-05-18 20:10:18,707 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/f0622044-e126-43df-b394-8158f807c8f5/in_use.lock acquired by nodename 6@e715d0592117
dn2_1    | 2023-05-18 20:10:18,759 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=15e7697f-640b-4664-b91f-bd9ea47c523c} from /data/metadata/ratis/f0622044-e126-43df-b394-8158f807c8f5/current/raft-meta
dn2_1    | 2023-05-18 20:10:19,010 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServer$Division: 15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5: set configuration 0: peers:[15e7697f-640b-4664-b91f-bd9ea47c523c|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-18 20:10:19,086 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:19,123 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO ratis.ContainerStateMachine: group-8158F807C8F5: Setting the last applied index to (t:1, i:0)
dn2_1    | 2023-05-18 20:10:20,105 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:20,152 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-05-18 20:10:20,269 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-05-18 20:10:20,269 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-05-18 20:10:20,278 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-05-18 20:10:20,279 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-05-18 20:10:20,313 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-05-18 20:10:20,364 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-05-18 20:10:20,366 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-05-18 20:10:20,366 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-05-18 20:10:20,421 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/f0622044-e126-43df-b394-8158f807c8f5
dn2_1    | 2023-05-18 20:10:20,428 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-05-18 20:09:54,897 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-05-18 20:09:54,902 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-05-18 20:09:54,932 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23: new RaftServerImpl for group-F31DFB92D20C:[] with ContainerStateMachine:uninitialized
dn3_1    | 2023-05-18 20:09:54,947 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-05-18 20:09:54,954 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-05-18 20:09:54,957 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-05-18 20:09:54,969 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-05-18 20:09:54,969 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-05-18 20:09:54,972 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-05-18 20:09:54,973 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-05-18 20:09:54,975 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-05-18 20:09:54,975 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-05-18 20:09:54,975 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-05-18 20:09:54,976 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-05-18 20:09:54,976 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn3_1    | 2023-05-18 20:09:54,976 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-05-18 20:09:54,979 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-05-18 20:09:55,023 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn3_1    | 2023-05-18 20:09:55,034 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-05-18 20:09:55,043 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-05-18 20:09:55,043 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-05-18 20:09:55,044 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-05-18 20:09:55,044 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-05-18 20:09:55,046 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-05-18 20:09:55,112 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23: new RaftServerImpl for group-9DB787F5D8F4:[] with ContainerStateMachine:uninitialized
dn3_1    | 2023-05-18 20:09:55,132 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-05-18 20:09:55,154 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-05-18 20:09:55,155 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-05-18 20:09:55,158 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-05-18 20:09:55,158 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-05-18 20:09:55,128 [main] INFO util.log: Logging initialized @38997ms to org.eclipse.jetty.util.log.Slf4jLog
dn3_1    | 2023-05-18 20:09:55,197 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-05-18 20:09:55,197 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-05-18 20:09:55,201 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-05-18 20:09:55,201 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-05-18 20:09:55,201 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-05-18 20:09:55,201 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-05-18 20:09:55,202 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn3_1    | 2023-05-18 20:09:55,217 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-05-18 20:09:55,219 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-05-18 20:09:55,226 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn3_1    | 2023-05-18 20:09:55,227 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-05-18 20:09:55,228 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-05-18 20:09:55,228 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn4_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn4_1    | 2023-05-18 20:09:27,827 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn4_1    | /************************************************************
dn4_1    | STARTUP_MSG: Starting HddsDatanodeService
dn4_1    | STARTUP_MSG:   host = 5dd7996c298c/10.9.0.20
dn4_1    | STARTUP_MSG:   args = []
dn4_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn2_1    | 2023-05-18 20:10:20,440 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-05-18 20:10:20,452 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-05-18 20:10:20,455 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-05-18 20:10:20,456 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-05-18 20:10:20,462 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-05-18 20:10:20,469 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-05-18 20:10:20,469 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-05-18 20:10:20,587 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-05-18 20:10:20,591 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-05-18 20:10:20,725 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-05-18 20:10:20,729 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-05-18 20:10:20,731 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-05-18 20:10:21,045 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServer$Division: 15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5: set configuration 0: peers:[15e7697f-640b-4664-b91f-bd9ea47c523c|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-18 20:10:21,067 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/f0622044-e126-43df-b394-8158f807c8f5/current/log_inprogress_0
dn2_1    | 2023-05-18 20:10:21,098 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-05-18 20:10:21,110 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:22,121 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:22,171 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServer$Division: 15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5: start as a follower, conf=0: peers:[15e7697f-640b-4664-b91f-bd9ea47c523c|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-18 20:10:22,173 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServer$Division: 15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5: changes role from      null to FOLLOWER at term 1 for startAsFollower
dn2_1    | 2023-05-18 20:10:22,194 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO impl.RoleInfo: 15e7697f-640b-4664-b91f-bd9ea47c523c: start 15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-FollowerState
dn2_1    | 2023-05-18 20:10:22,216 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-05-18 20:10:22,216 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-05-18 20:10:22,250 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8158F807C8F5,id=15e7697f-640b-4664-b91f-bd9ea47c523c
dn2_1    | 2023-05-18 20:10:22,256 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-05-18 20:10:22,258 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-05-18 20:10:22,261 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-05-18 20:10:22,267 [15e7697f-640b-4664-b91f-bd9ea47c523c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-05-18 20:10:22,322 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 15e7697f-640b-4664-b91f-bd9ea47c523c: start RPC server
dn2_1    | 2023-05-18 20:10:22,445 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 15e7697f-640b-4664-b91f-bd9ea47c523c: GrpcService started, listening on 9858
dn2_1    | 2023-05-18 20:10:22,450 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 15e7697f-640b-4664-b91f-bd9ea47c523c: GrpcService started, listening on 9856
dn2_1    | 2023-05-18 20:10:22,472 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 15e7697f-640b-4664-b91f-bd9ea47c523c: GrpcService started, listening on 9857
dn2_1    | 2023-05-18 20:10:22,554 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-15e7697f-640b-4664-b91f-bd9ea47c523c: Started
dn2_1    | 2023-05-18 20:10:22,596 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 15e7697f-640b-4664-b91f-bd9ea47c523c is started using port 9858 for RATIS
dn2_1    | 2023-05-18 20:10:22,596 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 15e7697f-640b-4664-b91f-bd9ea47c523c is started using port 9857 for RATIS_ADMIN
dn2_1    | 2023-05-18 20:10:22,596 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 15e7697f-640b-4664-b91f-bd9ea47c523c is started using port 9856 for RATIS_SERVER
dn2_1    | 2023-05-18 20:10:22,756 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn2_1    | 2023-05-18 20:10:22,800 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn2_1    | 2023-05-18 20:10:23,123 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn4_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/0f84f49736c9dcc7cfaf75d16080f1fd578598c2 ; compiled by 'runner' on 2023-05-18T19:28Z
dn4_1    | STARTUP_MSG:   java = 11.0.14.1
dn1_1    | 2023-05-18 20:09:55,719 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-05-18 20:09:55,720 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-05-18 20:09:55,721 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn5_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn5_1    | 2023-05-18 20:09:28,087 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn5_1    | /************************************************************
dn5_1    | STARTUP_MSG: Starting HddsDatanodeService
dn5_1    | STARTUP_MSG:   host = cd98f882537f/10.9.0.21
dn5_1    | STARTUP_MSG:   args = []
dn5_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn4_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn4_1    | ************************************************************/
dn4_1    | 2023-05-18 20:09:27,889 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn4_1    | 2023-05-18 20:09:28,286 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn4_1    | 2023-05-18 20:09:29,062 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn4_1    | 2023-05-18 20:09:29,934 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn4_1    | 2023-05-18 20:09:29,934 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn4_1    | 2023-05-18 20:09:30,928 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:5dd7996c298c ip:10.9.0.20
dn4_1    | 2023-05-18 20:09:32,172 [main] INFO reflections.Reflections: Reflections took 968 ms to scan 2 urls, producing 104 keys and 226 values 
dn4_1    | 2023-05-18 20:09:34,948 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
dn4_1    | 2023-05-18 20:09:35,281 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn4_1    | 2023-05-18 20:09:36,284 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 4207 at 2023-05-18T20:09:00.024Z
dn4_1    | 2023-05-18 20:09:36,512 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn4_1    | 2023-05-18 20:09:36,530 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn4_1    | 2023-05-18 20:09:36,533 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn4_1    | 2023-05-18 20:09:36,607 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn4_1    | 2023-05-18 20:09:36,782 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn4_1    | 2023-05-18 20:09:36,860 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-05-18T20:08:59.996Z
dn4_1    | 2023-05-18 20:09:36,869 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn4_1    | 2023-05-18 20:09:36,871 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn4_1    | 2023-05-18 20:09:36,904 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn4_1    | 2023-05-18 20:09:39,876 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/DS-aee401d9-603e-42b4-bfb5-4bf2fed6237f/container.db to cache
dn4_1    | 2023-05-18 20:09:39,881 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/DS-aee401d9-603e-42b4-bfb5-4bf2fed6237f/container.db for volume DS-aee401d9-603e-42b4-bfb5-4bf2fed6237f
dn4_1    | 2023-05-18 20:09:39,999 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn4_1    | 2023-05-18 20:09:40,774 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn4_1    | 2023-05-18 20:09:40,785 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn4_1    | 2023-05-18 20:09:49,688 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn4_1    | 2023-05-18 20:09:50,594 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn4_1    | 2023-05-18 20:09:50,936 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn4_1    | 2023-05-18 20:09:51,604 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn4_1    | 2023-05-18 20:09:51,621 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn4_1    | 2023-05-18 20:09:51,621 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn4_1    | 2023-05-18 20:09:51,631 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn4_1    | 2023-05-18 20:09:51,631 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn4_1    | 2023-05-18 20:09:51,635 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn4_1    | 2023-05-18 20:09:51,636 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn4_1    | 2023-05-18 20:09:51,641 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-18 20:09:51,657 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn4_1    | 2023-05-18 20:09:51,662 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-05-18 20:09:51,771 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-05-18 20:09:51,814 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn4_1    | 2023-05-18 20:09:51,826 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn4_1    | 2023-05-18 20:09:53,693 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn4_1    | 2023-05-18 20:09:53,741 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn4_1    | 2023-05-18 20:09:53,742 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn4_1    | 2023-05-18 20:09:53,758 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-05-18 20:09:53,762 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-05-18 20:09:53,787 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-05-18 20:09:53,834 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServer: 07404b21-602d-47db-8a51-48d80ebd16ac: found a subdirectory /data/metadata/ratis/78d664ca-7989-4249-9a90-24fe3dd84618
dn4_1    | 2023-05-18 20:09:53,930 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServer: 07404b21-602d-47db-8a51-48d80ebd16ac: addNew group-24FE3DD84618:[] returns group-24FE3DD84618:java.util.concurrent.CompletableFuture@7666aa76[Not completed]
dn4_1    | 2023-05-18 20:09:53,930 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServer: 07404b21-602d-47db-8a51-48d80ebd16ac: found a subdirectory /data/metadata/ratis/db288034-f1fd-42b9-aec0-461572f3e524
dn4_1    | 2023-05-18 20:09:53,931 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServer: 07404b21-602d-47db-8a51-48d80ebd16ac: addNew group-461572F3E524:[] returns group-461572F3E524:java.util.concurrent.CompletableFuture@502ae5ab[Not completed]
dn4_1    | 2023-05-18 20:09:53,933 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServer: 07404b21-602d-47db-8a51-48d80ebd16ac: found a subdirectory /data/metadata/ratis/a2756a42-16da-435f-a15a-f31dfb92d20c
dn4_1    | 2023-05-18 20:09:53,944 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServer: 07404b21-602d-47db-8a51-48d80ebd16ac: addNew group-F31DFB92D20C:[] returns group-F31DFB92D20C:java.util.concurrent.CompletableFuture@7eb1e0ba[Not completed]
dn4_1    | 2023-05-18 20:09:54,227 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn4_1    | 2023-05-18 20:09:54,545 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac: new RaftServerImpl for group-24FE3DD84618:[] with ContainerStateMachine:uninitialized
dn4_1    | 2023-05-18 20:09:54,592 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-05-18 20:09:54,615 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-05-18 20:09:54,616 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-05-18 20:09:54,618 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-05-18 20:09:54,705 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-05-18 20:09:54,705 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-05-18 20:09:54,698 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn4_1    | 2023-05-18 20:09:54,878 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-05-18 20:09:54,890 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-05-18 20:09:54,922 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-05-18 20:09:54,926 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-05-18 20:09:55,060 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-05-18 20:09:55,085 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn4_1    | 2023-05-18 20:09:55,233 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-05-18 20:09:55,277 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-05-18 20:09:55,588 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn4_1    | 2023-05-18 20:09:55,777 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn4_1    | 2023-05-18 20:09:56,010 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn4_1    | 2023-05-18 20:09:56,018 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-05-18 20:09:56,083 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-05-18 20:09:56,117 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-05-18 20:09:56,131 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-05-18 20:09:56,135 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-05-18 20:09:56,136 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-05-18 20:09:56,142 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac: new RaftServerImpl for group-461572F3E524:[] with ContainerStateMachine:uninitialized
dn4_1    | 2023-05-18 20:09:56,170 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-05-18 20:09:56,185 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-05-18 20:09:56,185 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-05-18 20:09:56,185 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-05-18 20:09:56,185 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-05-18 20:09:56,185 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-05-18 20:09:56,185 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-05-18 20:09:56,186 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-05-18 20:09:56,187 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-05-18 20:09:56,193 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-05-18 20:09:56,218 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-05-18 20:09:56,218 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn4_1    | 2023-05-18 20:09:56,218 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-05-18 20:09:56,218 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-05-18 20:09:56,218 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn4_1    | 2023-05-18 20:09:56,224 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-05-18 20:09:56,228 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-05-18 20:09:56,228 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-05-18 20:09:56,263 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-05-18 20:09:56,263 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-05-18 20:09:56,267 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-05-18 20:09:56,275 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac: new RaftServerImpl for group-F31DFB92D20C:[] with ContainerStateMachine:uninitialized
dn4_1    | 2023-05-18 20:09:56,329 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-05-18 20:09:56,339 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-05-18 20:09:56,339 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-05-18 20:09:56,339 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-05-18 20:09:56,342 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-05-18 20:09:56,343 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-05-18 20:09:56,343 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-05-18 20:09:56,343 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-05-18 20:09:56,361 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-05-18 20:09:56,361 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-05-18 20:09:56,361 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-05-18 20:09:56,361 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn4_1    | 2023-05-18 20:09:56,361 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-05-18 20:09:55,721 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-05-18 20:09:55,721 [76248598-2f32-4c49-8f2a-b1fddb9f8151-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-05-18 20:09:55,861 [main] INFO util.log: Logging initialized @37751ms to org.eclipse.jetty.util.log.Slf4jLog
dn1_1    | 2023-05-18 20:09:56,554 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn1_1    | 2023-05-18 20:09:56,653 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn1_1    | 2023-05-18 20:09:56,686 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn1_1    | 2023-05-18 20:09:56,700 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn1_1    | 2023-05-18 20:09:56,716 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn1_1    | 2023-05-18 20:09:56,723 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn1_1    | 2023-05-18 20:09:57,009 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn1_1    | 2023-05-18 20:09:57,032 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn1_1    | 2023-05-18 20:09:57,061 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
dn1_1    | 2023-05-18 20:09:57,241 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn1_1    | 2023-05-18 20:09:57,249 [main] INFO server.session: No SessionScavenger set, using defaults
dn1_1    | 2023-05-18 20:09:57,262 [main] INFO server.session: node0 Scavenging every 600000ms
dn1_1    | 2023-05-18 20:09:57,361 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@183fc2fa{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn1_1    | 2023-05-18 20:09:57,362 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@53dd42d6{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn1_1    | 2023-05-18 20:09:58,248 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6cdee57{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-6147152297739966303/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn1_1    | 2023-05-18 20:09:58,348 [main] INFO server.AbstractConnector: Started ServerConnector@50598a1a{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn1_1    | 2023-05-18 20:09:58,354 [main] INFO server.Server: Started @40238ms
dn1_1    | 2023-05-18 20:09:58,378 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn1_1    | 2023-05-18 20:09:58,378 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn1_1    | 2023-05-18 20:09:58,386 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn1_1    | 2023-05-18 20:09:58,468 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn1_1    | 2023-05-18 20:09:58,548 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn1_1    | 2023-05-18 20:09:58,549 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn1_1    | 2023-05-18 20:09:58,658 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7da3b00e] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn1_1    | 2023-05-18 20:09:59,368 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn1_1    | 2023-05-18 20:09:59,561 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn1_1    | 2023-05-18 20:10:02,036 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:02,036 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:02,036 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:02,036 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:03,037 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:03,038 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:03,040 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:04,038 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:04,039 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:04,040 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:05,039 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:05,039 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn5_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/0f84f49736c9dcc7cfaf75d16080f1fd578598c2 ; compiled by 'runner' on 2023-05-18T19:28Z
dn5_1    | STARTUP_MSG:   java = 11.0.14.1
dn2_1    | 2023-05-18 20:10:24,129 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:25,130 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:26,154 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:27,154 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:27,282 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-FollowerState] INFO impl.FollowerState: 15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5089566514ns, electionTimeout:5060ms
dn2_1    | 2023-05-18 20:10:27,284 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-FollowerState] INFO impl.RoleInfo: 15e7697f-640b-4664-b91f-bd9ea47c523c: shutdown 15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-FollowerState
dn2_1    | 2023-05-18 20:10:27,286 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-FollowerState] INFO server.RaftServer$Division: 15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
dn2_1    | 2023-05-18 20:10:27,299 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn2_1    | 2023-05-18 20:10:27,300 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-FollowerState] INFO impl.RoleInfo: 15e7697f-640b-4664-b91f-bd9ea47c523c: start 15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1
dn2_1    | 2023-05-18 20:10:27,325 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1] INFO impl.LeaderElection: 15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[15e7697f-640b-4664-b91f-bd9ea47c523c|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-18 20:10:27,335 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1] INFO impl.LeaderElection: 15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1 PRE_VOTE round 0: result PASSED (term=1)
dn2_1    | 2023-05-18 20:10:27,454 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1] INFO impl.LeaderElection: 15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[15e7697f-640b-4664-b91f-bd9ea47c523c|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-18 20:10:27,455 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1] INFO impl.LeaderElection: 15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1 ELECTION round 0: result PASSED (term=2)
dn2_1    | 2023-05-18 20:10:27,455 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1] INFO impl.RoleInfo: 15e7697f-640b-4664-b91f-bd9ea47c523c: shutdown 15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1
dn2_1    | 2023-05-18 20:10:27,456 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1] INFO server.RaftServer$Division: 15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
dn2_1    | 2023-05-18 20:10:27,459 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8158F807C8F5 with new leaderId: 15e7697f-640b-4664-b91f-bd9ea47c523c
dn2_1    | 2023-05-18 20:10:27,470 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1] INFO server.RaftServer$Division: 15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5: change Leader from null to 15e7697f-640b-4664-b91f-bd9ea47c523c at term 2 for becomeLeader, leader elected after 31751ms
dn2_1    | 2023-05-18 20:10:27,564 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn2_1    | 2023-05-18 20:10:27,595 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-05-18 20:10:27,606 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn2_1    | 2023-05-18 20:10:27,653 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn2_1    | 2023-05-18 20:10:27,655 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn2_1    | 2023-05-18 20:10:27,679 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn2_1    | 2023-05-18 20:10:27,888 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-05-18 20:10:27,947 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn2_1    | 2023-05-18 20:10:28,002 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1] INFO impl.RoleInfo: 15e7697f-640b-4664-b91f-bd9ea47c523c: start 15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderStateImpl
dn2_1    | 2023-05-18 20:10:28,120 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
dn2_1    | 2023-05-18 20:10:28,168 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/f0622044-e126-43df-b394-8158f807c8f5/current/log_inprogress_0 to /data/metadata/ratis/f0622044-e126-43df-b394-8158f807c8f5/current/log_0-0
dn2_1    | 2023-05-18 20:10:28,203 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:28,207 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-LeaderElection1] INFO server.RaftServer$Division: 15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5: set configuration 1: peers:[15e7697f-640b-4664-b91f-bd9ea47c523c|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-18 20:10:05,041 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:06,040 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:06,042 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:07,041 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:07,043 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:07,094 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From a1dc68dc7bff/10.9.0.17 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:44536 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn1_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:44536 remote=recon/10.9.0.22:9891]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn1_1    | 2023-05-18 20:10:08,042 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:08,044 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:09,043 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:09,044 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:10,043 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:10,045 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:10,053 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From a1dc68dc7bff/10.9.0.17 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:53004 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn1_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 2023-05-18 20:10:28,230 [15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 15e7697f-640b-4664-b91f-bd9ea47c523c@group-8158F807C8F5-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/f0622044-e126-43df-b394-8158f807c8f5/current/log_inprogress_1
dn2_1    | 2023-05-18 20:10:29,215 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:30,216 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:31,216 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:32,217 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:33,220 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-18 20:10:44,296 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn2_1    | 2023-05-18 20:11:22,801 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn2_1    | 2023-05-18 20:12:22,801 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn5_1    | ************************************************************/
dn5_1    | 2023-05-18 20:09:28,145 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn5_1    | 2023-05-18 20:09:28,363 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn5_1    | 2023-05-18 20:09:29,111 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn5_1    | 2023-05-18 20:09:30,337 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn5_1    | 2023-05-18 20:09:30,337 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn5_1    | 2023-05-18 20:09:31,735 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:cd98f882537f ip:10.9.0.21
dn5_1    | 2023-05-18 20:09:33,113 [main] INFO reflections.Reflections: Reflections took 1107 ms to scan 2 urls, producing 104 keys and 226 values 
dn5_1    | 2023-05-18 20:09:35,798 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
dn5_1    | 2023-05-18 20:09:36,244 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn5_1    | 2023-05-18 20:09:37,752 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 4096 at 2023-05-18T20:08:59.931Z
dn5_1    | 2023-05-18 20:09:37,910 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn5_1    | 2023-05-18 20:09:37,918 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn5_1    | 2023-05-18 20:09:37,930 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn5_1    | 2023-05-18 20:09:38,115 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn5_1    | 2023-05-18 20:09:38,229 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 2023-05-18 20:09:38,279 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-05-18T20:09:00.064Z
dn5_1    | 2023-05-18 20:09:38,331 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn5_1    | 2023-05-18 20:09:38,331 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn5_1    | 2023-05-18 20:09:38,331 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn5_1    | 2023-05-18 20:09:39,136 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/DS-989d8f12-c20b-4f31-bd63-df3824a4fd48/container.db to cache
dn5_1    | 2023-05-18 20:09:39,141 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/DS-989d8f12-c20b-4f31-bd63-df3824a4fd48/container.db for volume DS-989d8f12-c20b-4f31-bd63-df3824a4fd48
dn5_1    | 2023-05-18 20:09:39,225 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn5_1    | 2023-05-18 20:09:39,247 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn5_1    | 2023-05-18 20:09:39,247 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn5_1    | 2023-05-18 20:09:50,743 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn5_1    | 2023-05-18 20:09:51,400 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 2023-05-18 20:09:52,034 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn5_1    | 2023-05-18 20:09:52,756 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-05-18 20:09:52,775 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn5_1    | 2023-05-18 20:09:52,787 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-05-18 20:09:52,788 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn5_1    | 2023-05-18 20:09:52,802 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn5_1    | 2023-05-18 20:09:52,812 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn5_1    | 2023-05-18 20:09:52,813 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn5_1    | 2023-05-18 20:09:52,840 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-18 20:09:52,855 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn5_1    | 2023-05-18 20:09:52,872 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-05-18 20:09:52,969 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-05-18 20:09:53,045 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn5_1    | 2023-05-18 20:09:53,073 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn5_1    | 2023-05-18 20:09:55,191 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn5_1    | 2023-05-18 20:09:55,243 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn5_1    | 2023-05-18 20:09:55,244 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn5_1    | 2023-05-18 20:09:55,245 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-05-18 20:09:55,253 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-05-18 20:09:55,634 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-05-18 20:09:55,720 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServer: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f: found a subdirectory /data/metadata/ratis/bd9fbce3-479f-4a75-8657-2fbbef77032f
dn5_1    | 2023-05-18 20:09:55,792 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServer: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f: addNew group-2FBBEF77032F:[] returns group-2FBBEF77032F:java.util.concurrent.CompletableFuture@25f8790e[Not completed]
dn5_1    | 2023-05-18 20:09:56,037 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn5_1    | 2023-05-18 20:09:56,443 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-groupManagement] INFO server.RaftServer$Division: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f: new RaftServerImpl for group-2FBBEF77032F:[] with ContainerStateMachine:uninitialized
dn5_1    | 2023-05-18 20:09:56,541 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-05-18 20:09:56,565 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-05-18 20:09:56,565 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-05-18 20:09:56,565 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-05-18 20:09:56,565 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-05-18 20:09:56,565 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-05-18 20:09:56,626 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn5_1    | 2023-05-18 20:09:56,653 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-groupManagement] INFO server.RaftServer$Division: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-05-18 20:09:56,664 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-05-18 20:09:56,725 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-05-18 20:09:56,735 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-05-18 20:09:56,901 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-05-18 20:09:56,974 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn5_1    | 2023-05-18 20:09:57,003 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-05-18 20:09:57,014 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-05-18 20:09:57,325 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn5_1    | 2023-05-18 20:09:57,771 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-05-18 20:09:57,771 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn5_1    | 2023-05-18 20:09:57,838 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-05-18 20:09:57,869 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-05-18 20:09:57,889 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-05-18 20:09:57,905 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-05-18 20:09:57,938 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-05-18 20:09:57,940 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn5_1    | 2023-05-18 20:09:58,206 [main] INFO util.log: Logging initialized @40265ms to org.eclipse.jetty.util.log.Slf4jLog
dn5_1    | 2023-05-18 20:09:59,178 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn5_1    | 2023-05-18 20:09:59,231 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn5_1    | 2023-05-18 20:09:59,292 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn3_1    | 2023-05-18 20:09:55,228 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-05-18 20:09:55,228 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-05-18 20:09:55,228 [caabb77a-6190-403e-a139-2e30ca474b23-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-05-18 20:09:56,363 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn3_1    | 2023-05-18 20:09:56,380 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn3_1    | 2023-05-18 20:09:56,512 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn3_1    | 2023-05-18 20:09:56,528 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn3_1    | 2023-05-18 20:09:56,528 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn3_1    | 2023-05-18 20:09:56,561 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn3_1    | 2023-05-18 20:09:56,853 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn3_1    | 2023-05-18 20:09:56,892 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn3_1    | 2023-05-18 20:09:56,902 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
dn3_1    | 2023-05-18 20:09:57,140 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn3_1    | 2023-05-18 20:09:57,140 [main] INFO server.session: No SessionScavenger set, using defaults
dn3_1    | 2023-05-18 20:09:57,143 [main] INFO server.session: node0 Scavenging every 660000ms
dn3_1    | 2023-05-18 20:09:57,278 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3a11c0eb{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn3_1    | 2023-05-18 20:09:57,296 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1e76afeb{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn3_1    | 2023-05-18 20:09:58,297 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@482f7af0{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-16240821044299357430/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn3_1    | 2023-05-18 20:09:58,378 [main] INFO server.AbstractConnector: Started ServerConnector@14de1901{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn3_1    | 2023-05-18 20:09:58,381 [main] INFO server.Server: Started @42250ms
dn3_1    | 2023-05-18 20:09:58,383 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn3_1    | 2023-05-18 20:09:58,391 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn3_1    | 2023-05-18 20:09:58,392 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn3_1    | 2023-05-18 20:09:58,433 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn3_1    | 2023-05-18 20:09:58,567 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn3_1    | 2023-05-18 20:09:58,588 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn3_1    | 2023-05-18 20:09:58,656 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@b800c51] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn3_1    | 2023-05-18 20:09:59,317 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn3_1    | 2023-05-18 20:09:59,426 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn3_1    | 2023-05-18 20:10:02,029 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:02,029 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:02,033 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:02,039 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:03,030 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:03,031 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:03,041 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:04,032 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:04,032 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:04,042 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:05,033 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:05,033 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:05,048 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:06,035 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:06,048 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:07,036 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:07,049 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:07,083 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From 54c4478f5de9/10.9.0.19 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:39254 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn3_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:39254 remote=recon/10.9.0.22:9891]
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn3_1    | 2023-05-18 20:10:08,036 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:08,050 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:09,037 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:09,051 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:10,038 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:10,051 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:09:56,361 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-05-18 20:09:56,366 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn4_1    | 2023-05-18 20:09:56,367 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-05-18 20:09:56,367 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-05-18 20:09:56,367 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-05-18 20:09:56,367 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-05-18 20:09:56,367 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-05-18 20:09:56,367 [07404b21-602d-47db-8a51-48d80ebd16ac-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-05-18 20:09:56,391 [main] INFO util.log: Logging initialized @38869ms to org.eclipse.jetty.util.log.Slf4jLog
dn4_1    | 2023-05-18 20:09:57,191 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn4_1    | 2023-05-18 20:09:57,269 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn4_1    | 2023-05-18 20:09:57,300 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn4_1    | 2023-05-18 20:09:57,330 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn4_1    | 2023-05-18 20:09:57,341 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn4_1    | 2023-05-18 20:09:57,341 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn4_1    | 2023-05-18 20:09:57,809 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn4_1    | 2023-05-18 20:09:57,828 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn4_1    | 2023-05-18 20:09:57,839 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
dn4_1    | 2023-05-18 20:09:58,080 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn4_1    | 2023-05-18 20:09:58,101 [main] INFO server.session: No SessionScavenger set, using defaults
dn4_1    | 2023-05-18 20:09:58,110 [main] INFO server.session: node0 Scavenging every 600000ms
dn4_1    | 2023-05-18 20:09:58,229 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2c2c3947{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn4_1    | 2023-05-18 20:09:58,265 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3e4d40ea{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn4_1    | 2023-05-18 20:09:59,258 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@251a90ce{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-14922549614412118856/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn4_1    | 2023-05-18 20:09:59,335 [main] INFO server.AbstractConnector: Started ServerConnector@11f23038{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn4_1    | 2023-05-18 20:09:59,335 [main] INFO server.Server: Started @41814ms
dn4_1    | 2023-05-18 20:09:59,366 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn4_1    | 2023-05-18 20:09:59,366 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn4_1    | 2023-05-18 20:09:59,373 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn4_1    | 2023-05-18 20:09:59,423 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn4_1    | 2023-05-18 20:09:59,695 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn4_1    | 2023-05-18 20:09:59,696 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn4_1    | 2023-05-18 20:09:59,799 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@10c5d0ff] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn4_1    | 2023-05-18 20:10:00,269 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn4_1    | 2023-05-18 20:10:00,328 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn4_1    | 2023-05-18 20:10:03,121 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:03,124 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:03,124 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:04,122 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:04,125 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:04,125 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:05,123 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:05,129 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:05,131 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:06,132 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:06,133 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:07,134 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:07,134 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:07,187 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn4_1    | java.net.SocketTimeoutException: Call From 5dd7996c298c/10.9.0.20 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:56542 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn4_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:56542 remote=recon/10.9.0.22:9891]
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn4_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn4_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn4_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn4_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn4_1    | 2023-05-18 20:10:08,134 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:09:59,368 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn5_1    | 2023-05-18 20:09:59,370 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn5_1    | 2023-05-18 20:09:59,371 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn5_1    | 2023-05-18 20:09:59,687 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn5_1    | 2023-05-18 20:09:59,731 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn5_1    | 2023-05-18 20:09:59,732 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
dn5_1    | 2023-05-18 20:09:59,988 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn5_1    | 2023-05-18 20:09:59,988 [main] INFO server.session: No SessionScavenger set, using defaults
dn5_1    | 2023-05-18 20:10:00,006 [main] INFO server.session: node0 Scavenging every 600000ms
dn5_1    | 2023-05-18 20:10:00,034 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1511d157{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn5_1    | 2023-05-18 20:10:00,035 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2bab618{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn5_1    | 2023-05-18 20:10:00,615 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@458031da{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-3090488386375488329/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn5_1    | 2023-05-18 20:10:00,685 [main] INFO server.AbstractConnector: Started ServerConnector@6de7778f{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn5_1    | 2023-05-18 20:10:00,701 [main] INFO server.Server: Started @42761ms
dn5_1    | 2023-05-18 20:10:00,705 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn5_1    | 2023-05-18 20:10:00,711 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn5_1    | 2023-05-18 20:10:00,716 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn5_1    | 2023-05-18 20:10:00,737 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn5_1    | 2023-05-18 20:10:00,904 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn5_1    | 2023-05-18 20:10:00,908 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn5_1    | 2023-05-18 20:10:00,994 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1afb9262] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn5_1    | 2023-05-18 20:10:01,289 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn5_1    | 2023-05-18 20:10:01,339 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn5_1    | 2023-05-18 20:10:04,094 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:04,099 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:04,109 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:04,954 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn5_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:328)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:525)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	... 1 more
dn5_1    | 2023-05-18 20:10:04,955 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn5_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:328)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:525)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn3_1    | 2023-05-18 20:10:10,062 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From 54c4478f5de9/10.9.0.19 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:53328 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn3_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:53328 remote=scm1/10.9.0.14:9861]
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn3_1    | 2023-05-18 20:10:11,039 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:11,052 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:12,040 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:12,053 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:13,041 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:13,053 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:14,054 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:15,055 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:16,056 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:16,057 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn3_1    | java.net.ConnectException: Call From 54c4478f5de9/10.9.0.19 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn3_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1    | 2023-05-18 20:09:28,582 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1    | /************************************************************
om2_1    | STARTUP_MSG: Starting OzoneManager
s3g_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1    | STARTUP_MSG:   host = a6ec66d4f37a/10.9.0.12
om2_1    | STARTUP_MSG:   args = [--upgrade]
om2_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/0f84f49736c9dcc7cfaf75d16080f1fd578598c2 ; compiled by 'runner' on 2023-05-18T19:42Z
om2_1    | STARTUP_MSG:   java = 11.0.14.1
s3g_1    | 2023-05-18 20:09:27,987 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1    | 2023-05-18 20:09:28,004 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:53004 remote=scm1/10.9.0.14:9861]
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om2_1    | ************************************************************/
om2_1    | 2023-05-18 20:09:28,641 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
om3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn4_1    | 2023-05-18 20:10:08,135 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 	... 1 more
s3g_1    | 2023-05-18 20:09:28,236 [main] INFO util.log: Logging initialized @10711ms to org.eclipse.jetty.util.log.Slf4jLog
om2_1    | 2023-05-18 20:09:37,698 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1    | 2023-05-18 20:09:41,387 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om2_1    | 2023-05-18 20:09:42,229 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
recon_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
om1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn4_1    | 2023-05-18 20:10:09,135 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:05,094 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 2023-05-18 20:09:29,682 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1    | 2023-05-18 20:09:29,839 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
scm1_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2_1   | Waiting for the service scm1:9894
recon_1  | 2023-05-18 20:09:28,595 [main] INFO recon.ReconServer: STARTUP_MSG: 
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn4_1    | 2023-05-18 20:10:09,136 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:05,100 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:05,110 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:06,102 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-05-18 20:09:34,854 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm1_1   | /************************************************************
scm2_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
recon_1  | /************************************************************
om3_1    | 2023-05-18 20:09:25,829 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
dn4_1    | 2023-05-18 20:10:10,136 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:06,110 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:06,956 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn5_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
scm1_1   | STARTUP_MSG: Starting StorageContainerManager
scm1_1   | STARTUP_MSG:   host = de2588b9b9b7/10.9.0.14
scm1_1   | STARTUP_MSG:   args = []
om1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1    | 2023-05-18 20:09:29,353 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1    | /************************************************************
recon_1  | STARTUP_MSG: Starting ReconServer
om3_1    | /************************************************************
dn4_1    | 2023-05-18 20:10:10,137 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
scm1_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm1_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
om1_1    | STARTUP_MSG: Starting OzoneManager
om1_1    | STARTUP_MSG:   host = 32984e5f9c2e/10.9.0.11
om1_1    | STARTUP_MSG:   args = [--upgrade]
recon_1  | STARTUP_MSG:   host = cbc88b50144b/10.9.0.22
om3_1    | STARTUP_MSG: Starting OzoneManager
dn4_1    | 2023-05-18 20:10:10,139 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
s3g_1    | 2023-05-18 20:09:29,971 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1    | 2023-05-18 20:09:29,994 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
om2_1    | 2023-05-18 20:09:42,246 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
om2_1    | 2023-05-18 20:09:42,321 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-05-18 20:09:42,803 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = HSYNC (version = 4)
scm1_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/0f84f49736c9dcc7cfaf75d16080f1fd578598c2 ; compiled by 'runner' on 2023-05-18T19:28Z
scm1_1   | STARTUP_MSG:   java = 11.0.14.1
om1_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm2_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2_1   | 2023-05-18 20:10:08,874 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
om3_1    | STARTUP_MSG:   host = 27bf3cdde8a3/10.9.0.13
dn4_1    | java.net.SocketTimeoutException: Call From 5dd7996c298c/10.9.0.20 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:45856 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
s3g_1    | 2023-05-18 20:09:30,027 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1    | 2023-05-18 20:09:30,027 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1    | 2023-05-18 20:09:30,403 [main] INFO http.BaseHttpServer: HTTP server of s3gateway uses base directory /opt/hadoop/ozone_s3g_tmp_base_dir4403586553003160792
s3g_1    | 2023-05-18 20:09:31,537 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1    | /************************************************************
om1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
scm1_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=30m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm2_1   | /************************************************************
scm2_1   | STARTUP_MSG: Starting StorageContainerManager
scm2_1   | STARTUP_MSG:   host = ec06bddf6a82/10.9.0.15
om3_1    | STARTUP_MSG:   args = [--upgrade]
om3_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/0f84f49736c9dcc7cfaf75d16080f1fd578598c2 ; compiled by 'runner' on 2023-05-18T19:42Z
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
scm3_1   | Waiting for the service scm2:9894
om1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/0f84f49736c9dcc7cfaf75d16080f1fd578598c2 ; compiled by 'runner' on 2023-05-18T19:42Z
om1_1    | STARTUP_MSG:   java = 11.0.14.1
scm2_1   | STARTUP_MSG:   args = []
scm2_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1  | STARTUP_MSG:   args = []
om3_1    | STARTUP_MSG:   java = 11.0.14.1
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn1_1    | 2023-05-18 20:10:11,044 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
scm3_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1_1   | ************************************************************/
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om1_1    | ************************************************************/
om1_1    | 2023-05-18 20:09:29,413 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1    | 2023-05-18 20:09:38,172 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
dn1_1    | 2023-05-18 20:10:11,045 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:12,045 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:12,046 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
scm3_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1_1   | 2023-05-18 20:09:35,023 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
dn3_1    | Caused by: java.net.ConnectException: Connection refused
recon_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn1_1    | 2023-05-18 20:10:13,046 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:13,046 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:14,047 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:15,048 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:16,050 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:16,051 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn1_1    | java.net.ConnectException: Call From a1dc68dc7bff/10.9.0.17 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
scm1_1   | 2023-05-18 20:09:36,003 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-05-18 20:09:39,781 [main] INFO reflections.Reflections: Reflections took 3193 ms to scan 3 urls, producing 129 keys and 282 values 
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
om3_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om3_1    | ************************************************************/
om3_1    | 2023-05-18 20:09:25,836 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1    | 2023-05-18 20:09:34,402 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
scm1_1   | 2023-05-18 20:09:41,052 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1_1   | 2023-05-18 20:09:41,173 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm1_1   | 2023-05-18 20:09:41,764 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1:9894 and Ratis port: 9894
recon_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-5.1.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.27.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.27.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.27.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.27.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/guice-5.1.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/guice-servlet-5.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
om3_1    | 2023-05-18 20:09:37,815 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om3_1    | 2023-05-18 20:09:38,077 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1    | 2023-05-18 20:09:38,077 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om3_1    | 2023-05-18 20:09:38,110 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-05-18 20:09:38,307 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = HSYNC (version = 4)
scm1_1   | 2023-05-18 20:09:41,820 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1
scm1_1   | 2023-05-18 20:09:51,159 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-05-18 20:09:53,515 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
om2_1    | 2023-05-18 20:09:46,109 [main] INFO reflections.Reflections: Reflections took 2714 ms to scan 1 urls, producing 131 keys and 384 values [using 2 cores]
om2_1    | 2023-05-18 20:09:46,434 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/0f84f49736c9dcc7cfaf75d16080f1fd578598c2 ; compiled by 'runner' on 2023-05-18T19:42Z
om3_1    | 2023-05-18 20:09:40,927 [main] INFO reflections.Reflections: Reflections took 2091 ms to scan 1 urls, producing 131 keys and 384 values [using 2 cores]
om3_1    | 2023-05-18 20:09:41,379 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-05-18 20:09:42,930 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om3_1    | 2023-05-18 20:09:43,287 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om3_1    | 2023-05-18 20:09:49,848 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27bf3cdde8a3/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
scm1_1   | 2023-05-18 20:09:55,950 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm1_1   | 2023-05-18 20:09:55,971 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
om2_1    | 2023-05-18 20:09:48,597 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om2_1    | 2023-05-18 20:09:48,846 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
recon_1  | STARTUP_MSG:   java = 11.0.14.1
om3_1    | 2023-05-18 20:09:51,855 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27bf3cdde8a3/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
s3g_1    | STARTUP_MSG: Starting Gateway
s3g_1    | STARTUP_MSG:   host = 26264000652b/10.9.0.23
s3g_1    | STARTUP_MSG:   args = []
s3g_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm1_1   | 2023-05-18 20:09:57,159 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1_1   | 2023-05-18 20:09:57,368 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d
om1_1    | 2023-05-18 20:09:40,997 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om1_1    | 2023-05-18 20:09:41,247 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1    | 2023-05-18 20:09:41,269 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
scm1_1   | 2023-05-18 20:09:58,764 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1_1   | 2023-05-18 20:09:59,842 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm1_1   | 2023-05-18 20:09:59,851 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:328)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:525)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
recon_1  | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=30m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.sql.db.auto.commit=true, ozone.recon.sql.db.conn.idle.max.age=3600s, ozone.recon.sql.db.conn.idle.test=SELECT 1, ozone.recon.sql.db.conn.idle.test.period=60s, ozone.recon.sql.db.conn.max.active=5, ozone.recon.sql.db.conn.max.age=1800s, ozone.recon.sql.db.conn.timeout=30000ms, ozone.recon.sql.db.driver=org.apache.derby.jdbc.EmbeddedDriver, ozone.recon.sql.db.jdbc.url=jdbc:derby:/data/metadata/recon/ozone_recon_derby.db, ozone.recon.sql.db.jooq.dialect=DERBY, ozone.recon.task.containercounttask.interval=60s, ozone.recon.task.missingcontainer.interval=300s, ozone.recon.task.pipelinesync.interval=300s, ozone.recon.task.safemode.wait.threshold=300s, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om1_1    | 2023-05-18 20:09:41,282 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-05-18 20:09:59,863 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm1_1   | 2023-05-18 20:09:59,866 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
om2_1    | 2023-05-18 20:09:55,285 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a6ec66d4f37a/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
dn5_1    | Caused by: java.util.concurrent.TimeoutException
s3g_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-shaded-3.1.9.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/cdi-api-2.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
s3g_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/0f84f49736c9dcc7cfaf75d16080f1fd578598c2 ; compiled by 'runner' on 2023-05-18T19:42Z
scm2_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/0f84f49736c9dcc7cfaf75d16080f1fd578598c2 ; compiled by 'runner' on 2023-05-18T19:28Z
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:790)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:363)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
om2_1    | 2023-05-18 20:09:57,287 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a6ec66d4f37a/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
s3g_1    | STARTUP_MSG:   java = 11.0.14.1
s3g_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.basedir=/opt/hadoop/ozone_s3g_tmp_base_dir4403586553003160792, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
s3g_1    | ************************************************************/
s3g_1    | 2023-05-18 20:09:31,591 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
scm2_1   | STARTUP_MSG:   java = 11.0.14.1
om1_1    | 2023-05-18 20:09:41,467 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = HSYNC (version = 4)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
recon_1  | ************************************************************/
recon_1  | 2023-05-18 20:09:28,663 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1  | 2023-05-18 20:09:33,376 [main] INFO reflections.Reflections: Reflections took 609 ms to scan 1 urls, producing 17 keys and 57 values 
om2_1    | 2023-05-18 20:09:59,288 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a6ec66d4f37a/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
s3g_1    | 2023-05-18 20:09:31,714 [main] INFO s3.Gateway: Starting Ozone S3 gateway
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
scm2_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=30m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn3_1    | 	... 12 more
dn3_1    | 2023-05-18 20:10:17,061 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-05-18 20:09:37,615 [main] INFO reflections.Reflections: Reflections took 655 ms to scan 3 urls, producing 129 keys and 282 values 
scm1_1   | 2023-05-18 20:09:59,867 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm1_1   | 2023-05-18 20:09:59,867 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
om2_1    | 2023-05-18 20:10:01,290 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a6ec66d4f37a/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
s3g_1    | 2023-05-18 20:09:32,383 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn1_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 2023-05-18 20:10:18,051 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From 54c4478f5de9/10.9.0.19 to scm2:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:46828 remote=scm2/10.9.0.15:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn4_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
s3g_1    | 2023-05-18 20:09:33,649 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
scm1_1   | 2023-05-18 20:09:59,875 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
om2_1    | 2023-05-18 20:10:03,292 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a6ec66d4f37a/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
s3g_1    | 2023-05-18 20:09:33,649 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
om1_1    | 2023-05-18 20:09:44,006 [main] INFO reflections.Reflections: Reflections took 2271 ms to scan 1 urls, producing 131 keys and 384 values [using 2 cores]
om1_1    | 2023-05-18 20:09:44,355 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-05-18 20:09:46,874 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om1_1    | 2023-05-18 20:09:47,124 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om1_1    | 2023-05-18 20:09:53,498 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 32984e5f9c2e/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
om1_1    | 2023-05-18 20:09:55,499 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 32984e5f9c2e/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
om1_1    | 2023-05-18 20:09:57,506 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 32984e5f9c2e/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
scm1_1   | 2023-05-18 20:09:59,897 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-05-18 20:10:05,294 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a6ec66d4f37a/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
dn5_1    | 	... 1 more
dn5_1    | 2023-05-18 20:10:07,103 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:07,111 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 2023-05-18 20:09:33,907 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1    | 2023-05-18 20:09:33,934 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
s3g_1    | 2023-05-18 20:09:34,208 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1    | 2023-05-18 20:09:34,229 [main] INFO server.session: No SessionScavenger set, using defaults
scm2_1   | ************************************************************/
scm2_1   | 2023-05-18 20:10:08,883 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2_1   | 2023-05-18 20:10:08,926 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn5_1    | 2023-05-18 20:10:08,104 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:08,112 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:08,151 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn5_1    | java.net.SocketTimeoutException: Call From cd98f882537f/10.9.0.21 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:55808 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
om1_1    | 2023-05-18 20:09:59,508 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 32984e5f9c2e/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
s3g_1    | 2023-05-18 20:09:34,232 [main] INFO server.session: node0 Scavenging every 600000ms
om3_1    | 2023-05-18 20:09:53,857 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27bf3cdde8a3/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
om3_1    | 2023-05-18 20:09:55,859 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27bf3cdde8a3/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
om3_1    | 2023-05-18 20:09:57,860 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27bf3cdde8a3/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
om3_1    | 2023-05-18 20:09:59,862 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27bf3cdde8a3/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
scm2_1   | 2023-05-18 20:10:09,170 [main] INFO reflections.Reflections: Reflections took 206 ms to scan 3 urls, producing 129 keys and 282 values 
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
om1_1    | 2023-05-18 20:10:01,510 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 32984e5f9c2e/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
s3g_1    | 2023-05-18 20:09:34,372 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@291a7e3c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1    | 2023-05-18 20:09:34,424 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@21337f7b{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1    | WARNING: An illegal reflective access operation has occurred
s3g_1    | WARNING: Illegal reflective access by com.sun.xml.bind.v2.runtime.reflect.opt.Injector (file:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int)
s3g_1    | WARNING: Please consider reporting this to the maintainers of com.sun.xml.bind.v2.runtime.reflect.opt.Injector
om2_1    | 2023-05-18 20:10:07,717 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d is not the leader. Could not determine the leader node.
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
om1_1    | 2023-05-18 20:10:03,511 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 32984e5f9c2e/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
s3g_1    | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1    | WARNING: All illegal access operations will be denied in a future release
s3g_1    | 2023-05-18 20:09:58,899 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6c6c2a73{s3gateway,/,file:///opt/hadoop/ozone_s3g_tmp_base_dir4403586553003160792/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-17351321570105373200/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1    | 2023-05-18 20:09:58,999 [main] INFO server.AbstractConnector: Started ServerConnector@6d025197{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1    | 2023-05-18 20:09:58,999 [main] INFO server.Server: Started @41473ms
om3_1    | 2023-05-18 20:10:01,866 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27bf3cdde8a3/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
om2_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om2_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om2_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om2_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
s3g_1    | 2023-05-18 20:09:59,017 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1    | 2023-05-18 20:09:59,017 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1    | 2023-05-18 20:09:59,049 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
s3g_1    | 2023-05-18 20:11:15,838 [qtp605052357-21] INFO audit.AuditLogger: Refresh DebugCmdSet for S3GAudit to [].
om1_1    | 2023-05-18 20:10:07,729 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d is not the leader. Could not determine the leader node.
om3_1    | 2023-05-18 20:10:03,867 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27bf3cdde8a3/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om2_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.ConnectException: Connection refused
om1_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om3_1    | 2023-05-18 20:10:05,868 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27bf3cdde8a3/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
om2_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om2_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn5_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
om1_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om3_1    | 2023-05-18 20:10:07,925 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d is not the leader. Could not determine the leader node.
om2_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om2_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om2_1    | , while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
om1_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om3_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om2_1    | 2023-05-18 20:10:09,719 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a6ec66d4f37a/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:790)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:363)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
om1_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om3_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om3_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om3_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
dn1_1    | 	... 12 more
dn1_1    | 2023-05-18 20:10:17,052 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:18,053 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:18,055 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From a1dc68dc7bff/10.9.0.17 to scm2:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:53852 remote=scm2/10.9.0.15:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
om3_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om3_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om2_1    | 2023-05-18 20:10:11,720 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a6ec66d4f37a/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
om2_1    | 2023-05-18 20:10:13,726 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d is not the leader. Could not determine the leader node.
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
s3g_1    | 2023-05-18 20:11:15,864 [qtp605052357-21] INFO audit.AuditLogger: Refresh DebugCmdSet for S3GAudit to [].
s3g_1    | 2023-05-18 20:11:15,870 [qtp605052357-21] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
s3g_1    | 2023-05-18 20:11:15,870 [qtp605052357-21] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
s3g_1    | 2023-05-18 20:11:16,981 [qtp605052357-21] INFO protocolPB.GrpcOmTransport: GrpcOmTransport: started
s3g_1    | 2023-05-18 20:11:17,616 [qtp605052357-21] ERROR protocolPB.GrpcOmTransport: Failed to submit request
s3g_1    | io.grpc.StatusRuntimeException: INTERNAL: org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Suggested leader is OM:om2[om2/10.9.0.12].
om3_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om1_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om2_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om2_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om2_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om3_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2_1   | 2023-05-18 20:10:09,242 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm2_1   | 2023-05-18 20:10:09,248 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm2_1   | 2023-05-18 20:10:09,293 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2:9894 and Ratis port: 9894
om3_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
scm1_1   | 2023-05-18 20:09:59,904 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn5_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:55808 remote=recon/10.9.0.22:9891]
scm2_1   | 2023-05-18 20:10:09,293 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2
scm2_1   | 2023-05-18 20:10:09,888 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2_1   | 2023-05-18 20:10:10,104 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | , while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
scm1_1   | 2023-05-18 20:09:59,906 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
om1_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
om3_1    | 2023-05-18 20:10:09,931 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27bf3cdde8a3/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
om1_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om1_1    | , while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
om1_1    | 2023-05-18 20:10:09,731 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 32984e5f9c2e/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
om3_1    | 2023-05-18 20:10:11,933 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27bf3cdde8a3/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
om1_1    | 2023-05-18 20:10:11,732 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 32984e5f9c2e/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
om1_1    | 2023-05-18 20:10:13,737 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d is not the leader. Could not determine the leader node.
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
s3g_1    | 	at io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:271)
s3g_1    | 	at io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:252)
s3g_1    | 	at io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:165)
s3g_1    | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerServiceGrpc$OzoneManagerServiceBlockingStub.submitRequest(OzoneManagerServiceGrpc.java:182)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
scm1_1   | 2023-05-18 20:10:00,030 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
om3_1    | 2023-05-18 20:10:13,938 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d is not the leader. Could not determine the leader node.
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
om1_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
dn4_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:45856 remote=scm1/10.9.0.14:9861]
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn5_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
scm1_1   | 2023-05-18 20:10:00,048 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om2_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
s3g_1    | 	at org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransport.submitRequest(GrpcOmTransport.java:186)
scm2_1   | 2023-05-18 20:10:10,273 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm2_1   | 2023-05-18 20:10:10,274 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm2_1   | 2023-05-18 20:10:10,435 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn5_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
s3g_1    | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:299)
s3g_1    | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceInfo(OzoneManagerProtocolClientSideTranslatorPB.java:1578)
s3g_1    | 	at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:239)
scm2_1   | 2023-05-18 20:10:10,459 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:78aeffd2-0882-43d0-ab48-78e394e586e6
dn5_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn5_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
om1_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
s3g_1    | 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:248)
s3g_1    | 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:115)
s3g_1    | 	at org.apache.hadoop.ozone.s3.OzoneClientCache.<init>(OzoneClientCache.java:83)
scm2_1   | 2023-05-18 20:10:10,512 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm2_1   | 2023-05-18 20:10:10,592 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm2_1   | 2023-05-18 20:10:10,593 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
scm3_1   | 2023-05-18 20:10:15,800 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
scm1_1   | 2023-05-18 20:10:00,077 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om3_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1  | 2023-05-18 20:09:38,214 [main] INFO recon.ReconServer: Initializing Recon server...
dn5_1    | 2023-05-18 20:10:09,105 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:09,113 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-05-18 20:10:10,594 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
s3g_1    | 	at org.apache.hadoop.ozone.s3.OzoneClientCache.getOzoneClientInstance(OzoneClientCache.java:98)
s3g_1    | 	at org.apache.hadoop.ozone.s3.OzoneClientProducer.getClient(OzoneClientProducer.java:121)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
om3_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om3_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om3_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm2_1   | 2023-05-18 20:10:10,594 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm2_1   | 2023-05-18 20:10:10,594 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm2_1   | 2023-05-18 20:10:10,594 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
om1_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   | 2023-05-18 20:10:01,700 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm1_1   | 2023-05-18 20:10:01,709 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn4_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn4_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
om2_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
s3g_1    | 	at org.apache.hadoop.ozone.s3.OzoneClientProducer.createClient(OzoneClientProducer.java:71)
scm2_1   | 2023-05-18 20:10:10,595 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm3_1   | /************************************************************
om1_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | 2023-05-18 20:10:01,719 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm1_1   | 2023-05-18 20:10:01,722 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
dn4_1    | 2023-05-18 20:10:11,137 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:11,138 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om2_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
s3g_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm2_1   | 2023-05-18 20:10:10,596 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | STARTUP_MSG: Starting StorageContainerManager
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:53852 remote=scm2/10.9.0.15:9861]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om3_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om2_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om2_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm2_1   | 2023-05-18 20:10:10,597 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm3_1   | STARTUP_MSG:   host = 989cc3f1e886/10.9.0.16
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
scm1_1   | 2023-05-18 20:10:01,725 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | , while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
om2_1    | 2023-05-18 20:10:16,016 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:78aeffd2-0882-43d0-ab48-78e394e586e6 is not the leader. Could not determine the leader node.
scm2_1   | 2023-05-18 20:10:10,597 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm3_1   | STARTUP_MSG:   args = []
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om3_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om3_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn5_1    | 2023-05-18 20:10:10,105 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
scm1_1   | 2023-05-18 20:10:01,757 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
om2_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
s3g_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm3_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm2_1   | 2023-05-18 20:10:10,606 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om3_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om3_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om3_1    | , while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 15.
dn5_1    | java.net.SocketTimeoutException: Call From cd98f882537f/10.9.0.21 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:39034 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
scm1_1   | 2023-05-18 20:10:01,778 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServer: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d: found a subdirectory /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f
om2_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
s3g_1    | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
om1_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
scm2_1   | 2023-05-18 20:10:10,609 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm2_1   | 2023-05-18 20:10:10,609 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om3_1    | 2023-05-18 20:10:16,000 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:78aeffd2-0882-43d0-ab48-78e394e586e6 is not the leader. Could not determine the leader node.
om3_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om3_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om3_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om3_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn1_1    | 2023-05-18 20:10:18,174 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
om2_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
s3g_1    | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
scm3_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm3_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/0f84f49736c9dcc7cfaf75d16080f1fd578598c2 ; compiled by 'runner' on 2023-05-18T19:28Z
scm3_1   | STARTUP_MSG:   java = 11.0.14.1
scm3_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=30m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm3_1   | ************************************************************/
scm3_1   | 2023-05-18 20:10:15,808 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1_1   | 2023-05-18 20:10:01,797 [main] INFO server.RaftServer: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d: addNew group-E2B0BF76E94F:[] returns group-E2B0BF76E94F:java.util.concurrent.CompletableFuture@3baf6936[Not completed]
dn1_1    | 2023-05-18 20:10:18,193 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn1_1    | 2023-05-18 20:10:18,564 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
s3g_1    | 	at org.jboss.weld.injection.StaticMethodInjectionPoint.invoke(StaticMethodInjectionPoint.java:95)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm2_1   | 2023-05-18 20:10:10,826 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
recon_1  | 2023-05-18 20:09:38,231 [main] INFO impl.ReconDBProvider: Last known Recon DB : /data/metadata/recon/recon-container-key.db_1684440377430
recon_1  | 2023-05-18 20:09:40,045 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
scm3_1   | 2023-05-18 20:10:15,890 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-05-18 20:10:01,955 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-groupManagement] INFO ha.SCMStateMachine: Updated lastAppliedTermIndex 2#46 with transactionInfo term andIndex
om2_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
dn1_1    | 2023-05-18 20:10:18,565 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 76248598-2f32-4c49-8f2a-b1fddb9f8151
dn1_1    | 2023-05-18 20:10:18,659 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/78d664ca-7989-4249-9a90-24fe3dd84618/in_use.lock acquired by nodename 7@a1dc68dc7bff
om1_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 2023-05-18 20:10:16,167 [main] INFO reflections.Reflections: Reflections took 219 ms to scan 3 urls, producing 129 keys and 282 values 
scm1_1   | 2023-05-18 20:10:01,959 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-groupManagement] INFO server.RaftServer$Division: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d: new RaftServerImpl for group-E2B0BF76E94F:[] with SCMStateMachine:uninitialized
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
dn1_1    | 2023-05-18 20:10:18,683 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=caabb77a-6190-403e-a139-2e30ca474b23} from /data/metadata/ratis/78d664ca-7989-4249-9a90-24fe3dd84618/current/raft-meta
dn1_1    | 2023-05-18 20:10:18,708 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/467c8493-9e44-42c8-850d-8edf1c67e99f/in_use.lock acquired by nodename 7@a1dc68dc7bff
dn1_1    | 2023-05-18 20:10:18,723 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=76248598-2f32-4c49-8f2a-b1fddb9f8151} from /data/metadata/ratis/467c8493-9e44-42c8-850d-8edf1c67e99f/current/raft-meta
om1_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 2023-05-18 20:10:16,225 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1_1   | 2023-05-18 20:10:01,969 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
dn1_1    | 2023-05-18 20:10:18,738 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a2756a42-16da-435f-a15a-f31dfb92d20c/in_use.lock acquired by nodename 7@a1dc68dc7bff
s3g_1    | 	at org.jboss.weld.injection.StaticMethodInjectionPoint.invoke(StaticMethodInjectionPoint.java:85)
om1_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 2023-05-18 20:10:16,229 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om2_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
dn1_1    | 2023-05-18 20:10:18,739 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=76248598-2f32-4c49-8f2a-b1fddb9f8151} from /data/metadata/ratis/a2756a42-16da-435f-a15a-f31dfb92d20c/current/raft-meta
s3g_1    | 	at org.jboss.weld.injection.producer.ProducerMethodProducer.produce(ProducerMethodProducer.java:103)
om1_1    | , while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
om1_1    | 2023-05-18 20:10:16,004 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:78aeffd2-0882-43d0-ab48-78e394e586e6 is not the leader. Could not determine the leader node.
om1_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om1_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
scm3_1   | 2023-05-18 20:10:16,258 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3:9894 and Ratis port: 9894
scm1_1   | 2023-05-18 20:10:01,979 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1_1   | 2023-05-18 20:10:01,980 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om2_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
s3g_1    | 	at org.jboss.weld.injection.producer.AbstractMemberProducer.produce(AbstractMemberProducer.java:161)
om1_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om1_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
scm1_1   | 2023-05-18 20:10:01,980 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om2_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om2_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
s3g_1    | 	at org.jboss.weld.bean.AbstractProducerBean.create(AbstractProducerBean.java:180)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om1_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
recon_1  | 2023-05-18 20:09:45,747 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1  | WARNING: An illegal reflective access operation has occurred
scm3_1   | 2023-05-18 20:10:16,258 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3
dn1_1    | 2023-05-18 20:10:18,867 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C: set configuration 0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-18 20:10:01,982 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm2_1   | 2023-05-18 20:10:10,831 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
s3g_1    | 	at org.jboss.weld.contexts.unbound.DependentContextImpl.get(DependentContextImpl.java:64)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
scm3_1   | 2023-05-18 20:10:17,109 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-05-18 20:10:01,982 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om2_1    | , while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
om2_1    | 2023-05-18 20:10:18,023 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a6ec66d4f37a/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
s3g_1    | 	at org.jboss.weld.bean.ContextualInstanceStrategy$DefaultContextualInstanceStrategy.get(ContextualInstanceStrategy.java:100)
s3g_1    | 	at org.jboss.weld.bean.ContextualInstance.get(ContextualInstance.java:50)
s3g_1    | 	at org.jboss.weld.manager.BeanManagerImpl.getReference(BeanManagerImpl.java:694)
s3g_1    | 	at org.jboss.weld.manager.BeanManagerImpl.getInjectableReference(BeanManagerImpl.java:794)
s3g_1    | 	at org.jboss.weld.injection.FieldInjectionPoint.inject(FieldInjectionPoint.java:92)
dn4_1    | 2023-05-18 20:10:12,138 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:12,139 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-05-18 20:10:17,602 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-05-18 20:10:02,032 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-groupManagement] INFO server.RaftServer$Division: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
om3_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn4_1    | 2023-05-18 20:10:13,139 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:13,140 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-05-18 20:10:17,924 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm1_1   | 2023-05-18 20:10:02,067 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1_1   | 2023-05-18 20:10:02,106 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
recon_1  | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om1_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn3_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn5_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn4_1    | 2023-05-18 20:10:14,141 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:15,147 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:16,148 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-05-18 20:10:17,928 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm1_1   | 2023-05-18 20:10:02,106 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om1_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
om2_1    | 2023-05-18 20:10:24,865 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 2023-05-18 20:10:17,149 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:17,150 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
scm3_1   | 2023-05-18 20:10:18,153 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1_1   | 2023-05-18 20:10:02,190 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1_1   | 2023-05-18 20:10:02,229 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm2_1   | 2023-05-18 20:10:10,833 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om1_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
om2_1    | 2023-05-18 20:10:25,062 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn4_1    | java.net.ConnectException: Call From 5dd7996c298c/10.9.0.20 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
scm3_1   | 2023-05-18 20:10:18,275 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:2620a8e2-fcef-44ea-bfea-c3742904e700
scm1_1   | 2023-05-18 20:10:02,250 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1_1   | 2023-05-18 20:10:02,252 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm2_1   | 2023-05-18 20:10:10,833 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
om1_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1  | WARNING: All illegal access operations will be denied in a future release
recon_1  | 2023-05-18 20:09:48,621 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
scm3_1   | 2023-05-18 20:10:18,665 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1_1   | 2023-05-18 20:10:02,387 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm1_1   | 2023-05-18 20:10:02,774 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2_1   | 2023-05-18 20:10:10,834 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | , while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
om1_1    | 2023-05-18 20:10:18,008 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 32984e5f9c2e/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
om1_1    | 2023-05-18 20:10:24,495 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om1_1    | 2023-05-18 20:10:24,677 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
scm3_1   | 2023-05-18 20:10:19,048 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm1_1   | 2023-05-18 20:10:02,783 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2_1   | 2023-05-18 20:10:10,838 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
om1_1    | 2023-05-18 20:10:26,438 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om1_1    | 2023-05-18 20:10:26,441 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om1_1    | 2023-05-18 20:10:27,843 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om1_1    | 2023-05-18 20:10:28,096 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om2_1    | 2023-05-18 20:10:26,141 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm2_1   | 2023-05-18 20:10:10,842 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServer: 78aeffd2-0882-43d0-ab48-78e394e586e6: found a subdirectory /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f
scm2_1   | 2023-05-18 20:10:10,849 [main] INFO server.RaftServer: 78aeffd2-0882-43d0-ab48-78e394e586e6: addNew group-E2B0BF76E94F:[] returns group-E2B0BF76E94F:java.util.concurrent.CompletableFuture@37d871c2[Not completed]
scm2_1   | 2023-05-18 20:10:10,871 [78aeffd2-0882-43d0-ab48-78e394e586e6-groupManagement] INFO ha.SCMStateMachine: Updated lastAppliedTermIndex 2#46 with transactionInfo term andIndex
recon_1  | 2023-05-18 20:09:48,634 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1  | 2023-05-18 20:09:48,652 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1  | 2023-05-18 20:09:51,938 [main] INFO codegen.SqlDbUtils: RECON_TASK_STATUS table already exists, skipping creation.
recon_1  | 2023-05-18 20:09:52,142 [main] INFO codegen.SqlDbUtils: GLOBAL_STATS table already exists, skipping creation.
recon_1  | 2023-05-18 20:09:52,307 [main] INFO codegen.SqlDbUtils: UNHEALTHY_CONTAINERS table already exists, skipping creation.
s3g_1    | 	at org.jboss.weld.util.Beans.injectBoundFields(Beans.java:345)
s3g_1    | 	at org.jboss.weld.util.Beans.injectFieldsAndInitializers(Beans.java:356)
s3g_1    | 	at org.jboss.weld.injection.producer.ResourceInjector$1.proceed(ResourceInjector.java:69)
scm2_1   | 2023-05-18 20:10:10,873 [78aeffd2-0882-43d0-ab48-78e394e586e6-groupManagement] INFO server.RaftServer$Division: 78aeffd2-0882-43d0-ab48-78e394e586e6: new RaftServerImpl for group-E2B0BF76E94F:[] with SCMStateMachine:uninitialized
scm2_1   | 2023-05-18 20:10:10,874 [78aeffd2-0882-43d0-ab48-78e394e586e6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm2_1   | 2023-05-18 20:10:10,875 [78aeffd2-0882-43d0-ab48-78e394e586e6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
recon_1  | 2023-05-18 20:09:52,697 [main] INFO codegen.SqlDbUtils: FILE_COUNT_BY_SIZE table already exists, skipping creation.
om2_1    | 2023-05-18 20:10:26,169 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om1_1    | 2023-05-18 20:10:28,114 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2_1   | 2023-05-18 20:10:10,875 [78aeffd2-0882-43d0-ab48-78e394e586e6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm2_1   | 2023-05-18 20:10:10,876 [78aeffd2-0882-43d0-ab48-78e394e586e6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2_1   | 2023-05-18 20:10:10,876 [78aeffd2-0882-43d0-ab48-78e394e586e6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1  | 2023-05-18 20:09:52,860 [main] INFO codegen.SqlDbUtils: CLUSTER_GROWTH_DAILY table already exists, skipping creation.
om2_1    | 2023-05-18 20:10:27,409 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:39034 remote=scm1/10.9.0.14:9861]
dn1_1    | 2023-05-18 20:10:18,862 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618: set configuration 0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-18 20:10:18,907 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F: set configuration 0: peers:[76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
s3g_1    | 	at org.jboss.weld.injection.InjectionContextImpl.run(InjectionContextImpl.java:48)
scm2_1   | 2023-05-18 20:10:10,876 [78aeffd2-0882-43d0-ab48-78e394e586e6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm2_1   | 2023-05-18 20:10:10,885 [78aeffd2-0882-43d0-ab48-78e394e586e6-groupManagement] INFO server.RaftServer$Division: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm2_1   | 2023-05-18 20:10:10,885 [78aeffd2-0882-43d0-ab48-78e394e586e6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
recon_1  | 2023-05-18 20:09:53,753 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
om2_1    | 2023-05-18 20:10:27,618 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
om3_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om3_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om3_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om3_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
s3g_1    | 	at org.jboss.weld.injection.producer.ResourceInjector.inject(ResourceInjector.java:71)
scm2_1   | 2023-05-18 20:10:10,889 [78aeffd2-0882-43d0-ab48-78e394e586e6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm3_1   | 2023-05-18 20:10:19,052 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm3_1   | 2023-05-18 20:10:19,056 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
recon_1  | 2023-05-18 20:09:53,786 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
om2_1    | 2023-05-18 20:10:27,630 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-05-18 20:10:28,303 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om1_1    | 2023-05-18 20:10:28,322 [main] WARN utils.NativeLibraryLoader: Unable to load library: ozone_rocksdb_tools
om1_1    | java.io.IOException: Permission denied
om1_1    | 	at java.base/java.io.UnixFileSystem.createFileExclusively(Native Method)
om1_1    | 	at java.base/java.io.File.createTempFile(File.java:2129)
om3_1    | , while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 16 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 16.
scm1_1   | 2023-05-18 20:10:02,787 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm3_1   | 2023-05-18 20:10:19,058 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm3_1   | 2023-05-18 20:10:19,058 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om2_1    | 2023-05-18 20:10:27,835 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn5_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
om3_1    | 2023-05-18 20:10:18,003 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27bf3cdde8a3/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 17 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 17.
om3_1    | 2023-05-18 20:10:25,004 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
scm1_1   | 2023-05-18 20:10:02,788 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm3_1   | 2023-05-18 20:10:19,058 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
recon_1  | 2023-05-18 20:09:53,846 [main] INFO util.log: Logging initialized @36239ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1  | 2023-05-18 20:09:54,395 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om2_1    | 2023-05-18 20:10:27,851 [main] WARN utils.NativeLibraryLoader: Unable to load library: ozone_rocksdb_tools
dn1_1    | 2023-05-18 20:10:18,944 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO ratis.ContainerStateMachine: group-8EDF1C67E99F: Setting the last applied index to (t:1, i:0)
dn1_1    | 2023-05-18 20:10:18,951 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO ratis.ContainerStateMachine: group-F31DFB92D20C: Setting the last applied index to (t:3, i:12)
dn1_1    | 2023-05-18 20:10:18,958 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO ratis.ContainerStateMachine: group-24FE3DD84618: Setting the last applied index to (t:1, i:0)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
s3g_1    | 	at org.jboss.weld.injection.producer.BasicInjectionTarget.inject(BasicInjectionTarget.java:117)
s3g_1    | 	at org.glassfish.jersey.ext.cdi1x.internal.CdiComponentProvider$InjectionManagerInjectedCdiTarget.inject(CdiComponentProvider.java:665)
scm1_1   | 2023-05-18 20:10:02,790 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm3_1   | 2023-05-18 20:10:19,067 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
recon_1  | 2023-05-18 20:09:54,429 [main] INFO http.HttpRequestLog: Http request log for http.requests.recon is not defined
recon_1  | 2023-05-18 20:09:54,478 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om2_1    | java.io.IOException: Permission denied
om3_1    | 2023-05-18 20:10:25,231 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-05-18 20:10:26,712 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om3_1    | 2023-05-18 20:10:26,730 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn4_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
recon_1  | 2023-05-18 20:09:54,479 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1    | 	at java.base/java.io.UnixFileSystem.createFileExclusively(Native Method)
dn5_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 2023-05-18 20:10:19,055 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 	at org.apache.hadoop.hdds.utils.NativeLibraryLoader.copyResourceFromJarToTemp(NativeLibraryLoader.java:138)
om1_1    | 	at org.apache.hadoop.hdds.utils.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:114)
scm1_1   | 2023-05-18 20:10:02,791 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm3_1   | 2023-05-18 20:10:19,072 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1  | 2023-05-18 20:09:54,479 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1  | 2023-05-18 20:09:54,486 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om2_1    | 	at java.base/java.io.File.createTempFile(File.java:2129)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn5_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn5_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
s3g_1    | 	at org.jboss.weld.bean.ManagedBean.create(ManagedBean.java:161)
s3g_1    | 	at org.jboss.weld.contexts.unbound.DependentContextImpl.get(DependentContextImpl.java:64)
s3g_1    | 	at org.jboss.weld.bean.ContextualInstanceStrategy$DefaultContextualInstanceStrategy.get(ContextualInstanceStrategy.java:100)
scm1_1   | 2023-05-18 20:10:02,793 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm3_1   | 2023-05-18 20:10:19,078 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
recon_1  | 2023-05-18 20:09:54,790 [main] INFO http.BaseHttpServer: HTTP server of recon uses base directory /data/metadata/webserver
recon_1  | 2023-05-18 20:09:54,811 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
om2_1    | 	at org.apache.hadoop.hdds.utils.NativeLibraryLoader.copyResourceFromJarToTemp(NativeLibraryLoader.java:138)
om1_1    | 	at org.apache.hadoop.ozone.om.snapshot.SnapshotDiffManager.initSSTDumpTool(SnapshotDiffManager.java:253)
om1_1    | 	at org.apache.hadoop.ozone.om.snapshot.SnapshotDiffManager.<init>(SnapshotDiffManager.java:232)
om1_1    | 	at org.apache.hadoop.ozone.om.OmSnapshotManager.<init>(OmSnapshotManager.java:262)
dn1_1    | 2023-05-18 20:10:19,608 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
scm1_1   | 2023-05-18 20:10:02,794 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm3_1   | 2023-05-18 20:10:19,081 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
recon_1  | 2023-05-18 20:09:55,063 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1  | 2023-05-18 20:09:55,087 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
om2_1    | 	at org.apache.hadoop.hdds.utils.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:114)
s3g_1    | 	at org.jboss.weld.bean.ContextualInstance.get(ContextualInstance.java:50)
s3g_1    | 	at org.jboss.weld.manager.BeanManagerImpl.getReference(BeanManagerImpl.java:694)
dn1_1    | 2023-05-18 20:10:19,609 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm2_1   | 2023-05-18 20:10:10,890 [78aeffd2-0882-43d0-ab48-78e394e586e6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm1_1   | 2023-05-18 20:10:02,794 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm1_1   | 2023-05-18 20:10:02,864 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
scm1_1   | 2023-05-18 20:10:03,028 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1  | 2023-05-18 20:09:55,112 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1  | 2023-05-18 20:09:55,373 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om2_1    | 	at org.apache.hadoop.ozone.om.snapshot.SnapshotDiffManager.initSSTDumpTool(SnapshotDiffManager.java:253)
om1_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.instantiateServices(OzoneManager.java:817)
om1_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:652)
om1_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:737)
scm2_1   | 2023-05-18 20:10:10,905 [78aeffd2-0882-43d0-ab48-78e394e586e6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.net.ConnectException: Connection refused
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 2023-05-18 20:09:55,373 [main] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
om2_1    | 	at org.apache.hadoop.ozone.om.snapshot.SnapshotDiffManager.<init>(SnapshotDiffManager.java:232)
om1_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.startAndCancelPrepare(OzoneManagerStarter.java:231)
om1_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.startOmUpgrade(OzoneManagerStarter.java:121)
om1_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm2_1   | 2023-05-18 20:10:10,910 [78aeffd2-0882-43d0-ab48-78e394e586e6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
om3_1    | 2023-05-18 20:10:28,504 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om3_1    | 2023-05-18 20:10:28,625 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-05-18 20:09:58,177 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 	at org.apache.hadoop.ozone.om.OmSnapshotManager.<init>(OmSnapshotManager.java:262)
dn1_1    | 2023-05-18 20:10:19,637 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-05-18 20:10:19,639 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-05-18 20:10:19,640 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-05-18 20:10:10,913 [78aeffd2-0882-43d0-ab48-78e394e586e6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn5_1    | 2023-05-18 20:10:10,107 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:10,113 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:46828 remote=scm2/10.9.0.15:9861]
recon_1  | 2023-05-18 20:09:58,742 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-05-18 20:09:59,023 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
dn1_1    | 2023-05-18 20:10:19,640 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-05-18 20:10:19,641 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-05-18 20:10:19,641 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm2_1   | 2023-05-18 20:10:10,914 [78aeffd2-0882-43d0-ab48-78e394e586e6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-05-18 20:10:11,107 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:11,115 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:12,108 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:12,115 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
recon_1  | 2023-05-18 20:09:59,051 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
om2_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.instantiateServices(OzoneManager.java:817)
dn1_1    | 2023-05-18 20:10:19,642 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
s3g_1    | 	at org.jboss.weld.manager.BeanManagerImpl.getReference(BeanManagerImpl.java:717)
s3g_1    | 	at org.jboss.weld.util.ForwardingBeanManager.getReference(ForwardingBeanManager.java:64)
s3g_1    | 	at org.jboss.weld.bean.builtin.BeanManagerProxy.getReference(BeanManagerProxy.java:87)
scm2_1   | 2023-05-18 20:10:10,936 [78aeffd2-0882-43d0-ab48-78e394e586e6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om3_1    | 2023-05-18 20:10:28,641 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-05-18 20:10:28,740 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om3_1    | 2023-05-18 20:10:28,746 [main] WARN utils.NativeLibraryLoader: Unable to load library: ozone_rocksdb_tools
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
recon_1  | 2023-05-18 20:09:59,389 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-05-18 20:09:59,539 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
dn1_1    | 2023-05-18 20:10:19,645 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
s3g_1    | 	at org.glassfish.jersey.ext.cdi1x.internal.CdiUtil.getBeanReference(CdiUtil.java:129)
s3g_1    | 	at org.glassfish.jersey.ext.cdi1x.internal.AbstractCdiBeanSupplier$1.getInstance(AbstractCdiBeanSupplier.java:72)
s3g_1    | 	at org.glassfish.jersey.ext.cdi1x.internal.AbstractCdiBeanSupplier._provide(AbstractCdiBeanSupplier.java:112)
s3g_1    | 	at org.glassfish.jersey.ext.cdi1x.internal.RequestScopedCdiBeanSupplier.get(RequestScopedCdiBeanSupplier.java:46)
scm2_1   | 2023-05-18 20:10:11,064 [78aeffd2-0882-43d0-ab48-78e394e586e6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm3_1   | 2023-05-18 20:10:19,150 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm3_1   | 2023-05-18 20:10:19,178 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
recon_1  | 2023-05-18 20:09:59,694 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1  | 2023-05-18 20:09:59,882 [main] INFO node.SCMNodeManager: Entering startup safe mode.
dn1_1    | 2023-05-18 20:10:19,644 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-05-18 20:10:19,673 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-05-18 20:10:19,673 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-05-18 20:10:19,673 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-05-18 20:10:19,674 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm2_1   | 2023-05-18 20:10:11,067 [78aeffd2-0882-43d0-ab48-78e394e586e6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3_1   | 2023-05-18 20:10:19,183 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm3_1   | 2023-05-18 20:10:21,452 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
om2_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:652)
recon_1  | 2023-05-18 20:09:59,962 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/07404b21-602d-47db-8a51-48d80ebd16ac
dn1_1    | 2023-05-18 20:10:19,691 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
om3_1    | java.io.IOException: Permission denied
om3_1    | 	at java.base/java.io.UnixFileSystem.createFileExclusively(Native Method)
om3_1    | 	at java.base/java.io.File.createTempFile(File.java:2129)
om3_1    | 	at org.apache.hadoop.hdds.utils.NativeLibraryLoader.copyResourceFromJarToTemp(NativeLibraryLoader.java:138)
scm2_1   | 2023-05-18 20:10:11,068 [78aeffd2-0882-43d0-ab48-78e394e586e6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm3_1   | 2023-05-18 20:10:21,523 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm3_1   | 2023-05-18 20:10:21,555 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
om2_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:737)
recon_1  | 2023-05-18 20:09:59,963 [main] INFO node.SCMNodeManager: Registered Data node : 07404b21-602d-47db-8a51-48d80ebd16ac{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
dn1_1    | 2023-05-18 20:10:19,692 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
om3_1    | 	at org.apache.hadoop.hdds.utils.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:114)
om3_1    | 	at org.apache.hadoop.ozone.om.snapshot.SnapshotDiffManager.initSSTDumpTool(SnapshotDiffManager.java:253)
om3_1    | 	at org.apache.hadoop.ozone.om.snapshot.SnapshotDiffManager.<init>(SnapshotDiffManager.java:232)
om3_1    | 	at org.apache.hadoop.ozone.om.OmSnapshotManager.<init>(OmSnapshotManager.java:262)
scm2_1   | 2023-05-18 20:10:11,069 [78aeffd2-0882-43d0-ab48-78e394e586e6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm3_1   | 2023-05-18 20:10:21,559 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
scm1_1   | 2023-05-18 20:10:03,209 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1  | 2023-05-18 20:09:59,970 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/15e7697f-640b-4664-b91f-bd9ea47c523c
dn1_1    | 2023-05-18 20:10:19,692 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
om3_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.instantiateServices(OzoneManager.java:817)
om3_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:652)
om3_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:737)
om3_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.startAndCancelPrepare(OzoneManagerStarter.java:231)
scm2_1   | 2023-05-18 20:10:11,069 [78aeffd2-0882-43d0-ab48-78e394e586e6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm3_1   | 2023-05-18 20:10:21,569 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3_1   | 2023-05-18 20:10:21,586 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
om2_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.startAndCancelPrepare(OzoneManagerStarter.java:231)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
recon_1  | 2023-05-18 20:09:59,970 [main] INFO node.SCMNodeManager: Registered Data node : 15e7697f-640b-4664-b91f-bd9ea47c523c{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
dn1_1    | 2023-05-18 20:10:19,721 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om1_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
om1_1    | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
om1_1    | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
om1_1    | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
scm2_1   | 2023-05-18 20:10:11,070 [78aeffd2-0882-43d0-ab48-78e394e586e6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm3_1   | 2023-05-18 20:10:21,752 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServer: 2620a8e2-fcef-44ea-bfea-c3742904e700: found a subdirectory /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f
scm3_1   | 2023-05-18 20:10:21,820 [main] INFO server.RaftServer: 2620a8e2-fcef-44ea-bfea-c3742904e700: addNew group-E2B0BF76E94F:[] returns group-E2B0BF76E94F:java.util.concurrent.CompletableFuture@37d871c2[Not completed]
om2_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.startOmUpgrade(OzoneManagerStarter.java:121)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
recon_1  | 2023-05-18 20:09:59,977 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/76248598-2f32-4c49-8f2a-b1fddb9f8151
dn1_1    | 2023-05-18 20:10:19,724 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om1_1    | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
om1_1    | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
om1_1    | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
om1_1    | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm2_1   | 2023-05-18 20:10:11,071 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm3_1   | 2023-05-18 20:10:22,111 [2620a8e2-fcef-44ea-bfea-c3742904e700-groupManagement] INFO ha.SCMStateMachine: Updated lastAppliedTermIndex 2#46 with transactionInfo term andIndex
dn4_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
recon_1  | 2023-05-18 20:09:59,977 [main] INFO node.SCMNodeManager: Registered Data node : 76248598-2f32-4c49-8f2a-b1fddb9f8151{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
dn1_1    | 2023-05-18 20:10:19,731 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1    | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
om1_1    | 	at picocli.CommandLine.execute(CommandLine.java:2078)
om1_1    | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
om1_1    | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm2_1   | 2023-05-18 20:10:11,072 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm3_1   | 2023-05-18 20:10:22,125 [2620a8e2-fcef-44ea-bfea-c3742904e700-groupManagement] INFO server.RaftServer$Division: 2620a8e2-fcef-44ea-bfea-c3742904e700: new RaftServerImpl for group-E2B0BF76E94F:[] with SCMStateMachine:uninitialized
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:790)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:363)
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
om2_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
om2_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
recon_1  | 2023-05-18 20:09:59,977 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/9cd97d2e-fca5-4ac1-8a4f-be6d224f499f
dn1_1    | 2023-05-18 20:10:19,731 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:58)
s3g_1    | 	at org.glassfish.jersey.inject.hk2.InstanceSupplierFactoryBridge.provide(InstanceSupplierFactoryBridge.java:53)
s3g_1    | 	at org.jvnet.hk2.internal.FactoryCreator.create(FactoryCreator.java:129)
s3g_1    | 	at org.jvnet.hk2.internal.SystemDescriptor.create(SystemDescriptor.java:463)
scm2_1   | 2023-05-18 20:10:11,072 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm3_1   | 2023-05-18 20:10:22,144 [2620a8e2-fcef-44ea-bfea-c3742904e700-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm3_1   | 2023-05-18 20:10:22,161 [2620a8e2-fcef-44ea-bfea-c3742904e700-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
recon_1  | 2023-05-18 20:09:59,978 [main] INFO node.SCMNodeManager: Registered Data node : 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
dn1_1    | 2023-05-18 20:10:19,731 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-05-18 20:10:28,419 [main] WARN om.OzoneManager: Prepare marker file index 24 does not match DB prepare index 23. Writing DB index to prepare file and maintaining prepared state.
om1_1    | 2023-05-18 20:10:28,422 [main] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 23 to file /data/metadata/current/prepareMarker
om1_1    | 2023-05-18 20:10:28,697 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1    | 2023-05-18 20:10:28,704 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om1_1    | 2023-05-18 20:10:28,785 [main] INFO utils.RDBSnapshotProvider: Cleaning up the candidate dir: /data/metadata/snapshot/om.db.candidate
scm2_1   | 2023-05-18 20:10:11,116 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
om2_1    | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
om2_1    | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm1_1   | 2023-05-18 20:10:03,234 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
recon_1  | 2023-05-18 20:09:59,985 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/caabb77a-6190-403e-a139-2e30ca474b23
dn4_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)
om1_1    | 2023-05-18 20:10:28,791 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
om1_1    | 2023-05-18 20:10:29,904 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om1_1    | 2023-05-18 20:10:29,981 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1    | 2023-05-18 20:10:30,354 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om1:9872, om3:9872, om2:9872
om1_1    | 2023-05-18 20:10:30,460 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:2, i:24)
scm2_1   | 2023-05-18 20:10:11,196 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn1_1    | 2023-05-18 20:10:19,735 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1  | 2023-05-18 20:09:59,986 [main] INFO node.SCMNodeManager: Registered Data node : caabb77a-6190-403e-a139-2e30ca474b23{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-05-18 20:10:03,242 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1_1   | 2023-05-18 20:10:03,400 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
om1_1    | 2023-05-18 20:10:30,663 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om1_1    | 2023-05-18 20:10:31,303 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om1_1    | 2023-05-18 20:10:31,317 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om1_1    | 2023-05-18 20:10:31,318 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om1_1    | 2023-05-18 20:10:31,320 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
scm2_1   | 2023-05-18 20:10:11,272 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm2_1   | 2023-05-18 20:10:11,294 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm2_1   | 2023-05-18 20:10:11,296 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
om2_1    | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
dn1_1    | 2023-05-18 20:10:19,735 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
recon_1  | 2023-05-18 20:09:59,986 [main] INFO scm.ReconNodeManager: Loaded 5 nodes from node DB.
recon_1  | 2023-05-18 20:10:00,000 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
recon_1  | 2023-05-18 20:10:01,369 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
dn5_1    | 2023-05-18 20:10:13,109 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:13,116 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 	at org.jvnet.hk2.internal.PerLookupContext.findOrCreate(PerLookupContext.java:46)
s3g_1    | 	at org.jvnet.hk2.internal.Utilities.createService(Utilities.java:2102)
s3g_1    | 	at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:758)
scm2_1   | 2023-05-18 20:10:11,367 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
om2_1    | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
dn1_1    | 2023-05-18 20:10:19,738 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
recon_1  | 2023-05-18 20:10:01,432 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1_1   | 2023-05-18 20:10:03,400 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1_1   | 2023-05-18 20:10:03,419 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
s3g_1    | 	at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:721)
s3g_1    | 	at org.jvnet.hk2.internal.ServiceLocatorImpl.getService(ServiceLocatorImpl.java:691)
s3g_1    | 	at org.glassfish.jersey.inject.hk2.AbstractHk2InjectionManager.getInstance(AbstractHk2InjectionManager.java:160)
s3g_1    | 	at org.glassfish.jersey.inject.hk2.ImmediateHk2InjectionManager.getInstance(ImmediateHk2InjectionManager.java:30)
scm2_1   | 2023-05-18 20:10:11,367 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
om1_1    | 2023-05-18 20:10:31,320 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om1_1    | 2023-05-18 20:10:31,324 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
dn1_1    | 2023-05-18 20:10:19,739 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-05-18 20:10:03,419 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm1_1   | 2023-05-18 20:10:03,427 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm1_1   | 2023-05-18 20:10:03,434 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
s3g_1    | 	at org.glassfish.jersey.internal.inject.Injections.getOrCreate(Injections.java:105)
s3g_1    | 	at org.glassfish.jersey.server.model.MethodHandler$ClassBasedMethodHandler.getInstance(MethodHandler.java:260)
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.PushMethodHandlerRouter.apply(PushMethodHandlerRouter.java:51)
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:86)
scm2_1   | 2023-05-18 20:10:11,375 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
om1_1    | 2023-05-18 20:10:31,325 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
dn3_1    | 2023-05-18 20:10:18,067 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:19,775 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/78d664ca-7989-4249-9a90-24fe3dd84618
scm1_1   | 2023-05-18 20:10:03,438 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
recon_1  | 2023-05-18 20:10:01,493 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:69)
scm2_1   | 2023-05-18 20:10:11,376 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm2_1   | 2023-05-18 20:10:11,378 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm2_1   | 2023-05-18 20:10:11,379 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
dn1_1    | 2023-05-18 20:10:19,776 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm1_1   | 2023-05-18 20:10:03,441 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
dn4_1    | 	... 12 more
dn4_1    | 2023-05-18 20:10:18,121 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn4_1    | 2023-05-18 20:10:18,128 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn4_1    | 2023-05-18 20:10:18,149 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn4_1    | java.net.SocketTimeoutException: Call From 5dd7996c298c/10.9.0.20 to scm2:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:40988 remote=scm2/10.9.0.15:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
recon_1  | 2023-05-18 20:10:01,752 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
scm2_1   | 2023-05-18 20:10:11,384 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm2_1   | 2023-05-18 20:10:11,384 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm2_1   | 2023-05-18 20:10:11,443 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
dn1_1    | 2023-05-18 20:10:19,778 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a2756a42-16da-435f-a15a-f31dfb92d20c
scm1_1   | 2023-05-18 20:10:03,529 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
dn5_1    | 2023-05-18 20:10:14,110 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:15,111 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:16,111 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:17,112 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-05-18 20:10:01,753 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
scm2_1   | 2023-05-18 20:10:11,481 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm2_1   | 2023-05-18 20:10:11,562 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
dn3_1    | 2023-05-18 20:10:18,183 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
om2_1    | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
om2_1    | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
dn1_1    | 2023-05-18 20:10:19,788 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-05-18 20:10:19,788 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm1_1   | 2023-05-18 20:10:03,568 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
dn5_1    | 2023-05-18 20:10:18,113 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:18,114 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn5_1    | java.net.ConnectException: Call From cd98f882537f/10.9.0.21 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
recon_1  | 2023-05-18 20:10:01,915 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm2_1   | 2023-05-18 20:10:11,582 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
dn3_1    | 2023-05-18 20:10:18,201 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn3_1    | 2023-05-18 20:10:18,507 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
om2_1    | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
om2_1    | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
dn1_1    | 2023-05-18 20:10:19,788 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-05-18 20:10:19,791 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm1_1   | 2023-05-18 20:10:03,674 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
recon_1  | 2023-05-18 20:10:01,974 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm2_1   | 2023-05-18 20:10:11,582 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
scm2_1   | WARNING: An illegal reflective access operation has occurred
scm2_1   | WARNING: Illegal reflective access by org.apache.hadoop.hdds.utils.MetricsUtil (file:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar) to method java.lang.Class.annotationData()
scm2_1   | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.hdds.utils.MetricsUtil
dn1_1    | 2023-05-18 20:10:19,792 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-05-18 20:10:19,793 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm1_1   | 2023-05-18 20:10:03,737 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
om3_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.startOmUpgrade(OzoneManagerStarter.java:121)
om3_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
om3_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
om3_1    | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
om3_1    | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
om2_1    | 	at picocli.CommandLine.execute(CommandLine.java:2078)
scm2_1   | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
scm2_1   | WARNING: All illegal access operations will be denied in a future release
scm2_1   | 2023-05-18 20:10:11,602 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
dn1_1    | 2023-05-18 20:10:19,794 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
om3_1    | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
om3_1    | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
om3_1    | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
om3_1    | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
om2_1    | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm2_1   | 2023-05-18 20:10:11,608 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 2, healthy pipeline threshold count is 1
dn3_1    | 2023-05-18 20:10:18,507 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis caabb77a-6190-403e-a139-2e30ca474b23
dn3_1    | 2023-05-18 20:10:18,620 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/9693fb8d-2a36-45cd-aee5-9db787f5d8f4/in_use.lock acquired by nodename 6@54c4478f5de9
dn1_1    | 2023-05-18 20:10:19,794 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
om3_1    | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
scm3_1   | 2023-05-18 20:10:22,162 [2620a8e2-fcef-44ea-bfea-c3742904e700-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm3_1   | 2023-05-18 20:10:22,166 [2620a8e2-fcef-44ea-bfea-c3742904e700-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3_1   | 2023-05-18 20:10:22,167 [2620a8e2-fcef-44ea-bfea-c3742904e700-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
dn3_1    | 2023-05-18 20:10:18,630 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a2756a42-16da-435f-a15a-f31dfb92d20c/in_use.lock acquired by nodename 6@54c4478f5de9
dn3_1    | 2023-05-18 20:10:18,637 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/78d664ca-7989-4249-9a90-24fe3dd84618/in_use.lock acquired by nodename 6@54c4478f5de9
dn3_1    | 2023-05-18 20:10:18,639 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=caabb77a-6190-403e-a139-2e30ca474b23} from /data/metadata/ratis/78d664ca-7989-4249-9a90-24fe3dd84618/current/raft-meta
dn1_1    | 2023-05-18 20:10:19,799 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
om3_1    | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
om3_1    | 	at picocli.CommandLine.execute(CommandLine.java:2078)
om3_1    | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
om3_1    | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
om2_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:58)
scm2_1   | 2023-05-18 20:10:11,609 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
dn3_1    | 2023-05-18 20:10:18,652 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=76248598-2f32-4c49-8f2a-b1fddb9f8151} from /data/metadata/ratis/a2756a42-16da-435f-a15a-f31dfb92d20c/current/raft-meta
dn3_1    | 2023-05-18 20:10:18,665 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=caabb77a-6190-403e-a139-2e30ca474b23} from /data/metadata/ratis/9693fb8d-2a36-45cd-aee5-9db787f5d8f4/current/raft-meta
dn1_1    | 2023-05-18 20:10:19,799 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-05-18 20:10:19,801 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1_1   | 2023-05-18 20:10:03,758 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
scm1_1   | WARNING: An illegal reflective access operation has occurred
scm1_1   | WARNING: Illegal reflective access by org.apache.hadoop.hdds.utils.MetricsUtil (file:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar) to method java.lang.Class.annotationData()
scm1_1   | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.hdds.utils.MetricsUtil
scm1_1   | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
scm1_1   | WARNING: All illegal access operations will be denied in a future release
scm3_1   | 2023-05-18 20:10:22,168 [2620a8e2-fcef-44ea-bfea-c3742904e700-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm2_1   | 2023-05-18 20:10:11,679 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
dn1_1    | 2023-05-18 20:10:19,802 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-05-18 20:10:19,806 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:38)
s3g_1    | 	at org.glassfish.jersey.process.internal.Stages.process(Stages.java:173)
s3g_1    | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:247)
s3g_1    | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1    | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
om2_1    | 2023-05-18 20:10:27,961 [main] WARN om.OzoneManager: Prepare marker file index 24 does not match DB prepare index 23. Writing DB index to prepare file and maintaining prepared state.
om2_1    | 2023-05-18 20:10:27,970 [main] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 23 to file /data/metadata/current/prepareMarker
dn3_1    | 2023-05-18 20:10:18,797 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4: set configuration 0: peers:[caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-18 20:10:18,796 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C: set configuration 0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-05-18 20:10:12,492 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3_1   | 2023-05-18 20:10:22,331 [2620a8e2-fcef-44ea-bfea-c3742904e700-groupManagement] INFO server.RaftServer$Division: 2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
dn1_1    | 2023-05-18 20:10:19,810 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
s3g_1    | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
scm1_1   | 2023-05-18 20:10:03,781 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
om3_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:58)
om3_1    | 2023-05-18 20:10:28,791 [main] WARN om.OzoneManager: Prepare marker file index 24 does not match DB prepare index 23. Writing DB index to prepare file and maintaining prepared state.
om3_1    | 2023-05-18 20:10:28,824 [main] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 23 to file /data/metadata/current/prepareMarker
om2_1    | 2023-05-18 20:10:28,315 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1    | 2023-05-18 20:10:28,321 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
dn3_1    | 2023-05-18 20:10:18,803 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618: set configuration 0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
s3g_1    | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
scm1_1   | 2023-05-18 20:10:03,793 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 2, healthy pipeline threshold count is 1
om3_1    | 2023-05-18 20:10:29,119 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1    | 2023-05-18 20:10:29,123 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om3_1    | 2023-05-18 20:10:29,196 [main] INFO utils.RDBSnapshotProvider: Cleaning up the candidate dir: /data/metadata/snapshot/om.db.candidate
om2_1    | 2023-05-18 20:10:28,393 [main] INFO utils.RDBSnapshotProvider: Cleaning up the candidate dir: /data/metadata/snapshot/om.db.candidate
om2_1    | 2023-05-18 20:10:28,396 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
om2_1    | 2023-05-18 20:10:29,405 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm3_1   | 2023-05-18 20:10:22,337 [2620a8e2-fcef-44ea-bfea-c3742904e700-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2_1   | 2023-05-18 20:10:12,513 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
s3g_1    | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1    | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
om2_1    | 2023-05-18 20:10:29,486 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1    | 2023-05-18 20:10:29,797 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om2:9872, om1:9872, om3:9872
recon_1  | 2023-05-18 20:10:01,975 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
om1_1    | 2023-05-18 20:10:31,347 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-05-18 20:10:12,546 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm1_1   | 2023-05-18 20:10:03,798 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
scm1_1   | 2023-05-18 20:10:03,884 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
s3g_1    | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1    | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
dn5_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
om2_1    | 2023-05-18 20:10:30,196 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:2, i:24)
om2_1    | 2023-05-18 20:10:31,079 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
recon_1  | 2023-05-18 20:10:03,247 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
scm3_1   | 2023-05-18 20:10:22,360 [2620a8e2-fcef-44ea-bfea-c3742904e700-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm3_1   | 2023-05-18 20:10:22,363 [2620a8e2-fcef-44ea-bfea-c3742904e700-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om1_1    | 2023-05-18 20:10:31,350 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1_1   | 2023-05-18 20:10:04,918 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1_1   | 2023-05-18 20:10:04,951 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
s3g_1    | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1    | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
om2_1    | 2023-05-18 20:10:32,031 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om2_1    | 2023-05-18 20:10:32,047 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
recon_1  | 2023-05-18 20:10:03,250 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
scm3_1   | 2023-05-18 20:10:22,672 [2620a8e2-fcef-44ea-bfea-c3742904e700-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm2_1   | 2023-05-18 20:10:12,589 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
om1_1    | 2023-05-18 20:10:31,355 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
scm1_1   | 2023-05-18 20:10:05,010 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm1_1   | 2023-05-18 20:10:05,102 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
s3g_1    | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
dn1_1    | 2023-05-18 20:10:19,814 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
om2_1    | 2023-05-18 20:10:32,059 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om2_1    | 2023-05-18 20:10:32,064 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
scm2_1   | 2023-05-18 20:10:12,593 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om1_1    | 2023-05-18 20:10:31,444 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1_1   | 2023-05-18 20:10:05,110 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1    | 2023-05-18 20:10:29,208 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
om3_1    | 2023-05-18 20:10:30,660 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om2_1    | 2023-05-18 20:10:32,070 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om2_1    | 2023-05-18 20:10:32,071 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
scm3_1   | 2023-05-18 20:10:22,738 [2620a8e2-fcef-44ea-bfea-c3742904e700-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm2_1   | 2023-05-18 20:10:12,594 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
om1_1    | 2023-05-18 20:10:31,475 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm1_1   | 2023-05-18 20:10:05,114 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
om3_1    | 2023-05-18 20:10:30,704 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1    | 2023-05-18 20:10:31,211 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om3:9872, om1:9872, om2:9872
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 2023-05-18 20:10:19,839 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/467c8493-9e44-42c8-850d-8edf1c67e99f
dn1_1    | 2023-05-18 20:10:19,841 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
om2_1    | 2023-05-18 20:10:32,071 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn4_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
scm2_1   | 2023-05-18 20:10:12,631 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2_1   | 2023-05-18 20:10:12,637 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1    | 2023-05-18 20:10:31,457 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:2, i:24)
om3_1    | 2023-05-18 20:10:32,328 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 2023-05-18 20:10:18,854 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO ratis.ContainerStateMachine: group-24FE3DD84618: Setting the last applied index to (t:1, i:0)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
scm3_1   | 2023-05-18 20:10:22,761 [2620a8e2-fcef-44ea-bfea-c3742904e700-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1_1   | 2023-05-18 20:10:05,168 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1_1   | 2023-05-18 20:10:05,182 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1    | 2023-05-18 20:10:33,109 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om3_1    | 2023-05-18 20:10:33,118 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-05-18 20:10:18,901 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO ratis.ContainerStateMachine: group-9DB787F5D8F4: Setting the last applied index to (t:1, i:0)
om1_1    | 2023-05-18 20:10:31,484 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om1_1    | 2023-05-18 20:10:34,538 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om1_1    | 2023-05-18 20:10:34,564 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm1_1   | 2023-05-18 20:10:05,240 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm1_1   | 2023-05-18 20:10:05,345 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
s3g_1    | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1    | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1459)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
om1_1    | 2023-05-18 20:10:34,566 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om1_1    | 2023-05-18 20:10:34,602 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1    | 2023-05-18 20:10:34,614 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | 2023-05-18 20:10:34,674 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm1_1   | 2023-05-18 20:10:05,345 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1656)
s3g_1    | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
dn5_1    | Caused by: java.net.ConnectException: Connection refused
om3_1    | 2023-05-18 20:10:33,120 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om2_1    | 2023-05-18 20:10:32,114 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-05-18 20:10:32,129 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
dn1_1    | 2023-05-18 20:10:19,854 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-05-18 20:10:19,855 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
om1_1    | 2023-05-18 20:10:34,761 [om1-impl-thread1] INFO server.RaftServer: om1: found a subdirectory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om1_1    | 2023-05-18 20:10:34,847 [main] INFO server.RaftServer: om1: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@58687fb7[Not completed]
om1_1    | 2023-05-18 20:10:34,849 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
scm1_1   | Container Balancer status:
s3g_1    | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
om3_1    | 2023-05-18 20:10:33,123 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om3_1    | 2023-05-18 20:10:33,123 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om3_1    | 2023-05-18 20:10:33,130 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
recon_1  | 2023-05-18 20:10:03,334 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1  | 2023-05-18 20:10:03,335 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1  | 2023-05-18 20:10:03,341 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 660000ms
om1_1    | 2023-05-18 20:10:34,865 [main] INFO om.OzoneManager: Creating RPC Server
scm3_1   | 2023-05-18 20:10:22,763 [2620a8e2-fcef-44ea-bfea-c3742904e700-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm3_1   | 2023-05-18 20:10:22,898 [2620a8e2-fcef-44ea-bfea-c3742904e700-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm1_1   | Key                            Value
s3g_1    | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1    | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
om3_1    | 2023-05-18 20:10:33,134 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om3_1    | 2023-05-18 20:10:33,147 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-05-18 20:10:33,149 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
recon_1  | 2023-05-18 20:10:03,368 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1625789b{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1  | 2023-05-18 20:10:03,370 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5f6b53ed{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1  | 2023-05-18 20:10:06,665 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@695c8b32{recon,/,file:///data/metadata/webserver/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-321215365898740408/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
recon_1  | 2023-05-18 20:10:06,681 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@5a090f62{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm1_1   | Running                        false
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
dn3_1    | 2023-05-18 20:10:18,890 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO ratis.ContainerStateMachine: group-F31DFB92D20C: Setting the last applied index to (t:3, i:12)
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
om3_1    | 2023-05-18 20:10:33,152 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1    | 2023-05-18 20:10:33,213 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om3_1    | 2023-05-18 20:10:33,270 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om3_1    | 2023-05-18 20:10:33,282 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
recon_1  | 2023-05-18 20:10:06,681 [Listener at 0.0.0.0/9891] INFO server.Server: Started @49075ms
recon_1  | 2023-05-18 20:10:06,687 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn1_1    | 2023-05-18 20:10:19,855 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2_1   | 2023-05-18 20:10:12,641 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm2_1   | 2023-05-18 20:10:12,693 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
dn3_1    | 2023-05-18 20:10:19,069 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:19,476 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-05-18 20:10:19,497 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-05-18 20:10:19,517 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-05-18 20:10:19,528 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-05-18 20:10:19,533 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-05-18 20:10:19,530 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
recon_1  | 2023-05-18 20:10:06,687 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn1_1    | 2023-05-18 20:10:19,856 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2_1   | 2023-05-18 20:10:12,693 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm2_1   | Container Balancer status:
scm2_1   | Key                            Value
dn5_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
s3g_1    | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1681)
s3g_1    | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1    | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
dn1_1    | 2023-05-18 20:10:19,856 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:40988 remote=scm2/10.9.0.15:9861]
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 2023-05-18 20:10:19,530 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-05-18 20:10:19,537 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 2023-05-18 20:10:23,813 [2620a8e2-fcef-44ea-bfea-c3742904e700-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm3_1   | 2023-05-18 20:10:23,837 [2620a8e2-fcef-44ea-bfea-c3742904e700-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3_1   | 2023-05-18 20:10:23,859 [2620a8e2-fcef-44ea-bfea-c3742904e700-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)
s3g_1    | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
recon_1  | 2023-05-18 20:10:06,688 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
om3_1    | 2023-05-18 20:10:35,205 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om1_1    | 2023-05-18 20:10:35,129 [om1-groupManagement] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om1_1    | 2023-05-18 20:10:35,173 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-05-18 20:10:19,856 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-05-18 20:10:19,538 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | Container Balancer Configuration values:
scm1_1   | Key                                                Value
scm3_1   | 2023-05-18 20:10:23,862 [2620a8e2-fcef-44ea-bfea-c3742904e700-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm3_1   | 2023-05-18 20:10:23,867 [2620a8e2-fcef-44ea-bfea-c3742904e700-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm3_1   | 2023-05-18 20:10:23,867 [2620a8e2-fcef-44ea-bfea-c3742904e700-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:790)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
recon_1  | 2023-05-18 20:10:06,688 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
om3_1    | 2023-05-18 20:10:35,217 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om3_1    | 2023-05-18 20:10:35,220 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om3_1    | 2023-05-18 20:10:35,222 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
dn1_1    | 2023-05-18 20:10:19,857 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-05-18 20:10:19,542 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1_1   | Threshold                                          10
scm2_1   | Running                        false
scm3_1   | 2023-05-18 20:10:23,876 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm3_1   | 2023-05-18 20:10:23,903 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm3_1   | 2023-05-18 20:10:23,904 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:363)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)
recon_1  | 2023-05-18 20:10:06,706 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
om3_1    | 2023-05-18 20:10:35,225 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | 2023-05-18 20:10:35,178 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om1_1    | 2023-05-18 20:10:35,179 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-05-18 20:10:19,903 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-05-18 20:10:19,543 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1_1   | Max Datanodes to Involve per Iteration(percent)    20
scm2_1   | Container Balancer Configuration values:
scm3_1   | 2023-05-18 20:10:24,097 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)
s3g_1    | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
recon_1  | 2023-05-18 20:10:06,706 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-05-18 20:10:06,707 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Last known snapshot for OM : /data/metadata/om.snapshot.db_1684440465318
recon_1  | 2023-05-18 20:10:06,712 [Listener at 0.0.0.0/9891] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 2023-05-18 20:10:19,904 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-05-18 20:10:19,543 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1_1   | Max Size to Move per Iteration                     500GB
scm2_1   | Key                                                Value
scm2_1   | Threshold                                          10
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
s3g_1    | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:600)
recon_1  | 2023-05-18 20:10:06,713 [Listener at 0.0.0.0/9891] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om1_1    | 2023-05-18 20:10:35,181 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1    | 2023-05-18 20:10:35,241 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 2023-05-18 20:10:19,915 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-05-18 20:10:19,547 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1_1   | Max Size Entering Target per Iteration             26GB
scm3_1   | 2023-05-18 20:10:24,392 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm2_1   | Max Datanodes to Involve per Iteration(percent)    20
dn5_1    | 	... 12 more
s3g_1    | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
recon_1  | 2023-05-18 20:10:06,886 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1684440465318.
om1_1    | 2023-05-18 20:10:35,182 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1    | 2023-05-18 20:10:35,260 [om3-impl-thread1] INFO server.RaftServer: om3: found a subdirectory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om3_1    | 2023-05-18 20:10:35,284 [main] INFO server.RaftServer: om3: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@58687fb7[Not completed]
om3_1    | 2023-05-18 20:10:35,285 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om3_1    | 2023-05-18 20:10:35,295 [main] INFO om.OzoneManager: Creating RPC Server
om3_1    | 2023-05-18 20:10:35,397 [om3-groupManagement] INFO server.RaftServer$Division: om3: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
scm1_1   | Max Size Leaving Source per Iteration              26GB
scm1_1   | 
dn5_1    | 2023-05-18 20:10:18,133 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
s3g_1    | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
recon_1  | 2023-05-18 20:10:06,924 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
om1_1    | 2023-05-18 20:10:35,185 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om3_1    | 2023-05-18 20:10:35,411 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om3_1    | 2023-05-18 20:10:35,414 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om3_1    | 2023-05-18 20:10:35,415 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om3_1    | 2023-05-18 20:10:35,415 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
dn1_1    | 2023-05-18 20:10:19,930 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-05-18 20:10:05,345 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm1_1   | 2023-05-18 20:10:05,364 [Listener at 0.0.0.0/9860] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
recon_1  | 2023-05-18 20:10:06,926 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
om1_1    | 2023-05-18 20:10:35,326 [om1-groupManagement] INFO server.RaftServer$Division: om1@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om3_1    | 2023-05-18 20:10:35,415 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3_1   | 2023-05-18 20:10:24,963 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm3_1   | 2023-05-18 20:10:25,112 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm3_1   | 2023-05-18 20:10:25,113 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
dn1_1    | 2023-05-18 20:10:21,115 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:21,116 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-05-18 20:10:21,137 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | java.net.SocketTimeoutException: Call From cd98f882537f/10.9.0.21 to scm2:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:52210 remote=scm2/10.9.0.15:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
om1_1    | 2023-05-18 20:10:35,332 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1    | 2023-05-18 20:10:35,416 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm3_1   | 2023-05-18 20:10:25,734 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
dn3_1    | 2023-05-18 20:10:19,547 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-05-18 20:10:19,553 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm2_1   | Max Size to Move per Iteration                     500GB
scm1_1   | 2023-05-18 20:10:05,366 [Listener at 0.0.0.0/9860] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn1_1    | 2023-05-18 20:10:21,136 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
om1_1    | 2023-05-18 20:10:35,400 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om3_1    | 2023-05-18 20:10:35,469 [om3-groupManagement] INFO server.RaftServer$Division: om3@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm3_1   | 2023-05-18 20:10:25,748 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
dn3_1    | 2023-05-18 20:10:19,564 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-05-18 20:10:19,565 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm2_1   | Max Size Entering Target per Iteration             26GB
scm2_1   | Max Size Leaving Source per Iteration              26GB
om2_1    | 2023-05-18 20:10:32,142 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1    | 2023-05-18 20:10:32,277 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 2023-05-18 20:10:21,157 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
om1_1    | 2023-05-18 20:10:35,428 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om3_1    | 2023-05-18 20:10:35,472 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm3_1   | 2023-05-18 20:10:25,820 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
dn3_1    | 2023-05-18 20:10:19,570 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
scm2_1   | 
scm2_1   | 2023-05-18 20:10:12,694 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
dn1_1    | 2023-05-18 20:10:21,158 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
om1_1    | 2023-05-18 20:10:35,769 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om3_1    | 2023-05-18 20:10:35,495 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm3_1   | 2023-05-18 20:10:25,833 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
dn3_1    | 2023-05-18 20:10:19,592 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
scm1_1   | 2023-05-18 20:10:05,367 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm1_1   | 2023-05-18 20:10:05,373 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
s3g_1    | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1    | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
dn1_1    | 2023-05-18 20:10:21,296 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om1_1    | 2023-05-18 20:10:35,883 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om3_1    | 2023-05-18 20:10:35,496 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm3_1   | 2023-05-18 20:10:25,871 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
dn3_1    | 2023-05-18 20:10:19,593 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-05-18 20:10:19,593 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-05-18 20:10:12,757 [Listener at 0.0.0.0/9860] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
scm2_1   | 2023-05-18 20:10:12,759 [Listener at 0.0.0.0/9860] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
s3g_1    | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
recon_1  | 2023-05-18 20:10:13,871 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d is not the leader. Could not determine the leader node.
dn1_1    | 2023-05-18 20:10:21,321 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om1_1    | 2023-05-18 20:10:35,946 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om3_1    | 2023-05-18 20:10:35,643 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
scm3_1   | 2023-05-18 20:10:25,896 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
dn3_1    | 2023-05-18 20:10:19,605 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
scm2_1   | 2023-05-18 20:10:12,760 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm2_1   | 2023-05-18 20:10:12,765 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
s3g_1    | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
recon_1  | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1  | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om1_1    | 2023-05-18 20:10:36,043 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om3_1    | 2023-05-18 20:10:35,683 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm3_1   | 2023-05-18 20:10:25,972 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
dn3_1    | 2023-05-18 20:10:19,609 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
scm1_1   | 2023-05-18 20:10:05,379 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/in_use.lock acquired by nodename 7@de2588b9b9b7
scm1_1   | 2023-05-18 20:10:05,388 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=2, votedFor=dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d} from /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/current/raft-meta
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
recon_1  | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
dn1_1    | 2023-05-18 20:10:21,322 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
om1_1    | 2023-05-18 20:10:36,396 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om3_1    | 2023-05-18 20:10:35,728 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
scm3_1   | 2023-05-18 20:10:25,985 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
dn3_1    | 2023-05-18 20:10:19,609 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
scm2_1   | 2023-05-18 20:10:12,770 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/in_use.lock acquired by nodename 6@ec06bddf6a82
om2_1    | 2023-05-18 20:10:32,340 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
s3g_1    | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
recon_1  | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
dn1_1    | 2023-05-18 20:10:21,331 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om1_1    | 2023-05-18 20:10:36,791 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1    | 2023-05-18 20:10:35,739 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm3_1   | 2023-05-18 20:10:26,247 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
dn3_1    | 2023-05-18 20:10:19,627 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-05-18 20:10:19,627 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-05-18 20:10:19,628 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-05-18 20:10:12,776 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=2, votedFor=} from /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/current/raft-meta
om2_1    | 2023-05-18 20:10:32,340 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om2_1    | 2023-05-18 20:10:35,390 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
s3g_1    | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
dn1_1    | 2023-05-18 20:10:21,495 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-05-18 20:10:21,495 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
om3_1    | 2023-05-18 20:10:36,090 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om3_1    | 2023-05-18 20:10:36,416 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1    | 2023-05-18 20:10:36,451 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1    | 2023-05-18 20:10:36,454 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om3_1    | 2023-05-18 20:10:36,469 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om2_1    | 2023-05-18 20:10:35,414 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om2_1    | 2023-05-18 20:10:35,417 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om2_1    | 2023-05-18 20:10:35,421 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
s3g_1    | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
s3g_1    | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
dn1_1    | 2023-05-18 20:10:22,421 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-05-18 20:10:36,832 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1    | 2023-05-18 20:10:36,840 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om1_1    | 2023-05-18 20:10:36,852 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om1_1    | 2023-05-18 20:10:36,860 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om1_1    | 2023-05-18 20:10:36,866 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-05-18 20:10:19,656 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/9693fb8d-2a36-45cd-aee5-9db787f5d8f4
om2_1    | 2023-05-18 20:10:35,422 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | 2023-05-18 20:10:35,438 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1    | 2023-05-18 20:10:35,467 [om2-impl-thread1] INFO server.RaftServer: om2: found a subdirectory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
s3g_1    | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
dn1_1    | 2023-05-18 20:10:22,611 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F: set configuration 0: peers:[76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-18 20:10:05,476 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServer$Division: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F: set configuration 33: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2620a8e2-fcef-44ea-bfea-c3742904e700|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-18 20:10:26,318 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm3_1   | 2023-05-18 20:10:26,575 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm3_1   | 2023-05-18 20:10:26,677 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm3_1   | 2023-05-18 20:10:26,684 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
scm3_1   | WARNING: An illegal reflective access operation has occurred
om2_1    | 2023-05-18 20:10:35,491 [main] INFO server.RaftServer: om2: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@58687fb7[Not completed]
om2_1    | 2023-05-18 20:10:35,496 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om2_1    | 2023-05-18 20:10:35,513 [main] INFO om.OzoneManager: Creating RPC Server
s3g_1    | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
recon_1  | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
dn1_1    | 2023-05-18 20:10:22,619 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/467c8493-9e44-42c8-850d-8edf1c67e99f/current/log_inprogress_0
dn3_1    | 2023-05-18 20:10:19,657 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-05-18 20:10:19,657 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm3_1   | WARNING: Illegal reflective access by org.apache.hadoop.hdds.utils.MetricsUtil (file:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar) to method java.lang.Class.annotationData()
om3_1    | 2023-05-18 20:10:36,473 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om1_1    | 2023-05-18 20:10:38,304 [main] INFO reflections.Reflections: Reflections took 3072 ms to scan 8 urls, producing 24 keys and 609 values [using 2 cores]
om1_1    | 2023-05-18 20:10:38,977 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
s3g_1    | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
dn1_1    | 2023-05-18 20:10:22,613 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C: set configuration 0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-18 20:10:05,491 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1_1   | 2023-05-18 20:10:05,514 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm3_1   | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.hdds.utils.MetricsUtil
om3_1    | 2023-05-18 20:10:36,500 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om3_1    | 2023-05-18 20:10:38,144 [main] INFO reflections.Reflections: Reflections took 2672 ms to scan 8 urls, producing 24 keys and 609 values [using 2 cores]
om3_1    | 2023-05-18 20:10:38,608 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
s3g_1    | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
s3g_1    | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
dn3_1    | 2023-05-18 20:10:19,658 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-05-18 20:10:22,613 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618: set configuration 0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
om1_1    | 2023-05-18 20:10:39,022 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om1_1    | 2023-05-18 20:10:41,249 [Listener at om1/9862] INFO om.OzoneManagerPrepareState: Deleted prepare marker file: /data/metadata/current/prepareMarker
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn5_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
s3g_1    | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn1_1    | 2023-05-18 20:10:22,693 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-05-18 20:10:19,658 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-05-18 20:10:19,658 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1    | 2023-05-18 20:10:41,623 [Listener at om1/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om3_1    | 2023-05-18 20:10:38,666 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om3_1    | 2023-05-18 20:10:41,156 [Listener at om3/9862] INFO om.OzoneManagerPrepareState: Deleted prepare marker file: /data/metadata/current/prepareMarker
scm2_1   | 2023-05-18 20:10:12,828 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServer$Division: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F: set configuration 33: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2620a8e2-fcef-44ea-bfea-c3742904e700|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
s3g_1    | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn1_1    | 2023-05-18 20:10:22,650 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/78d664ca-7989-4249-9a90-24fe3dd84618/current/log_inprogress_0
dn1_1    | 2023-05-18 20:10:22,803 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om1_1    | 2023-05-18 20:10:41,688 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om3_1    | 2023-05-18 20:10:41,589 [Listener at om3/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om3_1    | 2023-05-18 20:10:41,660 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm2_1   | 2023-05-18 20:10:12,831 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
s3g_1    | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn1_1    | 2023-05-18 20:10:22,897 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO segmented.LogSegment: Successfully read 13 entries from segment file /data/metadata/ratis/a2756a42-16da-435f-a15a-f31dfb92d20c/current/log_inprogress_0
scm3_1   | WARNING: All illegal access operations will be denied in a future release
om1_1    | 2023-05-18 20:10:41,690 [Listener at om1/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om1_1    | 2023-05-18 20:10:41,973 [Listener at om1/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om1/10.9.0.11:9862
om1_1    | 2023-05-18 20:10:41,973 [Listener at om1/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
scm2_1   | 2023-05-18 20:10:12,843 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
s3g_1    | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn1_1    | 2023-05-18 20:10:22,898 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 12
dn1_1    | 2023-05-18 20:10:22,898 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om1_1    | 2023-05-18 20:10:42,020 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@32984e5f9c2e
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
scm2_1   | 2023-05-18 20:10:12,843 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
s3g_1    | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn1_1    | 2023-05-18 20:10:23,498 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:23,988 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618: start as a follower, conf=0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-18 20:10:23,989 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618: changes role from      null to FOLLOWER at term 1 for startAsFollower
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
om3_1    | 2023-05-18 20:10:41,660 [Listener at om3/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
scm2_1   | 2023-05-18 20:10:12,847 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
s3g_1    | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
recon_1  | , while invoking $Proxy42.submitRequest over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
om2_1    | 2023-05-18 20:10:35,596 [om2-groupManagement] INFO server.RaftServer$Division: om2: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om2_1    | 2023-05-18 20:10:35,604 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om2_1    | 2023-05-18 20:10:35,608 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om2_1    | 2023-05-18 20:10:35,608 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om1_1    | 2023-05-18 20:10:42,116 [om1-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=2, votedFor=om1} from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/raft-meta
om1_1    | 2023-05-18 20:10:42,263 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
om3_1    | 2023-05-18 20:10:41,993 [Listener at om3/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om3/10.9.0.13:9862
scm2_1   | 2023-05-18 20:10:12,848 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
s3g_1    | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
s3g_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1    | 2023-05-18 20:10:35,608 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1    | 2023-05-18 20:10:35,608 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | 2023-05-18 20:10:35,609 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1    | 2023-05-18 20:10:35,679 [om2-groupManagement] INFO server.RaftServer$Division: om2@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om2_1    | 2023-05-18 20:10:35,679 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1    | 2023-05-18 20:10:35,719 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-05-18 20:10:24,013 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C: start as a follower, conf=0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-05-18 20:10:42,266 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om3_1    | 2023-05-18 20:10:41,993 [Listener at om3/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om3 at port 9872
scm2_1   | 2023-05-18 20:10:12,854 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
s3g_1    | 2023-05-18 20:11:17,620 [qtp605052357-21] ERROR protocolPB.GrpcOmTransport: error unwrapping exception from OMResponse {}
recon_1  | 2023-05-18 20:10:16,196 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:78aeffd2-0882-43d0-ab48-78e394e586e6 is not the leader. Could not determine the leader node.
om2_1    | 2023-05-18 20:10:35,721 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1    | 2023-05-18 20:10:35,870 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
scm2_1   | 2023-05-18 20:10:12,871 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
recon_1  | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1  | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm3_1   | 2023-05-18 20:10:26,734 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm2_1   | 2023-05-18 20:10:12,872 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm2_1   | 2023-05-18 20:10:12,872 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1  | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
scm1_1   | 2023-05-18 20:10:05,515 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-05-18 20:10:05,520 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn4_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
om2_1    | 2023-05-18 20:10:35,927 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om2_1    | 2023-05-18 20:10:35,996 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om2_1    | 2023-05-18 20:10:35,998 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om2_1    | 2023-05-18 20:10:36,310 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om2_1    | 2023-05-18 20:10:36,642 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1    | 2023-05-18 20:10:36,666 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-05-18 20:10:05,521 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1_1   | 2023-05-18 20:10:05,532 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
dn3_1    | 2023-05-18 20:10:19,659 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-05-18 20:10:19,659 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-05-18 20:10:19,668 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-05-18 20:10:19,659 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a2756a42-16da-435f-a15a-f31dfb92d20c
om2_1    | 2023-05-18 20:10:36,680 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm3_1   | 2023-05-18 20:10:26,762 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 2, healthy pipeline threshold count is 1
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:52210 remote=scm2/10.9.0.15:9861]
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
om2_1    | 2023-05-18 20:10:36,686 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm3_1   | 2023-05-18 20:10:26,763 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
recon_1  | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
scm1_1   | 2023-05-18 20:10:05,550 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | 2023-05-18 20:10:05,552 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-05-18 20:10:18,165 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:18,617 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn4_1    | 2023-05-18 20:10:18,618 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 07404b21-602d-47db-8a51-48d80ebd16ac
dn4_1    | 2023-05-18 20:10:18,751 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/78d664ca-7989-4249-9a90-24fe3dd84618/in_use.lock acquired by nodename 6@5dd7996c298c
dn4_1    | 2023-05-18 20:10:18,767 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=caabb77a-6190-403e-a139-2e30ca474b23} from /data/metadata/ratis/78d664ca-7989-4249-9a90-24fe3dd84618/current/raft-meta
om2_1    | 2023-05-18 20:10:36,696 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm3_1   | 2023-05-18 20:10:27,033 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn5_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn5_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 2023-05-18 20:10:19,669 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-05-18 20:10:19,669 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-05-18 20:10:19,669 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
om2_1    | 2023-05-18 20:10:36,699 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm3_1   | 2023-05-18 20:10:31,932 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
scm2_1   | 2023-05-18 20:10:12,877 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f
scm2_1   | 2023-05-18 20:10:12,878 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm2_1   | 2023-05-18 20:10:12,878 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om2_1    | 2023-05-18 20:10:38,357 [main] INFO reflections.Reflections: Reflections took 2682 ms to scan 8 urls, producing 24 keys and 609 values [using 2 cores]
scm3_1   | 2023-05-18 20:10:32,184 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn5_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn5_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
om2_1    | 2023-05-18 20:10:38,990 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn3_1    | 2023-05-18 20:10:19,669 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn5_1    | 2023-05-18 20:10:18,156 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1  | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
scm3_1   | 2023-05-18 20:10:32,725 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
om3_1    | 2023-05-18 20:10:42,022 [om3-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 6@27bf3cdde8a3
dn3_1    | 2023-05-18 20:10:19,669 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/78d664ca-7989-4249-9a90-24fe3dd84618
dn3_1    | 2023-05-18 20:10:19,669 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-05-18 20:10:19,674 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm1_1   | 2023-05-18 20:10:05,552 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-05-18 20:10:05,572 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f
scm3_1   | 2023-05-18 20:10:33,006 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
om2_1    | 2023-05-18 20:10:39,040 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om3_1    | 2023-05-18 20:10:42,083 [om3-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=2, votedFor=om1} from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/raft-meta
om3_1    | 2023-05-18 20:10:42,341 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-05-18 20:10:42,359 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-05-18 20:10:18,179 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn1_1    | 2023-05-18 20:10:24,068 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C: changes role from      null to FOLLOWER at term 3 for startAsFollower
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn4_1    | 2023-05-18 20:10:18,777 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/db288034-f1fd-42b9-aec0-461572f3e524/in_use.lock acquired by nodename 6@5dd7996c298c
dn4_1    | 2023-05-18 20:10:18,791 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=07404b21-602d-47db-8a51-48d80ebd16ac} from /data/metadata/ratis/db288034-f1fd-42b9-aec0-461572f3e524/current/raft-meta
dn4_1    | 2023-05-18 20:10:18,805 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a2756a42-16da-435f-a15a-f31dfb92d20c/in_use.lock acquired by nodename 6@5dd7996c298c
dn4_1    | 2023-05-18 20:10:18,812 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=76248598-2f32-4c49-8f2a-b1fddb9f8151} from /data/metadata/ratis/a2756a42-16da-435f-a15a-f31dfb92d20c/current/raft-meta
dn4_1    | 2023-05-18 20:10:18,949 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524: set configuration 0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-18 20:10:33,059 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om2_1    | 2023-05-18 20:10:41,241 [Listener at om2/9862] INFO om.OzoneManagerPrepareState: Deleted prepare marker file: /data/metadata/current/prepareMarker
om3_1    | 2023-05-18 20:10:42,402 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-05-18 20:10:19,675 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-05-18 20:10:19,675 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-05-18 20:10:18,497 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn1_1    | 2023-05-18 20:10:24,067 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO impl.RoleInfo: 76248598-2f32-4c49-8f2a-b1fddb9f8151: start 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-FollowerState
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn4_1    | 2023-05-18 20:10:18,976 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C: set configuration 0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-18 20:10:05,574 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1_1   | 2023-05-18 20:10:05,576 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
s3g_1    | 2023-05-18 20:11:18,032 [qtp605052357-21] WARN impl.MetricsSystemImpl: S3Gateway metrics system already initialized!
s3g_1    | 2023-05-18 20:11:40,841 [qtp605052357-24] INFO rpc.RpcClient: Creating Bucket: s3v/old1-bucket, with bucket layout OBJECT_STORE, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false, Replication Type set to server-side default replication type, Namespace Quota set to -1, Space Quota set to -1 
scm3_1   | 2023-05-18 20:10:33,065 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
om2_1    | 2023-05-18 20:10:41,602 [Listener at om2/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om3_1    | 2023-05-18 20:10:42,402 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-05-18 20:10:42,403 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om3_1    | 2023-05-18 20:10:42,404 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-05-18 20:10:18,497 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f
dn1_1    | 2023-05-18 20:10:24,021 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F: start as a follower, conf=0: peers:[76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm1_1   | 2023-05-18 20:10:05,578 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1_1   | 2023-05-18 20:10:05,581 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1_1   | 2023-05-18 20:10:05,582 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | 2023-05-18 20:10:05,585 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1_1   | 2023-05-18 20:10:05,585 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-05-18 20:10:19,679 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-05-18 20:10:18,978 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618: set configuration 0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-18 20:10:19,046 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO ratis.ContainerStateMachine: group-24FE3DD84618: Setting the last applied index to (t:1, i:0)
dn4_1    | 2023-05-18 20:10:19,052 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO ratis.ContainerStateMachine: group-461572F3E524: Setting the last applied index to (t:1, i:0)
dn4_1    | 2023-05-18 20:10:19,055 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO ratis.ContainerStateMachine: group-F31DFB92D20C: Setting the last applied index to (t:3, i:12)
dn4_1    | 2023-05-18 20:10:19,170 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-05-18 20:10:05,585 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1_1   | 2023-05-18 20:10:05,610 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1_1   | 2023-05-18 20:10:05,610 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-18 20:10:19,664 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-05-18 20:10:19,670 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm2_1   | 2023-05-18 20:10:12,879 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2_1   | 2023-05-18 20:10:12,879 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm2_1   | 2023-05-18 20:10:12,880 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm2_1   | 2023-05-18 20:10:12,885 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm2_1   | 2023-05-18 20:10:12,885 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm2_1   | 2023-05-18 20:10:12,886 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-05-18 20:10:19,686 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om2_1    | 2023-05-18 20:10:41,659 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om2_1    | 2023-05-18 20:10:41,659 [Listener at om2/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om2_1    | 2023-05-18 20:10:42,062 [Listener at om2/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om2/10.9.0.12:9862
scm1_1   | 2023-05-18 20:10:05,799 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-05-18 20:10:19,705 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm2_1   | 2023-05-18 20:10:12,897 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm2_1   | 2023-05-18 20:10:12,898 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-05-18 20:10:13,088 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm2_1   | 2023-05-18 20:10:13,089 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm2_1   | 2023-05-18 20:10:13,090 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm2_1   | 2023-05-18 20:10:13,121 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServer$Division: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F: set configuration 0: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-18 20:10:19,686 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-05-18 20:10:19,694 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-05-18 20:10:19,697 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-05-18 20:10:19,697 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-05-18 20:10:19,811 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1_1   | 2023-05-18 20:10:05,799 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-05-18 20:10:19,711 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-05-18 20:10:13,126 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/current/log_0-0
scm2_1   | 2023-05-18 20:10:13,134 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServer$Division: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F: set configuration 1: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-05-18 20:10:13,147 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServer$Division: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F: set configuration 17: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm2_1   | 2023-05-18 20:10:13,148 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServer$Division: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F: set configuration 19: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-05-18 20:10:13,149 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServer$Division: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F: set configuration 31: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2620a8e2-fcef-44ea-bfea-c3742904e700|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm2_1   | 2023-05-18 20:10:13,150 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServer$Division: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F: set configuration 33: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2620a8e2-fcef-44ea-bfea-c3742904e700|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-05-18 20:10:13,198 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO segmented.LogSegment: Successfully read 46 entries from segment file /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/current/log_inprogress_1
scm1_1   | 2023-05-18 20:10:05,800 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
dn4_1    | 2023-05-18 20:10:19,708 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-05-18 20:10:19,720 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-18 20:10:19,722 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-05-18 20:10:19,812 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om1_1    | 2023-05-18 20:10:42,296 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1    | 2023-05-18 20:10:42,299 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-05-18 20:10:42,302 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om1_1    | 2023-05-18 20:10:42,303 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om2_1    | 2023-05-18 20:10:42,064 [Listener at om2/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om2 at port 9872
scm3_1   | 2023-05-18 20:10:33,273 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
dn4_1    | 2023-05-18 20:10:19,722 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm2_1   | 2023-05-18 20:10:13,203 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 46
dn3_1    | 2023-05-18 20:10:19,882 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
om1_1    | 2023-05-18 20:10:42,316 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1    | 2023-05-18 20:10:42,326 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
s3g_1    | 2023-05-18 20:11:41,883 [qtp605052357-20] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
s3g_1    | 2023-05-18 20:12:12,728 [qtp605052357-24] INFO rpc.RpcClient: Creating Bucket: s3v/new1-bucket, with bucket layout OBJECT_STORE, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false, Replication Type set to server-side default replication type, Namespace Quota set to -1, Space Quota set to -1 
scm1_1   | 2023-05-18 20:10:05,830 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServer$Division: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F: set configuration 0: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | , while invoking $Proxy42.submitRequest over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
recon_1  | 2023-05-18 20:10:18,198 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From cbc88b50144b/10.9.0.22 to scm3:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
recon_1  | 2023-05-18 20:10:22,117 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 7 pipelines from SCM.
dn4_1    | 2023-05-18 20:10:19,728 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm2_1   | 2023-05-18 20:10:13,259 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServer$Division: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F: start as a follower, conf=33: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2620a8e2-fcef-44ea-bfea-c3742904e700|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-18 20:10:19,887 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-05-18 20:10:24,075 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F: changes role from      null to FOLLOWER at term 1 for startAsFollower
dn1_1    | 2023-05-18 20:10:24,086 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-24FE3DD84618,id=76248598-2f32-4c49-8f2a-b1fddb9f8151
om1_1    | 2023-05-18 20:10:42,326 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1_1   | 2023-05-18 20:10:05,831 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/current/log_0-0
recon_1  | 2023-05-18 20:10:22,144 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 7 pipelines in house.
scm3_1   | 2023-05-18 20:10:33,306 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn5_1    | 2023-05-18 20:10:18,617 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bd9fbce3-479f-4a75-8657-2fbbef77032f/in_use.lock acquired by nodename 7@cd98f882537f
dn4_1    | 2023-05-18 20:10:19,736 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-05-18 20:10:19,747 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-05-18 20:10:19,899 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-05-18 20:10:24,089 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-05-18 20:10:24,125 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO impl.RoleInfo: 76248598-2f32-4c49-8f2a-b1fddb9f8151: start 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-FollowerState
dn1_1    | 2023-05-18 20:10:24,075 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO impl.RoleInfo: 76248598-2f32-4c49-8f2a-b1fddb9f8151: start 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-FollowerState
om2_1    | 2023-05-18 20:10:42,101 [om2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 8@a6ec66d4f37a
scm1_1   | 2023-05-18 20:10:05,834 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServer$Division: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F: set configuration 1: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 2023-05-18 20:10:22,660 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
scm3_1   | 2023-05-18 20:10:33,310 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
dn5_1    | 2023-05-18 20:10:18,639 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=9cd97d2e-fca5-4ac1-8a4f-be6d224f499f} from /data/metadata/ratis/bd9fbce3-479f-4a75-8657-2fbbef77032f/current/raft-meta
dn5_1    | 2023-05-18 20:10:18,836 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServer$Division: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F: set configuration 0: peers:[9cd97d2e-fca5-4ac1-8a4f-be6d224f499f|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-18 20:10:18,917 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO ratis.ContainerStateMachine: group-2FBBEF77032F: Setting the last applied index to (t:1, i:0)
dn3_1    | 2023-05-18 20:10:19,902 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-05-18 20:10:24,134 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-05-18 20:10:24,135 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-05-18 20:10:42,417 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1    | 2023-05-18 20:10:42,181 [om2-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=2, votedFor=} from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/raft-meta
om2_1    | 2023-05-18 20:10:42,397 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-05-18 20:10:42,413 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
recon_1  | 2023-05-18 20:10:22,728 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
scm3_1   | 2023-05-18 20:10:33,593 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
dn5_1    | 2023-05-18 20:10:19,116 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:19,749 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-05-18 20:10:19,751 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-05-18 20:10:19,903 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-05-18 20:10:24,135 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1    | 2023-05-18 20:10:42,326 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-05-18 20:10:42,346 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om2_1    | 2023-05-18 20:10:42,461 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1_1   | 2023-05-18 20:10:05,842 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServer$Division: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F: set configuration 17: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
recon_1  | 2023-05-18 20:10:22,777 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3_1   | 2023-05-18 20:10:33,648 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
dn5_1    | 2023-05-18 20:10:19,671 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-05-18 20:10:19,749 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-05-18 20:10:19,749 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-05-18 20:10:21,057 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:24,138 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 2023-05-18 20:10:42,350 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om3_1    | 2023-05-18 20:10:42,457 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om2_1    | 2023-05-18 20:10:42,466 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-05-18 20:10:05,843 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServer$Division: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F: set configuration 19: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-18 20:10:05,845 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServer$Division: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F: set configuration 31: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2620a8e2-fcef-44ea-bfea-c3742904e700|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1_1   | 2023-05-18 20:10:05,847 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServer$Division: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F: set configuration 33: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2620a8e2-fcef-44ea-bfea-c3742904e700|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-18 20:10:05,892 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO segmented.LogSegment: Successfully read 46 entries from segment file /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/current/log_inprogress_1
dn5_1    | 2023-05-18 20:10:19,724 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-05-18 20:10:19,761 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-05-18 20:10:19,761 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-05-18 20:10:21,058 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-05-18 20:10:24,193 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-05-18 20:10:24,194 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-05-18 20:10:24,164 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F31DFB92D20C,id=76248598-2f32-4c49-8f2a-b1fddb9f8151
om2_1    | 2023-05-18 20:10:42,475 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
recon_1  | 2023-05-18 20:10:22,788 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1  | 2023-05-18 20:10:24,967 [IPC Server handler 6 on default port 9891] WARN ipc.Server: IPC Server handler 6 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:55360: output error
scm3_1   | Container Balancer status:
dn5_1    | 2023-05-18 20:10:19,725 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-18 20:10:19,728 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-05-18 20:10:19,736 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-05-18 20:10:21,059 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-05-18 20:10:24,215 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om1_1    | 2023-05-18 20:10:42,350 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om3_1    | 2023-05-18 20:10:42,457 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om2_1    | 2023-05-18 20:10:42,480 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1_1   | 2023-05-18 20:10:05,897 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO segmented.SegmentedRaftLogWorker: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 46
recon_1  | 2023-05-18 20:10:25,011 [IPC Server handler 28 on default port 9891] WARN ipc.Server: IPC Server handler 28 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:52118: output error
scm3_1   | Key                            Value
dn5_1    | 2023-05-18 20:10:19,751 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-05-18 20:10:19,791 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-05-18 20:10:19,793 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-05-18 20:10:21,080 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-05-18 20:10:24,216 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
om1_1    | 2023-05-18 20:10:42,352 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1    | 2023-05-18 20:10:42,352 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om3_1    | 2023-05-18 20:10:42,458 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-05-18 20:10:42,481 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om3@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
recon_1  | 2023-05-18 20:10:25,013 [IPC Server handler 31 on default port 9891] WARN ipc.Server: IPC Server handler 31 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:57916: output error
recon_1  | 2023-05-18 20:10:25,055 [IPC Server handler 31 on default port 9891] INFO ipc.Server: IPC Server handler 31 on default port 9891 caught an exception
scm3_1   | Running                        false
scm3_1   | Container Balancer Configuration values:
scm3_1   | Key                                                Value
dn3_1    | 2023-05-18 20:10:21,113 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-05-18 20:10:19,764 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-05-18 20:10:24,218 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-05-18 20:10:24,220 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
om3_1    | 2023-05-18 20:10:42,481 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om3_1    | 2023-05-18 20:10:42,482 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om3_1    | 2023-05-18 20:10:42,483 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
recon_1  | java.nio.channels.ClosedChannelException
scm3_1   | Threshold                                          10
scm3_1   | Max Datanodes to Involve per Iteration(percent)    20
scm3_1   | Max Size to Move per Iteration                     500GB
scm3_1   | Max Size Entering Target per Iteration             26GB
om1_1    | 2023-05-18 20:10:42,354 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om3_1    | 2023-05-18 20:10:42,505 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1_1   | 2023-05-18 20:10:05,969 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServer$Division: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F: start as a follower, conf=33: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2620a8e2-fcef-44ea-bfea-c3742904e700|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-18 20:10:05,969 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServer$Division: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F: changes role from      null to FOLLOWER at term 2 for startAsFollower
scm1_1   | 2023-05-18 20:10:05,971 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO impl.RoleInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d: start dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-FollowerState
scm2_1   | 2023-05-18 20:10:13,260 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServer$Division: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F: changes role from      null to FOLLOWER at term 2 for startAsFollower
scm3_1   | Max Size Leaving Source per Iteration              26GB
scm3_1   | 
scm3_1   | 2023-05-18 20:10:33,672 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
om1_1    | 2023-05-18 20:10:42,355 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om1_1    | 2023-05-18 20:10:42,355 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om2_1    | 2023-05-18 20:10:42,489 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1    | 2023-05-18 20:10:42,499 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om3_1    | 2023-05-18 20:10:42,506 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-05-18 20:10:19,794 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-18 20:10:19,822 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bd9fbce3-479f-4a75-8657-2fbbef77032f
scm1_1   | 2023-05-18 20:10:05,974 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-E2B0BF76E94F,id=dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d
scm1_1   | 2023-05-18 20:10:05,976 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1   | 2023-05-18 20:10:05,976 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1_1   | 2023-05-18 20:10:05,977 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm2_1   | 2023-05-18 20:10:13,261 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO impl.RoleInfo: 78aeffd2-0882-43d0-ab48-78e394e586e6: start 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F-FollowerState
dn4_1    | 2023-05-18 20:10:19,789 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om2_1    | 2023-05-18 20:10:42,499 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om2_1    | 2023-05-18 20:10:42,499 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-05-18 20:10:42,522 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-05-18 20:10:19,824 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-05-18 20:10:19,826 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-05-18 20:10:21,199 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-05-18 20:10:21,199 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-05-18 20:10:21,199 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-05-18 20:10:21,322 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm2_1   | 2023-05-18 20:10:13,262 [78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2_1   | 2023-05-18 20:10:13,262 [78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-05-18 20:10:42,523 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 2023-05-18 20:10:33,756 [Listener at 0.0.0.0/9860] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
scm3_1   | 2023-05-18 20:10:33,780 [Listener at 0.0.0.0/9860] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
scm3_1   | 2023-05-18 20:10:33,791 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm3_1   | 2023-05-18 20:10:34,223 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm2_1   | 2023-05-18 20:10:13,264 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-E2B0BF76E94F,id=78aeffd2-0882-43d0-ab48-78e394e586e6
scm2_1   | 2023-05-18 20:10:13,266 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om3_1    | 2023-05-18 20:10:42,525 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn5_1    | 2023-05-18 20:10:19,831 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm1_1   | 2023-05-18 20:10:05,978 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1   | 2023-05-18 20:10:05,985 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-18 20:10:05,988 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-18 20:10:05,989 [Listener at 0.0.0.0/9860] INFO server.RaftServer: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d: start RPC server
scm1_1   | 2023-05-18 20:10:06,031 [Listener at 0.0.0.0/9860] INFO server.GrpcService: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d: GrpcService started, listening on 9894
dn4_1    | 2023-05-18 20:10:19,790 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om3_1    | 2023-05-18 20:10:42,652 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
om1_1    | 2023-05-18 20:10:42,356 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-05-18 20:10:19,833 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-05-18 20:10:19,834 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-05-18 20:10:21,322 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-05-18 20:10:21,378 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-05-18 20:10:21,448 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C: set configuration 0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-18 20:10:24,165 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8EDF1C67E99F,id=76248598-2f32-4c49-8f2a-b1fddb9f8151
dn1_1    | 2023-05-18 20:10:24,279 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm2_1   | 2023-05-18 20:10:13,267 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm2_1   | 2023-05-18 20:10:13,268 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
dn5_1    | 2023-05-18 20:10:19,846 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-05-18 20:10:19,791 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-18 20:10:19,793 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om2_1    | 2023-05-18 20:10:42,524 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om2@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
dn1_1    | 2023-05-18 20:10:24,289 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-05-18 20:10:24,289 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
om1_1    | 2023-05-18 20:10:42,383 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm3_1   | 2023-05-18 20:10:34,438 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/in_use.lock acquired by nodename 8@989cc3f1e886
scm3_1   | 2023-05-18 20:10:34,517 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=2, votedFor=} from /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/current/raft-meta
dn5_1    | 2023-05-18 20:10:19,846 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-05-18 20:10:19,794 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-05-18 20:10:19,794 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-05-18 20:10:42,524 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
dn1_1    | 2023-05-18 20:10:24,294 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-05-18 20:10:24,296 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
om1_1    | 2023-05-18 20:10:42,383 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-05-18 20:10:42,486 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om1_1    | 2023-05-18 20:10:42,487 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-05-18 20:10:19,847 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-05-18 20:10:19,804 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-05-18 20:10:21,454 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4: set configuration 0: peers:[caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-18 20:10:21,473 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618: set configuration 0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-18 20:10:06,036 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d: Started
scm1_1   | 2023-05-18 20:10:06,048 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2620a8e2-fcef-44ea-bfea-c3742904e700|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
dn1_1    | 2023-05-18 20:10:24,296 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-05-18 20:10:24,314 [76248598-2f32-4c49-8f2a-b1fddb9f8151-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
om1_1    | 2023-05-18 20:10:42,487 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om1_1    | 2023-05-18 20:10:42,520 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 24
om1_1    | 2023-05-18 20:10:42,520 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-05-18 20:10:19,897 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-05-18 20:10:19,805 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om2_1    | 2023-05-18 20:10:42,532 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om2_1    | 2023-05-18 20:10:42,532 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1    | 2023-05-18 20:10:42,536 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om2_1    | 2023-05-18 20:10:42,536 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om2_1    | 2023-05-18 20:10:42,537 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm2_1   | 2023-05-18 20:10:13,268 [78aeffd2-0882-43d0-ab48-78e394e586e6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm2_1   | 2023-05-18 20:10:13,273 [Listener at 0.0.0.0/9860] INFO server.RaftServer: 78aeffd2-0882-43d0-ab48-78e394e586e6: start RPC server
scm3_1   | 2023-05-18 20:10:34,958 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServer$Division: 2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F: set configuration 33: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2620a8e2-fcef-44ea-bfea-c3742904e700|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-18 20:10:19,899 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-18 20:10:19,807 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 2023-05-18 20:10:06,049 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm1_1   | 2023-05-18 20:10:06,467 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm1_1   | 2023-05-18 20:10:06,498 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm1_1   | 2023-05-18 20:10:06,498 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm2_1   | 2023-05-18 20:10:13,318 [Listener at 0.0.0.0/9860] INFO server.GrpcService: 78aeffd2-0882-43d0-ab48-78e394e586e6: GrpcService started, listening on 9894
scm2_1   | 2023-05-18 20:10:13,324 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-78aeffd2-0882-43d0-ab48-78e394e586e6: Started
scm3_1   | 2023-05-18 20:10:35,014 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-05-18 20:10:19,954 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-05-18 20:10:19,847 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a2756a42-16da-435f-a15a-f31dfb92d20c
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm1_1   | 2023-05-18 20:10:07,008 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm1_1   | 2023-05-18 20:10:07,011 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-05-18 20:10:07,012 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm1_1   | 2023-05-18 20:10:07,074 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm2_1   | 2023-05-18 20:10:13,334 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2620a8e2-fcef-44ea-bfea-c3742904e700|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm2_1   | 2023-05-18 20:10:13,334 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm3_1   | 2023-05-18 20:10:35,125 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-05-18 20:10:19,958 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-05-18 20:10:19,851 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm1_1   | 2023-05-18 20:10:07,332 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm1_1   | 2023-05-18 20:10:07,336 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-05-18 20:10:07,342 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
dn3_1    | 2023-05-18 20:10:21,475 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/78d664ca-7989-4249-9a90-24fe3dd84618/current/log_inprogress_0
scm3_1   | 2023-05-18 20:10:35,130 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 2023-05-18 20:10:35,143 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-05-18 20:10:19,959 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-05-18 20:10:20,117 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om2_1    | 2023-05-18 20:10:42,537 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-05-18 20:10:21,474 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/9693fb8d-2a36-45cd-aee5-9db787f5d8f4/current/log_inprogress_0
dn3_1    | 2023-05-18 20:10:21,541 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO segmented.SegmentedRaftLogWorker: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-05-18 20:10:21,561 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO segmented.SegmentedRaftLogWorker: caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-05-18 20:10:21,677 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO segmented.LogSegment: Successfully read 13 entries from segment file /data/metadata/ratis/a2756a42-16da-435f-a15a-f31dfb92d20c/current/log_inprogress_0
dn3_1    | 2023-05-18 20:10:21,677 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO segmented.SegmentedRaftLogWorker: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 12
dn3_1    | 2023-05-18 20:10:21,681 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO segmented.SegmentedRaftLogWorker: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-05-18 20:10:22,061 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:22,553 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618: start as a follower, conf=0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-05-18 20:10:13,549 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om2_1    | 2023-05-18 20:10:42,537 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om2_1    | 2023-05-18 20:10:42,576 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om2_1    | 2023-05-18 20:10:42,576 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-05-18 20:10:42,654 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-05-18 20:10:42,841 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om3_1    | 2023-05-18 20:10:42,843 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om3_1    | 2023-05-18 20:10:42,856 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 2023-05-18 20:10:07,609 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1d99ee1b] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn1_1    | 2023-05-18 20:10:24,351 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 76248598-2f32-4c49-8f2a-b1fddb9f8151: start RPC server
dn1_1    | 2023-05-18 20:10:24,354 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 76248598-2f32-4c49-8f2a-b1fddb9f8151: GrpcService started, listening on 9858
om1_1    | 2023-05-18 20:10:42,543 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: start as a follower, conf=0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-05-18 20:10:42,543 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from      null to FOLLOWER at term 2 for startAsFollower
om1_1    | 2023-05-18 20:10:42,551 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
scm3_1   | 2023-05-18 20:10:35,144 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm3_1   | 2023-05-18 20:10:35,161 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2_1   | 2023-05-18 20:10:13,569 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm2_1   | 2023-05-18 20:10:13,569 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm2_1   | 2023-05-18 20:10:13,902 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm2_1   | 2023-05-18 20:10:13,902 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2_1   | 2023-05-18 20:10:13,917 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
dn1_1    | 2023-05-18 20:10:24,360 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 76248598-2f32-4c49-8f2a-b1fddb9f8151: GrpcService started, listening on 9856
dn1_1    | 2023-05-18 20:10:24,366 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 76248598-2f32-4c49-8f2a-b1fddb9f8151: GrpcService started, listening on 9857
dn1_1    | 2023-05-18 20:10:24,415 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 76248598-2f32-4c49-8f2a-b1fddb9f8151 is started using port 9858 for RATIS
dn1_1    | 2023-05-18 20:10:24,430 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 76248598-2f32-4c49-8f2a-b1fddb9f8151 is started using port 9857 for RATIS_ADMIN
dn1_1    | 2023-05-18 20:10:24,431 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 76248598-2f32-4c49-8f2a-b1fddb9f8151 is started using port 9856 for RATIS_SERVER
dn1_1    | 2023-05-18 20:10:24,433 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-76248598-2f32-4c49-8f2a-b1fddb9f8151: Started
dn1_1    | 2023-05-18 20:10:24,501 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:24,509 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn4_1    | 2023-05-18 20:10:19,853 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-05-18 20:10:19,863 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new 07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/db288034-f1fd-42b9-aec0-461572f3e524
om2_1    | 2023-05-18 20:10:42,708 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om2_1    | 2023-05-18 20:10:42,723 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om2_1    | 2023-05-18 20:10:42,723 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om2_1    | 2023-05-18 20:10:42,760 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 24
om2_1    | 2023-05-18 20:10:42,760 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om2_1    | 2023-05-18 20:10:42,778 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: start as a follower, conf=0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-18 20:10:35,208 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm3_1   | 2023-05-18 20:10:35,214 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm3_1   | 2023-05-18 20:10:35,218 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 2023-05-18 20:10:35,257 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f
scm3_1   | 2023-05-18 20:10:35,261 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm3_1   | 2023-05-18 20:10:35,273 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm3_1   | 2023-05-18 20:10:35,276 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1    | 2023-05-18 20:10:42,909 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 24
om3_1    | 2023-05-18 20:10:42,920 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-05-18 20:10:24,547 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-05-18 20:10:25,502 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:26,503 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:27,504 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:22,565 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618: changes role from      null to FOLLOWER at term 1 for startAsFollower
dn3_1    | 2023-05-18 20:10:22,567 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO impl.RoleInfo: caabb77a-6190-403e-a139-2e30ca474b23: start caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-FollowerState
dn5_1    | 2023-05-18 20:10:20,499 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServer$Division: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F: set configuration 0: peers:[9cd97d2e-fca5-4ac1-8a4f-be6d224f499f|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-18 20:10:20,521 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/bd9fbce3-479f-4a75-8657-2fbbef77032f/current/log_inprogress_0
dn4_1    | 2023-05-18 20:10:19,869 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
om1_1    | 2023-05-18 20:10:42,573 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om1
om1_1    | 2023-05-18 20:10:42,575 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm2_1   | 2023-05-18 20:10:13,944 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm2_1   | 2023-05-18 20:10:13,944 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm2_1   | 2023-05-18 20:10:13,946 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2_1   | 2023-05-18 20:10:13,946 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
dn3_1    | 2023-05-18 20:10:22,592 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C: start as a follower, conf=0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-18 20:10:22,593 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-24FE3DD84618,id=caabb77a-6190-403e-a139-2e30ca474b23
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn4_1    | 2023-05-18 20:10:19,869 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-05-18 20:10:20,680 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om3_1    | 2023-05-18 20:10:42,957 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: start as a follower, conf=0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-05-18 20:10:42,575 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 2023-05-18 20:10:42,587 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-05-18 20:10:28,506 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:29,238 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-FollowerState] INFO impl.FollowerState: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5163125350ns, electionTimeout:5102ms
dn1_1    | 2023-05-18 20:10:29,239 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-FollowerState] INFO impl.RoleInfo: 76248598-2f32-4c49-8f2a-b1fddb9f8151: shutdown 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-FollowerState
dn3_1    | 2023-05-18 20:10:24,339 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-05-18 20:10:24,368 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-05-18 20:10:24,363 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4: start as a follower, conf=0: peers:[caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn5_1    | 2023-05-18 20:10:21,125 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-05-18 20:10:42,958 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from      null to FOLLOWER at term 2 for startAsFollower
om3_1    | 2023-05-18 20:10:42,964 [om3-impl-thread1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
om3_1    | 2023-05-18 20:10:43,022 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-05-18 20:10:42,779 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from      null to FOLLOWER at term 2 for startAsFollower
om2_1    | 2023-05-18 20:10:42,895 [om2-impl-thread1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
om2_1    | 2023-05-18 20:10:42,985 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-05-18 20:10:42,987 [om2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om2
om2_1    | 2023-05-18 20:10:43,006 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om2_1    | 2023-05-18 20:10:43,011 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om2_1    | 2023-05-18 20:10:43,015 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om2_1    | 2023-05-18 20:10:43,020 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om2_1    | 2023-05-18 20:10:43,019 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-05-18 20:10:43,022 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-05-18 20:10:25,055 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:57374: output error
om2_1    | 2023-05-18 20:10:43,059 [Listener at om2/9862] INFO server.RaftServer: om2: start RPC server
dn4_1    | 2023-05-18 20:10:19,869 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-05-18 20:10:19,871 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-05-18 20:10:19,872 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-05-18 20:10:19,875 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | 2023-05-18 20:10:07,648 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm1_1   | 2023-05-18 20:10:07,648 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm1_1   | 2023-05-18 20:10:07,777 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @51718ms to org.eclipse.jetty.util.log.Slf4jLog
scm1_1   | 2023-05-18 20:10:07,976 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om2_1    | 2023-05-18 20:10:43,463 [Listener at om2/9862] INFO server.GrpcService: om2: GrpcService started, listening on 9872
scm2_1   | 2023-05-18 20:10:14,029 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@53feeac9] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm2_1   | 2023-05-18 20:10:14,040 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
om1_1    | 2023-05-18 20:10:42,587 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om1_1    | 2023-05-18 20:10:42,588 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om1_1    | 2023-05-18 20:10:42,588 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om1_1    | 2023-05-18 20:10:42,631 [Listener at om1/9862] INFO server.RaftServer: om1: start RPC server
om1_1    | 2023-05-18 20:10:42,819 [Listener at om1/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om1_1    | 2023-05-18 20:10:42,834 [Listener at om1/9862] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
om1_1    | 2023-05-18 20:10:42,836 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om2_1    | 2023-05-18 20:10:43,501 [Listener at om2/9862] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
om2_1    | 2023-05-18 20:10:43,512 [Listener at om2/9862] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
scm3_1   | 2023-05-18 20:10:35,285 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm3_1   | 2023-05-18 20:10:35,289 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm3_1   | 2023-05-18 20:10:35,292 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm3_1   | 2023-05-18 20:10:35,295 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3_1   | 2023-05-18 20:10:35,296 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3_1   | 2023-05-18 20:10:35,372 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm3_1   | 2023-05-18 20:10:35,373 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-05-18 20:10:14,040 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1  | 2023-05-18 20:10:25,047 [IPC Server handler 25 on default port 9891] WARN ipc.Server: IPC Server handler 25 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:55808: output error
scm3_1   | 2023-05-18 20:10:35,999 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3_1   | 2023-05-18 20:10:36,000 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-05-18 20:10:29,239 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-FollowerState] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn1_1    | 2023-05-18 20:10:29,242 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-05-18 20:10:29,242 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-FollowerState] INFO impl.RoleInfo: 76248598-2f32-4c49-8f2a-b1fddb9f8151: start 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1
dn1_1    | 2023-05-18 20:10:29,246 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO impl.LeaderElection: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 3 for 0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-18 20:10:29,303 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-FollowerState] INFO impl.FollowerState: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5177452253ns, electionTimeout:5108ms
scm2_1   | 2023-05-18 20:10:14,154 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @7206ms to org.eclipse.jetty.util.log.Slf4jLog
scm2_1   | 2023-05-18 20:10:14,447 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1  | 2023-05-18 20:10:25,078 [IPC Server handler 25 on default port 9891] INFO ipc.Server: IPC Server handler 25 on default port 9891 caught an exception
dn4_1    | 2023-05-18 20:10:19,875 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-05-18 20:10:19,878 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-05-18 20:10:19,879 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-05-18 20:10:19,879 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-05-18 20:10:19,879 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-05-18 20:10:19,882 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-05-18 20:10:19,884 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-05-18 20:10:19,889 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm2_1   | 2023-05-18 20:10:14,466 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm2_1   | 2023-05-18 20:10:14,492 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn4_1    | 2023-05-18 20:10:19,914 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/78d664ca-7989-4249-9a90-24fe3dd84618
dn3_1    | 2023-05-18 20:10:24,374 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4: changes role from      null to FOLLOWER at term 1 for startAsFollower
dn3_1    | 2023-05-18 20:10:24,340 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:24,404 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn3_1    | 2023-05-18 20:10:24,404 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-05-18 20:10:24,405 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO impl.RoleInfo: caabb77a-6190-403e-a139-2e30ca474b23: start caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-FollowerState
dn3_1    | 2023-05-18 20:10:24,374 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO impl.RoleInfo: caabb77a-6190-403e-a139-2e30ca474b23: start caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-FollowerState
dn3_1    | 2023-05-18 20:10:24,440 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-05-18 20:10:24,447 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-05-18 20:10:19,921 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-05-18 20:10:19,922 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm1_1   | 2023-05-18 20:10:07,982 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm1_1   | 2023-05-18 20:10:07,989 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm1_1   | 2023-05-18 20:10:07,991 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm1_1   | 2023-05-18 20:10:07,991 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm1_1   | 2023-05-18 20:10:07,992 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm1_1   | 2023-05-18 20:10:08,059 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm1_1   | 2023-05-18 20:10:08,062 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
om3_1    | 2023-05-18 20:10:43,028 [om3-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om3
om3_1    | 2023-05-18 20:10:43,045 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om3_1    | 2023-05-18 20:10:43,048 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om3_1    | 2023-05-18 20:10:43,054 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om3_1    | 2023-05-18 20:10:43,062 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om1_1    | 2023-05-18 20:10:42,868 [Listener at om1/9862] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
om1_1    | 2023-05-18 20:10:42,873 [Listener at om1/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om1_1    | 2023-05-18 20:10:43,094 [Listener at om1/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om1_1    | 2023-05-18 20:10:43,098 [Listener at om1/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn5_1    | 2023-05-18 20:10:22,129 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:22,209 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServer$Division: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F: start as a follower, conf=0: peers:[9cd97d2e-fca5-4ac1-8a4f-be6d224f499f|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-18 20:10:29,306 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-FollowerState] INFO impl.RoleInfo: 76248598-2f32-4c49-8f2a-b1fddb9f8151: shutdown 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-FollowerState
dn1_1    | 2023-05-18 20:10:29,307 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-FollowerState] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm2_1   | 2023-05-18 20:10:14,498 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm2_1   | 2023-05-18 20:10:14,498 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm2_1   | 2023-05-18 20:10:14,499 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn5_1    | 2023-05-18 20:10:22,210 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServer$Division: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F: changes role from      null to FOLLOWER at term 1 for startAsFollower
om3_1    | 2023-05-18 20:10:43,110 [Listener at om3/9862] INFO server.RaftServer: om3: start RPC server
om3_1    | 2023-05-18 20:10:43,429 [Listener at om3/9862] INFO server.GrpcService: om3: GrpcService started, listening on 9872
dn1_1    | 2023-05-18 20:10:29,307 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-05-18 20:10:29,307 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-FollowerState] INFO impl.RoleInfo: 76248598-2f32-4c49-8f2a-b1fddb9f8151: start 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2
dn1_1    | 2023-05-18 20:10:29,317 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-FollowerState] INFO impl.FollowerState: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5250919690ns, electionTimeout:5169ms
dn1_1    | 2023-05-18 20:10:29,338 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-FollowerState] INFO impl.RoleInfo: 76248598-2f32-4c49-8f2a-b1fddb9f8151: shutdown 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-FollowerState
dn1_1    | 2023-05-18 20:10:29,338 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-FollowerState] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
dn1_1    | 2023-05-18 20:10:29,338 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-05-18 20:10:29,339 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-FollowerState] INFO impl.RoleInfo: 76248598-2f32-4c49-8f2a-b1fddb9f8151: start 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-LeaderElection3
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn5_1    | 2023-05-18 20:10:22,242 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO impl.RoleInfo: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f: start 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-FollowerState
om3_1    | 2023-05-18 20:10:43,441 [Listener at om3/9862] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
om3_1    | 2023-05-18 20:10:43,445 [Listener at om3/9862] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
om3_1    | 2023-05-18 20:10:43,445 [Listener at om3/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om3_1    | 2023-05-18 20:10:43,456 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om3: Started
om3_1    | 2023-05-18 20:10:43,742 [Listener at om3/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om3_1    | 2023-05-18 20:10:43,743 [Listener at om3/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om3_1    | 2023-05-18 20:10:43,820 [Listener at om3/9862] INFO util.log: Logging initialized @89469ms to org.eclipse.jetty.util.log.Slf4jLog
om3_1    | 2023-05-18 20:10:44,365 [Listener at om3/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm2_1   | 2023-05-18 20:10:14,558 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
om3_1    | 2023-05-18 20:10:44,380 [Listener at om3/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
scm1_1   | 2023-05-18 20:10:08,064 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
scm1_1   | 2023-05-18 20:10:08,133 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm1_1   | 2023-05-18 20:10:08,133 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
dn3_1    | 2023-05-18 20:10:24,453 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-05-18 20:10:24,457 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@b800c51] INFO util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1214ms
dn3_1    | GC pool 'ParNew' had collection(s): count=1 time=1683ms
om2_1    | 2023-05-18 20:10:43,512 [Listener at om2/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm2_1   | 2023-05-18 20:10:14,559 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
dn5_1    | 2023-05-18 20:10:22,243 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-05-18 20:10:22,243 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-05-18 20:10:44,414 [Listener at om3/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om3_1    | 2023-05-18 20:10:44,417 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om3_1    | 2023-05-18 20:10:44,425 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om3_1    | 2023-05-18 20:10:44,426 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om3_1    | 2023-05-18 20:10:44,472 [Listener at om3/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
scm3_1   | 2023-05-18 20:10:36,000 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm3_1   | 2023-05-18 20:10:36,154 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServer$Division: 2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F: set configuration 0: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-18 20:10:36,162 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/current/log_0-0
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn4_1    | 2023-05-18 20:10:19,924 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-05-18 20:10:22,258 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-2FBBEF77032F,id=9cd97d2e-fca5-4ac1-8a4f-be6d224f499f
om1_1    | 2023-05-18 20:10:43,215 [Listener at om1/9862] INFO util.log: Logging initialized @85469ms to org.eclipse.jetty.util.log.Slf4jLog
om1_1    | 2023-05-18 20:10:43,853 [Listener at om1/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om1_1    | 2023-05-18 20:10:43,900 [Listener at om1/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om1_1    | 2023-05-18 20:10:43,931 [Listener at om1/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om1_1    | 2023-05-18 20:10:43,956 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om1_1    | 2023-05-18 20:10:43,957 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om1_1    | 2023-05-18 20:10:43,958 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm1_1   | 2023-05-18 20:10:08,137 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn4_1    | 2023-05-18 20:10:19,932 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-05-18 20:10:22,292 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3_1   | 2023-05-18 20:10:36,175 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServer$Division: 2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F: set configuration 1: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-18 20:10:24,533 [caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-05-18 20:10:24,540 [caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-05-18 20:10:24,611 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-05-18 20:10:24,611 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-05-18 20:10:43,531 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om2: Started
om2_1    | 2023-05-18 20:10:43,808 [Listener at om2/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
scm1_1   | 2023-05-18 20:10:08,166 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@38f617f4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn4_1    | 2023-05-18 20:10:19,932 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-05-18 20:10:22,304 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm3_1   | 2023-05-18 20:10:36,243 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServer$Division: 2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F: set configuration 17: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3_1   | 2023-05-18 20:10:36,249 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServer$Division: 2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F: set configuration 19: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-18 20:10:36,265 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServer$Division: 2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F: set configuration 31: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2620a8e2-fcef-44ea-bfea-c3742904e700|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm2_1   | 2023-05-18 20:10:14,561 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
scm2_1   | 2023-05-18 20:10:14,630 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm2_1   | 2023-05-18 20:10:14,630 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm2_1   | 2023-05-18 20:10:14,635 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
scm2_1   | 2023-05-18 20:10:14,661 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2eebce87{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn4_1    | 2023-05-18 20:10:19,935 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-05-18 20:10:22,312 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-05-18 20:10:29,362 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2] INFO impl.LeaderElection: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-18 20:10:29,364 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2] INFO impl.LeaderElection: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2 PRE_VOTE round 0: result PASSED (term=1)
dn1_1    | 2023-05-18 20:10:29,407 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-LeaderElection3] INFO impl.LeaderElection: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-18 20:10:29,489 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-LeaderElection3-1] INFO server.GrpcServerProtocolClient: Build channel for 07404b21-602d-47db-8a51-48d80ebd16ac
dn1_1    | 2023-05-18 20:10:29,498 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-05-18 20:10:29,544 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-05-18 20:10:29,534 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-05-18 20:10:29,549 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-18 20:10:19,936 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-05-18 20:10:22,325 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-05-18 20:10:29,508 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-05-18 20:10:43,808 [Listener at om2/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om2_1    | 2023-05-18 20:10:43,974 [Listener at om2/9862] INFO util.log: Logging initialized @86547ms to org.eclipse.jetty.util.log.Slf4jLog
om2_1    | 2023-05-18 20:10:44,648 [Listener at om2/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om2_1    | 2023-05-18 20:10:44,686 [Listener at om2/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om2_1    | 2023-05-18 20:10:44,710 [Listener at om2/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om2_1    | 2023-05-18 20:10:44,738 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om2_1    | 2023-05-18 20:10:44,738 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn4_1    | 2023-05-18 20:10:20,029 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-05-18 20:10:22,814 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f: start RPC server
dn1_1    | 2023-05-18 20:10:29,498 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-LeaderElection3-2] INFO server.GrpcServerProtocolClient: Build channel for caabb77a-6190-403e-a139-2e30ca474b23
dn1_1    | 2023-05-18 20:10:29,609 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2] INFO impl.LeaderElection: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for 0: peers:[76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-18 20:10:08,167 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@13d289c7{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm1_1   | 2023-05-18 20:10:08,392 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@41e8d917{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-6943345815470119069/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm1_1   | 2023-05-18 20:10:08,408 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@18907af2{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm1_1   | 2023-05-18 20:10:08,409 [Listener at 0.0.0.0/9860] INFO server.Server: Started @52350ms
scm1_1   | 2023-05-18 20:10:08,416 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm1_1   | 2023-05-18 20:10:08,416 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn4_1    | 2023-05-18 20:10:20,088 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-05-18 20:10:20,090 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-05-18 20:10:44,738 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn1_1    | 2023-05-18 20:10:29,615 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2] INFO impl.LeaderElection: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2 ELECTION round 0: result PASSED (term=2)
scm1_1   | 2023-05-18 20:10:08,418 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
om1_1    | 2023-05-18 20:10:44,161 [Listener at om1/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
dn3_1    | 2023-05-18 20:10:24,612 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F31DFB92D20C,id=caabb77a-6190-403e-a139-2e30ca474b23
scm3_1   | 2023-05-18 20:10:36,268 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServer$Division: 2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F: set configuration 33: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2620a8e2-fcef-44ea-bfea-c3742904e700|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-18 20:10:36,468 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO segmented.LogSegment: Successfully read 46 entries from segment file /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/current/log_inprogress_1
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn4_1    | 2023-05-18 20:10:20,108 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-05-18 20:10:22,897 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f: GrpcService started, listening on 9858
om2_1    | 2023-05-18 20:10:45,050 [Listener at om2/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
dn1_1    | 2023-05-18 20:10:29,617 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2] INFO impl.RoleInfo: 76248598-2f32-4c49-8f2a-b1fddb9f8151: shutdown 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2
scm1_1   | 2023-05-18 20:10:11,184 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-FollowerState] INFO impl.FollowerState: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5213355153ns, electionTimeout:5194ms
om1_1    | 2023-05-18 20:10:44,173 [Listener at om1/9862] INFO http.HttpServer2: Jetty bound to port 9874
om1_1    | 2023-05-18 20:10:44,178 [Listener at om1/9862] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
om1_1    | 2023-05-18 20:10:44,480 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-om1: Detected pause in JVM or host machine approximately 0.127s without any GCs.
om1_1    | 2023-05-18 20:10:44,482 [Listener at om1/9862] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
om2_1    | 2023-05-18 20:10:45,058 [Listener at om2/9862] INFO http.HttpServer2: Jetty bound to port 9874
dn1_1    | 2023-05-18 20:10:29,617 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm1_1   | 2023-05-18 20:10:11,186 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-FollowerState] INFO impl.RoleInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d: shutdown dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-FollowerState
scm1_1   | 2023-05-18 20:10:11,189 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-FollowerState] INFO server.RaftServer$Division: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
scm1_1   | 2023-05-18 20:10:11,193 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm1_1   | 2023-05-18 20:10:11,194 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-FollowerState] INFO impl.RoleInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d: start dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection1
scm1_1   | 2023-05-18 20:10:11,200 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection1] INFO impl.LeaderElection: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 2 for 33: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2620a8e2-fcef-44ea-bfea-c3742904e700|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om2_1    | 2023-05-18 20:10:45,063 [Listener at om2/9862] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
dn1_1    | 2023-05-18 20:10:29,627 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8EDF1C67E99F with new leaderId: 76248598-2f32-4c49-8f2a-b1fddb9f8151
scm1_1   | 2023-05-18 20:10:11,295 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-05-18 20:10:24,613 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-05-18 20:10:24,613 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-05-18 20:10:24,613 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-05-18 20:10:24,612 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-9DB787F5D8F4,id=caabb77a-6190-403e-a139-2e30ca474b23
dn3_1    | 2023-05-18 20:10:24,615 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-05-18 20:10:24,621 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-05-18 20:10:22,907 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f: GrpcService started, listening on 9856
om2_1    | 2023-05-18 20:10:45,277 [Listener at om2/9862] INFO server.session: DefaultSessionIdManager workerName=node0
dn1_1    | 2023-05-18 20:10:29,683 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F: change Leader from null to 76248598-2f32-4c49-8f2a-b1fddb9f8151 at term 2 for becomeLeader, leader elected after 35249ms
scm1_1   | 2023-05-18 20:10:11,295 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-18 20:10:11,303 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 78aeffd2-0882-43d0-ab48-78e394e586e6
scm1_1   | 2023-05-18 20:10:11,313 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 2620a8e2-fcef-44ea-bfea-c3742904e700
scm1_1   | 2023-05-18 20:10:11,575 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection1] INFO impl.LeaderElection: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:11,575 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection1] INFO impl.LeaderElection: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:11,577 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection1] INFO impl.LeaderElection: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection1: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
scm1_1   | 2023-05-18 20:10:11,578 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection1] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-05-18 20:10:20,117 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-05-18 20:10:45,277 [Listener at om2/9862] INFO server.session: No SessionScavenger set, using defaults
dn1_1    | 2023-05-18 20:10:29,829 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1_1   | 2023-05-18 20:10:11,578 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection1] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:11,579 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection1] INFO impl.LeaderElection: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection1 PRE_VOTE round 0: result REJECTED
scm3_1   | 2023-05-18 20:10:36,513 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 46
scm3_1   | 2023-05-18 20:10:36,744 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServer$Division: 2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F: start as a follower, conf=33: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2620a8e2-fcef-44ea-bfea-c3742904e700|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-18 20:10:36,744 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServer$Division: 2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F: changes role from      null to FOLLOWER at term 2 for startAsFollower
scm3_1   | 2023-05-18 20:10:36,749 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO impl.RoleInfo: 2620a8e2-fcef-44ea-bfea-c3742904e700: start 2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState
scm3_1   | 2023-05-18 20:10:36,765 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-E2B0BF76E94F,id=2620a8e2-fcef-44ea-bfea-c3742904e700
dn5_1    | 2023-05-18 20:10:22,914 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f: GrpcService started, listening on 9857
dn4_1    | 2023-05-18 20:10:21,629 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:21,631 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
om2_1    | 2023-05-18 20:10:45,284 [Listener at om2/9862] INFO server.session: node0 Scavenging every 660000ms
dn1_1    | 2023-05-18 20:10:29,898 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm2_1   | 2023-05-18 20:10:14,661 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@74500e4f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm1_1   | 2023-05-18 20:10:11,580 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection1] INFO server.RaftServer$Division: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
scm3_1   | 2023-05-18 20:10:36,767 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-18 20:10:36,768 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-18 20:10:36,771 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3_1   | 2023-05-18 20:10:36,773 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
om1_1    | 2023-05-18 20:10:44,483 [Listener at om1/9862] INFO server.session: No SessionScavenger set, using defaults
om1_1    | 2023-05-18 20:10:44,492 [Listener at om1/9862] INFO server.session: node0 Scavenging every 660000ms
dn4_1    | 2023-05-18 20:10:21,632 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-18 20:10:21,699 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@10c5d0ff] INFO util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1299ms
om2_1    | 2023-05-18 20:10:45,389 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@48d5ca17{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn1_1    | 2023-05-18 20:10:29,909 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
scm2_1   | 2023-05-18 20:10:14,991 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5ab63a04{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-10737952902856043286/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm1_1   | 2023-05-18 20:10:11,580 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection1] INFO impl.RoleInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d: shutdown dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection1
scm1_1   | 2023-05-18 20:10:11,584 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection1] INFO impl.RoleInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d: start dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-FollowerState
scm1_1   | 2023-05-18 20:10:16,689 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-FollowerState] INFO impl.FollowerState: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5105285584ns, electionTimeout:5094ms
scm1_1   | 2023-05-18 20:10:16,689 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-FollowerState] INFO impl.RoleInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d: shutdown dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-FollowerState
scm1_1   | 2023-05-18 20:10:16,690 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-FollowerState] INFO server.RaftServer$Division: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
dn3_1    | 2023-05-18 20:10:24,621 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-05-18 20:10:24,626 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | GC pool 'ParNew' had collection(s): count=2 time=169ms
om2_1    | 2023-05-18 20:10:45,396 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4e8afdad{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om2_1    | 2023-05-18 20:10:46,222 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@407f2029{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-11281083286411731885/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
scm2_1   | 2023-05-18 20:10:15,005 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@4c063cb9{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
om1_1    | 2023-05-18 20:10:44,602 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@48d5ca17{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om3_1    | 2023-05-18 20:10:44,492 [Listener at om3/9862] INFO http.HttpServer2: Jetty bound to port 9874
om3_1    | 2023-05-18 20:10:44,499 [Listener at om3/9862] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
om3_1    | 2023-05-18 20:10:44,667 [Listener at om3/9862] INFO server.session: DefaultSessionIdManager workerName=node0
scm1_1   | 2023-05-18 20:10:16,690 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-05-18 20:10:24,626 [caabb77a-6190-403e-a139-2e30ca474b23-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-05-18 20:10:24,633 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: caabb77a-6190-403e-a139-2e30ca474b23: start RPC server
dn4_1    | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1295ms
om2_1    | 2023-05-18 20:10:46,292 [Listener at om2/9862] INFO server.AbstractConnector: Started ServerConnector@5402612e{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om2_1    | 2023-05-18 20:10:46,293 [Listener at om2/9862] INFO server.Server: Started @88866ms
scm2_1   | 2023-05-18 20:10:15,005 [Listener at 0.0.0.0/9860] INFO server.Server: Started @8057ms
scm2_1   | 2023-05-18 20:10:15,010 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
om1_1    | 2023-05-18 20:10:44,609 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4e8afdad{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om1_1    | 2023-05-18 20:10:45,720 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@407f2029{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-8938627163894008000/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om1_1    | 2023-05-18 20:10:45,774 [Listener at om1/9862] INFO server.AbstractConnector: Started ServerConnector@5402612e{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om1_1    | 2023-05-18 20:10:45,775 [Listener at om1/9862] INFO server.Server: Started @88039ms
scm1_1   | 2023-05-18 20:10:16,690 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-FollowerState] INFO impl.RoleInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d: start dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2
dn3_1    | 2023-05-18 20:10:24,670 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: caabb77a-6190-403e-a139-2e30ca474b23: GrpcService started, listening on 9858
dn5_1    | 2023-05-18 20:10:22,928 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-9cd97d2e-fca5-4ac1-8a4f-be6d224f499f: Started
dn4_1    | 2023-05-18 20:10:21,719 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-05-18 20:10:21,743 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-05-18 20:10:21,748 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-05-18 20:10:21,744 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om1_1    | 2023-05-18 20:10:45,799 [Listener at om1/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm1_1   | 2023-05-18 20:10:16,696 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO impl.LeaderElection: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 2 for 33: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2620a8e2-fcef-44ea-bfea-c3742904e700|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-18 20:10:24,679 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: caabb77a-6190-403e-a139-2e30ca474b23: GrpcService started, listening on 9856
dn3_1    | 2023-05-18 20:10:24,683 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: caabb77a-6190-403e-a139-2e30ca474b23: GrpcService started, listening on 9857
dn4_1    | 2023-05-18 20:10:21,754 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm2_1   | 2023-05-18 20:10:15,010 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm2_1   | 2023-05-18 20:10:15,011 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm2_1   | 2023-05-18 20:10:16,928 [grpc-default-executor-0] INFO server.RaftServer$Division: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F: receive requestVote(PRE_VOTE, dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d, group-E2B0BF76E94F, 2, (t:2, i:46))
om2_1    | 2023-05-18 20:10:46,317 [Listener at om2/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om2_1    | 2023-05-18 20:10:46,317 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1  | 2023-05-18 20:10:25,051 [IPC Server handler 13 on default port 9891] WARN ipc.Server: IPC Server handler 13 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:55818: output error
recon_1  | 2023-05-18 20:10:25,079 [IPC Server handler 13 on default port 9891] INFO ipc.Server: IPC Server handler 13 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
scm1_1   | 2023-05-18 20:10:16,711 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO impl.LeaderElection: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-05-18 20:10:24,686 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis caabb77a-6190-403e-a139-2e30ca474b23 is started using port 9858 for RATIS
dn3_1    | 2023-05-18 20:10:24,689 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis caabb77a-6190-403e-a139-2e30ca474b23 is started using port 9857 for RATIS_ADMIN
dn4_1    | 2023-05-18 20:10:21,754 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm2_1   | 2023-05-18 20:10:16,936 [grpc-default-executor-0] INFO impl.VoteContext: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F-FOLLOWER: accept PRE_VOTE from dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d: our priority 0 <= candidate's priority 0
scm2_1   | 2023-05-18 20:10:16,970 [grpc-default-executor-0] INFO server.RaftServer$Division: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F replies to PRE_VOTE vote request: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d<-78aeffd2-0882-43d0-ab48-78e394e586e6#0:OK-t2. Peer's state: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F:t2, leader=null, voted=, raftlog=Memoized:78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F-SegmentedRaftLog:OPENED:c46, conf=33: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2620a8e2-fcef-44ea-bfea-c3742904e700|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-05-18 20:10:17,142 [grpc-default-executor-0] INFO server.RaftServer$Division: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F: receive requestVote(ELECTION, dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d, group-E2B0BF76E94F, 3, (t:2, i:46))
om2_1    | 2023-05-18 20:10:46,320 [Listener at om2/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om2_1    | 2023-05-18 20:10:46,334 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
om3_1    | 2023-05-18 20:10:44,668 [Listener at om3/9862] INFO server.session: No SessionScavenger set, using defaults
om3_1    | 2023-05-18 20:10:44,673 [Listener at om3/9862] INFO server.session: node0 Scavenging every 660000ms
om3_1    | 2023-05-18 20:10:44,841 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@48d5ca17{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om3_1    | 2023-05-18 20:10:44,852 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4e8afdad{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn5_1    | 2023-05-18 20:10:22,939 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f is started using port 9858 for RATIS
scm2_1   | 2023-05-18 20:10:17,142 [grpc-default-executor-0] INFO impl.VoteContext: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F-FOLLOWER: accept ELECTION from dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d: our priority 0 <= candidate's priority 0
dn1_1    | 2023-05-18 20:10:29,986 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
om2_1    | 2023-05-18 20:10:46,351 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
scm3_1   | 2023-05-18 20:10:36,774 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1_1   | 2023-05-18 20:10:17,057 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO impl.LeaderElection: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 1 exception(s):
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn5_1    | 2023-05-18 20:10:22,939 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f is started using port 9857 for RATIS_ADMIN
dn5_1    | 2023-05-18 20:10:22,941 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f is started using port 9856 for RATIS_SERVER
scm2_1   | 2023-05-18 20:10:17,142 [grpc-default-executor-0] INFO server.RaftServer$Division: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d
dn1_1    | 2023-05-18 20:10:29,995 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om2_1    | 2023-05-18 20:10:46,650 [Listener at om2/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om2_1    | 2023-05-18 20:10:46,943 [Listener at om2/9862] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om2_1    | 2023-05-18 20:10:46,984 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@78bd02c8] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om2_1    | 2023-05-18 20:10:48,087 [om2@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om2@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5206948152ns, electionTimeout:5059ms
om1_1    | 2023-05-18 20:10:45,799 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om1_1    | 2023-05-18 20:10:45,801 [Listener at om1/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om1_1    | 2023-05-18 20:10:45,817 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1    | 2023-05-18 20:10:45,897 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om1_1    | 2023-05-18 20:10:46,180 [Listener at om1/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om1_1    | 2023-05-18 20:10:46,814 [Listener at om1/9862] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om1_1    | 2023-05-18 20:10:46,855 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@78bd02c8] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om1_1    | 2023-05-18 20:10:47,644 [om1@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om1@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5093023978ns, electionTimeout:5068ms
om1_1    | 2023-05-18 20:10:47,646 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-FollowerState
dn5_1    | 2023-05-18 20:10:23,042 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn5_1    | 2023-05-18 20:10:23,134 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:23,155 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 2023-05-18 20:10:24,136 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:25,137 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-05-18 20:10:36,775 [2620a8e2-fcef-44ea-bfea-c3742904e700-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
om2_1    | 2023-05-18 20:10:48,088 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-FollowerState
scm1_1   | 2023-05-18 20:10:17,062 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO impl.LeaderElection:   Response 0: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d<-78aeffd2-0882-43d0-ab48-78e394e586e6#0:OK-t2
om3_1    | 2023-05-18 20:10:45,928 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@407f2029{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-12097251668863145796/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om3_1    | 2023-05-18 20:10:45,975 [Listener at om3/9862] INFO server.AbstractConnector: Started ServerConnector@5402612e{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
dn3_1    | 2023-05-18 20:10:24,689 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis caabb77a-6190-403e-a139-2e30ca474b23 is started using port 9856 for RATIS_SERVER
dn3_1    | 2023-05-18 20:10:24,690 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-caabb77a-6190-403e-a139-2e30ca474b23: Started
scm2_1   | 2023-05-18 20:10:17,142 [grpc-default-executor-0] INFO impl.RoleInfo: 78aeffd2-0882-43d0-ab48-78e394e586e6: shutdown 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F-FollowerState
dn5_1    | 2023-05-18 20:10:26,138 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-05-18 20:10:36,787 [Listener at 0.0.0.0/9860] INFO server.RaftServer: 2620a8e2-fcef-44ea-bfea-c3742904e700: start RPC server
scm3_1   | 2023-05-18 20:10:37,042 [Listener at 0.0.0.0/9860] INFO server.GrpcService: 2620a8e2-fcef-44ea-bfea-c3742904e700: GrpcService started, listening on 9894
scm3_1   | 2023-05-18 20:10:37,070 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-2620a8e2-fcef-44ea-bfea-c3742904e700: Started
om2_1    | 2023-05-18 20:10:48,089 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
om2_1    | 2023-05-18 20:10:48,091 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om3_1    | 2023-05-18 20:10:45,980 [Listener at om3/9862] INFO server.Server: Started @91628ms
om3_1    | 2023-05-18 20:10:46,010 [Listener at om3/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om3_1    | 2023-05-18 20:10:46,010 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om3_1    | 2023-05-18 20:10:46,012 [Listener at om3/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om3_1    | 2023-05-18 20:10:46,013 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om3_1    | 2023-05-18 20:10:46,058 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
scm3_1   | 2023-05-18 20:10:37,090 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2620a8e2-fcef-44ea-bfea-c3742904e700|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm3_1   | 2023-05-18 20:10:37,091 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm3_1   | 2023-05-18 20:10:37,892 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-2620a8e2-fcef-44ea-bfea-c3742904e700: Detected pause in JVM or host machine approximately 0.275s with 0.593s GC time.
om2_1    | 2023-05-18 20:10:48,092 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-LeaderElection1
om2_1    | 2023-05-18 20:10:48,095 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 2 for 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-05-18 20:10:46,213 [Listener at om3/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
dn3_1    | 2023-05-18 20:10:24,742 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
scm2_1   | 2023-05-18 20:10:17,144 [grpc-default-executor-0] INFO impl.RoleInfo: 78aeffd2-0882-43d0-ab48-78e394e586e6: start 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F-FollowerState
dn5_1    | 2023-05-18 20:10:27,138 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | GC pool 'ParNew' had collection(s): count=1 time=593ms
scm3_1   | 2023-05-18 20:10:38,235 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm3_1   | 2023-05-18 20:10:38,496 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn3_1    | 2023-05-18 20:10:24,747 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm1_1   | 2023-05-18 20:10:17,063 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-05-18 20:10:27,275 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-FollowerState] INFO impl.FollowerState: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5033931850ns, electionTimeout:5031ms
om1_1    | 2023-05-18 20:10:47,647 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
om1_1    | 2023-05-18 20:10:47,651 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-05-18 20:10:29,997 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn1_1    | 2023-05-18 20:10:30,105 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-05-18 20:10:21,768 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om3_1    | 2023-05-18 20:10:46,765 [Listener at om3/9862] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om3_1    | 2023-05-18 20:10:46,793 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@78bd02c8] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn3_1    | 2023-05-18 20:10:25,375 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-05-18 20:10:17,144 [78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F-FollowerState] INFO impl.FollowerState: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F-FollowerState was interrupted
dn5_1    | 2023-05-18 20:10:27,276 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-FollowerState] INFO impl.RoleInfo: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f: shutdown 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-FollowerState
dn5_1    | 2023-05-18 20:10:27,277 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-FollowerState] INFO server.RaftServer$Division: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
dn5_1    | 2023-05-18 20:10:27,279 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm3_1   | 2023-05-18 20:10:38,496 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm3_1   | 2023-05-18 20:10:40,617 [2620a8e2-fcef-44ea-bfea-c3742904e700-server-thread2] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm3_1   | 2023-05-18 20:10:40,617 [2620a8e2-fcef-44ea-bfea-c3742904e700-server-thread2] INFO server.RaftServer$Division: 2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F: change Leader from null to dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d at term 3 for appendEntries, leader elected after 17944ms
om3_1    | 2023-05-18 20:10:48,032 [om3@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om3@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5068521458ns, electionTimeout:5009ms
om2_1    | 2023-05-18 20:10:48,205 [om2@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
dn3_1    | 2023-05-18 20:10:26,388 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-05-18 20:10:17,183 [grpc-default-executor-0] INFO server.RaftServer$Division: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F replies to ELECTION vote request: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d<-78aeffd2-0882-43d0-ab48-78e394e586e6#0:OK-t3. Peer's state: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F:t3, leader=null, voted=dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d, raftlog=Memoized:78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F-SegmentedRaftLog:OPENED:c46, conf=33: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2620a8e2-fcef-44ea-bfea-c3742904e700|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-18 20:10:17,063 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO impl.LeaderElection: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2 PRE_VOTE round 0: result PASSED
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 2023-05-18 20:10:40,652 [2620a8e2-fcef-44ea-bfea-c3742904e700-server-thread2] INFO server.RaftServer$Division: 2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F: Failed appendEntries as the first entry (index 0) already exists (snapshotIndex: 46, commitIndex: 46)
scm3_1   | 2023-05-18 20:10:40,913 [2620a8e2-fcef-44ea-bfea-c3742904e700-server-thread2] INFO server.RaftServer$Division: 2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F: inconsistency entries. Reply:dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d<-2620a8e2-fcef-44ea-bfea-c3742904e700#47:FAIL-t2,INCONSISTENCY,nextIndex=47,followerCommit=46,matchIndex=-1
scm3_1   | 2023-05-18 20:10:41,122 [2620a8e2-fcef-44ea-bfea-c3742904e700-server-thread3] INFO server.RaftServer$Division: 2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F: set configuration 47: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2620a8e2-fcef-44ea-bfea-c3742904e700|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-18 20:10:41,171 [2620a8e2-fcef-44ea-bfea-c3742904e700-server-thread3] INFO segmented.SegmentedRaftLogWorker: 2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-SegmentedRaftLogWorker: Rolling segment log-1_46 to index:46
scm3_1   | 2023-05-18 20:10:41,194 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/current/log_inprogress_1 to /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/current/log_1-46
dn4_1    | 2023-05-18 20:10:21,769 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm2_1   | 2023-05-18 20:10:17,446 [78aeffd2-0882-43d0-ab48-78e394e586e6-server-thread1] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm2_1   | 2023-05-18 20:10:17,446 [78aeffd2-0882-43d0-ab48-78e394e586e6-server-thread1] INFO server.RaftServer$Division: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F: change Leader from null to dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d at term 3 for appendEntries, leader elected after 6541ms
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om1_1    | 2023-05-18 20:10:47,651 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-LeaderElection1
om1_1    | 2023-05-18 20:10:47,655 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 2 for 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-05-18 20:10:48,211 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-05-18 20:10:30,140 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn1_1    | 2023-05-18 20:10:30,162 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2] INFO impl.RoleInfo: 76248598-2f32-4c49-8f2a-b1fddb9f8151: start 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderStateImpl
om3_1    | 2023-05-18 20:10:48,034 [om3@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-FollowerState
scm3_1   | 2023-05-18 20:10:41,349 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/current/log_inprogress_47
dn4_1    | 2023-05-18 20:10:21,771 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm2_1   | 2023-05-18 20:10:17,496 [78aeffd2-0882-43d0-ab48-78e394e586e6-server-thread2] INFO server.RaftServer$Division: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F: set configuration 47: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2620a8e2-fcef-44ea-bfea-c3742904e700|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-18 20:10:17,114 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO impl.LeaderElection: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2 ELECTION round 0: submit vote requests at term 3 for 33: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2620a8e2-fcef-44ea-bfea-c3742904e700|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om2_1    | 2023-05-18 20:10:48,211 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-05-18 20:10:48,222 [om2@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om3
dn3_1    | 2023-05-18 20:10:27,388 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:28,392 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-05-18 20:10:41,413 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 2, healthy pipeline threshold count is 1
scm3_1   | 2023-05-18 20:10:41,419 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm2_1   | 2023-05-18 20:10:17,519 [78aeffd2-0882-43d0-ab48-78e394e586e6-server-thread2] INFO segmented.SegmentedRaftLogWorker: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F-SegmentedRaftLogWorker: Rolling segment log-1_46 to index:46
scm2_1   | 2023-05-18 20:10:17,533 [78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/current/log_inprogress_1 to /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/current/log_1-46
scm2_1   | 2023-05-18 20:10:17,588 [78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/current/log_inprogress_47
dn5_1    | 2023-05-18 20:10:27,279 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-FollowerState] INFO impl.RoleInfo: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f: start 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn5_1    | 2023-05-18 20:10:27,282 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1] INFO impl.LeaderElection: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[9cd97d2e-fca5-4ac1-8a4f-be6d224f499f|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-05-18 20:10:49,212 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-om2: Detected pause in JVM or host machine approximately 0.109s with 0.142s GC time.
dn1_1    | 2023-05-18 20:10:30,269 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
om3_1    | 2023-05-18 20:10:48,034 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
scm3_1   | 2023-05-18 20:10:41,420 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
scm3_1   | 2023-05-18 20:10:41,424 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
dn4_1    | 2023-05-18 20:10:22,160 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524: set configuration 0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-18 20:10:17,153 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO impl.LeaderElection: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-05-18 20:10:17,622 [78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 2, healthy pipeline threshold count is 1
dn3_1    | 2023-05-18 20:10:29,392 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:29,448 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-FollowerState] INFO impl.FollowerState: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-FollowerState: change to CANDIDATE, lastRpcElapsedTime:6882092593ns, electionTimeout:5073ms
dn3_1    | 2023-05-18 20:10:29,450 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-FollowerState] INFO impl.RoleInfo: caabb77a-6190-403e-a139-2e30ca474b23: shutdown caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-FollowerState
dn3_1    | 2023-05-18 20:10:29,450 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-FollowerState] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
dn1_1    | 2023-05-18 20:10:30,308 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/467c8493-9e44-42c8-850d-8edf1c67e99f/current/log_inprogress_0 to /data/metadata/ratis/467c8493-9e44-42c8-850d-8edf1c67e99f/current/log_0-0
om3_1    | 2023-05-18 20:10:48,037 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om1_1    | 2023-05-18 20:10:47,729 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-18 20:10:41,798 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn4_1    | 2023-05-18 20:10:22,163 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C: set configuration 0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-18 20:10:17,187 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO impl.LeaderElection: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2: ELECTION PASSED received 1 response(s) and 1 exception(s):
scm1_1   | 2023-05-18 20:10:17,187 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO impl.LeaderElection:   Response 0: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d<-78aeffd2-0882-43d0-ab48-78e394e586e6#0:OK-t3
dn3_1    | 2023-05-18 20:10:29,452 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
recon_1  | 2023-05-18 20:10:25,049 [IPC Server handler 26 on default port 9891] WARN ipc.Server: IPC Server handler 26 on default port 9891, call Call#9 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:56406: output error
recon_1  | 2023-05-18 20:10:25,080 [IPC Server handler 26 on default port 9891] INFO ipc.Server: IPC Server handler 26 on default port 9891 caught an exception
om2_1    | GC pool 'ParNew' had collection(s): count=1 time=142ms
om2_1    | 2023-05-18 20:10:50,074 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(PRE_VOTE, om1, group-D66704EFC61C, 2, (t:2, i:24))
dn1_1    | 2023-05-18 20:10:30,359 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/467c8493-9e44-42c8-850d-8edf1c67e99f/current/log_inprogress_1
om3_1    | 2023-05-18 20:10:48,037 [om3@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-LeaderElection1
om1_1    | 2023-05-18 20:10:47,729 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-18 20:10:41,810 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm3_1   | 2023-05-18 20:10:41,925 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-18 20:10:41,945 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-18 20:10:22,163 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618: set configuration 0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-18 20:10:29,452 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-FollowerState] INFO impl.RoleInfo: caabb77a-6190-403e-a139-2e30ca474b23: start caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
om2_1    | 2023-05-18 20:10:50,090 [grpc-default-executor-1] INFO impl.VoteContext: om2@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om1: our priority 0 <= candidate's priority 0
dn1_1    | 2023-05-18 20:10:30,373 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F-LeaderElection2] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-8EDF1C67E99F: set configuration 1: peers:[76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-05-18 20:10:48,046 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 2 for 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-05-18 20:10:47,734 [om1@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om3
scm3_1   | 2023-05-18 20:10:42,170 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm3_1   | 2023-05-18 20:10:42,191 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-05-18 20:10:17,188 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-05-18 20:10:22,187 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/db288034-f1fd-42b9-aec0-461572f3e524/current/log_inprogress_0
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn5_1    | 2023-05-18 20:10:27,284 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1] INFO impl.LeaderElection: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1 PRE_VOTE round 0: result PASSED (term=1)
dn3_1    | 2023-05-18 20:10:29,455 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO impl.LeaderElection: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-05-18 20:10:50,098 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(PRE_VOTE, om3, group-D66704EFC61C, 2, (t:2, i:24))
dn1_1    | 2023-05-18 20:10:30,555 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-05-18 20:10:48,148 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-05-18 20:10:48,148 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2_1   | 2023-05-18 20:10:17,625 [78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm2_1   | 2023-05-18 20:10:17,626 [78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
scm3_1   | 2023-05-18 20:10:42,298 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
dn4_1    | 2023-05-18 20:10:22,218 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/78d664ca-7989-4249-9a90-24fe3dd84618/current/log_inprogress_0
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn5_1    | 2023-05-18 20:10:27,382 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1] INFO impl.LeaderElection: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[9cd97d2e-fca5-4ac1-8a4f-be6d224f499f|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-18 20:10:29,550 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-05-18 20:10:29,550 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-05-18 20:10:29,554 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 07404b21-602d-47db-8a51-48d80ebd16ac
om3_1    | 2023-05-18 20:10:48,150 [om3@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
om3_1    | 2023-05-18 20:10:48,154 [om3@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
scm2_1   | 2023-05-18 20:10:17,626 [78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm2_1   | 2023-05-18 20:10:17,630 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3_1   | 2023-05-18 20:10:42,298 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm3_1   | 2023-05-18 20:10:42,300 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn5_1    | 2023-05-18 20:10:27,382 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1] INFO impl.LeaderElection: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1 ELECTION round 0: result PASSED (term=2)
dn3_1    | 2023-05-18 20:10:29,565 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 76248598-2f32-4c49-8f2a-b1fddb9f8151
dn1_1    | 2023-05-18 20:10:31,556 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-05-18 20:10:50,108 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to PRE_VOTE vote request: om1<-om2#0:OK-t2. Peer's state: om2@group-D66704EFC61C:t2, leader=null, voted=, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c24, conf=0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-05-18 20:10:50,060 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(PRE_VOTE, om1, group-D66704EFC61C, 2, (t:2, i:24))
om3_1    | 2023-05-18 20:10:50,065 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(PRE_VOTE, om2, group-D66704EFC61C, 2, (t:2, i:24))
scm3_1   | 2023-05-18 20:10:42,303 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm3_1   | 2023-05-18 20:10:42,310 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn5_1    | 2023-05-18 20:10:27,385 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1] INFO impl.RoleInfo: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f: shutdown 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1
dn3_1    | 2023-05-18 20:10:29,616 [caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-FollowerState] INFO impl.FollowerState: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5210845968ns, electionTimeout:5072ms
dn1_1    | 2023-05-18 20:10:32,559 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-05-18 20:10:50,115 [grpc-default-executor-0] INFO impl.VoteContext: om2@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om3: our priority 0 <= candidate's priority 0
om3_1    | 2023-05-18 20:10:50,070 [grpc-default-executor-1] INFO impl.VoteContext: om3@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om1: our priority 0 <= candidate's priority 0
om3_1    | 2023-05-18 20:10:50,103 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to PRE_VOTE vote request: om1<-om3#0:OK-t2. Peer's state: om3@group-D66704EFC61C:t2, leader=null, voted=om1, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c24, conf=0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-18 20:10:42,584 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861: skipped Call#8 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:46448
dn4_1    | 2023-05-18 20:10:23,172 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn5_1    | 2023-05-18 20:10:27,390 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1] INFO server.RaftServer$Division: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
dn3_1    | 2023-05-18 20:10:29,632 [caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-FollowerState] INFO impl.RoleInfo: caabb77a-6190-403e-a139-2e30ca474b23: shutdown caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-FollowerState
dn1_1    | 2023-05-18 20:10:33,368 [grpc-default-executor-1] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C: receive requestVote(PRE_VOTE, caabb77a-6190-403e-a139-2e30ca474b23, group-F31DFB92D20C, 3, (t:3, i:12))
om2_1    | 2023-05-18 20:10:50,117 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to PRE_VOTE vote request: om3<-om2#0:OK-t2. Peer's state: om2@group-D66704EFC61C:t2, leader=null, voted=, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c24, conf=0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-05-18 20:10:50,104 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
om1_1    | 2023-05-18 20:10:47,734 [om1@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
scm3_1   | 2023-05-18 20:10:42,594 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1abcd059] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm1_1   | 2023-05-18 20:10:17,188 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO impl.LeaderElection: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2 ELECTION round 0: result PASSED
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn5_1    | 2023-05-18 20:10:27,394 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-2FBBEF77032F with new leaderId: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f
dn5_1    | 2023-05-18 20:10:27,395 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1] INFO server.RaftServer$Division: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F: change Leader from null to 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f at term 2 for becomeLeader, leader elected after 30522ms
dn5_1    | 2023-05-18 20:10:27,457 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn5_1    | 2023-05-18 20:10:27,483 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-05-18 20:10:27,490 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
scm2_1   | 2023-05-18 20:10:17,642 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm3_1   | 2023-05-18 20:10:42,861 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm3_1   | 2023-05-18 20:10:42,861 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn3_1    | 2023-05-18 20:10:29,632 [caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-FollowerState] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
om3_1    | 2023-05-18 20:10:50,104 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om3<-om1#0:OK-t2
om3_1    | 2023-05-18 20:10:50,104 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result PASSED
om1_1    | 2023-05-18 20:10:50,040 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(PRE_VOTE, om2, group-D66704EFC61C, 2, (t:2, i:24))
om2_1    | 2023-05-18 20:10:50,169 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn5_1    | 2023-05-18 20:10:27,503 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
scm2_1   | 2023-05-18 20:10:18,280 [IPC Server handler 2 on default port 9861] WARN ipc.Server: IPC Server handler 2 on default port 9861, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:46828: output error
scm3_1   | 2023-05-18 20:10:43,287 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @29050ms to org.eclipse.jetty.util.log.Slf4jLog
scm3_1   | 2023-05-18 20:10:44,290 [IPC Server handler 62 on default port 9861] WARN ipc.Server: IPC Server handler 62 on default port 9861, call Call#21 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:55332: output error
scm3_1   | 2023-05-18 20:10:44,292 [IPC Server handler 62 on default port 9861] INFO ipc.Server: IPC Server handler 62 on default port 9861 caught an exception
om2_1    | 2023-05-18 20:10:50,171 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om2<-om1#0:OK-t2
scm1_1   | 2023-05-18 20:10:17,189 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO impl.RoleInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d: shutdown dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2
scm1_1   | 2023-05-18 20:10:17,190 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO server.RaftServer$Division: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F: changes role from CANDIDATE to LEADER at term 3 for changeToLeader
scm1_1   | 2023-05-18 20:10:17,190 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO ha.SCMStateMachine: current SCM becomes leader of term 3.
scm2_1   | 2023-05-18 20:10:18,284 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861 caught an exception
scm2_1   | java.nio.channels.ClosedChannelException
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
om1_1    | 2023-05-18 20:10:50,049 [grpc-default-executor-0] INFO impl.VoteContext: om1@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om2: our priority 0 <= candidate's priority 0
om1_1    | 2023-05-18 20:10:50,055 [grpc-default-executor-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(PRE_VOTE, om3, group-D66704EFC61C, 2, (t:2, i:24))
om1_1    | 2023-05-18 20:10:50,064 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to PRE_VOTE vote request: om2<-om1#0:OK-t2. Peer's state: om1@group-D66704EFC61C:t2, leader=null, voted=om1, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c24, conf=0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-05-18 20:10:50,066 [grpc-default-executor-1] INFO impl.VoteContext: om1@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om3: our priority 0 <= candidate's priority 0
om1_1    | 2023-05-18 20:10:50,067 [grpc-default-executor-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to PRE_VOTE vote request: om3<-om1#0:OK-t2. Peer's state: om1@group-D66704EFC61C:t2, leader=null, voted=om1, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c24, conf=0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-05-18 20:10:50,165 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
om1_1    | 2023-05-18 20:10:50,191 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om1<-om3#0:OK-t2
om1_1    | 2023-05-18 20:10:50,192 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result PASSED
om1_1    | 2023-05-18 20:10:50,198 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(ELECTION, om3, group-D66704EFC61C, 3, (t:2, i:24))
scm1_1   | 2023-05-18 20:10:17,190 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,3>
scm1_1   | 2023-05-18 20:10:17,195 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO server.RaftServer$Division: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F: change Leader from null to dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d at term 3 for becomeLeader, leader elected after 15000ms
om2_1    | 2023-05-18 20:10:50,171 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result PASSED
dn4_1    | 2023-05-18 20:10:23,210 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-05-18 20:10:23,175 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-05-18 20:10:23,360 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO segmented.LogSegment: Successfully read 13 entries from segment file /data/metadata/ratis/a2756a42-16da-435f-a15a-f31dfb92d20c/current/log_inprogress_0
dn4_1    | 2023-05-18 20:10:23,403 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 12
om1_1    | 2023-05-18 20:10:50,212 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 ELECTION round 0: submit vote requests at term 3 for 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-05-18 20:10:50,212 [grpc-default-executor-0] INFO impl.VoteContext: om1@group-D66704EFC61C-CANDIDATE: reject ELECTION from om3: already has voted for om1 at current term 3
om1_1    | 2023-05-18 20:10:50,212 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to ELECTION vote request: om3<-om1#0:FAIL-t3. Peer's state: om1@group-D66704EFC61C:t3, leader=null, voted=om1, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c24, conf=0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-05-18 20:10:50,236 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(ELECTION, om2, group-D66704EFC61C, 3, (t:2, i:24))
scm1_1   | 2023-05-18 20:10:17,215 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1_1   | 2023-05-18 20:10:17,237 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om2_1    | 2023-05-18 20:10:50,193 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 ELECTION round 0: submit vote requests at term 3 for 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-05-18 20:10:50,195 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-05-18 20:10:33,409 [grpc-default-executor-1] INFO impl.VoteContext: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-CANDIDATE: reject PRE_VOTE from caabb77a-6190-403e-a139-2e30ca474b23: our priority 1 > candidate's priority 0
dn3_1    | 2023-05-18 20:10:29,636 [caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-05-18 20:10:27,510 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 2023-05-18 20:10:27,511 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn5_1    | 2023-05-18 20:10:27,563 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-05-18 20:10:27,585 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn5_1    | 2023-05-18 20:10:27,618 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1] INFO impl.RoleInfo: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f: start 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderStateImpl
dn5_1    | 2023-05-18 20:10:27,692 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
dn5_1    | 2023-05-18 20:10:27,705 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/bd9fbce3-479f-4a75-8657-2fbbef77032f/current/log_inprogress_0 to /data/metadata/ratis/bd9fbce3-479f-4a75-8657-2fbbef77032f/current/log_0-0
scm1_1   | 2023-05-18 20:10:17,241 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm1_1   | 2023-05-18 20:10:17,249 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm1_1   | 2023-05-18 20:10:17,250 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn4_1    | 2023-05-18 20:10:23,403 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-05-18 20:10:24,228 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:24,342 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618: start as a follower, conf=0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-05-18 20:10:50,236 [grpc-default-executor-0] INFO impl.VoteContext: om1@group-D66704EFC61C-CANDIDATE: reject ELECTION from om2: already has voted for om1 at current term 3
om1_1    | 2023-05-18 20:10:50,237 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-05-18 20:10:50,106 [grpc-default-executor-0] INFO impl.VoteContext: om3@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om2: our priority 0 <= candidate's priority 0
dn5_1    | 2023-05-18 20:10:27,794 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-LeaderElection1] INFO server.RaftServer$Division: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F: set configuration 1: peers:[9cd97d2e-fca5-4ac1-8a4f-be6d224f499f|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-18 20:10:17,252 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1_1   | 2023-05-18 20:10:17,267 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1_1   | 2023-05-18 20:10:17,279 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm3_1   | java.nio.channels.ClosedChannelException
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om2_1    | 2023-05-18 20:10:50,195 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-05-18 20:10:50,109 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to PRE_VOTE vote request: om2<-om3#0:OK-t2. Peer's state: om3@group-D66704EFC61C:t2, leader=null, voted=om1, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c24, conf=0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-05-18 20:10:50,119 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 ELECTION round 0: submit vote requests at term 3 for 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-05-18 20:10:50,130 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-05-18 20:10:50,176 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om1_1    | 2023-05-18 20:10:50,237 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to ELECTION vote request: om2<-om1#0:FAIL-t3. Peer's state: om1@group-D66704EFC61C:t3, leader=null, voted=om1, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c24, conf=0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-05-18 20:10:50,255 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 2023-05-18 20:10:50,296 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1: ELECTION REJECTED received 2 response(s) and 0 exception(s):
om3_1    | 2023-05-18 20:10:50,237 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(ELECTION, om1, group-D66704EFC61C, 3, (t:2, i:24))
dn1_1    | 2023-05-18 20:10:33,483 [grpc-default-executor-0] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C: receive requestVote(PRE_VOTE, 07404b21-602d-47db-8a51-48d80ebd16ac, group-F31DFB92D20C, 3, (t:3, i:12))
dn1_1    | 2023-05-18 20:10:33,553 [grpc-default-executor-2] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618: receive requestVote(PRE_VOTE, 07404b21-602d-47db-8a51-48d80ebd16ac, group-24FE3DD84618, 1, (t:1, i:0))
dn1_1    | 2023-05-18 20:10:33,605 [grpc-default-executor-3] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618: receive requestVote(PRE_VOTE, caabb77a-6190-403e-a139-2e30ca474b23, group-24FE3DD84618, 1, (t:1, i:0))
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
om1_1    | 2023-05-18 20:10:50,296 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om1<-om3#0:FAIL-t3
dn5_1    | 2023-05-18 20:10:27,800 [9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f@group-2FBBEF77032F-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bd9fbce3-479f-4a75-8657-2fbbef77032f/current/log_inprogress_1
dn5_1    | 2023-05-18 20:10:28,140 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:29,141 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:33,605 [grpc-default-executor-2] INFO impl.VoteContext: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-CANDIDATE: accept PRE_VOTE from 07404b21-602d-47db-8a51-48d80ebd16ac: our priority 0 <= candidate's priority 0
dn3_1    | 2023-05-18 20:10:29,636 [caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-FollowerState] INFO impl.RoleInfo: caabb77a-6190-403e-a139-2e30ca474b23: start caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-LeaderElection2
dn3_1    | 2023-05-18 20:10:29,678 [caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-LeaderElection2] INFO impl.LeaderElection: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 3 for 0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-18 20:10:29,680 [caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn4_1    | 2023-05-18 20:10:24,342 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618: changes role from      null to FOLLOWER at term 1 for startAsFollower
dn4_1    | 2023-05-18 20:10:24,344 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524: start as a follower, conf=0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-18 20:10:24,383 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524: changes role from      null to FOLLOWER at term 1 for startAsFollower
dn4_1    | 2023-05-18 20:10:24,344 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO impl.RoleInfo: 07404b21-602d-47db-8a51-48d80ebd16ac: start 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-FollowerState
om3_1    | 2023-05-18 20:10:50,239 [grpc-default-executor-1] INFO impl.VoteContext: om3@group-D66704EFC61C-CANDIDATE: reject ELECTION from om1: already has voted for om3 at current term 3
dn1_1    | 2023-05-18 20:10:33,582 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO impl.LeaderElection: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn1_1    | 2023-05-18 20:10:33,582 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-05-18 20:10:17,307 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om1_1    | 2023-05-18 20:10:50,296 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 1: om1<-om2#0:FAIL-t3
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn3_1    | 2023-05-18 20:10:29,681 [caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-05-18 20:10:29,767 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-FollowerState] INFO impl.FollowerState: caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5392175065ns, electionTimeout:5147ms
dn3_1    | 2023-05-18 20:10:29,774 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-FollowerState] INFO impl.RoleInfo: caabb77a-6190-403e-a139-2e30ca474b23: shutdown caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-FollowerState
scm1_1   | 2023-05-18 20:10:17,307 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-18 20:10:24,384 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C: start as a follower, conf=0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-18 20:10:33,635 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO impl.LeaderElection:   Response 0: 76248598-2f32-4c49-8f2a-b1fddb9f8151<-07404b21-602d-47db-8a51-48d80ebd16ac#0:OK-t3
dn1_1    | 2023-05-18 20:10:33,636 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO impl.LeaderElection: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1 PRE_VOTE round 0: result PASSED
dn1_1    | 2023-05-18 20:10:33,638 [grpc-default-executor-2] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618 replies to PRE_VOTE vote request: 07404b21-602d-47db-8a51-48d80ebd16ac<-76248598-2f32-4c49-8f2a-b1fddb9f8151#0:OK-t1. Peer's state: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618:t1, leader=null, voted=caabb77a-6190-403e-a139-2e30ca474b23, raftlog=Memoized:76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-SegmentedRaftLog:OPENED:c0, conf=0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-18 20:10:33,639 [grpc-default-executor-3] INFO impl.VoteContext: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-CANDIDATE: accept PRE_VOTE from caabb77a-6190-403e-a139-2e30ca474b23: our priority 0 <= candidate's priority 1
dn1_1    | 2023-05-18 20:10:33,640 [grpc-default-executor-3] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618 replies to PRE_VOTE vote request: caabb77a-6190-403e-a139-2e30ca474b23<-76248598-2f32-4c49-8f2a-b1fddb9f8151#0:OK-t1. Peer's state: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618:t1, leader=null, voted=caabb77a-6190-403e-a139-2e30ca474b23, raftlog=Memoized:76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-SegmentedRaftLog:OPENED:c0, conf=0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-18 20:10:33,677 [grpc-default-executor-1] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C replies to PRE_VOTE vote request: caabb77a-6190-403e-a139-2e30ca474b23<-76248598-2f32-4c49-8f2a-b1fddb9f8151#0:FAIL-t3. Peer's state: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C:t3, leader=null, voted=76248598-2f32-4c49-8f2a-b1fddb9f8151, raftlog=Memoized:76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-SegmentedRaftLog:OPENED:c12, conf=0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-18 20:10:33,733 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-LeaderElection3] INFO impl.LeaderElection: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-LeaderElection3: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
dn1_1    | 2023-05-18 20:10:33,765 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-LeaderElection3] INFO impl.LeaderElection:   Response 0: 76248598-2f32-4c49-8f2a-b1fddb9f8151<-07404b21-602d-47db-8a51-48d80ebd16ac#0:OK-t1
dn4_1    | 2023-05-18 20:10:24,385 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn4_1    | 2023-05-18 20:10:24,383 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO impl.RoleInfo: 07404b21-602d-47db-8a51-48d80ebd16ac: start 07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-FollowerState
dn4_1    | 2023-05-18 20:10:24,399 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO impl.RoleInfo: 07404b21-602d-47db-8a51-48d80ebd16ac: start 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-FollowerState
dn4_1    | 2023-05-18 20:10:24,407 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-461572F3E524,id=07404b21-602d-47db-8a51-48d80ebd16ac
scm1_1   | 2023-05-18 20:10:17,308 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm1_1   | 2023-05-18 20:10:17,310 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm1_1   | 2023-05-18 20:10:17,310 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1_1   | 2023-05-18 20:10:17,311 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1_1   | 2023-05-18 20:10:17,311 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn3_1    | 2023-05-18 20:10:29,778 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-FollowerState] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
om3_1    | 2023-05-18 20:10:50,239 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to ELECTION vote request: om1<-om3#0:FAIL-t3. Peer's state: om3@group-D66704EFC61C:t3, leader=null, voted=om3, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c24, conf=0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-05-18 20:10:50,266 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(ELECTION, om2, group-D66704EFC61C, 3, (t:2, i:24))
om3_1    | 2023-05-18 20:10:50,266 [grpc-default-executor-1] INFO impl.VoteContext: om3@group-D66704EFC61C-CANDIDATE: reject ELECTION from om2: already has voted for om3 at current term 3
om3_1    | 2023-05-18 20:10:50,266 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to ELECTION vote request: om2<-om3#0:FAIL-t3. Peer's state: om3@group-D66704EFC61C:t3, leader=null, voted=om3, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c24, conf=0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-05-18 20:10:50,270 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1: ELECTION REJECTED received 2 response(s) and 0 exception(s):
om3_1    | 2023-05-18 20:10:50,271 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om3<-om1#0:FAIL-t3
om3_1    | 2023-05-18 20:10:50,271 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 1: om3<-om2#0:FAIL-t3
om3_1    | 2023-05-18 20:10:50,272 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 ELECTION round 0: result REJECTED
dn3_1    | 2023-05-18 20:10:29,785 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-05-18 20:10:29,786 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-FollowerState] INFO impl.RoleInfo: caabb77a-6190-403e-a139-2e30ca474b23: start caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3
dn3_1    | 2023-05-18 20:10:29,842 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3] INFO impl.LeaderElection: caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-18 20:10:17,311 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
scm1_1   | 2023-05-18 20:10:17,312 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-05-18 20:10:17,312 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1_1   | 2023-05-18 20:10:17,316 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1_1   | 2023-05-18 20:10:17,317 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-05-18 20:10:17,317 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om2_1    | 2023-05-18 20:10:50,249 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(ELECTION, om3, group-D66704EFC61C, 3, (t:2, i:24))
om1_1    | 2023-05-18 20:10:50,296 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 ELECTION round 0: result REJECTED
dn3_1    | 2023-05-18 20:10:29,855 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3] INFO impl.LeaderElection: caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3 PRE_VOTE round 0: result PASSED (term=1)
dn3_1    | 2023-05-18 20:10:30,031 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3] INFO impl.LeaderElection: caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3 ELECTION round 0: submit vote requests at term 2 for 0: peers:[caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-18 20:10:30,035 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3] INFO impl.LeaderElection: caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3 ELECTION round 0: result PASSED (term=2)
dn3_1    | 2023-05-18 20:10:30,035 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3] INFO impl.RoleInfo: caabb77a-6190-403e-a139-2e30ca474b23: shutdown caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3
dn4_1    | 2023-05-18 20:10:24,426 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-05-18 20:10:24,427 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-05-18 20:10:24,430 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-18 20:10:24,435 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-05-18 20:10:24,436 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-05-18 20:10:24,441 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
om3_1    | 2023-05-18 20:10:50,273 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from CANDIDATE to FOLLOWER at term 3 for REJECTED
om3_1    | 2023-05-18 20:10:50,281 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-LeaderElection1
om3_1    | 2023-05-18 20:10:50,281 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
om2_1    | 2023-05-18 20:10:50,250 [grpc-default-executor-1] INFO impl.VoteContext: om2@group-D66704EFC61C-CANDIDATE: reject ELECTION from om3: already has voted for om2 at current term 3
scm1_1   | 2023-05-18 20:10:17,317 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-05-18 20:10:25,047 [IPC Server handler 23 on default port 9891] WARN ipc.Server: IPC Server handler 23 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:41942: output error
recon_1  | 2023-05-18 20:10:25,082 [IPC Server handler 23 on default port 9891] INFO ipc.Server: IPC Server handler 23 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
om3_1    | 2023-05-18 20:10:55,455 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(PRE_VOTE, om2, group-D66704EFC61C, 3, (t:2, i:24))
om3_1    | 2023-05-18 20:10:55,455 [grpc-default-executor-1] INFO impl.VoteContext: om3@group-D66704EFC61C-FOLLOWER: accept PRE_VOTE from om2: our priority 0 <= candidate's priority 0
om3_1    | 2023-05-18 20:10:55,458 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to PRE_VOTE vote request: om2<-om3#0:OK-t3. Peer's state: om3@group-D66704EFC61C:t3, leader=null, voted=om3, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c24, conf=0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-05-18 20:10:55,480 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(ELECTION, om2, group-D66704EFC61C, 4, (t:2, i:24))
dn4_1    | 2023-05-18 20:10:24,536 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F31DFB92D20C,id=07404b21-602d-47db-8a51-48d80ebd16ac
dn4_1    | 2023-05-18 20:10:24,537 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-05-18 20:10:24,537 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-05-18 20:10:24,537 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-05-18 20:10:33,765 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-LeaderElection3] INFO impl.LeaderElection:   Response 1: 76248598-2f32-4c49-8f2a-b1fddb9f8151<-caabb77a-6190-403e-a139-2e30ca474b23#0:FAIL-t1
dn1_1    | 2023-05-18 20:10:33,765 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-LeaderElection3] INFO impl.LeaderElection: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-LeaderElection3 PRE_VOTE round 0: result REJECTED
dn1_1    | 2023-05-18 20:10:33,766 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-LeaderElection3] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
dn1_1    | 2023-05-18 20:10:33,766 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-LeaderElection3] INFO impl.RoleInfo: 76248598-2f32-4c49-8f2a-b1fddb9f8151: shutdown 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-LeaderElection3
dn3_1    | 2023-05-18 20:10:30,037 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
dn4_1    | 2023-05-18 20:10:24,538 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-05-18 20:10:24,539 [07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-05-18 20:10:50,250 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to ELECTION vote request: om3<-om2#0:FAIL-t3. Peer's state: om2@group-D66704EFC61C:t3, leader=null, voted=om2, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c24, conf=0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-05-18 20:10:50,271 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1: ELECTION REJECTED received 2 response(s) and 0 exception(s):
dn3_1    | 2023-05-18 20:10:30,049 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-9DB787F5D8F4 with new leaderId: caabb77a-6190-403e-a139-2e30ca474b23
dn3_1    | 2023-05-18 20:10:30,059 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4: change Leader from null to caabb77a-6190-403e-a139-2e30ca474b23 at term 2 for becomeLeader, leader elected after 34847ms
dn3_1    | 2023-05-18 20:10:30,235 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om1_1    | 2023-05-18 20:10:50,297 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from CANDIDATE to FOLLOWER at term 3 for REJECTED
om1_1    | 2023-05-18 20:10:50,297 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-LeaderElection1
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn1_1    | 2023-05-18 20:10:33,783 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-LeaderElection3] INFO impl.RoleInfo: 76248598-2f32-4c49-8f2a-b1fddb9f8151: start 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-FollowerState
dn1_1    | 2023-05-18 20:10:33,743 [grpc-default-executor-0] INFO impl.VoteContext: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-CANDIDATE: reject PRE_VOTE from 07404b21-602d-47db-8a51-48d80ebd16ac: our priority 1 > candidate's priority 0
om2_1    | 2023-05-18 20:10:50,271 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om2<-om1#0:FAIL-t3
om2_1    | 2023-05-18 20:10:50,272 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 1: om2<-om3#0:FAIL-t3
om2_1    | 2023-05-18 20:10:50,272 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 ELECTION round 0: result REJECTED
om2_1    | 2023-05-18 20:10:50,275 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from CANDIDATE to FOLLOWER at term 3 for REJECTED
om2_1    | 2023-05-18 20:10:50,275 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-LeaderElection1
dn5_1    | 2023-05-18 20:10:30,142 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-05-18 20:10:50,298 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm1_1   | 2023-05-18 20:10:17,318 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn1_1    | 2023-05-18 20:10:33,801 [grpc-default-executor-0] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C replies to PRE_VOTE vote request: 07404b21-602d-47db-8a51-48d80ebd16ac<-76248598-2f32-4c49-8f2a-b1fddb9f8151#0:FAIL-t4. Peer's state: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C:t4, leader=null, voted=76248598-2f32-4c49-8f2a-b1fddb9f8151, raftlog=Memoized:76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-SegmentedRaftLog:OPENED:c12, conf=0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-18 20:10:33,743 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO impl.LeaderElection: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1 ELECTION round 0: submit vote requests at term 4 for 0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-05-18 20:10:50,276 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn3_1    | 2023-05-18 20:10:30,294 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-05-18 20:10:30,295 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
om1_1    | 2023-05-18 20:10:55,438 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(PRE_VOTE, om2, group-D66704EFC61C, 3, (t:2, i:24))
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 2023-05-18 20:10:17,318 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
dn1_1    | 2023-05-18 20:10:33,817 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-05-18 20:10:33,821 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-05-18 20:10:55,480 [grpc-default-executor-1] INFO impl.VoteContext: om3@group-D66704EFC61C-FOLLOWER: accept ELECTION from om2: our priority 0 <= candidate's priority 0
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn3_1    | 2023-05-18 20:10:30,394 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:31,143 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:32,144 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-05-18 20:10:55,439 [grpc-default-executor-0] INFO impl.VoteContext: om1@group-D66704EFC61C-FOLLOWER: accept PRE_VOTE from om2: our priority 0 <= candidate's priority 0
om1_1    | 2023-05-18 20:10:55,440 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to PRE_VOTE vote request: om2<-om1#0:OK-t3. Peer's state: om1@group-D66704EFC61C:t3, leader=null, voted=om1, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c24, conf=0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn4_1    | 2023-05-18 20:10:24,539 [07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-05-18 20:10:55,480 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from  FOLLOWER to FOLLOWER at term 4 for candidate:om2
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn3_1    | 2023-05-18 20:10:30,475 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
om1_1    | 2023-05-18 20:10:55,466 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(ELECTION, om2, group-D66704EFC61C, 4, (t:2, i:24))
om2_1    | 2023-05-18 20:10:50,276 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(ELECTION, om1, group-D66704EFC61C, 3, (t:2, i:24))
dn1_1    | 2023-05-18 20:10:33,895 [grpc-default-executor-0] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618: receive requestVote(ELECTION, caabb77a-6190-403e-a139-2e30ca474b23, group-24FE3DD84618, 2, (t:1, i:0))
dn1_1    | 2023-05-18 20:10:33,898 [grpc-default-executor-0] INFO impl.VoteContext: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-FOLLOWER: accept ELECTION from caabb77a-6190-403e-a139-2e30ca474b23: our priority 0 <= candidate's priority 1
om3_1    | 2023-05-18 20:10:55,480 [grpc-default-executor-1] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-FollowerState
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn3_1    | 2023-05-18 20:10:30,476 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om2_1    | 2023-05-18 20:10:50,285 [grpc-default-executor-1] INFO impl.VoteContext: om2@group-D66704EFC61C-FOLLOWER: reject ELECTION from om1: already has voted for om2 at current term 3
om2_1    | 2023-05-18 20:10:50,288 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to ELECTION vote request: om1<-om2#0:FAIL-t3. Peer's state: om2@group-D66704EFC61C:t3, leader=null, voted=om2, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c24, conf=0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-18 20:10:17,318 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
scm1_1   | 2023-05-18 20:10:17,319 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn4_1    | 2023-05-18 20:10:24,545 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-24FE3DD84618,id=07404b21-602d-47db-8a51-48d80ebd16ac
dn1_1    | 2023-05-18 20:10:33,899 [grpc-default-executor-0] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:caabb77a-6190-403e-a139-2e30ca474b23
om3_1    | 2023-05-18 20:10:55,481 [grpc-default-executor-1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn5_1    | 2023-05-18 20:10:33,145 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-18 20:10:44,322 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn3_1    | 2023-05-18 20:10:30,717 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om2_1    | 2023-05-18 20:10:55,425 [om2@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om2@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5149422006ns, electionTimeout:5132ms
scm1_1   | 2023-05-18 20:10:17,321 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-05-18 20:10:24,545 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-05-18 20:10:33,899 [grpc-default-executor-0] INFO impl.RoleInfo: 76248598-2f32-4c49-8f2a-b1fddb9f8151: shutdown 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-FollowerState
dn1_1    | 2023-05-18 20:10:33,900 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-FollowerState] INFO impl.FollowerState: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-FollowerState was interrupted
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn3_1    | 2023-05-18 20:10:30,857 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
om2_1    | 2023-05-18 20:10:55,426 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-FollowerState
scm1_1   | 2023-05-18 20:10:17,321 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1_1   | 2023-05-18 20:10:17,325 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO impl.RoleInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d: start dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderStateImpl
dn4_1    | 2023-05-18 20:10:24,545 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-05-18 20:11:23,159 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-05-18 20:10:30,877 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om2_1    | 2023-05-18 20:10:55,426 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
scm1_1   | 2023-05-18 20:10:17,331 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-SegmentedRaftLogWorker: Rolling segment log-1_46 to index:46
scm1_1   | 2023-05-18 20:10:17,338 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/current/log_inprogress_1 to /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/current/log_1-46
dn4_1    | 2023-05-18 20:10:24,548 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-05-18 20:12:23,159 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 2023-05-18 20:10:44,296 [IPC Server handler 68 on default port 9861] WARN ipc.Server: IPC Server handler 68 on default port 9861, call Call#8 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:54616: output error
dn1_1    | 2023-05-18 20:10:33,901 [grpc-default-executor-0] INFO impl.RoleInfo: 76248598-2f32-4c49-8f2a-b1fddb9f8151: start 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-FollowerState
dn1_1    | 2023-05-18 20:10:33,954 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO impl.LeaderElection: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
scm1_1   | 2023-05-18 20:10:17,362 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-LeaderElection2] INFO server.RaftServer$Division: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F: set configuration 47: peers:[dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 78aeffd2-0882-43d0-ab48-78e394e586e6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 2620a8e2-fcef-44ea-bfea-c3742904e700|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-18 20:10:17,362 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/7fde8ad8-2d67-471d-9d26-e2b0bf76e94f/current/log_inprogress_47
dn4_1    | 2023-05-18 20:10:24,548 [07404b21-602d-47db-8a51-48d80ebd16ac-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-05-18 20:10:24,550 [07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-05-18 20:10:24,551 [07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-18 20:10:24,552 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 07404b21-602d-47db-8a51-48d80ebd16ac: start RPC server
om3_1    | 2023-05-18 20:10:55,481 [om3@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om3@group-D66704EFC61C-FollowerState was interrupted
dn1_1    | 2023-05-18 20:10:33,956 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO impl.LeaderElection:   Response 0: 76248598-2f32-4c49-8f2a-b1fddb9f8151<-07404b21-602d-47db-8a51-48d80ebd16ac#0:OK-t4
dn3_1    | 2023-05-18 20:10:30,912 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3] INFO impl.RoleInfo: caabb77a-6190-403e-a139-2e30ca474b23: start caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderStateImpl
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn1_1    | 2023-05-18 20:10:33,958 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO impl.LeaderElection: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1 ELECTION round 0: result PASSED
dn1_1    | 2023-05-18 20:10:33,958 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO impl.RoleInfo: 76248598-2f32-4c49-8f2a-b1fddb9f8151: shutdown 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om3_1    | 2023-05-18 20:10:55,484 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to ELECTION vote request: om2<-om3#0:OK-t4. Peer's state: om3@group-D66704EFC61C:t4, leader=null, voted=om2, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c24, conf=0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-05-18 20:10:55,687 [om3-server-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: change Leader from null to om2 at term 4 for appendEntries, leader elected after 20031ms
scm3_1   | 2023-05-18 20:10:44,297 [IPC Server handler 68 on default port 9861] INFO ipc.Server: IPC Server handler 68 on default port 9861 caught an exception
scm3_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn3_1    | 2023-05-18 20:10:30,980 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
dn1_1    | 2023-05-18 20:10:33,960 [grpc-default-executor-0] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618 replies to ELECTION vote request: caabb77a-6190-403e-a139-2e30ca474b23<-76248598-2f32-4c49-8f2a-b1fddb9f8151#0:OK-t2. Peer's state: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618:t2, leader=null, voted=caabb77a-6190-403e-a139-2e30ca474b23, raftlog=Memoized:76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-SegmentedRaftLog:OPENED:c0, conf=0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-05-18 20:10:55,426 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm1_1   | 2023-05-18 20:10:17,391 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:17,430 [grpc-default-executor-0] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:17,438 [grpc-default-executor-1] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 47 -> 0
scm1_1   | 2023-05-18 20:10:17,450 [grpc-default-executor-0] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:17,451 [grpc-default-executor-4] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:17,454 [grpc-default-executor-6] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:17,471 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:17,474 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:17,475 [grpc-default-executor-5] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 48 -> 0
scm2_1   | 2023-05-18 20:10:18,291 [IPC Server handler 0 on default port 9861] WARN ipc.Server: IPC Server handler 0 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:53852: output error
scm2_1   | 2023-05-18 20:10:18,298 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861 caught an exception
scm1_1   | 2023-05-18 20:10:17,502 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn1_1    | 2023-05-18 20:10:33,962 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
scm1_1   | 2023-05-18 20:10:17,503 [grpc-default-executor-5] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 48 -> 0
scm2_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn1_1    | 2023-05-18 20:10:33,965 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-F31DFB92D20C with new leaderId: 76248598-2f32-4c49-8f2a-b1fddb9f8151
dn3_1    | 2023-05-18 20:10:31,052 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/9693fb8d-2a36-45cd-aee5-9db787f5d8f4/current/log_inprogress_0 to /data/metadata/ratis/9693fb8d-2a36-45cd-aee5-9db787f5d8f4/current/log_0-0
om2_1    | 2023-05-18 20:10:55,426 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-LeaderElection2
om2_1    | 2023-05-18 20:10:55,433 [om2@group-D66704EFC61C-LeaderElection2] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 3 for 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-05-18 20:10:55,448 [om2@group-D66704EFC61C-LeaderElection2] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn4_1    | 2023-05-18 20:10:24,572 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 07404b21-602d-47db-8a51-48d80ebd16ac: GrpcService started, listening on 9858
dn4_1    | 2023-05-18 20:10:24,588 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 07404b21-602d-47db-8a51-48d80ebd16ac: GrpcService started, listening on 9856
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn1_1    | 2023-05-18 20:10:33,972 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C: change Leader from null to 76248598-2f32-4c49-8f2a-b1fddb9f8151 at term 4 for becomeLeader, leader elected after 38258ms
dn3_1    | 2023-05-18 20:10:31,131 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9693fb8d-2a36-45cd-aee5-9db787f5d8f4/current/log_inprogress_1
dn3_1    | 2023-05-18 20:10:31,198 [caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4-LeaderElection3] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-9DB787F5D8F4: set configuration 1: peers:[caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-18 20:10:31,395 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-05-18 20:10:55,466 [grpc-default-executor-0] INFO impl.VoteContext: om1@group-D66704EFC61C-FOLLOWER: accept ELECTION from om2: our priority 0 <= candidate's priority 0
dn4_1    | 2023-05-18 20:10:24,595 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 07404b21-602d-47db-8a51-48d80ebd16ac: GrpcService started, listening on 9857
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 2023-05-18 20:10:17,538 [grpc-default-executor-6] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn1_1    | 2023-05-18 20:10:33,973 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om2_1    | 2023-05-18 20:10:55,448 [om2@group-D66704EFC61C-LeaderElection2] INFO impl.LeaderElection:   Response 0: om2<-om1#0:OK-t3
om2_1    | 2023-05-18 20:10:55,448 [om2@group-D66704EFC61C-LeaderElection2] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection2 PRE_VOTE round 0: result PASSED
om2_1    | 2023-05-18 20:10:55,450 [om2@group-D66704EFC61C-LeaderElection2] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection2 ELECTION round 0: submit vote requests at term 4 for 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-05-18 20:10:55,466 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from  FOLLOWER to FOLLOWER at term 4 for candidate:om2
om1_1    | 2023-05-18 20:10:55,466 [grpc-default-executor-0] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-FollowerState
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm1_1   | 2023-05-18 20:10:17,539 [grpc-default-executor-6] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 48 -> 0
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn1_1    | 2023-05-18 20:10:33,974 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
om3_1    | 2023-05-18 20:10:55,694 [om3-server-thread2] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 25: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-05-18 20:10:55,722 [om3-server-thread2] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:25
om3_1    | 2023-05-18 20:10:56,004 [om3@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_25
om3_1    | 2023-05-18 20:10:56,277 [om3@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om3_1    | [id: "om1"
om3_1    | address: "om1:9872"
scm1_1   | 2023-05-18 20:10:17,539 [grpc-default-executor-2] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn1_1    | 2023-05-18 20:10:33,975 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
om1_1    | 2023-05-18 20:10:55,466 [grpc-default-executor-0] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
om1_1    | 2023-05-18 20:10:55,467 [om1@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om1@group-D66704EFC61C-FollowerState was interrupted
dn3_1    | 2023-05-18 20:10:32,399 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:33,312 [grpc-default-executor-0] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618: receive requestVote(PRE_VOTE, 76248598-2f32-4c49-8f2a-b1fddb9f8151, group-24FE3DD84618, 1, (t:1, i:0))
dn3_1    | 2023-05-18 20:10:33,315 [grpc-default-executor-1] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C: receive requestVote(PRE_VOTE, 76248598-2f32-4c49-8f2a-b1fddb9f8151, group-F31DFB92D20C, 3, (t:3, i:12))
dn3_1    | 2023-05-18 20:10:33,321 [grpc-default-executor-1] INFO impl.VoteContext: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-CANDIDATE: accept PRE_VOTE from 76248598-2f32-4c49-8f2a-b1fddb9f8151: our priority 0 <= candidate's priority 1
scm1_1   | 2023-05-18 20:10:17,540 [grpc-default-executor-6] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
om1_1    | 2023-05-18 20:10:55,470 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to ELECTION vote request: om2<-om1#0:OK-t4. Peer's state: om1@group-D66704EFC61C:t4, leader=null, voted=om2, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c24, conf=0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-05-18 20:10:55,701 [om1-server-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: change Leader from null to om2 at term 4 for appendEntries, leader elected after 19927ms
om1_1    | 2023-05-18 20:10:55,706 [om1-server-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 25: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-05-18 20:10:55,487 [om2@group-D66704EFC61C-LeaderElection2] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
om2_1    | 2023-05-18 20:10:55,488 [om2@group-D66704EFC61C-LeaderElection2] INFO impl.LeaderElection:   Response 0: om2<-om3#0:OK-t4
om2_1    | 2023-05-18 20:10:55,488 [om2@group-D66704EFC61C-LeaderElection2] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection2 ELECTION round 0: result PASSED
scm1_1   | 2023-05-18 20:10:17,543 [grpc-default-executor-6] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:17,544 [grpc-default-executor-6] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 48 -> 0
scm1_1   | 2023-05-18 20:10:17,556 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:17,558 [grpc-default-executor-5] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 48 -> 0
scm1_1   | 2023-05-18 20:10:17,589 [grpc-default-executor-6] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:17,590 [grpc-default-executor-6] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 48 -> 0
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 2023-05-18 20:10:25,040 [IPC Server handler 14 on default port 9891] WARN ipc.Server: IPC Server handler 14 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:39254: output error
recon_1  | 2023-05-18 20:10:25,083 [IPC Server handler 14 on default port 9891] INFO ipc.Server: IPC Server handler 14 on default port 9891 caught an exception
dn3_1    | 2023-05-18 20:10:33,403 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-18 20:10:33,975 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn1_1    | 2023-05-18 20:10:33,976 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1_1   | 2023-05-18 20:10:17,609 [grpc-default-executor-4] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:17,610 [grpc-default-executor-0] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-05-18 20:10:24,596 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 07404b21-602d-47db-8a51-48d80ebd16ac is started using port 9858 for RATIS
dn4_1    | 2023-05-18 20:10:24,601 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 07404b21-602d-47db-8a51-48d80ebd16ac is started using port 9857 for RATIS_ADMIN
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
om3_1    | startupRole: FOLLOWER
om3_1    | , id: "om3"
om3_1    | address: "om3:9872"
om2_1    | 2023-05-18 20:10:55,488 [om2@group-D66704EFC61C-LeaderElection2] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-LeaderElection2
scm1_1   | 2023-05-18 20:10:17,610 [grpc-default-executor-2] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:17,610 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:17,610 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn3_1    | 2023-05-18 20:10:33,404 [grpc-default-executor-0] INFO impl.VoteContext: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-CANDIDATE: reject PRE_VOTE from 76248598-2f32-4c49-8f2a-b1fddb9f8151: our priority 1 > candidate's priority 0
dn3_1    | 2023-05-18 20:10:33,511 [grpc-default-executor-0] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618 replies to PRE_VOTE vote request: 76248598-2f32-4c49-8f2a-b1fddb9f8151<-caabb77a-6190-403e-a139-2e30ca474b23#0:FAIL-t1. Peer's state: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618:t1, leader=null, voted=caabb77a-6190-403e-a139-2e30ca474b23, raftlog=Memoized:caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-SegmentedRaftLog:OPENED:c0, conf=0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | java.nio.channels.ClosedChannelException
om2_1    | 2023-05-18 20:10:55,488 [om2@group-D66704EFC61C-LeaderElection2] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
scm1_1   | 2023-05-18 20:10:17,609 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm1_1   | 2023-05-18 20:10:17,617 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm1_1   | 2023-05-18 20:10:17,621 [grpc-default-executor-4] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 49 -> 0
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn3_1    | 2023-05-18 20:10:33,554 [grpc-default-executor-1] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C replies to PRE_VOTE vote request: 76248598-2f32-4c49-8f2a-b1fddb9f8151<-caabb77a-6190-403e-a139-2e30ca474b23#0:OK-t3. Peer's state: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C:t3, leader=null, voted=76248598-2f32-4c49-8f2a-b1fddb9f8151, raftlog=Memoized:caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-SegmentedRaftLog:OPENED:c12, conf=0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om2_1    | 2023-05-18 20:10:55,491 [om2@group-D66704EFC61C-LeaderElection2] INFO server.RaftServer$Division: om2@group-D66704EFC61C: change Leader from null to om2 at term 4 for becomeLeader, leader elected after 19618ms
scm1_1   | 2023-05-18 20:10:17,628 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 2, healthy pipeline threshold count is 1
om1_1    | 2023-05-18 20:10:55,717 [om1-server-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:25
om1_1    | 2023-05-18 20:10:56,109 [om1@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_25
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn3_1    | 2023-05-18 20:10:33,602 [grpc-default-executor-2] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618: receive requestVote(PRE_VOTE, 07404b21-602d-47db-8a51-48d80ebd16ac, group-24FE3DD84618, 1, (t:1, i:0))
dn3_1    | 2023-05-18 20:10:33,666 [grpc-default-executor-2] INFO impl.VoteContext: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-CANDIDATE: reject PRE_VOTE from 07404b21-602d-47db-8a51-48d80ebd16ac: our priority 1 > candidate's priority 0
dn3_1    | 2023-05-18 20:10:33,671 [grpc-default-executor-2] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618 replies to PRE_VOTE vote request: 07404b21-602d-47db-8a51-48d80ebd16ac<-caabb77a-6190-403e-a139-2e30ca474b23#0:FAIL-t1. Peer's state: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618:t1, leader=null, voted=caabb77a-6190-403e-a139-2e30ca474b23, raftlog=Memoized:caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-SegmentedRaftLog:OPENED:c0, conf=0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-05-18 20:10:55,499 [om2@group-D66704EFC61C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1_1   | 2023-05-18 20:10:17,631 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm1_1   | 2023-05-18 20:10:17,631 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
scm1_1   | 2023-05-18 20:10:17,631 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
om3_1    | startupRole: FOLLOWER
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn3_1    | 2023-05-18 20:10:33,628 [grpc-default-executor-3] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C: receive requestVote(PRE_VOTE, 07404b21-602d-47db-8a51-48d80ebd16ac, group-F31DFB92D20C, 3, (t:3, i:12))
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
om2_1    | 2023-05-18 20:10:55,505 [om2@group-D66704EFC61C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1_1   | 2023-05-18 20:10:17,634 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-05-18 20:10:17,634 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm1_1   | 2023-05-18 20:10:17,669 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:17,675 [grpc-default-executor-1] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 49 -> 0
dn4_1    | 2023-05-18 20:10:24,601 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 07404b21-602d-47db-8a51-48d80ebd16ac is started using port 9856 for RATIS_SERVER
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn3_1    | 2023-05-18 20:10:33,697 [grpc-default-executor-3] INFO impl.VoteContext: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-CANDIDATE: accept PRE_VOTE from 07404b21-602d-47db-8a51-48d80ebd16ac: our priority 0 <= candidate's priority 0
dn3_1    | 2023-05-18 20:10:33,697 [grpc-default-executor-3] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C replies to PRE_VOTE vote request: 07404b21-602d-47db-8a51-48d80ebd16ac<-caabb77a-6190-403e-a139-2e30ca474b23#0:OK-t3. Peer's state: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C:t3, leader=null, voted=76248598-2f32-4c49-8f2a-b1fddb9f8151, raftlog=Memoized:caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-SegmentedRaftLog:OPENED:c12, conf=0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-18 20:10:33,752 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO impl.LeaderElection: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
om1_1    | 2023-05-18 20:10:56,339 [om1@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om1_1    | [id: "om1"
scm1_1   | 2023-05-18 20:10:17,709 [IPC Server handler 8 on default port 9861] INFO ipc.Server: IPC Server handler 8 on default port 9861: skipped Call#5 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:53010
dn4_1    | 2023-05-18 20:10:24,602 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-07404b21-602d-47db-8a51-48d80ebd16ac: Started
dn1_1    | 2023-05-18 20:10:33,977 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn1_1    | 2023-05-18 20:10:33,977 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-05-18 20:10:33,978 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om3_1    | , id: "om2"
om3_1    | address: "om2:9872"
om2_1    | 2023-05-18 20:10:55,507 [om2@group-D66704EFC61C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om2_1    | 2023-05-18 20:10:55,518 [om2@group-D66704EFC61C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om1_1    | address: "om1:9872"
scm1_1   | 2023-05-18 20:10:17,709 [IPC Server handler 8 on default port 9861] INFO ipc.Server: IPC Server handler 8 on default port 9861: skipped Call#2 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:51658
dn4_1    | 2023-05-18 20:10:24,658 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
om3_1    | startupRole: FOLLOWER
om3_1    | ]
om3_1    | 2023-05-18 20:11:40,894 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:old1-bucket in volume:s3v
om3_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm1_1   | 2023-05-18 20:10:17,709 [IPC Server handler 8 on default port 9861] INFO ipc.Server: IPC Server handler 8 on default port 9861: skipped Call#5 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:53344
scm1_1   | 2023-05-18 20:10:17,709 [IPC Server handler 8 on default port 9861] INFO ipc.Server: IPC Server handler 8 on default port 9861: skipped Call#5 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:51672
om1_1    | startupRole: FOLLOWER
om1_1    | , id: "om3"
om1_1    | address: "om3:9872"
om1_1    | startupRole: FOLLOWER
om1_1    | , id: "om2"
scm1_1   | 2023-05-18 20:10:17,710 [IPC Server handler 7 on default port 9861] INFO ipc.Server: IPC Server handler 7 on default port 9861: skipped Call#2 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:39034
om2_1    | 2023-05-18 20:10:55,518 [om2@group-D66704EFC61C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1_1   | 2023-05-18 20:10:17,766 [grpc-default-executor-3] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om3_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:214)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm2_1   | 2023-05-18 20:10:18,287 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:52210: output error
scm2_1   | 2023-05-18 20:10:18,305 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
scm2_1   | java.nio.channels.ClosedChannelException
om2_1    | 2023-05-18 20:10:55,520 [om2@group-D66704EFC61C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om3_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:337)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn1_1    | 2023-05-18 20:10:34,101 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn1_1    | 2023-05-18 20:10:34,107 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-05-18 20:10:34,125 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
om1_1    | address: "om2:9872"
om1_1    | startupRole: FOLLOWER
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om3_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:567)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 2023-05-18 20:10:44,319 [IPC Server handler 69 on default port 9861] WARN ipc.Server: IPC Server handler 69 on default port 9861, call Call#20 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:46458: output error
scm3_1   | 2023-05-18 20:10:44,324 [IPC Server handler 69 on default port 9861] INFO ipc.Server: IPC Server handler 69 on default port 9861 caught an exception
scm3_1   | java.nio.channels.ClosedChannelException
om1_1    | ]
om1_1    | 2023-05-18 20:11:17,588 [grpc-default-executor-0] ERROR om.OzoneManagerServiceGrpc: Failed to submit request
om2_1    | 2023-05-18 20:10:55,525 [om2@group-D66704EFC61C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn4_1    | 2023-05-18 20:10:24,660 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn4_1    | 2023-05-18 20:10:25,230 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:26,231 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | com.google.protobuf.ServiceException: org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Suggested leader is OM:om2[om2/10.9.0.12].
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:250)
om2_1    | 2023-05-18 20:10:55,529 [om2@group-D66704EFC61C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
om3_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:358)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn4_1    | 2023-05-18 20:10:27,233 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:28,234 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-18 20:10:29,236 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:232)
om2_1    | 2023-05-18 20:10:55,541 [om2@group-D66704EFC61C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 2023-05-18 20:10:17,771 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:225)
om2_1    | 2023-05-18 20:10:55,541 [om2@group-D66704EFC61C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn1_1    | 2023-05-18 20:10:34,150 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn1_1    | 2023-05-18 20:10:34,152 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn1_1    | 2023-05-18 20:10:34,156 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-05-18 20:10:34,159 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn1_1    | 2023-05-18 20:10:34,160 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:177)
om1_1    | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
dn4_1    | 2023-05-18 20:10:29,586 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-FollowerState] INFO impl.FollowerState: 07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5202621194ns, electionTimeout:5154ms
dn4_1    | 2023-05-18 20:10:29,587 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-FollowerState] INFO impl.RoleInfo: 07404b21-602d-47db-8a51-48d80ebd16ac: shutdown 07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-FollowerState
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn1_1    | 2023-05-18 20:10:34,161 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
om2_1    | 2023-05-18 20:10:55,542 [om2@group-D66704EFC61C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
om1_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerServiceGrpc.submitRequest(OzoneManagerServiceGrpc.java:87)
om1_1    | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerServiceGrpc$MethodHandlers.invoke(OzoneManagerServiceGrpc.java:237)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn1_1    | 2023-05-18 20:10:34,167 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 2023-05-18 20:10:34,218 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn1_1    | 2023-05-18 20:10:34,224 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-05-18 20:10:34,224 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
om2_1    | 2023-05-18 20:10:55,544 [om2@group-D66704EFC61C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn4_1    | 2023-05-18 20:10:29,588 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-FollowerState] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
dn4_1    | 2023-05-18 20:10:29,591 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om1_1    | 	at io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
scm2_1   | 2023-05-18 20:10:18,287 [IPC Server handler 3 on default port 9861] WARN ipc.Server: IPC Server handler 3 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:40988: output error
dn1_1    | 2023-05-18 20:10:34,226 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn1_1    | 2023-05-18 20:10:34,226 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn1_1    | 2023-05-18 20:10:34,227 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-05-18 20:10:34,230 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn1_1    | 2023-05-18 20:10:34,230 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn1_1    | 2023-05-18 20:10:34,230 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-05-18 20:10:34,230 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 2023-05-18 20:10:34,250 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO impl.RoleInfo: 76248598-2f32-4c49-8f2a-b1fddb9f8151: start 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderStateImpl
scm2_1   | 2023-05-18 20:10:18,306 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861 caught an exception
scm2_1   | java.nio.channels.ClosedChannelException
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn4_1    | 2023-05-18 20:10:29,591 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-FollowerState] INFO impl.RoleInfo: 07404b21-602d-47db-8a51-48d80ebd16ac: start 07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1
dn4_1    | 2023-05-18 20:10:29,615 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1] INFO impl.LeaderElection: 07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn3_1    | 2023-05-18 20:10:33,781 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO impl.LeaderElection:   Response 0: caabb77a-6190-403e-a139-2e30ca474b23<-07404b21-602d-47db-8a51-48d80ebd16ac#0:OK-t1
dn3_1    | 2023-05-18 20:10:33,781 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO impl.LeaderElection: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1 PRE_VOTE round 0: result PASSED
dn3_1    | 2023-05-18 20:10:33,784 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO impl.LeaderElection: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-18 20:10:33,789 [caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-LeaderElection2] INFO impl.LeaderElection: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-LeaderElection2: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
scm1_1   | 2023-05-18 20:10:17,773 [grpc-default-executor-2] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:17,773 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:17,777 [grpc-default-executor-3] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 49 -> 0
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
om2_1    | 2023-05-18 20:10:55,545 [om2@group-D66704EFC61C-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om3_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
dn4_1    | 2023-05-18 20:10:29,618 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1] INFO impl.LeaderElection: 07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1 PRE_VOTE round 0: result PASSED (term=1)
dn4_1    | 2023-05-18 20:10:29,690 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1] INFO impl.LeaderElection: 07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-18 20:10:29,693 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1] INFO impl.LeaderElection: 07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1 ELECTION round 0: result PASSED (term=2)
dn3_1    | 2023-05-18 20:10:33,796 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-05-18 20:10:33,882 [caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-LeaderElection2] INFO impl.LeaderElection:   Response 0: caabb77a-6190-403e-a139-2e30ca474b23<-07404b21-602d-47db-8a51-48d80ebd16ac#0:OK-t3
om1_1    | 	at io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm1_1   | 2023-05-18 20:10:17,819 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:17,819 [grpc-default-executor-2] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om3_1    | 2023-05-18 20:11:50,177 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:new1-volume for user:hadoop
om3_1    | 2023-05-18 20:11:53,602 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new1-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: new1-volume
dn1_1    | 2023-05-18 20:10:34,254 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-SegmentedRaftLogWorker: Rolling segment log-0_12 to index:12
om1_1    | 	at io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
om2_1    | 2023-05-18 20:10:55,546 [om2@group-D66704EFC61C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1    | 2023-05-18 20:10:55,546 [om2@group-D66704EFC61C-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn4_1    | 2023-05-18 20:10:29,693 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1] INFO impl.RoleInfo: 07404b21-602d-47db-8a51-48d80ebd16ac: shutdown 07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1
dn3_1    | 2023-05-18 20:10:33,898 [caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-LeaderElection2] INFO impl.LeaderElection:   Response 1: caabb77a-6190-403e-a139-2e30ca474b23<-76248598-2f32-4c49-8f2a-b1fddb9f8151#0:FAIL-t3
scm1_1   | 2023-05-18 20:10:17,822 [grpc-default-executor-5] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 49 -> 0
dn1_1    | 2023-05-18 20:10:34,308 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/a2756a42-16da-435f-a15a-f31dfb92d20c/current/log_inprogress_0 to /data/metadata/ratis/a2756a42-16da-435f-a15a-f31dfb92d20c/current/log_0-12
dn1_1    | 2023-05-18 20:10:34,317 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-LeaderElection1] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C: set configuration 13: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-05-18 20:10:55,547 [om2@group-D66704EFC61C-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn4_1    | 2023-05-18 20:10:29,710 [07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-FollowerState] INFO impl.FollowerState: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5325295564ns, electionTimeout:5170ms
dn4_1    | 2023-05-18 20:10:29,721 [07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-FollowerState] INFO impl.RoleInfo: 07404b21-602d-47db-8a51-48d80ebd16ac: shutdown 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-FollowerState
dn4_1    | 2023-05-18 20:10:29,721 [07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-FollowerState] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
scm1_1   | 2023-05-18 20:10:18,046 [IPC Server handler 0 on default port 9861] WARN ipc.Server: IPC Server handler 0 on default port 9861, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:53004: output error
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
om1_1    | 	at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn3_1    | 2023-05-18 20:10:33,899 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-05-18 20:10:55,547 [om2@group-D66704EFC61C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-05-18 20:10:34,451 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-F31DFB92D20C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a2756a42-16da-435f-a15a-f31dfb92d20c/current/log_inprogress_13
om3_1    | 2023-05-18 20:12:03,399 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new1-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: s3v
om3_1    | 2023-05-18 20:12:12,763 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:new1-bucket in volume:s3v
om3_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om3_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:214)
om3_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:337)
scm1_1   | 2023-05-18 20:10:18,059 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861 caught an exception
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
om1_1    | 	at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 2023-05-18 20:10:34,732 [76248598-2f32-4c49-8f2a-b1fddb9f8151-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-24FE3DD84618 with new leaderId: caabb77a-6190-403e-a139-2e30ca474b23
om3_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:567)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-05-18 20:10:25,037 [IPC Server handler 12 on default port 9891] WARN ipc.Server: IPC Server handler 12 on default port 9891, call Call#9 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:48320: output error
scm1_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn1_1    | 2023-05-18 20:10:34,779 [76248598-2f32-4c49-8f2a-b1fddb9f8151-server-thread1] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618: change Leader from null to caabb77a-6190-403e-a139-2e30ca474b23 at term 2 for appendEntries, leader elected after 39194ms
om3_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:358)
recon_1  | 2023-05-18 20:10:25,087 [IPC Server handler 12 on default port 9891] INFO ipc.Server: IPC Server handler 12 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om2_1    | 2023-05-18 20:10:55,547 [om2@group-D66704EFC61C-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1    | 2023-05-18 20:10:55,551 [om2@group-D66704EFC61C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn1_1    | 2023-05-18 20:10:34,813 [76248598-2f32-4c49-8f2a-b1fddb9f8151-server-thread2] INFO server.RaftServer$Division: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618: set configuration 1: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
dn4_1    | 2023-05-18 20:10:29,723 [07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-05-18 20:10:33,899 [caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-LeaderElection2] INFO impl.LeaderElection: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-LeaderElection2 PRE_VOTE round 0: result REJECTED
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 2023-05-18 20:10:44,324 [IPC Server handler 63 on default port 9861] WARN ipc.Server: IPC Server handler 63 on default port 9861, call Call#21 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:54624: output error
scm3_1   | 2023-05-18 20:10:44,324 [IPC Server handler 67 on default port 9861] WARN ipc.Server: IPC Server handler 67 on default port 9861, call Call#24 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:39176: output error
dn1_1    | 2023-05-18 20:10:34,819 [76248598-2f32-4c49-8f2a-b1fddb9f8151-server-thread2] INFO segmented.SegmentedRaftLogWorker: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
om3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn3_1    | 2023-05-18 20:10:33,903 [caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-LeaderElection2] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C: changes role from CANDIDATE to FOLLOWER at term 3 for REJECTED
dn3_1    | 2023-05-18 20:10:33,903 [caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-LeaderElection2] INFO impl.RoleInfo: caabb77a-6190-403e-a139-2e30ca474b23: shutdown caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-LeaderElection2
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 2023-05-18 20:10:44,324 [IPC Server handler 66 on default port 9861] WARN ipc.Server: IPC Server handler 66 on default port 9861, call Call#21 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:33996: output error
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Suggested leader is OM:om2[om2/10.9.0.12].
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:245)
om2_1    | 2023-05-18 20:10:55,552 [om2@group-D66704EFC61C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-05-18 20:10:55,552 [om2@group-D66704EFC61C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
dn4_1    | 2023-05-18 20:10:29,724 [07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-FollowerState] INFO impl.RoleInfo: 07404b21-602d-47db-8a51-48d80ebd16ac: start 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-LeaderElection2
dn3_1    | 2023-05-18 20:10:33,904 [caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-LeaderElection2] INFO impl.RoleInfo: caabb77a-6190-403e-a139-2e30ca474b23: start caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-FollowerState
dn3_1    | 2023-05-18 20:10:33,926 [grpc-default-executor-3] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C: receive requestVote(ELECTION, 76248598-2f32-4c49-8f2a-b1fddb9f8151, group-F31DFB92D20C, 4, (t:3, i:12))
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn4_1    | 2023-05-18 20:10:29,740 [07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-FollowerState] INFO impl.FollowerState: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5395687637ns, electionTimeout:5186ms
om3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 2023-05-18 20:10:34,925 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/78d664ca-7989-4249-9a90-24fe3dd84618/current/log_inprogress_0 to /data/metadata/ratis/78d664ca-7989-4249-9a90-24fe3dd84618/current/log_0-0
scm3_1   | 2023-05-18 20:10:44,324 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#8 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:39172: output error
dn3_1    | 2023-05-18 20:10:33,926 [grpc-default-executor-3] INFO impl.VoteContext: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-FOLLOWER: accept ELECTION from 76248598-2f32-4c49-8f2a-b1fddb9f8151: our priority 0 <= candidate's priority 1
om1_1    | 	... 15 more
om1_1    | 2023-05-18 20:11:40,877 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:old1-bucket in volume:s3v
om3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | 2023-05-18 20:10:34,944 [76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 76248598-2f32-4c49-8f2a-b1fddb9f8151@group-24FE3DD84618-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/78d664ca-7989-4249-9a90-24fe3dd84618/current/log_inprogress_1
scm3_1   | 2023-05-18 20:10:44,329 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
dn3_1    | 2023-05-18 20:10:33,926 [grpc-default-executor-3] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C: changes role from  FOLLOWER to FOLLOWER at term 4 for candidate:76248598-2f32-4c49-8f2a-b1fddb9f8151
om2_1    | 2023-05-18 20:10:55,553 [om2@group-D66704EFC61C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
om1_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om1_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:214)
om3_1    | 2023-05-18 20:12:34,633 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om3 Received prepare request with log index 63
dn1_1    | 2023-05-18 20:10:44,321 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
scm3_1   | java.nio.channels.ClosedChannelException
dn3_1    | 2023-05-18 20:10:33,926 [grpc-default-executor-3] INFO impl.RoleInfo: caabb77a-6190-403e-a139-2e30ca474b23: shutdown caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-FollowerState
om2_1    | 2023-05-18 20:10:55,553 [om2@group-D66704EFC61C-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om2_1    | 2023-05-18 20:10:55,553 [om2@group-D66704EFC61C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1    | 2023-05-18 20:10:55,554 [om2@group-D66704EFC61C-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
om2_1    | 2023-05-18 20:10:55,554 [om2@group-D66704EFC61C-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
om2_1    | 2023-05-18 20:10:55,555 [om2@group-D66704EFC61C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-05-18 20:10:29,741 [07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-FollowerState] INFO impl.RoleInfo: 07404b21-602d-47db-8a51-48d80ebd16ac: shutdown 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-FollowerState
dn4_1    | 2023-05-18 20:10:29,742 [07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-FollowerState] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
om3_1    | 2023-05-18 20:12:34,634 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: om3 waiting for index 63 to flush to OM DB and index 64 to flush to Ratis state machine.
dn1_1    | 2023-05-18 20:11:24,552 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn3_1    | 2023-05-18 20:10:33,927 [grpc-default-executor-3] INFO impl.RoleInfo: caabb77a-6190-403e-a139-2e30ca474b23: start caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-FollowerState
om2_1    | 2023-05-18 20:10:55,555 [om2@group-D66704EFC61C-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-05-18 20:10:29,765 [07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:337)
om3_1    | 2023-05-18 20:12:39,635 [OM StateMachine ApplyTransaction Thread - 0] INFO ratis.OzoneManagerStateMachine: Current Snapshot Index (t:4, i:64)
dn1_1    | 2023-05-18 20:12:24,553 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn3_1    | 2023-05-18 20:10:33,929 [caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-FollowerState] INFO impl.FollowerState: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-FollowerState: Stopping now (isRunning? false, role = FOLLOWER)
om2_1    | 2023-05-18 20:10:55,556 [om2@group-D66704EFC61C-LeaderElection2] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-LeaderStateImpl
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn4_1    | 2023-05-18 20:10:29,766 [07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-FollowerState] INFO impl.RoleInfo: 07404b21-602d-47db-8a51-48d80ebd16ac: start 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-LeaderElection3
om3_1    | 2023-05-18 20:12:39,704 [OM StateMachine ApplyTransaction Thread - 0] INFO raftlog.RaftLog: om3@group-D66704EFC61C-SegmentedRaftLog: snapshotIndex: updateIncreasingly 24 -> 64
om1_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:567)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn3_1    | 2023-05-18 20:10:33,941 [grpc-default-executor-3] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C replies to ELECTION vote request: 76248598-2f32-4c49-8f2a-b1fddb9f8151<-caabb77a-6190-403e-a139-2e30ca474b23#0:OK-t4. Peer's state: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C:t4, leader=null, voted=76248598-2f32-4c49-8f2a-b1fddb9f8151, raftlog=Memoized:caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-SegmentedRaftLog:OPENED:c12, conf=0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-05-18 20:10:55,562 [om2@group-D66704EFC61C-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:25
om2_1    | 2023-05-18 20:10:55,599 [om2@group-D66704EFC61C-LeaderElection2] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 25: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn4_1    | 2023-05-18 20:10:29,788 [07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-LeaderElection2] INFO impl.LeaderElection: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 3 for 0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-05-18 20:12:39,704 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally -1 -> 64
om3_1    | 2023-05-18 20:12:39,704 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: Closing segment log_inprogress_25 to index: 64
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn3_1    | 2023-05-18 20:10:33,966 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO impl.LeaderElection: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
om2_1    | 2023-05-18 20:10:55,782 [om2@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_25
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn4_1    | 2023-05-18 20:10:29,800 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
om3_1    | 2023-05-18 20:12:39,707 [om3@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_25 to /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_25-64
om1_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:358)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn3_1    | 2023-05-18 20:10:33,967 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO impl.LeaderElection:   Response 0: caabb77a-6190-403e-a139-2e30ca474b23<-76248598-2f32-4c49-8f2a-b1fddb9f8151#0:OK-t2
dn3_1    | 2023-05-18 20:10:33,967 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO impl.LeaderElection: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1 ELECTION round 0: result PASSED
dn3_1    | 2023-05-18 20:10:33,968 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO impl.RoleInfo: caabb77a-6190-403e-a139-2e30ca474b23: shutdown caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1
om2_1    | 2023-05-18 20:10:56,245 [om2@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn4_1    | 2023-05-18 20:10:29,801 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-461572F3E524 with new leaderId: 07404b21-602d-47db-8a51-48d80ebd16ac
om3_1    | 2023-05-18 20:12:39,712 [OM StateMachine ApplyTransaction Thread - 0] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 63 to file /data/metadata/current/prepareMarker
om1_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn3_1    | 2023-05-18 20:10:33,968 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn4_1    | 2023-05-18 20:10:29,805 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524: change Leader from null to 07404b21-602d-47db-8a51-48d80ebd16ac at term 2 for becomeLeader, leader elected after 33583ms
om3_1    | 2023-05-18 20:12:39,714 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om3 prepared at log index 63. Returning response txnID: 63
om3_1    |  with log index 63
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn3_1    | 2023-05-18 20:10:33,968 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-24FE3DD84618 with new leaderId: caabb77a-6190-403e-a139-2e30ca474b23
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
om2_1    | [id: "om1"
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn4_1    | 2023-05-18 20:10:29,861 [07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-LeaderElection3] INFO impl.LeaderElection: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn3_1    | 2023-05-18 20:10:33,973 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618: change Leader from null to caabb77a-6190-403e-a139-2e30ca474b23 at term 2 for becomeLeader, leader elected after 40241ms
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om2_1    | address: "om1:9872"
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn4_1    | 2023-05-18 20:10:29,929 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | 2023-05-18 20:11:50,183 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:new1-volume for user:hadoop
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn4_1    | 2023-05-18 20:10:30,135 [07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1    | 2023-05-18 20:11:53,593 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new1-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: new1-volume
dn3_1    | 2023-05-18 20:10:33,976 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om2_1    | startupRole: FOLLOWER
dn4_1    | 2023-05-18 20:10:30,194 [07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 76248598-2f32-4c49-8f2a-b1fddb9f8151
om1_1    | 2023-05-18 20:12:03,393 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new1-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: s3v
dn3_1    | 2023-05-18 20:10:33,980 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm2_1   | 2023-05-18 20:10:18,313 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:39490: output error
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om2_1    | , id: "om3"
om2_1    | address: "om3:9872"
om1_1    | 2023-05-18 20:12:12,759 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:new1-bucket in volume:s3v
dn3_1    | 2023-05-18 20:10:33,980 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 2023-05-18 20:10:25,037 [IPC Server handler 28 on default port 9891] INFO ipc.Server: IPC Server handler 28 on default port 9891 caught an exception
scm2_1   | 2023-05-18 20:10:18,314 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om2_1    | startupRole: FOLLOWER
dn4_1    | 2023-05-18 20:10:30,184 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-05-18 20:10:30,213 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn4_1    | 2023-05-18 20:10:30,180 [07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | java.nio.channels.ClosedChannelException
scm2_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om2_1    | , id: "om2"
dn4_1    | 2023-05-18 20:10:30,271 [07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-05-18 20:10:33,982 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-05-18 20:10:33,982 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 2023-05-18 20:10:18,057 [IPC Server handler 6 on default port 9861] WARN ipc.Server: IPC Server handler 6 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:53328: output error
om2_1    | address: "om2:9872"
dn4_1    | 2023-05-18 20:10:30,170 [07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-LeaderElection3-1] INFO server.GrpcServerProtocolClient: Build channel for caabb77a-6190-403e-a139-2e30ca474b23
dn3_1    | 2023-05-18 20:10:33,983 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om1_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 2023-05-18 20:10:18,075 [IPC Server handler 6 on default port 9861] INFO ipc.Server: IPC Server handler 6 on default port 9861 caught an exception
om2_1    | startupRole: FOLLOWER
dn4_1    | 2023-05-18 20:10:30,272 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-18 20:10:33,983 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
om1_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:214)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm1_1   | java.nio.channels.ClosedChannelException
om2_1    | ]
dn4_1    | 2023-05-18 20:10:30,271 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-05-18 20:10:33,983 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:337)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn4_1    | 2023-05-18 20:10:30,292 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-05-18 20:10:34,072 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn3_1    | 2023-05-18 20:10:34,088 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
om2_1    | 2023-05-18 20:11:40,877 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:old1-bucket in volume:s3v
dn4_1    | 2023-05-18 20:10:30,227 [07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-05-18 20:10:34,093 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
om1_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:567)
scm3_1   | 2023-05-18 20:10:44,320 [IPC Server handler 65 on default port 9861] WARN ipc.Server: IPC Server handler 65 on default port 9861, call Call#25 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:34012: output error
scm3_1   | 2023-05-18 20:10:44,320 [IPC Server handler 64 on default port 9861] WARN ipc.Server: IPC Server handler 64 on default port 9861, call Call#20 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:55326: output error
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
om2_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om2_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:214)
dn3_1    | 2023-05-18 20:10:34,138 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm3_1   | 2023-05-18 20:10:44,332 [IPC Server handler 64 on default port 9861] INFO ipc.Server: IPC Server handler 64 on default port 9861 caught an exception
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn4_1    | 2023-05-18 20:10:30,301 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om2_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:337)
dn3_1    | 2023-05-18 20:10:34,145 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn4_1    | 2023-05-18 20:10:30,349 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:567)
dn3_1    | 2023-05-18 20:10:34,152 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm3_1   | java.nio.channels.ClosedChannelException
om1_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:358)
om1_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn4_1    | 2023-05-18 20:10:30,363 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:358)
dn3_1    | 2023-05-18 20:10:34,154 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn4_1    | 2023-05-18 20:10:30,423 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1] INFO impl.RoleInfo: 07404b21-602d-47db-8a51-48d80ebd16ac: start 07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderStateImpl
om2_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn3_1    | 2023-05-18 20:10:34,154 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 2023-05-18 20:10:30,495 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
dn4_1    | 2023-05-18 20:10:30,550 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/db288034-f1fd-42b9-aec0-461572f3e524/current/log_inprogress_0 to /data/metadata/ratis/db288034-f1fd-42b9-aec0-461572f3e524/current/log_0-0
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn3_1    | 2023-05-18 20:10:34,155 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-05-18 20:10:30,622 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/db288034-f1fd-42b9-aec0-461572f3e524/current/log_inprogress_1
dn4_1    | 2023-05-18 20:10:30,692 [07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524-LeaderElection1] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-461572F3E524: set configuration 1: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn3_1    | 2023-05-18 20:10:34,155 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1    | 2023-05-18 20:11:50,185 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:new1-volume for user:hadoop
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
om1_1    | 2023-05-18 20:12:34,630 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om1 Received prepare request with log index 63
dn4_1    | 2023-05-18 20:10:31,289 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn3_1    | 2023-05-18 20:10:34,220 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn4_1    | 2023-05-18 20:10:32,293 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn3_1    | 2023-05-18 20:10:34,220 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
om2_1    | 2023-05-18 20:11:53,568 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new1-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: new1-volume
om1_1    | 2023-05-18 20:12:34,634 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: om1 waiting for index 63 to flush to OM DB and index 64 to flush to Ratis state machine.
dn4_1    | 2023-05-18 20:10:33,288 [grpc-default-executor-1] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618: receive requestVote(PRE_VOTE, caabb77a-6190-403e-a139-2e30ca474b23, group-24FE3DD84618, 1, (t:1, i:0))
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn3_1    | 2023-05-18 20:10:34,221 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
om1_1    | 2023-05-18 20:12:39,635 [OM StateMachine ApplyTransaction Thread - 0] INFO ratis.OzoneManagerStateMachine: Current Snapshot Index (t:4, i:64)
dn4_1    | 2023-05-18 20:10:33,296 [grpc-default-executor-1] INFO impl.VoteContext: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-CANDIDATE: accept PRE_VOTE from caabb77a-6190-403e-a139-2e30ca474b23: our priority 0 <= candidate's priority 1
dn4_1    | 2023-05-18 20:10:33,297 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm2_1   | 2023-05-18 20:10:23,938 [IPC Server handler 5 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/15e7697f-640b-4664-b91f-bd9ea47c523c
dn3_1    | 2023-05-18 20:10:34,231 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
om2_1    | 2023-05-18 20:12:03,387 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new1-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: s3v
om1_1    | 2023-05-18 20:12:39,682 [OM StateMachine ApplyTransaction Thread - 0] INFO raftlog.RaftLog: om1@group-D66704EFC61C-SegmentedRaftLog: snapshotIndex: updateIncreasingly 24 -> 64
om1_1    | 2023-05-18 20:12:39,682 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally -1 -> 64
dn4_1    | 2023-05-18 20:10:33,332 [grpc-default-executor-0] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C: receive requestVote(PRE_VOTE, caabb77a-6190-403e-a139-2e30ca474b23, group-F31DFB92D20C, 3, (t:3, i:12))
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm2_1   | 2023-05-18 20:10:23,950 [IPC Server handler 5 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 15e7697f-640b-4664-b91f-bd9ea47c523c{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn3_1    | 2023-05-18 20:10:34,231 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
om2_1    | 2023-05-18 20:12:12,742 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:new1-bucket in volume:s3v
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
om1_1    | 2023-05-18 20:12:39,682 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: Closing segment log_inprogress_25 to index: 64
dn4_1    | 2023-05-18 20:10:33,362 [grpc-default-executor-1] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618 replies to PRE_VOTE vote request: caabb77a-6190-403e-a139-2e30ca474b23<-07404b21-602d-47db-8a51-48d80ebd16ac#0:OK-t1. Peer's state: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618:t1, leader=null, voted=caabb77a-6190-403e-a139-2e30ca474b23, raftlog=Memoized:07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-SegmentedRaftLog:OPENED:c0, conf=0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 2023-05-18 20:10:24,294 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn3_1    | 2023-05-18 20:10:34,237 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
om2_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om1_1    | 2023-05-18 20:12:39,693 [om1@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_25 to /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_25-64
om1_1    | 2023-05-18 20:12:39,699 [OM StateMachine ApplyTransaction Thread - 0] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 63 to file /data/metadata/current/prepareMarker
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 2023-05-18 20:10:24,353 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
dn3_1    | 2023-05-18 20:10:34,238 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
om2_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:214)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 2023-05-18 20:12:39,702 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om1 prepared at log index 63. Returning response txnID: 63
dn4_1    | 2023-05-18 20:10:33,336 [grpc-default-executor-0] INFO impl.VoteContext: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-CANDIDATE: accept PRE_VOTE from caabb77a-6190-403e-a139-2e30ca474b23: our priority 0 <= candidate's priority 0
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm2_1   | 2023-05-18 20:10:24,458 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
dn3_1    | 2023-05-18 20:10:34,238 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om2_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:337)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    |  with log index 63
dn4_1    | 2023-05-18 20:10:33,428 [grpc-default-executor-0] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C replies to PRE_VOTE vote request: caabb77a-6190-403e-a139-2e30ca474b23<-07404b21-602d-47db-8a51-48d80ebd16ac#0:OK-t3. Peer's state: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C:t3, leader=null, voted=76248598-2f32-4c49-8f2a-b1fddb9f8151, raftlog=Memoized:07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-SegmentedRaftLog:OPENED:c12, conf=0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm2_1   | 2023-05-18 20:10:24,505 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
dn3_1    | 2023-05-18 20:10:34,238 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:567)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn4_1    | 2023-05-18 20:10:33,430 [grpc-default-executor-0] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C: receive requestVote(PRE_VOTE, 76248598-2f32-4c49-8f2a-b1fddb9f8151, group-F31DFB92D20C, 3, (t:3, i:12))
scm1_1   | 2023-05-18 20:10:18,057 [IPC Server handler 2 on default port 9861] WARN ipc.Server: IPC Server handler 2 on default port 9861, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:39040: output error
dn3_1    | 2023-05-18 20:10:34,239 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:358)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-05-18 20:10:25,037 [IPC Server handler 6 on default port 9891] INFO ipc.Server: IPC Server handler 6 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
scm1_1   | 2023-05-18 20:10:18,079 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861 caught an exception
dn3_1    | 2023-05-18 20:10:34,262 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO impl.RoleInfo: caabb77a-6190-403e-a139-2e30ca474b23: start caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderStateImpl
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om2_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 2023-05-18 20:10:24,609 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn4_1    | 2023-05-18 20:10:33,432 [grpc-default-executor-0] INFO impl.VoteContext: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-CANDIDATE: accept PRE_VOTE from 76248598-2f32-4c49-8f2a-b1fddb9f8151: our priority 0 <= candidate's priority 1
scm1_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 2023-05-18 20:10:25,058 [IPC Server handler 7 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/9cd97d2e-fca5-4ac1-8a4f-be6d224f499f
dn4_1    | 2023-05-18 20:10:33,438 [grpc-default-executor-0] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C replies to PRE_VOTE vote request: 76248598-2f32-4c49-8f2a-b1fddb9f8151<-07404b21-602d-47db-8a51-48d80ebd16ac#0:OK-t3. Peer's state: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C:t3, leader=null, voted=76248598-2f32-4c49-8f2a-b1fddb9f8151, raftlog=Memoized:07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-SegmentedRaftLog:OPENED:c12, conf=0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn3_1    | 2023-05-18 20:10:34,264 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm2_1   | 2023-05-18 20:10:25,059 [IPC Server handler 7 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn4_1    | 2023-05-18 20:10:33,462 [grpc-default-executor-2] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618: receive requestVote(PRE_VOTE, 76248598-2f32-4c49-8f2a-b1fddb9f8151, group-24FE3DD84618, 1, (t:1, i:0))
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
om2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1    | 2023-05-18 20:12:34,623 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om2 Received prepare request with log index 63
dn4_1    | 2023-05-18 20:10:33,511 [grpc-default-executor-2] INFO impl.VoteContext: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-CANDIDATE: accept PRE_VOTE from 76248598-2f32-4c49-8f2a-b1fddb9f8151: our priority 0 <= candidate's priority 0
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn3_1    | 2023-05-18 20:10:34,288 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/78d664ca-7989-4249-9a90-24fe3dd84618/current/log_inprogress_0 to /data/metadata/ratis/78d664ca-7989-4249-9a90-24fe3dd84618/current/log_0-0
scm3_1   | 2023-05-18 20:10:44,332 [IPC Server handler 65 on default port 9861] INFO ipc.Server: IPC Server handler 65 on default port 9861 caught an exception
scm3_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 2023-05-18 20:10:25,060 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
dn4_1    | 2023-05-18 20:10:33,513 [grpc-default-executor-2] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618 replies to PRE_VOTE vote request: 76248598-2f32-4c49-8f2a-b1fddb9f8151<-07404b21-602d-47db-8a51-48d80ebd16ac#0:OK-t1. Peer's state: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618:t1, leader=null, voted=caabb77a-6190-403e-a139-2e30ca474b23, raftlog=Memoized:07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-SegmentedRaftLog:OPENED:c0, conf=0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-18 20:10:33,805 [07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-LeaderElection2] INFO impl.LeaderElection: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-LeaderElection2: PRE_VOTE DISCOVERED_A_NEW_TERM (term=4) received 1 response(s) and 0 exception(s):
dn3_1    | 2023-05-18 20:10:34,335 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-LeaderElection1] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618: set configuration 1: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
om2_1    | 2023-05-18 20:12:34,625 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: om2 waiting for index 63 to flush to OM DB and index 64 to flush to Ratis state machine.
scm2_1   | 2023-05-18 20:10:25,061 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn4_1    | 2023-05-18 20:10:33,806 [07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-LeaderElection2] INFO impl.LeaderElection:   Response 0: 07404b21-602d-47db-8a51-48d80ebd16ac<-76248598-2f32-4c49-8f2a-b1fddb9f8151#0:FAIL-t4
dn4_1    | 2023-05-18 20:10:33,806 [07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-LeaderElection2] INFO impl.LeaderElection: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-LeaderElection2 PRE_VOTE round 0: result DISCOVERED_A_NEW_TERM (term=4)
dn4_1    | 2023-05-18 20:10:33,806 [07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-LeaderElection2] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C: changes role from CANDIDATE to FOLLOWER at term 4 for DISCOVERED_A_NEW_TERM (term=4)
dn3_1    | 2023-05-18 20:10:34,438 [caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: caabb77a-6190-403e-a139-2e30ca474b23@group-24FE3DD84618-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/78d664ca-7989-4249-9a90-24fe3dd84618/current/log_inprogress_1
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
om2_1    | 2023-05-18 20:12:39,641 [OM StateMachine ApplyTransaction Thread - 0] INFO ratis.OzoneManagerStateMachine: Current Snapshot Index (t:4, i:64)
scm2_1   | 2023-05-18 20:10:25,061 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn4_1    | 2023-05-18 20:10:33,810 [07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-LeaderElection2] INFO impl.RoleInfo: 07404b21-602d-47db-8a51-48d80ebd16ac: shutdown 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-LeaderElection2
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn3_1    | 2023-05-18 20:10:34,774 [caabb77a-6190-403e-a139-2e30ca474b23-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-F31DFB92D20C with new leaderId: 76248598-2f32-4c49-8f2a-b1fddb9f8151
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
om2_1    | 2023-05-18 20:12:39,715 [OM StateMachine ApplyTransaction Thread - 0] INFO raftlog.RaftLog: om2@group-D66704EFC61C-SegmentedRaftLog: snapshotIndex: updateIncreasingly 24 -> 64
scm2_1   | 2023-05-18 20:10:25,061 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
dn4_1    | 2023-05-18 20:10:33,811 [07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-LeaderElection2] INFO impl.RoleInfo: 07404b21-602d-47db-8a51-48d80ebd16ac: start 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-FollowerState
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn3_1    | 2023-05-18 20:10:34,796 [caabb77a-6190-403e-a139-2e30ca474b23-server-thread1] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C: change Leader from null to 76248598-2f32-4c49-8f2a-b1fddb9f8151 at term 4 for appendEntries, leader elected after 39798ms
om2_1    | 2023-05-18 20:12:39,715 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally -1 -> 64
scm2_1   | 2023-05-18 20:10:25,214 [IPC Server handler 2 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/76248598-2f32-4c49-8f2a-b1fddb9f8151
dn4_1    | 2023-05-18 20:10:33,851 [07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-LeaderElection3] INFO impl.LeaderElection: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-LeaderElection3: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
dn4_1    | 2023-05-18 20:10:33,852 [07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-LeaderElection3] INFO impl.LeaderElection:   Response 0: 07404b21-602d-47db-8a51-48d80ebd16ac<-caabb77a-6190-403e-a139-2e30ca474b23#0:FAIL-t1
dn4_1    | 2023-05-18 20:10:33,852 [07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-LeaderElection3] INFO impl.LeaderElection:   Response 1: 07404b21-602d-47db-8a51-48d80ebd16ac<-76248598-2f32-4c49-8f2a-b1fddb9f8151#0:OK-t1
dn3_1    | 2023-05-18 20:10:34,907 [caabb77a-6190-403e-a139-2e30ca474b23-server-thread2] INFO server.RaftServer$Division: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C: set configuration 13: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
om2_1    | 2023-05-18 20:12:39,715 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: Closing segment log_inprogress_25 to index: 64
scm2_1   | 2023-05-18 20:10:25,216 [IPC Server handler 2 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 76248598-2f32-4c49-8f2a-b1fddb9f8151{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn4_1    | 2023-05-18 20:10:33,852 [07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-LeaderElection3] INFO impl.LeaderElection: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-LeaderElection3 PRE_VOTE round 0: result REJECTED
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn3_1    | 2023-05-18 20:10:34,913 [caabb77a-6190-403e-a139-2e30ca474b23-server-thread2] INFO segmented.SegmentedRaftLogWorker: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-SegmentedRaftLogWorker: Rolling segment log-0_12 to index:12
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
om2_1    | 2023-05-18 20:12:39,718 [om2@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_25 to /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_25-64
scm2_1   | 2023-05-18 20:10:25,217 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn3_1    | 2023-05-18 20:10:34,995 [caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/a2756a42-16da-435f-a15a-f31dfb92d20c/current/log_inprogress_0 to /data/metadata/ratis/a2756a42-16da-435f-a15a-f31dfb92d20c/current/log_0-12
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
om2_1    | 2023-05-18 20:12:39,719 [OM StateMachine ApplyTransaction Thread - 0] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 63 to file /data/metadata/current/prepareMarker
scm2_1   | 2023-05-18 20:10:25,259 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm2_1   | 2023-05-18 20:10:25,268 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
dn3_1    | 2023-05-18 20:10:35,016 [caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: caabb77a-6190-403e-a139-2e30ca474b23@group-F31DFB92D20C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a2756a42-16da-435f-a15a-f31dfb92d20c/current/log_inprogress_13
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
om2_1    | 2023-05-18 20:12:39,722 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om2 prepared at log index 63. Returning response txnID: 63
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn3_1    | 2023-05-18 20:10:44,340 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn3_1    | 2023-05-18 20:11:24,747 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn3_1    | 2023-05-18 20:12:24,749 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
om2_1    |  with log index 63
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm2_1   | 2023-05-18 20:10:25,268 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-05-18 20:10:25,018 [IPC Server handler 3 on default port 9891] WARN ipc.Server: IPC Server handler 3 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:55386: output error
recon_1  | 2023-05-18 20:10:25,018 [IPC Server handler 7 on default port 9891] WARN ipc.Server: IPC Server handler 7 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:56542: output error
recon_1  | 2023-05-18 20:10:25,091 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn4_1    | 2023-05-18 20:10:33,852 [07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-LeaderElection3] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
scm2_1   | 2023-05-18 20:10:25,269 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm2_1   | 2023-05-18 20:10:25,271 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-05-18 20:10:25,277 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 2, required at least one datanode reported per pipeline count is 2
scm2_1   | 2023-05-18 20:10:25,277 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn4_1    | 2023-05-18 20:10:33,852 [07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-LeaderElection3] INFO impl.RoleInfo: 07404b21-602d-47db-8a51-48d80ebd16ac: shutdown 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-LeaderElection3
dn4_1    | 2023-05-18 20:10:33,853 [07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-LeaderElection3] INFO impl.RoleInfo: 07404b21-602d-47db-8a51-48d80ebd16ac: start 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-FollowerState
dn4_1    | 2023-05-18 20:10:33,877 [grpc-default-executor-2] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C: receive requestVote(ELECTION, 76248598-2f32-4c49-8f2a-b1fddb9f8151, group-F31DFB92D20C, 4, (t:3, i:12))
dn4_1    | 2023-05-18 20:10:33,878 [grpc-default-executor-2] INFO impl.VoteContext: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-FOLLOWER: accept ELECTION from 76248598-2f32-4c49-8f2a-b1fddb9f8151: our priority 0 <= candidate's priority 1
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 2023-05-18 20:10:44,329 [IPC Server handler 63 on default port 9861] INFO ipc.Server: IPC Server handler 63 on default port 9861 caught an exception
scm3_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 2023-05-18 20:10:18,055 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:45870: output error
scm2_1   | 2023-05-18 20:10:25,279 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn4_1    | 2023-05-18 20:10:33,878 [grpc-default-executor-2] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C: changes role from  FOLLOWER to FOLLOWER at term 4 for candidate:76248598-2f32-4c49-8f2a-b1fddb9f8151
dn4_1    | 2023-05-18 20:10:33,878 [grpc-default-executor-2] INFO impl.RoleInfo: 07404b21-602d-47db-8a51-48d80ebd16ac: shutdown 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-FollowerState
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm1_1   | 2023-05-18 20:10:18,080 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
scm2_1   | 2023-05-18 20:10:25,411 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn4_1    | 2023-05-18 20:10:33,878 [grpc-default-executor-2] INFO impl.RoleInfo: 07404b21-602d-47db-8a51-48d80ebd16ac: start 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-FollowerState
scm1_1   | java.nio.channels.ClosedChannelException
scm2_1   | 2023-05-18 20:10:26,061 [IPC Server handler 7 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/07404b21-602d-47db-8a51-48d80ebd16ac
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn4_1    | 2023-05-18 20:10:33,878 [07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-FollowerState] INFO impl.FollowerState: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-FollowerState was interrupted
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 2023-05-18 20:10:26,061 [IPC Server handler 7 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 07404b21-602d-47db-8a51-48d80ebd16ac{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn4_1    | 2023-05-18 20:10:33,918 [grpc-default-executor-2] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C replies to ELECTION vote request: 76248598-2f32-4c49-8f2a-b1fddb9f8151<-07404b21-602d-47db-8a51-48d80ebd16ac#0:OK-t4. Peer's state: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C:t4, leader=null, voted=76248598-2f32-4c49-8f2a-b1fddb9f8151, raftlog=Memoized:07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-SegmentedRaftLog:OPENED:c12, conf=0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm2_1   | 2023-05-18 20:10:26,062 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn4_1    | 2023-05-18 20:10:33,984 [grpc-default-executor-2] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618: receive requestVote(ELECTION, caabb77a-6190-403e-a139-2e30ca474b23, group-24FE3DD84618, 2, (t:1, i:0))
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 2023-05-18 20:10:26,110 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-05-18 20:10:26,492 [IPC Server handler 10 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/caabb77a-6190-403e-a139-2e30ca474b23
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn4_1    | 2023-05-18 20:10:33,994 [grpc-default-executor-2] INFO impl.VoteContext: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-FOLLOWER: accept ELECTION from caabb77a-6190-403e-a139-2e30ca474b23: our priority 0 <= candidate's priority 1
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm2_1   | 2023-05-18 20:10:26,493 [IPC Server handler 10 on default port 9861] INFO node.SCMNodeManager: Registered Data node : caabb77a-6190-403e-a139-2e30ca474b23{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn4_1    | 2023-05-18 20:10:33,995 [grpc-default-executor-2] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:caabb77a-6190-403e-a139-2e30ca474b23
dn4_1    | 2023-05-18 20:10:33,996 [grpc-default-executor-2] INFO impl.RoleInfo: 07404b21-602d-47db-8a51-48d80ebd16ac: shutdown 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-FollowerState
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm2_1   | 2023-05-18 20:10:26,493 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn4_1    | 2023-05-18 20:10:33,996 [grpc-default-executor-2] INFO impl.RoleInfo: 07404b21-602d-47db-8a51-48d80ebd16ac: start 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-FollowerState
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn4_1    | 2023-05-18 20:10:33,997 [07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-FollowerState] INFO impl.FollowerState: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-FollowerState was interrupted
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm2_1   | 2023-05-18 20:10:26,511 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm2_1   | 2023-05-18 20:10:26,511 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn4_1    | 2023-05-18 20:10:34,040 [grpc-default-executor-2] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618 replies to ELECTION vote request: caabb77a-6190-403e-a139-2e30ca474b23<-07404b21-602d-47db-8a51-48d80ebd16ac#0:OK-t2. Peer's state: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618:t2, leader=null, voted=caabb77a-6190-403e-a139-2e30ca474b23, raftlog=Memoized:07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-SegmentedRaftLog:OPENED:c0, conf=0: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-18 20:10:34,646 [07404b21-602d-47db-8a51-48d80ebd16ac-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-24FE3DD84618 with new leaderId: caabb77a-6190-403e-a139-2e30ca474b23
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm2_1   | 2023-05-18 20:10:26,514 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | 2023-05-18 20:10:44,329 [IPC Server handler 67 on default port 9861] INFO ipc.Server: IPC Server handler 67 on default port 9861 caught an exception
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 2023-05-18 20:10:26,519 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
dn4_1    | 2023-05-18 20:10:34,651 [07404b21-602d-47db-8a51-48d80ebd16ac-server-thread1] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618: change Leader from null to caabb77a-6190-403e-a139-2e30ca474b23 at term 2 for appendEntries, leader elected after 39586ms
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 2023-05-18 20:10:26,520 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm2_1   | 2023-05-18 20:11:27,643 [78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for containerId, expected lastId is 0, actual lastId is 1000.
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 2023-05-18 20:11:27,810 [78aeffd2-0882-43d0-ab48-78e394e586e6@group-E2B0BF76E94F-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019201000.
dn4_1    | 2023-05-18 20:10:34,749 [07404b21-602d-47db-8a51-48d80ebd16ac-server-thread1] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618: set configuration 1: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn4_1    | 2023-05-18 20:10:34,751 [07404b21-602d-47db-8a51-48d80ebd16ac-server-thread1] INFO segmented.SegmentedRaftLogWorker: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn4_1    | 2023-05-18 20:10:34,785 [07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/78d664ca-7989-4249-9a90-24fe3dd84618/current/log_inprogress_0 to /data/metadata/ratis/78d664ca-7989-4249-9a90-24fe3dd84618/current/log_0-0
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn4_1    | 2023-05-18 20:10:34,892 [07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 07404b21-602d-47db-8a51-48d80ebd16ac@group-24FE3DD84618-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/78d664ca-7989-4249-9a90-24fe3dd84618/current/log_inprogress_1
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn4_1    | 2023-05-18 20:10:34,936 [07404b21-602d-47db-8a51-48d80ebd16ac-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-F31DFB92D20C with new leaderId: 76248598-2f32-4c49-8f2a-b1fddb9f8151
recon_1  | 2023-05-18 20:10:25,015 [IPC Server handler 9 on default port 9891] WARN ipc.Server: IPC Server handler 9 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:55400: output error
recon_1  | 2023-05-18 20:10:25,093 [IPC Server handler 9 on default port 9891] INFO ipc.Server: IPC Server handler 9 on default port 9891 caught an exception
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn4_1    | 2023-05-18 20:10:34,936 [07404b21-602d-47db-8a51-48d80ebd16ac-server-thread1] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C: change Leader from null to 76248598-2f32-4c49-8f2a-b1fddb9f8151 at term 4 for appendEntries, leader elected after 38574ms
recon_1  | java.nio.channels.ClosedChannelException
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn4_1    | 2023-05-18 20:10:35,004 [07404b21-602d-47db-8a51-48d80ebd16ac-server-thread2] INFO server.RaftServer$Division: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C: set configuration 13: peers:[07404b21-602d-47db-8a51-48d80ebd16ac|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, caabb77a-6190-403e-a139-2e30ca474b23|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 76248598-2f32-4c49-8f2a-b1fddb9f8151|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 2023-05-18 20:10:18,084 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:45856: output error
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn4_1    | 2023-05-18 20:10:35,010 [07404b21-602d-47db-8a51-48d80ebd16ac-server-thread2] INFO segmented.SegmentedRaftLogWorker: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-SegmentedRaftLogWorker: Rolling segment log-0_12 to index:12
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn4_1    | 2023-05-18 20:10:35,048 [07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/a2756a42-16da-435f-a15a-f31dfb92d20c/current/log_inprogress_0 to /data/metadata/ratis/a2756a42-16da-435f-a15a-f31dfb92d20c/current/log_0-12
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 2023-05-18 20:10:18,086 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn4_1    | 2023-05-18 20:10:35,058 [07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 07404b21-602d-47db-8a51-48d80ebd16ac@group-F31DFB92D20C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a2756a42-16da-435f-a15a-f31dfb92d20c/current/log_inprogress_13
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm1_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn4_1    | 2023-05-18 20:10:44,327 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn4_1    | 2023-05-18 20:11:24,663 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn4_1    | 2023-05-18 20:12:24,663 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 2023-05-18 20:10:44,329 [IPC Server handler 66 on default port 9861] INFO ipc.Server: IPC Server handler 66 on default port 9861 caught an exception
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 2023-05-18 20:10:25,015 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:55376: output error
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 2023-05-18 20:10:25,095 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | java.nio.channels.ClosedChannelException
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm1_1   | 2023-05-18 20:10:18,877 [grpc-default-executor-2] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 2023-05-18 20:10:18,877 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 2023-05-18 20:10:18,933 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm1_1   | 2023-05-18 20:10:18,944 [grpc-default-executor-1] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 49 -> 0
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 2023-05-18 20:10:18,949 [grpc-default-executor-2] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 2023-05-18 20:10:20,128 [grpc-default-executor-2] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 2023-05-18 20:10:20,129 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 2023-05-18 20:10:20,145 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:20,151 [grpc-default-executor-2] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 2023-05-18 20:10:20,152 [grpc-default-executor-1] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 49 -> 0
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm1_1   | 2023-05-18 20:10:20,202 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 2023-05-18 20:10:44,688 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm1_1   | 2023-05-18 20:10:20,205 [grpc-default-executor-2] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 2023-05-18 20:10:44,724 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm1_1   | 2023-05-18 20:10:20,206 [grpc-default-executor-1] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 49 -> 0
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 2023-05-18 20:10:44,771 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm1_1   | 2023-05-18 20:10:20,269 [grpc-default-executor-2] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 2023-05-18 20:10:44,778 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm1_1   | 2023-05-18 20:10:20,273 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 2023-05-18 20:10:44,780 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm1_1   | 2023-05-18 20:10:20,275 [grpc-default-executor-2] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 49 -> 0
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 2023-05-18 20:10:44,785 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm1_1   | 2023-05-18 20:10:21,379 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-05-18 20:10:25,015 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#9 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:39852: output error
scm3_1   | 2023-05-18 20:10:44,919 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm1_1   | 2023-05-18 20:10:21,380 [grpc-default-executor-2] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-05-18 20:10:25,014 [IPC Server handler 11 on default port 9891] WARN ipc.Server: IPC Server handler 11 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:57904: output error
scm1_1   | 2023-05-18 20:10:21,453 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-05-18 20:10:44,922 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm3_1   | 2023-05-18 20:10:44,926 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
recon_1  | 2023-05-18 20:10:25,129 [IPC Server handler 11 on default port 9891] INFO ipc.Server: IPC Server handler 11 on default port 9891 caught an exception
scm1_1   | 2023-05-18 20:10:21,463 [grpc-default-executor-1] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 49 -> 0
recon_1  | java.nio.channels.ClosedChannelException
scm3_1   | 2023-05-18 20:10:45,118 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm3_1   | 2023-05-18 20:10:45,130 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm1_1   | 2023-05-18 20:10:21,467 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:22,632 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:22,645 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:22,710 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:22,719 [grpc-default-executor-5] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 49 -> 0
scm1_1   | 2023-05-18 20:10:22,813 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:23,644 [IPC Server handler 11 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/15e7697f-640b-4664-b91f-bd9ea47c523c
scm1_1   | 2023-05-18 20:10:23,699 [IPC Server handler 11 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 15e7697f-640b-4664-b91f-bd9ea47c523c{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-05-18 20:10:23,883 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-05-18 20:10:45,146 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 2023-05-18 20:10:23,887 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:23,908 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-05-18 20:10:45,249 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7e2c6702{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm3_1   | 2023-05-18 20:10:45,255 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6ee37ca7{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm1_1   | 2023-05-18 20:10:23,916 [grpc-default-executor-5] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 49 -> 0
scm3_1   | 2023-05-18 20:10:45,888 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@15c16f19{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-8589627308644817999/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 2023-05-18 20:10:23,917 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-05-18 20:10:45,927 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@72a7aa4f{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm1_1   | 2023-05-18 20:10:23,939 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 2023-05-18 20:10:45,929 [Listener at 0.0.0.0/9860] INFO server.Server: Started @31695ms
scm1_1   | 2023-05-18 20:10:23,985 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 2023-05-18 20:10:24,010 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 2023-05-18 20:10:45,941 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm1_1   | 2023-05-18 20:10:24,061 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm3_1   | 2023-05-18 20:10:45,941 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm1_1   | 2023-05-18 20:10:24,194 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 2023-05-18 20:10:45,943 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm1_1   | 2023-05-18 20:10:25,064 [IPC Server handler 10 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/9cd97d2e-fca5-4ac1-8a4f-be6d224f499f
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | 2023-05-18 20:10:47,043 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-18 20:10:47,043 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm1_1   | 2023-05-18 20:10:25,065 [IPC Server handler 10 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-05-18 20:10:52,134 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 2023-05-18 20:10:25,066 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1_1   | 2023-05-18 20:10:25,067 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 2023-05-18 20:10:25,067 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm3_1   | 2023-05-18 20:10:52,134 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm1_1   | 2023-05-18 20:10:25,068 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-05-18 20:10:57,290 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm1_1   | 2023-05-18 20:10:25,134 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-05-18 20:10:57,290 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-18 20:10:57,479 [IPC Server handler 2 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/9cd97d2e-fca5-4ac1-8a4f-be6d224f499f
scm3_1   | 2023-05-18 20:10:57,486 [IPC Server handler 2 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 9cd97d2e-fca5-4ac1-8a4f-be6d224f499f{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-05-18 20:10:25,134 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 2023-05-18 20:10:25,154 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 2023-05-18 20:10:57,514 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm1_1   | 2023-05-18 20:10:25,160 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 2023-05-18 20:10:57,528 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-05-18 20:10:25,013 [IPC Server handler 10 on default port 9891] WARN ipc.Server: IPC Server handler 10 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:44536: output error
scm3_1   | 2023-05-18 20:10:57,525 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
recon_1  | 2023-05-18 20:10:25,013 [IPC Server handler 24 on default port 9891] WARN ipc.Server: IPC Server handler 24 on default port 9891, call Call#9 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:35838: output error
scm1_1   | 2023-05-18 20:10:25,168 [grpc-default-executor-5] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 49 -> 0
scm3_1   | 2023-05-18 20:10:57,540 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
recon_1  | 2023-05-18 20:10:25,154 [IPC Server handler 10 on default port 9891] INFO ipc.Server: IPC Server handler 10 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
scm3_1   | 2023-05-18 20:10:57,540 [IPC Server handler 4 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/15e7697f-640b-4664-b91f-bd9ea47c523c
scm1_1   | 2023-05-18 20:10:25,270 [IPC Server handler 9 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/76248598-2f32-4c49-8f2a-b1fddb9f8151
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 2023-05-18 20:10:25,275 [IPC Server handler 9 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 76248598-2f32-4c49-8f2a-b1fddb9f8151{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 2023-05-18 20:10:57,541 [IPC Server handler 4 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 15e7697f-640b-4664-b91f-bd9ea47c523c{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-05-18 20:10:57,544 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 2023-05-18 20:10:25,277 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 2023-05-18 20:10:57,525 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm1_1   | 2023-05-18 20:10:25,277 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 2023-05-18 20:10:25,283 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 2, required at least one datanode reported per pipeline count is 2
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 2023-05-18 20:10:57,545 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm1_1   | 2023-05-18 20:10:25,291 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm3_1   | 2023-05-18 20:10:57,542 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm1_1   | 2023-05-18 20:10:25,295 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm1_1   | 2023-05-18 20:10:25,295 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm3_1   | 2023-05-18 20:10:57,542 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm1_1   | 2023-05-18 20:10:25,299 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm1_1   | 2023-05-18 20:10:25,299 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | 2023-05-18 20:10:59,859 [IPC Server handler 77 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/07404b21-602d-47db-8a51-48d80ebd16ac
scm1_1   | 2023-05-18 20:10:25,300 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-05-18 20:10:25,013 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:41944: output error
recon_1  | 2023-05-18 20:10:25,156 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 2023-05-18 20:10:59,860 [IPC Server handler 77 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 07404b21-602d-47db-8a51-48d80ebd16ac{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-05-18 20:10:59,860 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm1_1   | 2023-05-18 20:10:25,339 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 1.
scm3_1   | 2023-05-18 20:10:59,862 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm3_1   | 2023-05-18 20:10:59,863 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm1_1   | 2023-05-18 20:10:25,408 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-05-18 20:10:59,863 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 2023-05-18 20:10:59,863 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm1_1   | 2023-05-18 20:10:26,129 [IPC Server handler 9 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/07404b21-602d-47db-8a51-48d80ebd16ac
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 2023-05-18 20:10:59,863 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-05-18 20:10:26,130 [IPC Server handler 9 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 07404b21-602d-47db-8a51-48d80ebd16ac{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 2023-05-18 20:10:59,863 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 2, required at least one datanode reported per pipeline count is 2
scm1_1   | 2023-05-18 20:10:26,149 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 2023-05-18 20:10:59,864 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 2023-05-18 20:10:26,152 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 2.
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 2023-05-18 20:10:59,864 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm1_1   | 2023-05-18 20:10:26,153 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-05-18 20:11:02,486 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-18 20:10:26,385 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 2023-05-18 20:11:02,486 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-18 20:10:26,386 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 2023-05-18 20:11:04,062 [IPC Server handler 5 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/caabb77a-6190-403e-a139-2e30ca474b23
scm3_1   | 2023-05-18 20:11:04,063 [IPC Server handler 5 on default port 9861] INFO node.SCMNodeManager: Registered Data node : caabb77a-6190-403e-a139-2e30ca474b23{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-05-18 20:10:26,399 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:26,401 [grpc-default-executor-5] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 49 -> 0
scm1_1   | 2023-05-18 20:10:26,399 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:26,504 [IPC Server handler 15 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/caabb77a-6190-403e-a139-2e30ca474b23
scm1_1   | 2023-05-18 20:10:26,504 [IPC Server handler 15 on default port 9861] INFO node.SCMNodeManager: Registered Data node : caabb77a-6190-403e-a139-2e30ca474b23{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-05-18 20:10:25,155 [IPC Server handler 24 on default port 9891] INFO ipc.Server: IPC Server handler 24 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 2023-05-18 20:11:04,063 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-05-18 20:11:04,073 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-05-18 20:11:04,074 [IPC Server handler 13 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/76248598-2f32-4c49-8f2a-b1fddb9f8151
scm3_1   | 2023-05-18 20:11:04,074 [IPC Server handler 13 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 76248598-2f32-4c49-8f2a-b1fddb9f8151{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-05-18 20:10:26,521 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm3_1   | 2023-05-18 20:11:04,075 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm1_1   | 2023-05-18 20:10:26,524 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 2023-05-18 20:11:04,078 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm1_1   | 2023-05-18 20:10:26,525 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm3_1   | 2023-05-18 20:11:04,078 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm1_1   | 2023-05-18 20:10:26,525 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 2023-05-18 20:11:04,078 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm1_1   | 2023-05-18 20:10:26,525 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | 2023-05-18 20:11:04,078 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm1_1   | 2023-05-18 20:10:26,534 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | 2023-05-18 20:11:04,078 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm1_1   | 2023-05-18 20:10:26,534 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 2023-05-18 20:11:07,545 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-18 20:10:26,534 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 2023-05-18 20:11:07,545 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-18 20:10:26,535 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 2023-05-18 20:11:12,616 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-18 20:10:26,535 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 2023-05-18 20:11:12,618 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-18 20:11:17,690 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 2023-05-18 20:11:17,690 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-18 20:10:26,565 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 2023-05-18 20:11:22,857 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-18 20:10:27,637 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-05-18 20:10:25,136 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
scm3_1   | 2023-05-18 20:11:22,857 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-18 20:10:27,639 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | java.nio.channels.ClosedChannelException
scm3_1   | 2023-05-18 20:11:27,657 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for containerId, expected lastId is 0, actual lastId is 1000.
scm1_1   | 2023-05-18 20:10:27,648 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 2023-05-18 20:11:27,796 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019201000.
scm1_1   | 2023-05-18 20:10:27,651 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 2023-05-18 20:11:27,990 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-18 20:10:27,658 [grpc-default-executor-1] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 49 -> 0
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 2023-05-18 20:11:27,991 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-18 20:10:28,893 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 2023-05-18 20:11:33,092 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-18 20:10:28,896 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm1_1   | 2023-05-18 20:10:28,945 [grpc-default-executor-2] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm3_1   | 2023-05-18 20:11:33,092 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-18 20:10:28,959 [grpc-default-executor-1] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 49 -> 0
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 2023-05-18 20:10:28,971 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-05-18 20:11:38,256 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 2023-05-18 20:11:38,256 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-18 20:10:30,154 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 2023-05-18 20:11:43,345 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-18 20:10:30,156 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 2023-05-18 20:11:43,345 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-18 20:10:30,239 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 2023-05-18 20:11:48,377 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-18 20:10:30,240 [grpc-default-executor-2] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 2023-05-18 20:11:48,378 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-18 20:10:30,249 [grpc-default-executor-5] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 49 -> 0
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 2023-05-18 20:11:53,441 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 2023-05-18 20:11:53,441 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-18 20:10:31,426 [grpc-default-executor-2] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-05-18 20:10:25,119 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
scm3_1   | 2023-05-18 20:11:58,529 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-18 20:10:31,430 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | java.nio.channels.ClosedChannelException
scm3_1   | 2023-05-18 20:11:58,529 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-18 20:12:03,724 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-18 20:12:03,725 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 2023-05-18 20:12:08,757 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 2023-05-18 20:12:08,758 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-18 20:10:31,474 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 2023-05-18 20:10:31,481 [grpc-default-executor-5] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 49 -> 0
scm1_1   | 2023-05-18 20:10:31,512 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-05-18 20:10:25,092 [IPC Server handler 3 on default port 9891] INFO ipc.Server: IPC Server handler 3 on default port 9891 caught an exception
scm1_1   | 2023-05-18 20:10:32,678 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:32,678 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:32,713 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:32,751 [grpc-default-executor-2] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:32,759 [grpc-default-executor-1] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 49 -> 0
scm1_1   | 2023-05-18 20:10:33,935 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:33,935 [grpc-default-executor-2] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:33,970 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-05-18 20:12:13,806 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-18 20:12:13,807 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-18 20:12:18,842 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-18 20:12:18,843 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-18 20:12:24,033 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-18 20:12:24,033 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-18 20:12:29,093 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-18 20:12:29,093 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-18 20:12:34,111 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-18 20:12:34,111 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-18 20:10:33,987 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:33,999 [grpc-default-executor-1] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 49 -> 0
scm1_1   | 2023-05-18 20:10:35,186 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:35,186 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:35,204 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:35,210 [grpc-default-executor-5] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 49 -> 0
scm1_1   | 2023-05-18 20:10:35,212 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:36,437 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-18 20:10:36,438 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-05-18 20:12:39,283 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
recon_1  | java.nio.channels.ClosedChannelException
scm1_1   | 2023-05-18 20:10:36,462 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 2023-05-18 20:12:39,284 [2620a8e2-fcef-44ea-bfea-c3742904e700@group-E2B0BF76E94F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm1_1   | 2023-05-18 20:10:36,463 [grpc-default-executor-5] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: decreaseNextIndex nextIndex: updateUnconditionally 49 -> 0
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 2023-05-18 20:10:36,469 [grpc-default-executor-1] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm1_1   | 2023-05-18 20:10:41,099 [grpc-default-executor-5] WARN server.GrpcLogAppender: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 47
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm1_1   | 2023-05-18 20:10:41,100 [grpc-default-executor-5] INFO leader.FollowerInfo: dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F->2620a8e2-fcef-44ea-bfea-c3742904e700: setNextIndex nextIndex: updateUnconditionally 49 -> 47
scm1_1   | 2023-05-18 20:10:56,555 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 2023-05-18 20:11:09,733 [IPC Server handler 20 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.14
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm1_1   | 2023-05-18 20:11:26,556 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm1_1   | 2023-05-18 20:11:27,604 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for containerId, expected lastId is 0, actual lastId is 1000.
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 2023-05-18 20:11:27,705 [IPC Server handler 15 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 1000 to 2000.
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 2023-05-18 20:11:27,756 [dd5c6639-35e3-42c5-8af0-b1f9e23f8c0d@group-E2B0BF76E94F-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019201000.
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 2023-05-18 20:11:27,782 [IPC Server handler 15 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019201000 to 111677748019202000.
scm1_1   | 2023-05-18 20:11:43,696 [IPC Server handler 26 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for delTxnId, change lastId from 0 to 1000.
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm1_1   | 2023-05-18 20:11:56,558 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
scm1_1   | 2023-05-18 20:12:20,940 [IPC Server handler 5 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.14
recon_1  | 2023-05-18 20:10:25,227 [IPC Server handler 5 on default port 9891] WARN ipc.Server: IPC Server handler 5 on default port 9891, call Call#10 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:57376: output error
scm1_1   | 2023-05-18 20:12:26,561 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
recon_1  | 2023-05-18 20:10:25,229 [IPC Server handler 5 on default port 9891] INFO ipc.Server: IPC Server handler 5 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-05-18 20:10:25,294 [IPC Server handler 79 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for a1dc68dc7bff
recon_1  | 2023-05-18 20:10:25,303 [IPC Server handler 57 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn2_1.ha_net
recon_1  | 2023-05-18 20:10:26,012 [IPC Server handler 8 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for 5dd7996c298c
recon_1  | 2023-05-18 20:10:26,507 [IPC Server handler 15 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for 54c4478f5de9
recon_1  | 2023-05-18 20:10:26,922 [IPC Server handler 32 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn5_1.ha_net
recon_1  | 2023-05-18 20:10:53,338 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered ContainerSizeCountTask task 
recon_1  | 2023-05-18 20:10:53,338 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerSizeCountTask Thread.
recon_1  | 2023-05-18 20:10:53,357 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1  | 2023-05-18 20:10:53,383 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1  | 2023-05-18 20:10:53,459 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 7 pipelines in house.
recon_1  | 2023-05-18 20:10:53,488 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 127 milliseconds to process 0 existing database records.
recon_1  | 2023-05-18 20:10:53,489 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 85 milliseconds.
recon_1  | 2023-05-18 20:10:53,594 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 106 milliseconds for processing 1 containers.
recon_1  | 2023-05-18 20:11:06,926 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1  | 2023-05-18 20:11:06,930 [pool-27-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-05-18 20:11:06,931 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-05-18 20:11:06,931 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-05-18 20:11:06,931 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-05-18 20:11:06,931 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-05-18 20:11:06,932 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-05-18 20:11:06,932 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1  | 2023-05-18 20:11:06,932 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 31 
recon_1  | 2023-05-18 20:11:07,204 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 3, SequenceNumber diff: 4, SequenceNumber Lag from OM 0.
recon_1  | 2023-05-18 20:11:07,205 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 4 records
recon_1  | 2023-05-18 20:11:07,215 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
recon_1  | 2023-05-18 20:11:07,215 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
recon_1  | 2023-05-18 20:11:07,624 [pool-49-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
recon_1  | 2023-05-18 20:11:07,627 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 0 OM DB update event(s).
recon_1  | 2023-05-18 20:11:07,629 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
recon_1  | 2023-05-18 20:11:31,060 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #1001 got from ha_dn4_1.ha_net.
recon_1  | 2023-05-18 20:11:31,090 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Successfully added container #1001 to Recon.
recon_1  | 2023-05-18 20:11:42,198 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: New container #1002 got from ha_dn1_1.ha_net.
recon_1  | 2023-05-18 20:11:42,243 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #1002 got from ha_dn3_1.ha_net.
recon_1  | 2023-05-18 20:11:42,250 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: Successfully added container #1002 to Recon.
recon_1  | 2023-05-18 20:11:42,268 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: Successfully added container #1002 to Recon.
recon_1  | 2023-05-18 20:11:53,387 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Deleted 0 records from "CONTAINER_COUNT_BY_SIZE"
recon_1  | 2023-05-18 20:11:53,422 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1  | 2023-05-18 20:11:53,423 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 35
recon_1  | 2023-05-18 20:12:07,650 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1  | 2023-05-18 20:12:07,650 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1  | 2023-05-18 20:12:07,650 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 35 
recon_1  | 2023-05-18 20:12:07,697 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 14, SequenceNumber diff: 42, SequenceNumber Lag from OM 0.
recon_1  | 2023-05-18 20:12:07,698 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 42 records
recon_1  | 2023-05-18 20:12:07,714 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
recon_1  | 2023-05-18 20:12:07,715 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
recon_1  | 2023-05-18 20:12:07,953 [pool-49-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
recon_1  | 2023-05-18 20:12:08,218 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 4 OM DB update event(s).
recon_1  | 2023-05-18 20:12:08,279 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
