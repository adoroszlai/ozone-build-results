Attaching to ha_s3g_1, ha_dn5_1, ha_dn2_1, ha_om1_1, ha_scm2_1, ha_recon_1, ha_om2_1, ha_scm1_1, ha_om3_1, ha_dn4_1, ha_scm3_1, ha_dn3_1, ha_dn1_1
dn1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn1_1    | 2023-05-31 01:05:30,740 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn1_1    | /************************************************************
dn1_1    | STARTUP_MSG: Starting HddsDatanodeService
dn1_1    | STARTUP_MSG:   host = 2af98bbedaed/10.9.0.17
dn1_1    | STARTUP_MSG:   args = []
dn1_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-beec17828-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/e76f312183ca29cda8abcf963963d32223388b77 ; compiled by 'runner' on 2023-05-31T00:25Z
dn1_1    | STARTUP_MSG:   java = 11.0.14.1
dn1_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn1_1    | ************************************************************/
dn1_1    | 2023-05-31 01:05:30,883 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn1_1    | 2023-05-31 01:05:31,241 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn1_1    | 2023-05-31 01:05:32,038 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn1_1    | 2023-05-31 01:05:33,476 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn1_1    | 2023-05-31 01:05:33,476 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn1_1    | 2023-05-31 01:05:34,653 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:2af98bbedaed ip:10.9.0.17
dn1_1    | 2023-05-31 01:05:36,075 [main] INFO reflections.Reflections: Reflections took 1021 ms to scan 2 urls, producing 104 keys and 224 values 
dn1_1    | 2023-05-31 01:05:38,998 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
dn1_1    | 2023-05-31 01:05:39,408 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn1_1    | 2023-05-31 01:05:40,660 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 4651 at 2023-05-31T01:05:07.024Z
dn1_1    | 2023-05-31 01:05:40,757 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn1_1    | 2023-05-31 01:05:40,797 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn1_1    | 2023-05-31 01:05:40,816 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn1_1    | 2023-05-31 01:05:41,009 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn1_1    | 2023-05-31 01:05:41,205 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-05-31 01:05:41,244 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-05-31T01:05:06.989Z
dn1_1    | 2023-05-31 01:05:41,247 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn1_1    | 2023-05-31 01:05:41,252 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn1_1    | 2023-05-31 01:05:41,286 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn1_1    | 2023-05-31 01:05:44,685 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/DS-c6fc308a-9c7b-4ec3-bbe9-f9baa54649b0/container.db to cache
dn1_1    | 2023-05-31 01:05:44,691 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/DS-c6fc308a-9c7b-4ec3-bbe9-f9baa54649b0/container.db for volume DS-c6fc308a-9c7b-4ec3-bbe9-f9baa54649b0
dn1_1    | 2023-05-31 01:05:44,949 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn1_1    | 2023-05-31 01:05:45,838 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn1_1    | 2023-05-31 01:05:45,841 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 1s
dn1_1    | 2023-05-31 01:05:56,311 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn1_1    | 2023-05-31 01:05:57,012 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-05-31 01:05:57,461 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn1_1    | 2023-05-31 01:05:58,340 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-05-31 01:05:58,434 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn1_1    | 2023-05-31 01:05:58,435 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-05-31 01:05:58,438 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn1_1    | 2023-05-31 01:05:58,442 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn1_1    | 2023-05-31 01:05:58,454 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn1_1    | 2023-05-31 01:05:58,455 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn1_1    | 2023-05-31 01:05:58,455 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-05-31 01:05:58,460 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn1_1    | 2023-05-31 01:05:58,465 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-05-31 01:05:58,594 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 2023-05-31 01:05:58,677 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn1_1    | 2023-05-31 01:05:58,695 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn1_1    | 2023-05-31 01:06:00,905 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn1_1    | 2023-05-31 01:06:00,993 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn1_1    | 2023-05-31 01:06:00,993 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn1_1    | 2023-05-31 01:06:00,994 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-05-31 01:06:00,994 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-05-31 01:06:01,019 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-05-31 01:06:01,079 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServer: b20d0805-391e-447d-a646-35a5d1a5cbcf: found a subdirectory /data/metadata/ratis/6a499bf5-10af-4dab-a3fd-a4e2998a83c9
dn1_1    | 2023-05-31 01:06:01,159 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServer: b20d0805-391e-447d-a646-35a5d1a5cbcf: addNew group-A4E2998A83C9:[] returns group-A4E2998A83C9:java.util.concurrent.CompletableFuture@422000af[Not completed]
dn1_1    | 2023-05-31 01:06:01,159 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServer: b20d0805-391e-447d-a646-35a5d1a5cbcf: found a subdirectory /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1
dn1_1    | 2023-05-31 01:06:01,159 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServer: b20d0805-391e-447d-a646-35a5d1a5cbcf: addNew group-DC0251522AF1:[] returns group-DC0251522AF1:java.util.concurrent.CompletableFuture@43c3a506[Not completed]
dn1_1    | 2023-05-31 01:06:01,159 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServer: b20d0805-391e-447d-a646-35a5d1a5cbcf: found a subdirectory /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444
dn1_1    | 2023-05-31 01:06:01,171 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServer: b20d0805-391e-447d-a646-35a5d1a5cbcf: addNew group-F1A4CF07E444:[] returns group-F1A4CF07E444:java.util.concurrent.CompletableFuture@71d8a4fb[Not completed]
dn1_1    | 2023-05-31 01:06:01,366 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn1_1    | 2023-05-31 01:06:01,573 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf: new RaftServerImpl for group-A4E2998A83C9:[] with ContainerStateMachine:uninitialized
dn1_1    | 2023-05-31 01:06:01,616 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-05-31 01:06:01,617 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-05-31 01:06:01,626 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-05-31 01:06:01,636 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-05-31 01:06:01,637 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-05-31 01:06:01,643 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-05-31 01:06:01,740 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-05-31 01:06:01,751 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-05-31 01:06:01,810 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-05-31 01:06:01,811 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-05-31 01:06:01,868 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn1_1    | 2023-05-31 01:06:02,016 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-05-31 01:06:02,045 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn1_1    | 2023-05-31 01:06:02,108 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-05-31 01:06:02,139 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-05-31 01:06:02,558 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn1_1    | 2023-05-31 01:06:03,059 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-05-31 01:06:03,062 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-05-31 01:06:03,081 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn4_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn4_1    | 2023-05-31 01:05:33,742 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn4_1    | /************************************************************
dn4_1    | STARTUP_MSG: Starting HddsDatanodeService
dn4_1    | STARTUP_MSG:   host = 955d7ba09e69/10.9.0.20
dn4_1    | STARTUP_MSG:   args = []
dn4_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn4_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-beec17828-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn4_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/e76f312183ca29cda8abcf963963d32223388b77 ; compiled by 'runner' on 2023-05-31T00:25Z
dn4_1    | STARTUP_MSG:   java = 11.0.14.1
dn1_1    | 2023-05-31 01:06:03,087 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn1_1    | 2023-05-31 01:06:03,087 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-05-31 01:06:03,096 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-05-31 01:06:03,101 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-05-31 01:06:03,138 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf: new RaftServerImpl for group-DC0251522AF1:[] with ContainerStateMachine:uninitialized
dn1_1    | 2023-05-31 01:06:03,144 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-05-31 01:06:03,157 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-05-31 01:06:03,166 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-05-31 01:06:03,169 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-05-31 01:06:03,169 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-05-31 01:06:03,170 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-05-31 01:06:03,173 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-05-31 01:06:03,175 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-05-31 01:06:03,240 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-05-31 01:06:03,240 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-05-31 01:06:03,250 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-05-31 01:06:03,250 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn1_1    | 2023-05-31 01:06:03,251 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-05-31 01:06:03,259 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-05-31 01:06:03,283 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn1_1    | 2023-05-31 01:06:03,309 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-05-31 01:06:03,311 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-05-31 01:06:03,319 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-05-31 01:06:03,324 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-05-31 01:06:03,327 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-05-31 01:06:03,334 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn1_1    | 2023-05-31 01:06:03,340 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-05-31 01:06:03,467 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf: new RaftServerImpl for group-F1A4CF07E444:[] with ContainerStateMachine:uninitialized
dn1_1    | 2023-05-31 01:06:03,477 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-05-31 01:06:03,477 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-05-31 01:06:03,492 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-05-31 01:06:03,493 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-05-31 01:06:03,493 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-05-31 01:06:03,507 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-05-31 01:06:03,523 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-05-31 01:06:03,525 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-05-31 01:06:03,533 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-05-31 01:06:03,533 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-05-31 01:06:03,536 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-05-31 01:06:03,536 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn1_1    | 2023-05-31 01:06:03,536 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-05-31 01:06:03,536 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-05-31 01:06:03,545 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn1_1    | 2023-05-31 01:06:03,553 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-05-31 01:06:03,553 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-05-31 01:06:03,553 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-05-31 01:06:03,553 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-05-31 01:06:03,572 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-05-31 01:06:03,572 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-05-31 01:06:03,608 [main] INFO util.log: Logging initialized @42210ms to org.eclipse.jetty.util.log.Slf4jLog
dn1_1    | 2023-05-31 01:06:04,642 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn1_1    | 2023-05-31 01:06:04,749 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn1_1    | 2023-05-31 01:06:04,809 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn1_1    | 2023-05-31 01:06:04,837 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn1_1    | 2023-05-31 01:06:04,861 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn1_1    | 2023-05-31 01:06:04,870 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn1_1    | 2023-05-31 01:06:05,220 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn1_1    | 2023-05-31 01:06:05,282 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn1_1    | 2023-05-31 01:06:05,312 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
dn1_1    | 2023-05-31 01:06:05,560 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn1_1    | 2023-05-31 01:06:05,574 [main] INFO server.session: No SessionScavenger set, using defaults
dn1_1    | 2023-05-31 01:06:05,576 [main] INFO server.session: node0 Scavenging every 600000ms
dn1_1    | 2023-05-31 01:06:05,700 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@22f02996{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn1_1    | 2023-05-31 01:06:05,701 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2ca3d826{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn1_1    | 2023-05-31 01:06:06,727 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3ca3648{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-506009014838474139/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn1_1    | 2023-05-31 01:06:06,846 [main] INFO server.AbstractConnector: Started ServerConnector@70881123{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn1_1    | 2023-05-31 01:06:06,847 [main] INFO server.Server: Started @45449ms
dn1_1    | 2023-05-31 01:06:06,872 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn1_1    | 2023-05-31 01:06:06,872 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn1_1    | 2023-05-31 01:06:06,873 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn1_1    | 2023-05-31 01:06:06,994 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn1_1    | 2023-05-31 01:06:07,121 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
dn1_1    | 2023-05-31 01:06:08,506 [Listener at 0.0.0.0/9864] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
dn1_1    | 2023-05-31 01:06:08,515 [Listener at 0.0.0.0/9864] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
dn1_1    | 2023-05-31 01:06:08,516 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn1_1    | 2023-05-31 01:06:08,518 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
dn1_1    | 2023-05-31 01:06:08,560 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn1_1    | 2023-05-31 01:06:08,665 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn1_1    | 2023-05-31 01:06:08,674 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn1_1    | 2023-05-31 01:06:08,689 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@29a9edde] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn1_1    | 2023-05-31 01:06:08,990 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn1_1    | 2023-05-31 01:06:09,049 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn1_1    | 2023-05-31 01:06:11,889 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:11,890 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:11,895 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:11,895 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:12,890 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:12,891 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:12,896 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:13,891 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:13,893 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:13,896 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:14,892 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:14,894 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:14,897 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:15,894 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:15,894 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:15,902 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:16,895 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:16,903 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:16,924 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From 2af98bbedaed/10.9.0.17 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:58656 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn1_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:58656 remote=recon/10.9.0.22:9891]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn1_1    | 2023-05-31 01:06:17,896 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:17,904 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:18,897 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn4_1    | ************************************************************/
dn4_1    | 2023-05-31 01:05:33,826 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn4_1    | 2023-05-31 01:05:34,099 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn4_1    | 2023-05-31 01:05:34,773 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn4_1    | 2023-05-31 01:05:36,081 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn4_1    | 2023-05-31 01:05:36,093 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn4_1    | 2023-05-31 01:05:37,773 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:955d7ba09e69 ip:10.9.0.20
dn4_1    | 2023-05-31 01:05:39,346 [main] INFO reflections.Reflections: Reflections took 1272 ms to scan 2 urls, producing 104 keys and 224 values 
dn4_1    | 2023-05-31 01:05:42,262 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
dn4_1    | 2023-05-31 01:05:42,624 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn4_1    | 2023-05-31 01:05:43,888 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 8747 at 2023-05-31T01:05:07.043Z
dn4_1    | 2023-05-31 01:05:44,008 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn4_1    | 2023-05-31 01:05:44,024 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn4_1    | 2023-05-31 01:05:44,063 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn4_1    | 2023-05-31 01:05:44,210 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn4_1    | 2023-05-31 01:05:44,385 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn4_1    | 2023-05-31 01:05:44,393 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-05-31T01:05:07.042Z
dn4_1    | 2023-05-31 01:05:44,439 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn4_1    | 2023-05-31 01:05:44,440 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn4_1    | 2023-05-31 01:05:44,463 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn4_1    | 2023-05-31 01:05:47,662 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/DS-c79d6d66-d534-4302-b2fa-dcc66ad7b759/container.db to cache
dn4_1    | 2023-05-31 01:05:47,662 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/DS-c79d6d66-d534-4302-b2fa-dcc66ad7b759/container.db for volume DS-c79d6d66-d534-4302-b2fa-dcc66ad7b759
dn4_1    | 2023-05-31 01:05:47,783 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn4_1    | 2023-05-31 01:05:48,579 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn4_1    | 2023-05-31 01:05:48,595 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn2_1    | 2023-05-31 01:05:36,601 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn2_1    | /************************************************************
dn2_1    | STARTUP_MSG: Starting HddsDatanodeService
dn2_1    | STARTUP_MSG:   host = 86166c590ca2/10.9.0.18
dn2_1    | STARTUP_MSG:   args = []
dn2_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-beec17828-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/e76f312183ca29cda8abcf963963d32223388b77 ; compiled by 'runner' on 2023-05-31T00:25Z
dn2_1    | STARTUP_MSG:   java = 11.0.14.1
dn4_1    | 2023-05-31 01:05:58,168 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn4_1    | 2023-05-31 01:05:59,145 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn4_1    | 2023-05-31 01:05:59,445 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn4_1    | 2023-05-31 01:06:00,274 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn4_1    | 2023-05-31 01:06:00,305 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn4_1    | 2023-05-31 01:06:00,306 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn4_1    | 2023-05-31 01:06:00,306 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn4_1    | 2023-05-31 01:06:00,306 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn4_1    | 2023-05-31 01:06:00,306 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn4_1    | 2023-05-31 01:06:00,307 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn4_1    | 2023-05-31 01:06:00,316 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-31 01:06:00,316 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn4_1    | 2023-05-31 01:06:00,317 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-05-31 01:06:00,354 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-05-31 01:06:00,364 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn4_1    | 2023-05-31 01:06:00,380 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn4_1    | 2023-05-31 01:06:02,588 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn4_1    | 2023-05-31 01:06:02,666 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn4_1    | 2023-05-31 01:06:02,673 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn4_1    | 2023-05-31 01:06:02,682 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-05-31 01:06:02,683 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-05-31 01:06:02,686 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-05-31 01:06:02,777 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServer: 3da1490a-d610-40de-95c1-bdfc259ba041: found a subdirectory /data/metadata/ratis/5509b2c4-975c-42f1-91be-39d463d01a51
dn4_1    | 2023-05-31 01:06:02,849 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServer: 3da1490a-d610-40de-95c1-bdfc259ba041: addNew group-39D463D01A51:[] returns group-39D463D01A51:java.util.concurrent.CompletableFuture@2db53200[Not completed]
dn4_1    | 2023-05-31 01:06:02,854 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServer: 3da1490a-d610-40de-95c1-bdfc259ba041: found a subdirectory /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1
dn4_1    | 2023-05-31 01:06:02,854 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServer: 3da1490a-d610-40de-95c1-bdfc259ba041: addNew group-DC0251522AF1:[] returns group-DC0251522AF1:java.util.concurrent.CompletableFuture@4c59829a[Not completed]
dn4_1    | 2023-05-31 01:06:02,855 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServer: 3da1490a-d610-40de-95c1-bdfc259ba041: found a subdirectory /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444
dn4_1    | 2023-05-31 01:06:02,855 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServer: 3da1490a-d610-40de-95c1-bdfc259ba041: addNew group-F1A4CF07E444:[] returns group-F1A4CF07E444:java.util.concurrent.CompletableFuture@23723d28[Not completed]
dn4_1    | 2023-05-31 01:06:03,072 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn4_1    | 2023-05-31 01:06:03,300 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041: new RaftServerImpl for group-39D463D01A51:[] with ContainerStateMachine:uninitialized
dn4_1    | 2023-05-31 01:06:03,362 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-05-31 01:06:03,397 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-05-31 01:06:03,397 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-05-31 01:06:03,399 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-05-31 01:06:03,400 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-05-31 01:06:03,400 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-05-31 01:06:03,450 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-05-31 01:06:03,467 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-05-31 01:06:03,482 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn4_1    | 2023-05-31 01:06:03,561 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-05-31 01:06:03,569 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-05-31 01:06:03,695 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-05-31 01:06:03,711 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn4_1    | 2023-05-31 01:06:03,774 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-05-31 01:06:03,806 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-05-31 01:06:04,186 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn4_1    | 2023-05-31 01:06:04,556 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn4_1    | 2023-05-31 01:06:04,620 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-05-31 01:06:04,622 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn3_1    | 2023-05-31 01:05:35,025 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn3_1    | /************************************************************
dn3_1    | STARTUP_MSG: Starting HddsDatanodeService
dn3_1    | STARTUP_MSG:   host = dcc4205663d5/10.9.0.19
dn3_1    | STARTUP_MSG:   args = []
dn3_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn4_1    | 2023-05-31 01:06:04,658 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-05-31 01:06:04,658 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-05-31 01:06:04,663 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-05-31 01:06:04,667 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-05-31 01:06:04,770 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041: new RaftServerImpl for group-DC0251522AF1:[] with ContainerStateMachine:uninitialized
dn4_1    | 2023-05-31 01:06:04,771 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-05-31 01:06:04,774 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-05-31 01:06:04,790 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-05-31 01:06:04,795 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-05-31 01:06:04,827 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-05-31 01:06:04,830 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-05-31 01:06:04,830 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-05-31 01:06:04,835 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-05-31 01:06:04,837 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-05-31 01:06:04,837 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-05-31 01:06:04,841 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn4_1    | 2023-05-31 01:06:04,846 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-05-31 01:06:04,855 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn4_1    | 2023-05-31 01:06:04,855 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-05-31 01:06:04,858 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-05-31 01:06:04,899 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn4_1    | 2023-05-31 01:06:04,942 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-05-31 01:06:04,943 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-05-31 01:06:04,943 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-05-31 01:06:04,951 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-05-31 01:06:04,958 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-05-31 01:06:04,968 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-05-31 01:06:04,982 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041: new RaftServerImpl for group-F1A4CF07E444:[] with ContainerStateMachine:uninitialized
dn4_1    | 2023-05-31 01:06:05,014 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-05-31 01:06:05,015 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-05-31 01:06:05,032 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-05-31 01:06:05,033 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-05-31 01:06:05,034 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-05-31 01:06:05,034 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-05-31 01:06:05,044 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-05-31 01:06:05,044 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-05-31 01:06:05,044 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-05-31 01:06:05,079 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-05-31 01:06:05,080 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-05-31 01:06:05,081 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn4_1    | 2023-05-31 01:06:05,083 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-05-31 01:06:05,083 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-05-31 01:06:05,087 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn4_1    | 2023-05-31 01:06:05,097 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-05-31 01:06:05,109 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-beec17828-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn2_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn5_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1    | 2023-05-31 01:05:37,233 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1    | /************************************************************
om1_1    | STARTUP_MSG: Starting OzoneManager
om1_1    | STARTUP_MSG:   host = 4c30cb93385d/10.9.0.11
om1_1    | STARTUP_MSG:   args = [--upgrade]
dn3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/e76f312183ca29cda8abcf963963d32223388b77 ; compiled by 'runner' on 2023-05-31T00:25Z
dn2_1    | ************************************************************/
dn5_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn5_1    | 2023-05-31 01:05:36,295 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn1_1    | 2023-05-31 01:06:18,906 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:19,900 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | STARTUP_MSG:   java = 11.0.14.1
dn2_1    | 2023-05-31 01:05:36,663 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn2_1    | 2023-05-31 01:05:37,012 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn5_1    | /************************************************************
dn4_1    | 2023-05-31 01:06:05,110 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-05-31 01:06:05,110 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-05-31 01:06:19,907 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:05:37,838 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om1_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-beec17828-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
dn4_1    | 2023-05-31 01:06:05,124 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn1_1    | 2023-05-31 01:06:20,900 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:05:39,007 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn5_1    | STARTUP_MSG: Starting HddsDatanodeService
om1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/e76f312183ca29cda8abcf963963d32223388b77 ; compiled by 'runner' on 2023-05-31T00:26Z
dn4_1    | 2023-05-31 01:06:05,124 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | ************************************************************/
om2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn1_1    | 2023-05-31 01:06:20,907 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:05:39,007 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
om3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn5_1    | STARTUP_MSG:   host = 51bd9aa00c0e/10.9.0.21
om1_1    | STARTUP_MSG:   java = 11.0.14.1
dn3_1    | 2023-05-31 01:05:35,143 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1    | 2023-05-31 01:05:36,311 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1    | /************************************************************
dn2_1    | 2023-05-31 01:05:40,404 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:86166c590ca2 ip:10.9.0.18
om3_1    | 2023-05-31 01:05:36,069 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
dn5_1    | STARTUP_MSG:   args = []
scm1_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn4_1    | 2023-05-31 01:06:05,234 [main] INFO util.log: Logging initialized @42901ms to org.eclipse.jetty.util.log.Slf4jLog
dn4_1    | 2023-05-31 01:06:06,439 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn4_1    | 2023-05-31 01:06:06,477 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
om1_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om1_1    | ************************************************************/
om1_1    | 2023-05-31 01:05:37,338 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
dn1_1    | 2023-05-31 01:06:20,909 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From 2af98bbedaed/10.9.0.17 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:48644 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
om3_1    | /************************************************************
dn5_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn5_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-beec17828-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn4_1    | 2023-05-31 01:06:06,521 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om1_1    | 2023-05-31 01:05:45,944 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
s3g_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1    | 2023-05-31 01:05:36,112 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
om2_1    | STARTUP_MSG: Starting OzoneManager
om2_1    | STARTUP_MSG:   host = 45ffe5496dee/10.9.0.12
dn5_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/e76f312183ca29cda8abcf963963d32223388b77 ; compiled by 'runner' on 2023-05-31T00:25Z
scm2_1   | Waiting for the service scm1:9894
scm2_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm2_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2_1   | 2023-05-31 01:06:19,970 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm2_1   | /************************************************************
recon_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1    | 2023-05-31 01:05:36,113 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1    | 2023-05-31 01:05:36,407 [main] INFO util.log: Logging initialized @11358ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1    | 2023-05-31 01:05:37,758 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om2_1    | STARTUP_MSG:   args = [--upgrade]
om3_1    | STARTUP_MSG: Starting OzoneManager
scm3_1   | Waiting for the service scm2:9894
dn5_1    | STARTUP_MSG:   java = 11.0.14.1
scm2_1   | STARTUP_MSG: Starting StorageContainerManager
dn3_1    | 2023-05-31 01:05:35,568 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn3_1    | 2023-05-31 01:05:36,310 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om1_1    | 2023-05-31 01:05:49,285 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
dn4_1    | 2023-05-31 01:06:06,531 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn4_1    | 2023-05-31 01:06:06,541 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn4_1    | 2023-05-31 01:06:06,541 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1    | 2023-05-31 01:05:37,995 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1    | 2023-05-31 01:05:38,073 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1    | 2023-05-31 01:05:38,102 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
om2_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-beec17828-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
scm3_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm2_1   | STARTUP_MSG:   host = 2e348dbfef56/10.9.0.15
dn3_1    | 2023-05-31 01:05:37,667 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om1_1    | 2023-05-31 01:05:49,714 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
dn5_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn5_1    | ************************************************************/
dn5_1    | 2023-05-31 01:05:36,400 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1    | 2023-05-31 01:05:38,144 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1    | 2023-05-31 01:05:38,145 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn2_1    | 2023-05-31 01:05:41,865 [main] INFO reflections.Reflections: Reflections took 1105 ms to scan 2 urls, producing 104 keys and 224 values 
om2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/e76f312183ca29cda8abcf963963d32223388b77 ; compiled by 'runner' on 2023-05-31T00:26Z
om3_1    | STARTUP_MSG:   host = 482910862efc/10.9.0.13
scm2_1   | STARTUP_MSG:   args = []
scm2_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn3_1    | 2023-05-31 01:05:37,667 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn3_1    | 2023-05-31 01:05:38,869 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:dcc4205663d5 ip:10.9.0.19
om1_1    | 2023-05-31 01:05:49,715 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
recon_1  | 2023-05-31 01:05:36,309 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1  | /************************************************************
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
om2_1    | STARTUP_MSG:   java = 11.0.14.1
om3_1    | STARTUP_MSG:   args = [--upgrade]
scm3_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn3_1    | 2023-05-31 01:05:40,083 [main] INFO reflections.Reflections: Reflections took 918 ms to scan 2 urls, producing 104 keys and 224 values 
dn4_1    | 2023-05-31 01:06:06,736 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn4_1    | 2023-05-31 01:06:06,768 [main] INFO http.HttpServer2: Jetty bound to port 9882
om1_1    | 2023-05-31 01:05:49,844 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn5_1    | 2023-05-31 01:05:36,736 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn5_1    | 2023-05-31 01:05:37,410 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn5_1    | 2023-05-31 01:05:38,758 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn2_1    | 2023-05-31 01:05:45,078 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
dn2_1    | 2023-05-31 01:05:45,560 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
om2_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om3_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn3_1    | 2023-05-31 01:05:42,982 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
dn3_1    | 2023-05-31 01:05:43,452 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn3_1    | 2023-05-31 01:05:44,703 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 8192 at 2023-05-31T01:05:06.946Z
dn3_1    | 2023-05-31 01:05:44,814 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
om1_1    | 2023-05-31 01:05:50,165 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = FILESYSTEM_SNAPSHOT (version = 5)
dn5_1    | 2023-05-31 01:05:38,759 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
recon_1  | STARTUP_MSG: Starting ReconServer
recon_1  | STARTUP_MSG:   host = 46d31cd800d0/10.9.0.22
om2_1    | ************************************************************/
dn3_1    | 2023-05-31 01:05:44,816 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn3_1    | 2023-05-31 01:05:44,844 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn3_1    | 2023-05-31 01:05:44,968 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn3_1    | 2023-05-31 01:05:45,077 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-beec17828-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om1_1    | 2023-05-31 01:05:53,127 [main] INFO reflections.Reflections: Reflections took 2517 ms to scan 1 urls, producing 133 keys and 388 values [using 2 cores]
om1_1    | 2023-05-31 01:05:53,325 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | STARTUP_MSG:   args = []
recon_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn2_1    | 2023-05-31 01:05:46,680 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 4096 at 2023-05-31T01:05:06.930Z
scm1_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1    | 2023-05-31 01:05:36,433 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
dn3_1    | 2023-05-31 01:05:45,110 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-05-31T01:05:07.023Z
dn3_1    | 2023-05-31 01:05:45,131 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
om1_1    | 2023-05-31 01:05:55,231 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
dn5_1    | 2023-05-31 01:05:40,151 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:51bd9aa00c0e ip:10.9.0.21
dn5_1    | 2023-05-31 01:05:41,505 [main] INFO reflections.Reflections: Reflections took 1073 ms to scan 2 urls, producing 104 keys and 224 values 
dn2_1    | 2023-05-31 01:05:46,810 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn2_1    | 2023-05-31 01:05:46,823 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
recon_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-5.1.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.27.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-beec17828-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.41.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.27.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.27.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.27.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/guice-5.1.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/guice-servlet-5.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
recon_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/e76f312183ca29cda8abcf963963d32223388b77 ; compiled by 'runner' on 2023-05-31T00:26Z
om2_1    | 2023-05-31 01:05:45,262 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
dn3_1    | 2023-05-31 01:05:45,131 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn3_1    | 2023-05-31 01:05:45,192 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
om1_1    | 2023-05-31 01:05:55,636 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
dn5_1    | 2023-05-31 01:05:44,043 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
dn5_1    | 2023-05-31 01:05:44,504 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn5_1    | 2023-05-31 01:05:45,710 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 4651 at 2023-05-31T01:05:06.971Z
dn5_1    | 2023-05-31 01:05:45,825 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
recon_1  | STARTUP_MSG:   java = 11.0.14.1
recon_1  | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.sql.db.auto.commit=true, ozone.recon.sql.db.conn.idle.max.age=3600s, ozone.recon.sql.db.conn.idle.test=SELECT 1, ozone.recon.sql.db.conn.idle.test.period=60s, ozone.recon.sql.db.conn.max.active=5, ozone.recon.sql.db.conn.max.age=1800s, ozone.recon.sql.db.conn.timeout=30000ms, ozone.recon.sql.db.driver=org.apache.derby.jdbc.EmbeddedDriver, ozone.recon.sql.db.jdbc.url=jdbc:derby:/data/metadata/recon/ozone_recon_derby.db, ozone.recon.sql.db.jooq.dialect=DERBY, ozone.recon.task.containercounttask.interval=60s, ozone.recon.task.missingcontainer.interval=300s, ozone.recon.task.pipelinesync.interval=300s, ozone.recon.task.safemode.wait.threshold=300s, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om2_1    | 2023-05-31 01:05:48,708 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
scm1_1   | 2023-05-31 01:05:47,166 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
dn3_1    | 2023-05-31 01:05:45,985 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/DS-3fb57306-a48a-438a-89c7-33929f039b19/container.db to cache
om1_1    | 2023-05-31 01:06:02,091 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4c30cb93385d/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
dn5_1    | 2023-05-31 01:05:45,841 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn5_1    | 2023-05-31 01:05:45,865 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn5_1    | 2023-05-31 01:05:46,125 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn5_1    | 2023-05-31 01:05:46,288 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
recon_1  | ************************************************************/
om2_1    | 2023-05-31 01:05:49,048 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/e76f312183ca29cda8abcf963963d32223388b77 ; compiled by 'runner' on 2023-05-31T00:26Z
om3_1    | STARTUP_MSG:   java = 11.0.14.1
scm1_1   | /************************************************************
dn3_1    | 2023-05-31 01:05:46,038 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/DS-3fb57306-a48a-438a-89c7-33929f039b19/container.db for volume DS-3fb57306-a48a-438a-89c7-33929f039b19
dn3_1    | 2023-05-31 01:05:46,097 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn3_1    | 2023-05-31 01:05:46,099 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn3_1    | 2023-05-31 01:05:46,115 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn3_1    | 2023-05-31 01:05:58,377 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
recon_1  | 2023-05-31 01:05:36,380 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1    | 2023-05-31 01:05:38,518 [main] INFO http.BaseHttpServer: HTTP server of s3gateway uses base directory /opt/hadoop/ozone_s3g_tmp_base_dir4492253588079406295
om2_1    | 2023-05-31 01:05:49,057 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
om1_1    | 2023-05-31 01:06:04,092 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4c30cb93385d/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
om1_1    | 2023-05-31 01:06:06,094 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4c30cb93385d/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
om3_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om3_1    | ************************************************************/
om3_1    | 2023-05-31 01:05:36,139 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
recon_1  | 2023-05-31 01:05:40,968 [main] INFO reflections.Reflections: Reflections took 647 ms to scan 1 urls, producing 18 keys and 67 values 
scm2_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-beec17828-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
dn4_1    | 2023-05-31 01:06:06,769 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
recon_1  | 2023-05-31 01:05:45,125 [main] INFO reflections.Reflections: Reflections took 1025 ms to scan 3 urls, producing 129 keys and 280 values 
s3g_1    | 2023-05-31 01:05:39,888 [main] INFO s3.Gateway: STARTUP_MSG: 
om2_1    | 2023-05-31 01:05:49,105 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-05-31 01:05:49,405 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = FILESYSTEM_SNAPSHOT (version = 5)
om2_1    | 2023-05-31 01:05:52,118 [main] INFO reflections.Reflections: Reflections took 2136 ms to scan 1 urls, producing 133 keys and 388 values [using 2 cores]
dn2_1    | 2023-05-31 01:05:46,829 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
om3_1    | 2023-05-31 01:05:44,798 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1    | 2023-05-31 01:05:48,322 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om3_1    | 2023-05-31 01:05:48,691 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1    | 2023-05-31 01:05:48,692 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om3_1    | 2023-05-31 01:05:48,746 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/e76f312183ca29cda8abcf963963d32223388b77 ; compiled by 'runner' on 2023-05-31T00:25Z
dn4_1    | 2023-05-31 01:06:06,936 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1    | /************************************************************
recon_1  | 2023-05-31 01:05:45,708 [main] INFO recon.ReconServer: Initializing Recon server...
om1_1    | 2023-05-31 01:06:08,096 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4c30cb93385d/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
om2_1    | 2023-05-31 01:05:52,480 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-05-31 01:05:55,215 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
dn2_1    | 2023-05-31 01:05:47,003 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn2_1    | 2023-05-31 01:05:47,149 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-05-31 01:05:47,188 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-05-31T01:05:07.090Z
om3_1    | 2023-05-31 01:05:49,265 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = FILESYSTEM_SNAPSHOT (version = 5)
dn3_1    | 2023-05-31 01:05:59,585 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-05-31 01:06:00,023 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm2_1   | STARTUP_MSG:   java = 11.0.14.1
dn4_1    | 2023-05-31 01:06:06,936 [main] INFO server.session: No SessionScavenger set, using defaults
dn4_1    | 2023-05-31 01:06:06,944 [main] INFO server.session: node0 Scavenging every 600000ms
dn4_1    | 2023-05-31 01:06:06,997 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@22f02996{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om2_1    | 2023-05-31 01:05:55,639 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
scm1_1   | STARTUP_MSG: Starting StorageContainerManager
dn2_1    | 2023-05-31 01:05:47,219 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn2_1    | 2023-05-31 01:05:47,223 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
om3_1    | 2023-05-31 01:05:52,212 [main] INFO reflections.Reflections: Reflections took 2293 ms to scan 1 urls, producing 133 keys and 388 values [using 2 cores]
dn3_1    | 2023-05-31 01:06:00,813 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-05-31 01:06:00,832 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn3_1    | 2023-05-31 01:06:00,834 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-05-31 01:06:00,855 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn3_1    | 2023-05-31 01:06:00,855 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm2_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om2_1    | 2023-05-31 01:06:02,417 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 45ffe5496dee/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
om2_1    | 2023-05-31 01:06:04,420 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 45ffe5496dee/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
scm1_1   | STARTUP_MSG:   host = be0664a92293/10.9.0.14
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
om3_1    | 2023-05-31 01:05:52,471 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-05-31 01:05:54,459 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
dn4_1    | 2023-05-31 01:06:07,006 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2ca3d826{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn4_1    | 2023-05-31 01:06:07,809 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3ca3648{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-12171670401716261943/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn4_1    | 2023-05-31 01:06:07,892 [main] INFO server.AbstractConnector: Started ServerConnector@70881123{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn4_1    | 2023-05-31 01:06:07,893 [main] INFO server.Server: Started @45560ms
dn4_1    | 2023-05-31 01:06:07,903 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn4_1    | 2023-05-31 01:06:07,904 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm2_1   | ************************************************************/
om2_1    | 2023-05-31 01:06:06,422 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 45ffe5496dee/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
om2_1    | 2023-05-31 01:06:08,424 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 45ffe5496dee/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
om2_1    | 2023-05-31 01:06:10,429 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 45ffe5496dee/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
dn1_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
s3g_1    | STARTUP_MSG: Starting Gateway
s3g_1    | STARTUP_MSG:   host = f3f2c2a53d57/10.9.0.23
scm2_1   | 2023-05-31 01:06:19,980 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
dn2_1    | 2023-05-31 01:05:47,244 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
scm1_1   | STARTUP_MSG:   args = []
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
s3g_1    | STARTUP_MSG:   args = []
s3g_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om2_1    | 2023-05-31 01:06:12,431 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 45ffe5496dee/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
om3_1    | 2023-05-31 01:05:54,897 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om1_1    | 2023-05-31 01:06:10,097 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4c30cb93385d/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
om1_1    | 2023-05-31 01:06:12,102 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4c30cb93385d/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
om1_1    | 2023-05-31 01:06:14,104 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4c30cb93385d/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
om1_1    | 2023-05-31 01:06:16,106 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4c30cb93385d/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
om1_1    | 2023-05-31 01:06:18,107 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4c30cb93385d/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
scm2_1   | 2023-05-31 01:06:20,027 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn2_1    | 2023-05-31 01:05:47,836 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/DS-b37add21-a612-4e15-b3a3-5303e4a1650f/container.db to cache
scm1_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om2_1    | 2023-05-31 01:06:14,433 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 45ffe5496dee/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
om2_1    | 2023-05-31 01:06:16,435 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 45ffe5496dee/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
s3g_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-beec17828-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-shaded-3.1.9.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/cdi-api-2.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
s3g_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/e76f312183ca29cda8abcf963963d32223388b77 ; compiled by 'runner' on 2023-05-31T00:26Z
om1_1    | 2023-05-31 01:06:20,149 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:7b17d04a-0b5d-44ed-b487-1bf0d959b978 is not the leader. Could not determine the leader node.
om1_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
dn3_1    | 2023-05-31 01:06:00,862 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn3_1    | 2023-05-31 01:06:00,863 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn3_1    | 2023-05-31 01:06:00,882 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-05-31 01:06:20,244 [main] INFO reflections.Reflections: Reflections took 182 ms to scan 3 urls, producing 129 keys and 280 values 
dn2_1    | 2023-05-31 01:05:47,836 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/DS-b37add21-a612-4e15-b3a3-5303e4a1650f/container.db for volume DS-b37add21-a612-4e15-b3a3-5303e4a1650f
dn2_1    | 2023-05-31 01:05:47,960 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm1_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-beec17828-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
om2_1    | 2023-05-31 01:06:18,436 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 45ffe5496dee/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
s3g_1    | STARTUP_MSG:   java = 11.0.14.1
dn4_1    | 2023-05-31 01:06:07,911 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
om1_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om3_1    | 2023-05-31 01:06:01,716 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 482910862efc/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
dn3_1    | 2023-05-31 01:06:00,895 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn3_1    | 2023-05-31 01:06:00,902 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-05-31 01:06:01,063 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm2_1   | 2023-05-31 01:06:20,302 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
dn2_1    | 2023-05-31 01:05:47,961 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn2_1    | 2023-05-31 01:05:47,961 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn2_1    | 2023-05-31 01:05:59,376 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
scm1_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/e76f312183ca29cda8abcf963963d32223388b77 ; compiled by 'runner' on 2023-05-31T00:25Z
om2_1    | 2023-05-31 01:06:20,503 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:7b17d04a-0b5d-44ed-b487-1bf0d959b978 is not the leader. Could not determine the leader node.
om2_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
dn4_1    | 2023-05-31 01:06:07,994 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om1_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
s3g_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.basedir=/opt/hadoop/ozone_s3g_tmp_base_dir4492253588079406295, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
s3g_1    | ************************************************************/
dn3_1    | 2023-05-31 01:06:01,093 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn3_1    | 2023-05-31 01:06:01,108 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn3_1    | 2023-05-31 01:06:03,180 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm2_1   | 2023-05-31 01:06:20,306 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
dn2_1    | 2023-05-31 01:05:59,941 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | STARTUP_MSG:   java = 11.0.14.1
om2_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
dn4_1    | 2023-05-31 01:06:08,091 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
dn4_1    | 2023-05-31 01:06:09,467 [Listener at 0.0.0.0/9864] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
dn4_1    | 2023-05-31 01:06:09,468 [Listener at 0.0.0.0/9864] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
dn3_1    | 2023-05-31 01:06:03,232 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
recon_1  | 2023-05-31 01:05:45,722 [main] INFO impl.ReconDBProvider: Last known Recon DB : /data/metadata/recon/recon-container-key.db_1685494412864
scm3_1   | 2023-05-31 01:06:27,772 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm2_1   | 2023-05-31 01:06:20,334 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2:9894 and Ratis port: 9894
scm2_1   | 2023-05-31 01:06:20,335 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2
scm2_1   | 2023-05-31 01:06:21,002 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2_1   | 2023-05-31 01:06:21,177 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2_1   | 2023-05-31 01:06:21,382 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
om2_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om1_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
dn4_1    | 2023-05-31 01:06:09,468 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn4_1    | 2023-05-31 01:06:09,478 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
dn4_1    | 2023-05-31 01:06:09,536 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn3_1    | 2023-05-31 01:06:03,270 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn3_1    | 2023-05-31 01:06:03,275 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-05-31 01:06:03,279 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:48644 remote=scm1/10.9.0.14:9861]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
om2_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
s3g_1    | 2023-05-31 01:05:39,941 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1    | 2023-05-31 01:05:40,076 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1    | 2023-05-31 01:05:40,739 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1    | 2023-05-31 01:05:42,369 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn2_1    | 2023-05-31 01:06:00,361 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn2_1    | 2023-05-31 01:06:01,517 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
dn4_1    | 2023-05-31 01:06:09,624 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
s3g_1    | 2023-05-31 01:05:42,370 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
om3_1    | 2023-05-31 01:06:03,718 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 482910862efc/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
recon_1  | 2023-05-31 01:05:47,482 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
dn2_1    | 2023-05-31 01:06:01,548 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn2_1    | 2023-05-31 01:06:01,555 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-05-31 01:05:46,290 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-05-31T01:05:07.181Z
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
dn4_1    | 2023-05-31 01:06:09,625 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
s3g_1    | 2023-05-31 01:05:42,712 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1    | 2023-05-31 01:05:42,714 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
dn2_1    | 2023-05-31 01:06:01,558 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn2_1    | 2023-05-31 01:06:01,560 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn2_1    | 2023-05-31 01:06:01,563 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn2_1    | 2023-05-31 01:06:01,571 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn2_1    | 2023-05-31 01:06:01,574 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-05-31 01:06:01,578 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
recon_1  | 2023-05-31 01:05:52,906 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
scm1_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm1_1   | ************************************************************/
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om1_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
dn4_1    | 2023-05-31 01:06:09,660 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@29e5e9e0] INFO util.JvmPauseMonitor: Starting JVM pause monitor
s3g_1    | 2023-05-31 01:05:42,943 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om3_1    | 2023-05-31 01:06:05,728 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 482910862efc/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
recon_1  | WARNING: An illegal reflective access operation has occurred
dn3_1    | 2023-05-31 01:06:03,344 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm1_1   | 2023-05-31 01:05:47,438 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
dn4_1    | 2023-05-31 01:06:09,845 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn4_1    | 2023-05-31 01:06:09,935 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn4_1    | 2023-05-31 01:06:12,782 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:05:46,309 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn5_1    | 2023-05-31 01:05:46,309 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn5_1    | 2023-05-31 01:05:46,332 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn5_1    | 2023-05-31 01:05:49,333 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/DS-62d48aa6-0f94-4fb5-a563-d40a6ff7ca20/container.db to cache
dn5_1    | 2023-05-31 01:05:49,333 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/DS-62d48aa6-0f94-4fb5-a563-d40a6ff7ca20/container.db for volume DS-62d48aa6-0f94-4fb5-a563-d40a6ff7ca20
dn5_1    | 2023-05-31 01:05:49,556 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
recon_1  | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
scm1_1   | 2023-05-31 01:05:48,703 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om3_1    | 2023-05-31 01:06:07,731 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 482910862efc/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
om3_1    | 2023-05-31 01:06:09,733 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 482910862efc/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
s3g_1    | 2023-05-31 01:05:42,943 [main] INFO server.session: No SessionScavenger set, using defaults
dn5_1    | 2023-05-31 01:05:50,507 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn1_1    | 2023-05-31 01:06:21,901 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:01,579 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-05-31 01:06:01,720 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
recon_1  | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
scm1_1   | 2023-05-31 01:05:52,166 [main] INFO reflections.Reflections: Reflections took 2939 ms to scan 3 urls, producing 129 keys and 280 values 
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om1_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om3_1    | 2023-05-31 01:06:11,735 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 482910862efc/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
om3_1    | 2023-05-31 01:06:13,738 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 482910862efc/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
om3_1    | 2023-05-31 01:06:15,744 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 482910862efc/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
dn5_1    | 2023-05-31 01:05:50,515 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 1s
dn1_1    | 2023-05-31 01:06:21,908 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:22,902 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:22,909 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:01,769 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn2_1    | 2023-05-31 01:06:01,772 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn2_1    | 2023-05-31 01:06:03,577 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
recon_1  | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
scm1_1   | 2023-05-31 01:05:53,760 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
om1_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
s3g_1    | 2023-05-31 01:05:42,994 [main] INFO server.session: node0 Scavenging every 660000ms
s3g_1    | 2023-05-31 01:05:43,147 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@291a7e3c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn4_1    | 2023-05-31 01:06:12,792 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:05:59,921 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn5_1    | 2023-05-31 01:06:00,778 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 2023-05-31 01:06:01,247 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn5_1    | 2023-05-31 01:06:02,083 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-05-31 01:06:02,105 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn5_1    | 2023-05-31 01:06:02,119 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm3_1   | /************************************************************
om2_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om1_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
s3g_1    | 2023-05-31 01:05:43,165 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@21337f7b{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1    | WARNING: An illegal reflective access operation has occurred
dn4_1    | 2023-05-31 01:06:12,813 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:02,123 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn1_1    | 2023-05-31 01:06:23,903 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:23,910 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:24,903 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:03,581 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
recon_1  | WARNING: All illegal access operations will be denied in a future release
scm3_1   | STARTUP_MSG: Starting StorageContainerManager
om1_1    | , while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
om1_1    | 2023-05-31 01:06:22,150 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4c30cb93385d/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
om3_1    | 2023-05-31 01:06:17,746 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 482910862efc/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
om3_1    | 2023-05-31 01:06:19,828 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:7b17d04a-0b5d-44ed-b487-1bf0d959b978 is not the leader. Could not determine the leader node.
dn4_1    | 2023-05-31 01:06:13,705 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn5_1    | 2023-05-31 01:06:02,124 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm2_1   | 2023-05-31 01:06:21,384 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm2_1   | 2023-05-31 01:06:21,450 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm2_1   | 2023-05-31 01:06:21,571 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2
dn2_1    | 2023-05-31 01:06:03,595 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
recon_1  | 2023-05-31 01:05:56,731 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
scm3_1   | STARTUP_MSG:   host = 6bc445deac8a/10.9.0.16
om2_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 2023-05-31 01:06:24,152 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4c30cb93385d/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
om1_1    | 2023-05-31 01:06:26,164 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:7b17d04a-0b5d-44ed-b487-1bf0d959b978 is not the leader. Could not determine the leader node.
dn5_1    | 2023-05-31 01:06:02,124 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn5_1    | 2023-05-31 01:06:02,124 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn5_1    | 2023-05-31 01:06:02,127 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:06:02,147 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn5_1    | 2023-05-31 01:06:02,150 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
recon_1  | 2023-05-31 01:05:56,752 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
scm3_1   | STARTUP_MSG:   args = []
om2_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om1_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om1_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
dn1_1    | 2023-05-31 01:06:25,904 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:25,905 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn1_1    | java.net.ConnectException: Call From 2af98bbedaed/10.9.0.17 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
recon_1  | 2023-05-31 01:05:56,765 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1  | 2023-05-31 01:06:00,187 [main] INFO codegen.SqlDbUtils: UNHEALTHY_CONTAINERS table already exists, skipping creation.
om1_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om1_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
dn2_1    | 2023-05-31 01:06:03,595 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-05-31 01:06:03,599 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-05-31 01:06:03,602 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-05-31 01:06:03,634 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServer: 4bc93e86-8989-4282-b7d8-050f4a19b92f: found a subdirectory /data/metadata/ratis/ccaf8839-8737-4f75-8b75-b011fdd1ea12
dn2_1    | 2023-05-31 01:06:03,658 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServer: 4bc93e86-8989-4282-b7d8-050f4a19b92f: addNew group-B011FDD1EA12:[] returns group-B011FDD1EA12:java.util.concurrent.CompletableFuture@4aaeb0fe[Not completed]
scm2_1   | 2023-05-31 01:06:21,649 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
recon_1  | 2023-05-31 01:06:00,550 [main] INFO codegen.SqlDbUtils: FILE_COUNT_BY_SIZE table already exists, skipping creation.
om2_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
dn2_1    | 2023-05-31 01:06:04,016 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn4_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
recon_1  | 2023-05-31 01:06:00,671 [main] INFO codegen.SqlDbUtils: CLUSTER_GROWTH_DAILY table already exists, skipping creation.
om2_1    | , while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om1_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om1_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn2_1    | 2023-05-31 01:06:04,332 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f: new RaftServerImpl for group-B011FDD1EA12:[] with ContainerStateMachine:uninitialized
dn2_1    | 2023-05-31 01:06:04,456 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn5_1    | 2023-05-31 01:06:02,267 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
recon_1  | 2023-05-31 01:06:00,746 [main] INFO codegen.SqlDbUtils: CONTAINER_COUNT_BY_SIZE table already exists, skipping creation.
recon_1  | 2023-05-31 01:06:00,837 [main] INFO codegen.SqlDbUtils: GLOBAL_STATS table already exists, skipping creation.
om1_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 2023-05-31 01:05:53,766 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
dn2_1    | 2023-05-31 01:06:04,498 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm2_1   | 2023-05-31 01:06:21,658 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn1_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
om2_1    | 2023-05-31 01:06:22,508 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 45ffe5496dee/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
om1_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 2023-05-31 01:05:54,321 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1:9894 and Ratis port: 9894
dn2_1    | 2023-05-31 01:06:04,503 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-05-31 01:06:04,506 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-05-31 01:06:04,507 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-05-31 01:06:04,515 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-05-31 01:06:04,666 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-05-31 01:06:04,705 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-05-31 01:06:04,757 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
scm2_1   | 2023-05-31 01:06:21,660 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm2_1   | 2023-05-31 01:06:21,662 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om3_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om3_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om3_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om1_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm1_1   | 2023-05-31 01:05:54,332 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1
dn5_1    | 2023-05-31 01:06:02,302 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn5_1    | 2023-05-31 01:06:02,350 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn5_1    | 2023-05-31 01:06:04,579 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn5_1    | 2023-05-31 01:06:04,585 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn5_1    | 2023-05-31 01:06:04,639 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn5_1    | 2023-05-31 01:06:04,643 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
s3g_1    | WARNING: Illegal reflective access by com.sun.xml.bind.v2.runtime.reflect.opt.Injector (file:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
om3_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om1_1    | , while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 15.
om2_1    | 2023-05-31 01:06:24,511 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 45ffe5496dee/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
scm2_1   | 2023-05-31 01:06:21,662 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm2_1   | 2023-05-31 01:06:21,663 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm2_1   | 2023-05-31 01:06:21,663 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm2_1   | 2023-05-31 01:06:21,663 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm2_1   | 2023-05-31 01:06:21,665 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-05-31 01:06:21,666 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
s3g_1    | WARNING: Please consider reporting this to the maintainers of com.sun.xml.bind.v2.runtime.reflect.opt.Injector
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om3_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om1_1    | 2023-05-31 01:06:28,173 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2 is not the leader. Could not determine the leader node.
scm1_1   | 2023-05-31 01:06:03,764 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-05-31 01:06:26,518 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:7b17d04a-0b5d-44ed-b487-1bf0d959b978 is not the leader. Could not determine the leader node.
scm2_1   | 2023-05-31 01:06:21,668 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2_1   | 2023-05-31 01:06:21,680 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm2_1   | 2023-05-31 01:06:21,686 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm2_1   | 2023-05-31 01:06:21,686 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm2_1   | 2023-05-31 01:06:22,035 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm2_1   | 2023-05-31 01:06:22,055 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:538)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
recon_1  | 2023-05-31 01:06:00,942 [main] INFO codegen.SqlDbUtils: RECON_TASK_STATUS table already exists, skipping creation.
om1_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
scm1_1   | 2023-05-31 01:06:06,301 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-05-31 01:06:08,621 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
dn3_1    | 2023-05-31 01:06:03,380 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServer: 97734618-7966-42ac-b50e-e49ead6ce4d2: found a subdirectory /data/metadata/ratis/9e886fff-1860-4674-bb59-7a8b4be06a04
dn3_1    | 2023-05-31 01:06:03,406 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServer: 97734618-7966-42ac-b50e-e49ead6ce4d2: addNew group-7A8B4BE06A04:[] returns group-7A8B4BE06A04:java.util.concurrent.CompletableFuture@18fd84af[Not completed]
dn3_1    | 2023-05-31 01:06:03,739 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn2_1    | 2023-05-31 01:06:04,845 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-05-31 01:06:04,846 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
s3g_1    | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1    | WARNING: All illegal access operations will be denied in a future release
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1  | 2023-05-31 01:06:01,360 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
om1_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om1_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
scm1_1   | 2023-05-31 01:06:08,638 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
dn5_1    | 2023-05-31 01:06:04,643 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-05-31 01:06:04,646 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
s3g_1    | 2023-05-31 01:06:10,095 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3b770d07{s3gateway,/,file:///opt/hadoop/ozone_s3g_tmp_base_dir4492253588079406295/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-1789412115721782947/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
dn4_1    | Caused by: java.util.concurrent.TimeoutException
scm3_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-beec17828-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
om1_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
dn3_1    | 2023-05-31 01:06:04,001 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2: new RaftServerImpl for group-7A8B4BE06A04:[] with ContainerStateMachine:uninitialized
dn3_1    | 2023-05-31 01:06:04,100 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-05-31 01:06:04,103 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-05-31 01:06:04,103 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-05-31 01:06:04,111 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-05-31 01:06:04,117 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
s3g_1    | 2023-05-31 01:06:10,158 [main] INFO server.AbstractConnector: Started ServerConnector@6d025197{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1    | 2023-05-31 01:06:10,170 [main] INFO server.Server: Started @45121ms
om3_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/e76f312183ca29cda8abcf963963d32223388b77 ; compiled by 'runner' on 2023-05-31T00:25Z
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
om2_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
dn3_1    | 2023-05-31 01:06:04,117 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-05-31 01:06:04,691 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServer: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: found a subdirectory /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1
dn5_1    | 2023-05-31 01:06:04,818 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServer: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: addNew group-DC0251522AF1:[] returns group-DC0251522AF1:java.util.concurrent.CompletableFuture@4697f588[Not completed]
dn5_1    | 2023-05-31 01:06:04,818 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServer: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: found a subdirectory /data/metadata/ratis/437d8269-2a0d-4058-8be6-ed0a589991d2
dn2_1    | 2023-05-31 01:06:05,075 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-05-31 01:06:05,130 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
s3g_1    | 2023-05-31 01:06:10,201 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1    | 2023-05-31 01:06:10,201 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1    | 2023-05-31 01:06:10,213 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
dn3_1    | 2023-05-31 01:06:04,194 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm2_1   | 2023-05-31 01:06:22,055 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm2_1   | 2023-05-31 01:06:22,056 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2_1   | 2023-05-31 01:06:22,061 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2_1   | 2023-05-31 01:06:22,071 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
om2_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
scm1_1   | 2023-05-31 01:06:09,459 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
s3g_1    | 2023-05-31 01:09:55,437 [qtp605052357-21] INFO audit.AuditLogger: Refresh DebugCmdSet for S3GAudit to [].
s3g_1    | 2023-05-31 01:09:55,522 [qtp605052357-21] INFO audit.AuditLogger: Refresh DebugCmdSet for S3GAudit to [].
s3g_1    | 2023-05-31 01:09:55,531 [qtp605052357-21] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
dn4_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm2_1   | 2023-05-31 01:06:22,124 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServer: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2: found a subdirectory /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e
scm2_1   | 2023-05-31 01:06:22,136 [main] INFO server.RaftServer: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2: addNew group-6A492392CC3E:[] returns group-6A492392CC3E:java.util.concurrent.CompletableFuture@295bf2a[Not completed]
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
scm1_1   | 2023-05-31 01:06:11,128 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:7b17d04a-0b5d-44ed-b487-1bf0d959b978
s3g_1    | 2023-05-31 01:09:55,531 [qtp605052357-21] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
om3_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om3_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om3_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om3_1    | , while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 2023-05-31 01:06:05,161 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-05-31 01:06:05,213 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-05-31 01:06:05,624 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn2_1    | 2023-05-31 01:06:05,963 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
om2_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
s3g_1    | 2023-05-31 01:09:56,970 [qtp605052357-21] INFO protocolPB.GrpcOmTransport: GrpcOmTransport: started
scm1_1   | 2023-05-31 01:06:11,599 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn3_1    | 2023-05-31 01:06:04,209 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1    | 2023-05-31 01:06:21,829 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 482910862efc/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
recon_1  | 2023-05-31 01:06:01,460 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm3_1   | STARTUP_MSG:   java = 11.0.14.1
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 2023-05-31 01:06:05,968 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn2_1    | 2023-05-31 01:06:05,979 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-05-31 01:06:06,003 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-05-31 01:06:04,818 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServer: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: addNew group-ED0A589991D2:[] returns group-ED0A589991D2:java.util.concurrent.CompletableFuture@368eb67e[Not completed]
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
dn3_1    | 2023-05-31 01:06:04,237 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn3_1    | 2023-05-31 01:06:04,331 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-05-31 01:06:04,333 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om3_1    | 2023-05-31 01:06:23,831 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 482910862efc/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
om3_1    | 2023-05-31 01:06:25,840 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:7b17d04a-0b5d-44ed-b487-1bf0d959b978 is not the leader. Could not determine the leader node.
recon_1  | 2023-05-31 01:06:01,557 [main] INFO util.log: Logging initialized @36551ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1  | 2023-05-31 01:06:02,490 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.ConnectException: Connection refused
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
s3g_1    | 2023-05-31 01:09:57,925 [qtp605052357-21] ERROR protocolPB.GrpcOmTransport: Failed to submit request
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
dn3_1    | 2023-05-31 01:06:04,473 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
recon_1  | 2023-05-31 01:06:02,519 [main] INFO http.HttpRequestLog: Http request log for http.requests.recon is not defined
dn2_1    | 2023-05-31 01:06:06,008 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-05-31 01:06:06,011 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
s3g_1    | io.grpc.StatusRuntimeException: INTERNAL: org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Suggested leader is OM:om2[om2/10.9.0.12].
dn5_1    | 2023-05-31 01:06:04,818 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServer: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: found a subdirectory /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444
dn5_1    | 2023-05-31 01:06:04,826 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServer: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: addNew group-F1A4CF07E444:[] returns group-F1A4CF07E444:java.util.concurrent.CompletableFuture@163ff6a[Not completed]
dn5_1    | 2023-05-31 01:06:05,000 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
recon_1  | 2023-05-31 01:06:02,583 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn2_1    | 2023-05-31 01:06:06,014 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-05-31 01:06:06,096 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1    | 	at io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:271)
om3_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om3_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
scm1_1   | 2023-05-31 01:06:11,666 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
recon_1  | 2023-05-31 01:06:02,590 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
dn2_1    | 2023-05-31 01:06:06,412 [main] INFO util.log: Logging initialized @40916ms to org.eclipse.jetty.util.log.Slf4jLog
dn2_1    | 2023-05-31 01:06:07,344 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1    | 	at io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:252)
om3_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)
om1_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
scm1_1   | 2023-05-31 01:06:11,669 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
s3g_1    | 	at io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:165)
s3g_1    | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerServiceGrpc$OzoneManagerServiceBlockingStub.submitRequest(OzoneManagerServiceGrpc.java:182)
dn4_1    | 	... 1 more
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:790)
recon_1  | 2023-05-31 01:06:02,600 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om3_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:363)
scm1_1   | 2023-05-31 01:06:11,681 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm2_1   | 2023-05-31 01:06:22,194 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-groupManagement] INFO ha.SCMStateMachine: Updated lastAppliedTermIndex 5#80 with transactionInfo term andIndex
dn5_1    | 2023-05-31 01:06:05,242 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: new RaftServerImpl for group-DC0251522AF1:[] with ContainerStateMachine:uninitialized
dn5_1    | 2023-05-31 01:06:05,279 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-05-31 01:06:05,279 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-05-31 01:06:05,296 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-05-31 01:06:05,296 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-05-31 01:06:05,299 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-05-31 01:06:05,300 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-05-31 01:06:13,783 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-05-31 01:06:02,601 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
scm1_1   | 2023-05-31 01:06:11,683 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm2_1   | 2023-05-31 01:06:22,199 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-groupManagement] INFO server.RaftServer$Division: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2: new RaftServerImpl for group-6A492392CC3E:[] with SCMStateMachine:uninitialized
om1_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn5_1    | 2023-05-31 01:06:05,377 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-05-31 01:06:05,412 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-05-31 01:06:05,463 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-05-31 01:06:05,464 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-05-31 01:06:04,543 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn3_1    | 2023-05-31 01:06:04,627 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-05-31 01:06:13,794 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-31 01:06:13,814 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-05-31 01:06:11,683 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm2_1   | 2023-05-31 01:06:22,202 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
om1_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn2_1    | 2023-05-31 01:06:07,386 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn2_1    | 2023-05-31 01:06:07,481 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn2_1    | 2023-05-31 01:06:07,483 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn2_1    | 2023-05-31 01:06:07,483 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn3_1    | 2023-05-31 01:06:04,629 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-05-31 01:06:04,901 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn4_1    | 2023-05-31 01:06:14,784 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-31 01:06:14,795 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-05-31 01:06:11,685 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm2_1   | 2023-05-31 01:06:22,207 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om1_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn5_1    | 2023-05-31 01:06:05,579 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn5_1    | 2023-05-31 01:06:05,617 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-05-31 01:06:05,660 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn5_1    | 2023-05-31 01:06:05,721 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-05-31 01:06:05,723 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-05-31 01:06:05,950 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn4_1    | 2023-05-31 01:06:14,815 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-31 01:06:15,707 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
scm1_1   | 2023-05-31 01:06:11,690 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm2_1   | 2023-05-31 01:06:22,207 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om1_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn5_1    | 2023-05-31 01:06:06,466 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-05-31 01:06:05,426 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-05-31 01:06:05,490 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-05-31 01:06:05,531 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-05-31 01:06:05,532 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-05-31 01:06:07,492 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn4_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
om3_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
scm1_1   | 2023-05-31 01:06:11,697 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-05-31 01:06:22,208 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
om1_1    | , while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 16 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 16.
dn5_1    | 2023-05-31 01:06:06,482 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-05-31 01:06:05,536 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-05-31 01:06:05,537 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-05-31 01:06:05,540 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn3_1    | 2023-05-31 01:06:05,641 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn2_1    | 2023-05-31 01:06:08,005 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
recon_1  | 2023-05-31 01:06:02,826 [main] INFO http.BaseHttpServer: HTTP server of recon uses base directory /data/metadata/webserver
dn1_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
scm1_1   | 2023-05-31 01:06:11,697 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm2_1   | 2023-05-31 01:06:22,208 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | 2023-05-31 01:06:30,176 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4c30cb93385d/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 17 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 17.
dn5_1    | 2023-05-31 01:06:06,525 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-05-31 01:06:06,527 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-05-31 01:06:06,528 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-05-31 01:06:06,529 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-05-31 01:06:06,532 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: new RaftServerImpl for group-ED0A589991D2:[] with ContainerStateMachine:uninitialized
dn5_1    | 2023-05-31 01:06:06,556 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
recon_1  | 2023-05-31 01:06:02,837 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
scm1_1   | 2023-05-31 01:06:11,700 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1_1   | 2023-05-31 01:06:11,804 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-05-31 01:06:05,828 [main] INFO util.log: Logging initialized @41840ms to org.eclipse.jetty.util.log.Slf4jLog
dn3_1    | 2023-05-31 01:06:06,843 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn3_1    | 2023-05-31 01:06:06,879 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn3_1    | 2023-05-31 01:06:06,931 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn3_1    | 2023-05-31 01:06:06,971 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn3_1    | 2023-05-31 01:06:06,971 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1  | 2023-05-31 01:06:03,111 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
dn1_1    | 	... 12 more
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
scm2_1   | 2023-05-31 01:06:22,208 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1_1   | 2023-05-31 01:06:11,828 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om1_1    | 2023-05-31 01:06:37,022 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
dn3_1    | 2023-05-31 01:06:06,973 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn3_1    | 2023-05-31 01:06:07,232 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn2_1    | 2023-05-31 01:06:08,149 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn2_1    | 2023-05-31 01:06:08,156 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
dn2_1    | 2023-05-31 01:06:08,476 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn2_1    | 2023-05-31 01:06:08,486 [main] INFO server.session: No SessionScavenger set, using defaults
dn2_1    | 2023-05-31 01:06:08,497 [main] INFO server.session: node0 Scavenging every 600000ms
dn2_1    | 2023-05-31 01:06:08,635 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6b2fdffc{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
scm2_1   | 2023-05-31 01:06:22,215 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-groupManagement] INFO server.RaftServer$Division: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
scm1_1   | 2023-05-31 01:06:11,832 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm1_1   | 2023-05-31 01:06:12,732 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
s3g_1    | 	at org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransport.submitRequest(GrpcOmTransport.java:186)
s3g_1    | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:299)
dn1_1    | 2023-05-31 01:06:26,907 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:27,908 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:28,908 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-05-31 01:06:03,114 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
scm3_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm3_1   | ************************************************************/
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
scm2_1   | 2023-05-31 01:06:22,215 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2_1   | 2023-05-31 01:06:22,225 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-05-31 01:06:07,268 [main] INFO http.HttpServer2: Jetty bound to port 9882
s3g_1    | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceInfo(OzoneManagerProtocolClientSideTranslatorPB.java:1578)
s3g_1    | 	at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:243)
dn1_1    | 2023-05-31 01:06:28,927 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From 2af98bbedaed/10.9.0.17 to scm2:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:42526 remote=scm2/10.9.0.15:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
recon_1  | 2023-05-31 01:06:03,137 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
scm3_1   | 2023-05-31 01:06:27,779 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm3_1   | 2023-05-31 01:06:27,856 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:538)
scm2_1   | 2023-05-31 01:06:22,230 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm2_1   | 2023-05-31 01:06:22,247 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
dn3_1    | 2023-05-31 01:06:07,269 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
dn3_1    | 2023-05-31 01:06:07,517 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om1_1    | 2023-05-31 01:06:37,110 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn5_1    | 2023-05-31 01:06:06,556 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
recon_1  | 2023-05-31 01:06:03,384 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
s3g_1    | 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:248)
s3g_1    | 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:115)
s3g_1    | 	at org.apache.hadoop.ozone.s3.OzoneClientCache.<init>(OzoneClientCache.java:83)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2_1   | 2023-05-31 01:06:22,254 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm2_1   | 2023-05-31 01:06:22,261 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1_1   | 2023-05-31 01:06:12,738 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn3_1    | 2023-05-31 01:06:07,519 [main] INFO server.session: No SessionScavenger set, using defaults
dn3_1    | 2023-05-31 01:06:07,521 [main] INFO server.session: node0 Scavenging every 660000ms
dn3_1    | 2023-05-31 01:06:07,601 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@76396509{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn3_1    | 2023-05-31 01:06:07,607 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4dc52559{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn5_1    | 2023-05-31 01:06:06,556 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
recon_1  | 2023-05-31 01:06:03,400 [main] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
s3g_1    | 	at org.apache.hadoop.ozone.s3.OzoneClientCache.getOzoneClientInstance(OzoneClientCache.java:98)
s3g_1    | 	at org.apache.hadoop.ozone.s3.OzoneClientProducer.getClient(OzoneClientProducer.java:121)
s3g_1    | 	at org.apache.hadoop.ozone.s3.OzoneClientProducer.createClient(OzoneClientProducer.java:71)
dn4_1    | Caused by: java.util.concurrent.TimeoutException
scm2_1   | 2023-05-31 01:06:22,262 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm2_1   | 2023-05-31 01:06:22,300 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm1_1   | 2023-05-31 01:06:12,740 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om1_1    | 2023-05-31 01:06:37,926 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
dn3_1    | 2023-05-31 01:06:08,418 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@73e1ecd0{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-13165353120061600347/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn3_1    | 2023-05-31 01:06:08,508 [main] INFO server.AbstractConnector: Started ServerConnector@43d76a92{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn3_1    | 2023-05-31 01:06:08,508 [main] INFO server.Server: Started @44520ms
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
s3g_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1    | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
scm2_1   | 2023-05-31 01:06:22,493 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2_1   | 2023-05-31 01:06:22,495 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-05-31 01:06:12,740 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
om1_1    | 2023-05-31 01:06:39,842 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
dn2_1    | 2023-05-31 01:06:08,639 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@463afa6e{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn2_1    | 2023-05-31 01:06:09,514 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4e958f08{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-10166854609635711607/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn2_1    | 2023-05-31 01:06:09,592 [main] INFO server.AbstractConnector: Started ServerConnector@2725ca05{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
s3g_1    | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1    | 	at org.jboss.weld.injection.StaticMethodInjectionPoint.invoke(StaticMethodInjectionPoint.java:95)
s3g_1    | 	at org.jboss.weld.injection.StaticMethodInjectionPoint.invoke(StaticMethodInjectionPoint.java:85)
s3g_1    | 	at org.jboss.weld.injection.producer.ProducerMethodProducer.produce(ProducerMethodProducer.java:103)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm1_1   | 2023-05-31 01:06:12,741 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | 2023-05-31 01:06:39,924 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
dn2_1    | 2023-05-31 01:06:09,604 [main] INFO server.Server: Started @44096ms
dn2_1    | 2023-05-31 01:06:09,607 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 2023-05-31 01:06:08,511 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1    | 	at org.jboss.weld.injection.producer.AbstractMemberProducer.produce(AbstractMemberProducer.java:161)
s3g_1    | 	at org.jboss.weld.bean.AbstractProducerBean.create(AbstractProducerBean.java:180)
s3g_1    | 	at org.jboss.weld.contexts.unbound.DependentContextImpl.get(DependentContextImpl.java:64)
s3g_1    | 	at org.jboss.weld.bean.ContextualInstanceStrategy$DefaultContextualInstanceStrategy.get(ContextualInstanceStrategy.java:100)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om3_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om3_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om3_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn2_1    | 2023-05-31 01:06:09,607 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn2_1    | 2023-05-31 01:06:09,623 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn2_1    | 2023-05-31 01:06:09,785 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn2_1    | 2023-05-31 01:06:09,927 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
scm2_1   | 2023-05-31 01:06:22,496 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm2_1   | 2023-05-31 01:06:22,497 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm2_1   | 2023-05-31 01:06:22,500 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
s3g_1    | 	at org.jboss.weld.bean.ContextualInstance.get(ContextualInstance.java:50)
dn4_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
om1_1    | 2023-05-31 01:06:39,954 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm1_1   | 2023-05-31 01:06:12,750 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
dn2_1    | 2023-05-31 01:06:10,912 [Listener at 0.0.0.0/9864] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 2023-05-31 01:06:06,557 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-05-31 01:06:06,563 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2_1   | 2023-05-31 01:06:22,500 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm2_1   | 2023-05-31 01:06:22,501 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm2_1   | 2023-05-31 01:06:22,501 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
s3g_1    | 	at org.jboss.weld.manager.BeanManagerImpl.getReference(BeanManagerImpl.java:694)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om1_1    | 2023-05-31 01:06:40,262 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om3_1    | , while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 15.
scm1_1   | 2023-05-31 01:06:12,779 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServer: 7b17d04a-0b5d-44ed-b487-1bf0d959b978: found a subdirectory /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e
dn2_1    | 2023-05-31 01:06:10,913 [Listener at 0.0.0.0/9864] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
scm2_1   | 2023-05-31 01:06:22,502 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm2_1   | 2023-05-31 01:06:22,585 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm2_1   | 2023-05-31 01:06:22,629 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
s3g_1    | 	at org.jboss.weld.manager.BeanManagerImpl.getInjectableReference(BeanManagerImpl.java:794)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1    | 2023-05-31 01:06:28,014 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2 is not the leader. Could not determine the leader node.
scm1_1   | 2023-05-31 01:06:12,791 [main] INFO server.RaftServer: 7b17d04a-0b5d-44ed-b487-1bf0d959b978: addNew group-6A492392CC3E:[] returns group-6A492392CC3E:java.util.concurrent.CompletableFuture@295bf2a[Not completed]
dn2_1    | 2023-05-31 01:06:10,915 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
recon_1  | 2023-05-31 01:06:06,839 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2_1   | 2023-05-31 01:06:22,687 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm2_1   | 2023-05-31 01:06:22,697 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm2_1   | 2023-05-31 01:06:22,698 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
s3g_1    | 	at org.jboss.weld.injection.FieldInjectionPoint.inject(FieldInjectionPoint.java:92)
s3g_1    | 	at org.jboss.weld.util.Beans.injectBoundFields(Beans.java:345)
s3g_1    | 	at org.jboss.weld.util.Beans.injectFieldsAndInitializers(Beans.java:356)
om3_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
scm1_1   | 2023-05-31 01:06:12,880 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-groupManagement] INFO ha.SCMStateMachine: Updated lastAppliedTermIndex 5#80 with transactionInfo term andIndex
dn2_1    | 2023-05-31 01:06:10,916 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
dn1_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
recon_1  | 2023-05-31 01:06:07,450 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2_1   | 2023-05-31 01:06:22,737 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm2_1   | 2023-05-31 01:06:22,737 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
om2_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om1_1    | 2023-05-31 01:06:40,271 [main] WARN utils.NativeLibraryLoader: Unable to load library: ozone_rocksdb_tools
om1_1    | java.io.IOException: Permission denied
om1_1    | 	at java.base/java.io.UnixFileSystem.createFileExclusively(Native Method)
om3_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om3_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
dn2_1    | 2023-05-31 01:06:10,939 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn5_1    | 2023-05-31 01:06:06,583 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
recon_1  | 2023-05-31 01:06:07,714 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm2_1   | 2023-05-31 01:06:22,741 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om1_1    | 	at java.base/java.io.File.createTempFile(File.java:2129)
om1_1    | 	at org.apache.hadoop.hdds.utils.NativeLibraryLoader.copyResourceFromJarToTemp(NativeLibraryLoader.java:138)
om1_1    | 	at org.apache.hadoop.hdds.utils.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:114)
om3_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
scm1_1   | 2023-05-31 01:06:12,881 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-groupManagement] INFO server.RaftServer$Division: 7b17d04a-0b5d-44ed-b487-1bf0d959b978: new RaftServerImpl for group-6A492392CC3E:[] with SCMStateMachine:uninitialized
dn2_1    | 2023-05-31 01:06:11,046 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn2_1    | 2023-05-31 01:06:11,047 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
recon_1  | 2023-05-31 01:06:07,727 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
om1_1    | 	at org.apache.hadoop.ozone.om.snapshot.SnapshotDiffManager.initSSTDumpTool(SnapshotDiffManager.java:254)
om1_1    | 	at org.apache.hadoop.ozone.om.snapshot.SnapshotDiffManager.<init>(SnapshotDiffManager.java:233)
om1_1    | 	at org.apache.hadoop.ozone.om.OmSnapshotManager.<init>(OmSnapshotManager.java:261)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
scm1_1   | 2023-05-31 01:06:12,884 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
dn5_1    | 2023-05-31 01:06:06,587 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-05-31 01:06:06,587 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-05-31 01:06:06,588 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-05-31 01:06:06,588 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-05-31 01:06:06,589 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-05-31 01:06:06,589 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn5_1    | 2023-05-31 01:06:06,601 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-05-31 01:06:06,601 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
recon_1  | 2023-05-31 01:06:08,485 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.instantiateServices(OzoneManager.java:833)
om1_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:667)
om1_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:752)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
scm1_1   | 2023-05-31 01:06:12,887 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn5_1    | 2023-05-31 01:06:06,602 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn5_1    | 2023-05-31 01:06:06,603 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-05-31 01:06:06,603 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-05-31 01:06:08,514 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn3_1    | 2023-05-31 01:06:08,521 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn3_1    | 2023-05-31 01:06:08,647 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1  | 2023-05-31 01:06:08,740 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
recon_1  | 2023-05-31 01:06:08,918 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1  | 2023-05-31 01:06:09,040 [main] INFO node.SCMNodeManager: Entering startup safe mode.
om1_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.startAndCancelPrepare(OzoneManagerStarter.java:231)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
scm1_1   | 2023-05-31 01:06:12,887 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1  | 2023-05-31 01:06:09,114 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn3_1    | 2023-05-31 01:06:08,768 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
om1_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.startOmUpgrade(OzoneManagerStarter.java:121)
om1_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
om3_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
scm1_1   | 2023-05-31 01:06:12,887 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	... 1 more
dn4_1    | 2023-05-31 01:06:15,785 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-31 01:06:15,795 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
scm1_1   | 2023-05-31 01:06:12,889 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1_1   | 2023-05-31 01:06:12,889 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om2_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om2_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om2_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om2_1    | , while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 15.
dn2_1    | 2023-05-31 01:06:11,066 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@fec56f8] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn2_1    | 2023-05-31 01:06:11,247 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn2_1    | 2023-05-31 01:06:11,319 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn4_1    | 2023-05-31 01:06:15,816 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
om1_1    | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   | 2023-05-31 01:06:12,916 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-groupManagement] INFO server.RaftServer$Division: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm2_1   | 2023-05-31 01:06:22,742 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm2_1   | 2023-05-31 01:06:22,744 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm3_1   | 2023-05-31 01:06:28,265 [main] INFO reflections.Reflections: Reflections took 306 ms to scan 3 urls, producing 129 keys and 280 values 
scm3_1   | 2023-05-31 01:06:28,351 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
recon_1  | 2023-05-31 01:06:09,117 [main] INFO node.SCMNodeManager: Registered Data node : 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-05-31 01:06:09,127 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3da1490a-d610-40de-95c1-bdfc259ba041
recon_1  | 2023-05-31 01:06:09,128 [main] INFO node.SCMNodeManager: Registered Data node : 3da1490a-d610-40de-95c1-bdfc259ba041{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-05-31 01:06:09,133 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/4bc93e86-8989-4282-b7d8-050f4a19b92f
dn4_1    | 2023-05-31 01:06:16,786 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-31 01:06:16,817 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 	at org.jboss.weld.injection.producer.ResourceInjector$1.proceed(ResourceInjector.java:69)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | 2023-05-31 01:06:12,920 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
om3_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
om1_1    | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
om1_1    | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm1_1   | 2023-05-31 01:06:12,932 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om3_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn4_1    | 2023-05-31 01:06:16,826 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
om1_1    | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:42526 remote=scm2/10.9.0.15:9861]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
scm1_1   | 2023-05-31 01:06:12,945 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om3_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn4_1    | java.net.SocketTimeoutException: Call From 955d7ba09e69/10.9.0.20 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:42906 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn5_1    | 2023-05-31 01:06:06,605 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-05-31 01:06:06,605 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm3_1   | 2023-05-31 01:06:28,355 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3_1   | 2023-05-31 01:06:28,377 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3:9894 and Ratis port: 9894
scm3_1   | 2023-05-31 01:06:28,378 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3
scm3_1   | 2023-05-31 01:06:29,618 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3_1   | 2023-05-31 01:06:30,793 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3_1   | 2023-05-31 01:06:32,162 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
om1_1    | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
s3g_1    | 	at org.jboss.weld.injection.InjectionContextImpl.run(InjectionContextImpl.java:48)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
scm1_1   | 2023-05-31 01:06:12,984 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1_1   | 2023-05-31 01:06:12,996 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om3_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn2_1    | 2023-05-31 01:06:14,239 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:14,240 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:14,245 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:15,104 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn2_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
scm2_1   | 2023-05-31 01:06:22,747 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm2_1   | 2023-05-31 01:06:22,750 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm2_1   | 2023-05-31 01:06:22,751 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
om1_1    | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
s3g_1    | 	at org.jboss.weld.injection.producer.ResourceInjector.inject(ResourceInjector.java:71)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
scm1_1   | 2023-05-31 01:06:13,006 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
om3_1    | , while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 16 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 16.
dn3_1    | 2023-05-31 01:06:10,090 [Listener at 0.0.0.0/9864] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
dn3_1    | 2023-05-31 01:06:10,117 [Listener at 0.0.0.0/9864] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
dn3_1    | 2023-05-31 01:06:10,132 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1  | 2023-05-31 01:06:09,135 [main] INFO node.SCMNodeManager: Registered Data node : 4bc93e86-8989-4282-b7d8-050f4a19b92f{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-05-31 01:06:09,135 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/97734618-7966-42ac-b50e-e49ead6ce4d2
recon_1  | 2023-05-31 01:06:09,136 [main] INFO node.SCMNodeManager: Registered Data node : 97734618-7966-42ac-b50e-e49ead6ce4d2{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-05-31 01:06:09,140 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/b20d0805-391e-447d-a646-35a5d1a5cbcf
recon_1  | 2023-05-31 01:06:09,142 [main] INFO node.SCMNodeManager: Registered Data node : b20d0805-391e-447d-a646-35a5d1a5cbcf{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
om1_1    | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
om1_1    | 	at picocli.CommandLine.execute(CommandLine.java:2078)
s3g_1    | 	at org.jboss.weld.injection.producer.BasicInjectionTarget.inject(BasicInjectionTarget.java:117)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
om3_1    | 2023-05-31 01:06:30,016 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 482910862efc/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 17 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 17.
dn5_1    | 2023-05-31 01:06:06,605 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-05-31 01:06:06,605 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-05-31 01:06:06,648 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: new RaftServerImpl for group-F1A4CF07E444:[] with ContainerStateMachine:uninitialized
dn5_1    | 2023-05-31 01:06:06,657 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-05-31 01:06:06,657 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn5_1    | 2023-05-31 01:06:06,659 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-05-31 01:06:06,666 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-05-31 01:06:06,667 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
scm3_1   | 2023-05-31 01:06:32,185 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
s3g_1    | 	at org.glassfish.jersey.ext.cdi1x.internal.CdiComponentProvider$InjectionManagerInjectedCdiTarget.inject(CdiComponentProvider.java:665)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
scm1_1   | 2023-05-31 01:06:13,011 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1_1   | 2023-05-31 01:06:13,088 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm1_1   | 2023-05-31 01:06:13,590 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1_1   | 2023-05-31 01:06:13,607 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-05-31 01:06:13,611 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1_1   | 2023-05-31 01:06:13,612 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1_1   | 2023-05-31 01:06:13,617 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1_1   | 2023-05-31 01:06:13,618 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm1_1   | 2023-05-31 01:06:13,622 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
om2_1    | 2023-05-31 01:06:28,528 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2 is not the leader. Could not determine the leader node.
om1_1    | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm3_1   | 2023-05-31 01:06:33,150 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
s3g_1    | 	at org.jboss.weld.bean.ManagedBean.create(ManagedBean.java:161)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
scm2_1   | 2023-05-31 01:06:22,787 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm2_1   | 2023-05-31 01:06:22,806 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
dn3_1    | 2023-05-31 01:06:10,168 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
dn3_1    | 2023-05-31 01:06:10,219 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn3_1    | 2023-05-31 01:06:10,366 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn3_1    | 2023-05-31 01:06:10,367 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn3_1    | 2023-05-31 01:06:10,486 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@17bdcde0] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
om1_1    | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
scm3_1   | 2023-05-31 01:06:34,982 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:d2fda184-2469-4fa2-af65-75268cb832ba
scm3_1   | 2023-05-31 01:06:36,121 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
om2_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om2_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
dn3_1    | 2023-05-31 01:06:10,636 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
scm1_1   | 2023-05-31 01:06:13,622 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:58)
scm3_1   | 2023-05-31 01:06:36,257 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm3_1   | 2023-05-31 01:06:36,274 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm2_1   | 2023-05-31 01:06:22,841 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm2_1   | 2023-05-31 01:06:22,855 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm2_1   | 2023-05-31 01:06:22,859 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
recon_1  | 2023-05-31 01:06:09,142 [main] INFO scm.ReconNodeManager: Loaded 5 nodes from node DB.
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
om2_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om2_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
scm3_1   | 2023-05-31 01:06:36,291 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
s3g_1    | 	at org.jboss.weld.contexts.unbound.DependentContextImpl.get(DependentContextImpl.java:64)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
scm2_1   | WARNING: An illegal reflective access operation has occurred
dn5_1    | 2023-05-31 01:06:06,667 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-05-31 01:06:10,749 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
om3_1    | 2023-05-31 01:06:37,183 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om1_1    | 2023-05-31 01:06:41,422 [main] WARN om.OzoneManager: Prepare marker file index 109 does not match DB prepare index 108. Writing DB index to prepare file and maintaining prepared state.
om1_1    | 2023-05-31 01:06:41,446 [main] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 108 to file /data/metadata/current/prepareMarker
scm3_1   | 2023-05-31 01:06:36,291 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm3_1   | 2023-05-31 01:06:36,303 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
scm2_1   | WARNING: Illegal reflective access by org.apache.hadoop.hdds.utils.MetricsUtil (file:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar) to method java.lang.Class.annotationData()
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn5_1    | 2023-05-31 01:06:06,667 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-05-31 01:06:13,601 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-05-31 01:06:37,408 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-05-31 01:06:39,388 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om3_1    | 2023-05-31 01:06:40,140 [Thread-19] WARN rocksdiff.RocksDBCheckpointDiffer: Snapshot info table column family handle is not set!
scm3_1   | 2023-05-31 01:06:36,303 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
s3g_1    | 	at org.jboss.weld.bean.ContextualInstanceStrategy$DefaultContextualInstanceStrategy.get(ContextualInstanceStrategy.java:100)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
recon_1  | 2023-05-31 01:06:09,174 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
scm2_1   | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.hdds.utils.MetricsUtil
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn3_1    | 2023-05-31 01:06:13,601 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:13,601 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:14,604 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:14,605 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-05-31 01:06:36,305 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
s3g_1    | 	at org.jboss.weld.bean.ContextualInstance.get(ContextualInstance.java:50)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om2_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
scm2_1   | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
dn1_1    | 2023-05-31 01:06:29,335 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn5_1    | 2023-05-31 01:06:06,667 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-05-31 01:06:14,605 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:15,605 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-05-31 01:06:40,156 [Thread-22] INFO rocksdiff.RocksDBCheckpointDiffer: Skipped the compaction entry. Compaction input files: [/data/metadata/om.db/000082.sst, /data/metadata/om.db/000073.sst, /data/metadata/om.db/000051.sst, /data/metadata/om.db/000047.sst] and output files: [/data/metadata/om.db/000082.sst, /data/metadata/om.db/000073.sst, /data/metadata/om.db/000051.sst, /data/metadata/om.db/000047.sst] are same.
om3_1    | 2023-05-31 01:06:40,157 [Thread-23] WARN rocksdiff.RocksDBCheckpointDiffer: Snapshot info table column family handle is not set!
om3_1    | 2023-05-31 01:06:40,184 [Thread-34] WARN rocksdiff.RocksDBCheckpointDiffer: Snapshot info table column family handle is not set!
om3_1    | 2023-05-31 01:06:40,184 [Thread-34] ERROR rocksdiff.RocksDBCheckpointDiffer: Unable to append compaction log. Compaction log path is not set. Please check initialization.
s3g_1    | 	at org.jboss.weld.manager.BeanManagerImpl.getReference(BeanManagerImpl.java:694)
recon_1  | 2023-05-31 01:06:11,049 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1  | 2023-05-31 01:06:11,109 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn4_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
scm2_1   | WARNING: All illegal access operations will be denied in a future release
dn5_1    | 2023-05-31 01:06:06,668 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-05-31 01:06:29,348 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om2_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om2_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om2_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
s3g_1    | 	at org.jboss.weld.manager.BeanManagerImpl.getReference(BeanManagerImpl.java:717)
recon_1  | 2023-05-31 01:06:11,255 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1  | 2023-05-31 01:06:11,716 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
scm2_1   | 2023-05-31 01:06:22,878 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
dn5_1    | 2023-05-31 01:06:06,670 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-05-31 01:06:29,676 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
s3g_1    | 	at org.jboss.weld.util.ForwardingBeanManager.getReference(ForwardingBeanManager.java:64)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:538)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
scm2_1   | 2023-05-31 01:06:22,885 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 2, healthy pipeline threshold count is 1
scm2_1   | 2023-05-31 01:06:22,888 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
dn1_1    | 2023-05-31 01:06:29,679 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis b20d0805-391e-447d-a646-35a5d1a5cbcf
dn5_1    | 2023-05-31 01:06:06,672 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-05-31 01:06:06,672 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
om2_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om2_1    | , while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 16 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 16.
om2_1    | 2023-05-31 01:06:30,529 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 45ffe5496dee/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 17 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 17.
om2_1    | 2023-05-31 01:06:37,916 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om2_1    | 2023-05-31 01:06:38,226 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
s3g_1    | 	at org.jboss.weld.bean.builtin.BeanManagerProxy.getReference(BeanManagerProxy.java:87)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn5_1    | 2023-05-31 01:06:06,701 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn5_1    | 2023-05-31 01:06:06,702 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-05-31 01:06:06,702 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-05-31 01:06:06,702 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn5_1    | 2023-05-31 01:06:06,706 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-05-31 01:06:06,712 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1    | 2023-05-31 01:06:39,390 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
s3g_1    | 	at org.glassfish.jersey.ext.cdi1x.internal.CdiUtil.getBeanReference(CdiUtil.java:129)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn1_1    | 2023-05-31 01:06:29,821 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/in_use.lock acquired by nodename 6@2af98bbedaed
dn1_1    | 2023-05-31 01:06:29,823 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/6a499bf5-10af-4dab-a3fd-a4e2998a83c9/in_use.lock acquired by nodename 6@2af98bbedaed
dn3_1    | 2023-05-31 01:06:15,606 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-05-31 01:06:36,356 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 2023-05-31 01:06:36,363 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm3_1   | 2023-05-31 01:06:36,378 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
dn5_1    | 2023-05-31 01:06:06,714 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-05-31 01:06:06,714 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
s3g_1    | 	at org.glassfish.jersey.ext.cdi1x.internal.AbstractCdiBeanSupplier$1.getInstance(AbstractCdiBeanSupplier.java:72)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1    | 2023-05-31 01:06:40,572 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn3_1    | 2023-05-31 01:06:15,606 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:16,606 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:16,607 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-05-31 01:06:13,626 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm3_1   | 2023-05-31 01:06:36,588 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om3_1    | Exception in thread "Thread-34" java.lang.RuntimeException: Compaction log path not set
s3g_1    | 	at org.glassfish.jersey.ext.cdi1x.internal.AbstractCdiBeanSupplier._provide(AbstractCdiBeanSupplier.java:112)
recon_1  | 2023-05-31 01:06:11,716 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1  | 2023-05-31 01:06:11,928 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1  | 2023-05-31 01:06:11,960 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1  | 2023-05-31 01:06:11,960 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
dn1_1    | 2023-05-31 01:06:29,861 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=b20d0805-391e-447d-a646-35a5d1a5cbcf} from /data/metadata/ratis/6a499bf5-10af-4dab-a3fd-a4e2998a83c9/current/raft-meta
om2_1    | 2023-05-31 01:06:40,684 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn5_1    | 2023-05-31 01:06:06,714 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1_1   | 2023-05-31 01:06:13,759 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
om1_1    | 2023-05-31 01:06:41,815 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm3_1   | 2023-05-31 01:06:36,638 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om3_1    | 	at org.apache.ozone.rocksdiff.RocksDBCheckpointDiffer.appendToCurrentCompactionLog(RocksDBCheckpointDiffer.java:375)
om3_1    | 	at org.apache.ozone.rocksdiff.RocksDBCheckpointDiffer.access$500(RocksDBCheckpointDiffer.java:100)
om3_1    | 	at org.apache.ozone.rocksdiff.RocksDBCheckpointDiffer$2.onCompactionCompleted(RocksDBCheckpointDiffer.java:588)
s3g_1    | 	at org.glassfish.jersey.ext.cdi1x.internal.RequestScopedCdiBeanSupplier.get(RequestScopedCdiBeanSupplier.java:46)
recon_1  | 2023-05-31 01:06:13,198 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1  | 2023-05-31 01:06:13,208 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
recon_1  | 2023-05-31 01:06:13,402 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
dn1_1    | 2023-05-31 01:06:29,871 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/in_use.lock acquired by nodename 6@2af98bbedaed
dn1_1    | 2023-05-31 01:06:29,881 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=11, votedFor=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd} from /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/current/raft-meta
scm2_1   | 2023-05-31 01:06:22,931 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
dn5_1    | 2023-05-31 01:06:06,714 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm1_1   | 2023-05-31 01:06:13,870 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
om1_1    | 2023-05-31 01:06:41,820 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
scm3_1   | 2023-05-31 01:06:36,640 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om3_1    | 	at org.rocksdb.AbstractEventListener.onCompactionCompletedProxy(AbstractEventListener.java:191)
s3g_1    | 	at org.glassfish.jersey.inject.hk2.InstanceSupplierFactoryBridge.provide(InstanceSupplierFactoryBridge.java:53)
scm2_1   | 2023-05-31 01:06:23,629 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2_1   | 2023-05-31 01:06:23,659 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2_1   | 2023-05-31 01:06:23,741 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm2_1   | 2023-05-31 01:06:23,801 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2_1   | 2023-05-31 01:06:23,806 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn5_1    | 2023-05-31 01:06:06,750 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm1_1   | 2023-05-31 01:06:14,059 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm3_1   | 2023-05-31 01:06:39,165 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om3_1    | 2023-05-31 01:06:40,365 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
s3g_1    | 	at org.jvnet.hk2.internal.FactoryCreator.create(FactoryCreator.java:129)
recon_1  | 2023-05-31 01:06:13,402 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1  | 2023-05-31 01:06:13,411 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 660000ms
recon_1  | 2023-05-31 01:06:13,461 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4dad8ec0{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1  | 2023-05-31 01:06:13,466 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@77b5148c{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn5_1    | 2023-05-31 01:06:06,976 [main] INFO util.log: Logging initialized @41745ms to org.eclipse.jetty.util.log.Slf4jLog
scm1_1   | 2023-05-31 01:06:14,088 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm3_1   | 2023-05-31 01:06:39,186 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om3_1    | 2023-05-31 01:06:40,445 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om3_1    | 2023-05-31 01:06:40,466 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-05-31 01:06:40,698 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om3_1    | 2023-05-31 01:06:40,719 [main] WARN utils.NativeLibraryLoader: Unable to load library: ozone_rocksdb_tools
om3_1    | java.io.IOException: Permission denied
s3g_1    | 	at org.jvnet.hk2.internal.SystemDescriptor.create(SystemDescriptor.java:463)
dn5_1    | 2023-05-31 01:06:08,006 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm3_1   | 2023-05-31 01:06:39,191 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm3_1   | 2023-05-31 01:06:39,191 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
om3_1    | 	at java.base/java.io.UnixFileSystem.createFileExclusively(Native Method)
om3_1    | 	at java.base/java.io.File.createTempFile(File.java:2129)
om3_1    | 	at org.apache.hadoop.hdds.utils.NativeLibraryLoader.copyResourceFromJarToTemp(NativeLibraryLoader.java:138)
om3_1    | 	at org.apache.hadoop.hdds.utils.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:114)
s3g_1    | 	at org.jvnet.hk2.internal.PerLookupContext.findOrCreate(PerLookupContext.java:46)
scm3_1   | 2023-05-31 01:06:39,206 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om3_1    | 	at org.apache.hadoop.ozone.om.snapshot.SnapshotDiffManager.initSSTDumpTool(SnapshotDiffManager.java:254)
om3_1    | 	at org.apache.hadoop.ozone.om.snapshot.SnapshotDiffManager.<init>(SnapshotDiffManager.java:233)
scm2_1   | 2023-05-31 01:06:23,821 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm2_1   | 2023-05-31 01:06:23,918 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
s3g_1    | 	at org.jvnet.hk2.internal.Utilities.createService(Utilities.java:2102)
recon_1  | 2023-05-31 01:06:17,048 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@40696fc6{recon,/,file:///data/metadata/webserver/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-10224032266117984806/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
om2_1    | 2023-05-31 01:06:40,700 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-05-31 01:06:40,984 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om2_1    | 2023-05-31 01:06:41,007 [main] WARN utils.NativeLibraryLoader: Unable to load library: ozone_rocksdb_tools
dn2_1    | 	... 1 more
om1_1    | 2023-05-31 01:06:41,926 [main] INFO utils.RDBSnapshotProvider: Cleaning up the candidate dir: /data/metadata/snapshot/om.db.candidate
dn5_1    | 2023-05-31 01:06:08,099 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn1_1    | 2023-05-31 01:06:29,894 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=8, votedFor=3da1490a-d610-40de-95c1-bdfc259ba041} from /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/current/raft-meta
scm3_1   | 2023-05-31 01:06:39,228 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
dn5_1    | 2023-05-31 01:06:08,165 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn2_1    | 2023-05-31 01:06:15,105 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn2_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
recon_1  | 2023-05-31 01:06:17,061 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@1c297897{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
om2_1    | java.io.IOException: Permission denied
dn3_1    | 2023-05-31 01:06:16,607 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:17,608 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-05-31 01:06:39,251 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServer: d2fda184-2469-4fa2-af65-75268cb832ba: found a subdirectory /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e
dn5_1    | 2023-05-31 01:06:08,173 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn5_1    | 2023-05-31 01:06:08,182 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm2_1   | 2023-05-31 01:06:23,925 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2_1   | 2023-05-31 01:06:23,925 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
recon_1  | 2023-05-31 01:06:17,062 [Listener at 0.0.0.0/9891] INFO server.Server: Started @52055ms
recon_1  | 2023-05-31 01:06:17,066 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
om2_1    | 	at java.base/java.io.UnixFileSystem.createFileExclusively(Native Method)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | 2023-05-31 01:06:29,910 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:29,995 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: set configuration 29: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-31 01:06:29,997 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9: set configuration 3: peers:[b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-31 01:06:39,289 [main] INFO server.RaftServer: d2fda184-2469-4fa2-af65-75268cb832ba: addNew group-6A492392CC3E:[] returns group-6A492392CC3E:java.util.concurrent.CompletableFuture@bbf9e07[Not completed]
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
scm2_1   | 2023-05-31 01:06:24,009 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm2_1   | 2023-05-31 01:06:24,010 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
recon_1  | 2023-05-31 01:06:17,066 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
om2_1    | 	at java.base/java.io.File.createTempFile(File.java:2129)
om2_1    | 	at org.apache.hadoop.hdds.utils.NativeLibraryLoader.copyResourceFromJarToTemp(NativeLibraryLoader.java:138)
om2_1    | 	at org.apache.hadoop.hdds.utils.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:114)
om2_1    | 	at org.apache.hadoop.ozone.om.snapshot.SnapshotDiffManager.initSSTDumpTool(SnapshotDiffManager.java:254)
om2_1    | 	at org.apache.hadoop.ozone.om.snapshot.SnapshotDiffManager.<init>(SnapshotDiffManager.java:233)
dn1_1    | 2023-05-31 01:06:29,998 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: set configuration 11: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-31 01:06:30,107 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO ratis.ContainerStateMachine: group-F1A4CF07E444: Setting the last applied index to (t:11, i:24)
scm3_1   | 2023-05-31 01:06:39,527 [d2fda184-2469-4fa2-af65-75268cb832ba-groupManagement] INFO ha.SCMStateMachine: Updated lastAppliedTermIndex 5#80 with transactionInfo term andIndex
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
s3g_1    | 	at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:758)
s3g_1    | 	at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:721)
recon_1  | 2023-05-31 01:06:17,068 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1  | 2023-05-31 01:06:17,068 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
dn3_1    | 2023-05-31 01:06:17,609 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:17,650 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From dcc4205663d5/10.9.0.19 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:35710 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
scm3_1   | 2023-05-31 01:06:39,547 [d2fda184-2469-4fa2-af65-75268cb832ba-groupManagement] INFO server.RaftServer$Division: d2fda184-2469-4fa2-af65-75268cb832ba: new RaftServerImpl for group-6A492392CC3E:[] with SCMStateMachine:uninitialized
scm2_1   | Container Balancer status:
scm2_1   | Key                            Value
scm2_1   | Running                        false
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:538)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-05-31 01:06:17,078 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
om1_1    | 2023-05-31 01:06:41,931 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
om1_1    | 2023-05-31 01:06:44,108 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om1_1    | 2023-05-31 01:06:44,275 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1    | 2023-05-31 01:06:44,696 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om1:9872, om3:9872, om2:9872
om1_1    | 2023-05-31 01:06:44,831 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:4, i:109)
om1_1    | 2023-05-31 01:06:45,325 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om1_1    | 2023-05-31 01:06:45,430 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm3_1   | 2023-05-31 01:06:39,577 [d2fda184-2469-4fa2-af65-75268cb832ba-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm3_1   | 2023-05-31 01:06:39,579 [d2fda184-2469-4fa2-af65-75268cb832ba-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-05-31 01:06:08,186 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm2_1   | Container Balancer Configuration values:
scm2_1   | Key                                                Value
dn2_1    | Caused by: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn1_1    | 2023-05-31 01:06:30,133 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO ratis.ContainerStateMachine: group-A4E2998A83C9: Setting the last applied index to (t:3, i:4)
dn1_1    | 2023-05-31 01:06:30,116 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO ratis.ContainerStateMachine: group-DC0251522AF1: Setting the last applied index to (t:8, i:42)
dn1_1    | 2023-05-31 01:06:30,903 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-05-31 01:06:30,910 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-05-31 01:06:30,913 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:30,922 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-05-31 01:06:30,942 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
s3g_1    | 	at org.jvnet.hk2.internal.ServiceLocatorImpl.getService(ServiceLocatorImpl.java:691)
dn5_1    | 2023-05-31 01:06:08,404 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
recon_1  | 2023-05-31 01:06:17,078 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-05-31 01:06:17,080 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Last known snapshot for OM : /data/metadata/om.snapshot.db_1685494503083
om3_1    | 	at org.apache.hadoop.ozone.om.OmSnapshotManager.<init>(OmSnapshotManager.java:261)
om3_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.instantiateServices(OzoneManager.java:833)
om1_1    | 2023-05-31 01:06:45,437 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om1_1    | 2023-05-31 01:06:45,441 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om1_1    | 2023-05-31 01:06:45,448 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om1_1    | 2023-05-31 01:06:45,449 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
s3g_1    | 	at org.glassfish.jersey.inject.hk2.AbstractHk2InjectionManager.getInstance(AbstractHk2InjectionManager.java:160)
s3g_1    | 	at org.glassfish.jersey.inject.hk2.ImmediateHk2InjectionManager.getInstance(ImmediateHk2InjectionManager.java:30)
s3g_1    | 	at org.glassfish.jersey.internal.inject.Injections.getOrCreate(Injections.java:105)
s3g_1    | 	at org.glassfish.jersey.server.model.MethodHandler$ClassBasedMethodHandler.getInstance(MethodHandler.java:260)
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.PushMethodHandlerRouter.apply(PushMethodHandlerRouter.java:51)
dn5_1    | 2023-05-31 01:06:08,498 [main] INFO http.HttpServer2: Jetty bound to port 9882
scm2_1   | Threshold                                          10
recon_1  | 2023-05-31 01:06:17,102 [Listener at 0.0.0.0/9891] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn1_1    | 2023-05-31 01:06:30,947 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:42906 remote=recon/10.9.0.22:9891]
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
om1_1    | 2023-05-31 01:06:45,450 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
dn5_1    | 2023-05-31 01:06:08,516 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
scm2_1   | Max Datanodes to Involve per Iteration(percent)    20
scm2_1   | Max Size to Move per Iteration                     500GB
recon_1  | 2023-05-31 01:06:17,292 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1685494503083.
recon_1  | 2023-05-31 01:06:17,345 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
dn1_1    | 2023-05-31 01:06:30,948 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-05-31 01:06:30,953 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-05-31 01:06:30,958 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om1_1    | 2023-05-31 01:06:45,468 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 2023-05-31 01:06:08,761 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om3_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:667)
om3_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:752)
om3_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.startAndCancelPrepare(OzoneManagerStarter.java:231)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm1_1   | 2023-05-31 01:06:14,090 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1_1   | 2023-05-31 01:06:14,192 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
dn1_1    | 2023-05-31 01:06:30,949 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om1_1    | 2023-05-31 01:06:45,575 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-05-31 01:06:45,593 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
scm2_1   | Max Size Entering Target per Iteration             26GB
scm2_1   | Max Size Leaving Source per Iteration              26GB
scm2_1   | 
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm1_1   | 2023-05-31 01:06:14,192 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1_1   | 2023-05-31 01:06:14,200 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
dn1_1    | 2023-05-31 01:06:30,960 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om1_1    | 2023-05-31 01:06:45,601 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
dn5_1    | 2023-05-31 01:06:08,772 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:86)
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
om2_1    | 	at org.apache.hadoop.ozone.om.OmSnapshotManager.<init>(OmSnapshotManager.java:261)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
recon_1  | 2023-05-31 01:06:17,349 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1  | 2023-05-31 01:06:24,225 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:7b17d04a-0b5d-44ed-b487-1bf0d959b978 is not the leader. Could not determine the leader node.
om3_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.startOmUpgrade(OzoneManagerStarter.java:121)
om3_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm1_1   | 2023-05-31 01:06:14,201 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm1_1   | 2023-05-31 01:06:14,205 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
om1_1    | 2023-05-31 01:06:45,704 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-05-31 01:06:08,790 [main] INFO server.session: node0 Scavenging every 600000ms
scm3_1   | 2023-05-31 01:06:39,586 [d2fda184-2469-4fa2-af65-75268cb832ba-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm3_1   | 2023-05-31 01:06:39,588 [d2fda184-2469-4fa2-af65-75268cb832ba-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
om2_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.instantiateServices(OzoneManager.java:833)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
scm2_1   | 2023-05-31 01:06:24,011 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm2_1   | 2023-05-31 01:06:24,018 [Listener at 0.0.0.0/9860] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
om3_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
om3_1    | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm1_1   | 2023-05-31 01:06:14,205 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
om1_1    | 2023-05-31 01:06:45,739 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn5_1    | 2023-05-31 01:06:08,904 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@22f02996{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1  | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1  | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	... 1 more
scm1_1   | 2023-05-31 01:06:14,254 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm1_1   | 2023-05-31 01:06:14,255 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm1_1   | 2023-05-31 01:06:14,301 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
dn4_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn4_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
om2_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:667)
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
scm3_1   | 2023-05-31 01:06:39,588 [d2fda184-2469-4fa2-af65-75268cb832ba-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2_1   | 2023-05-31 01:06:24,020 [Listener at 0.0.0.0/9860] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
recon_1  | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
dn2_1    | 2023-05-31 01:06:15,240 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:15,241 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:30,960 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-05-31 01:06:30,980 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-05-31 01:06:30,981 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-05-31 01:06:08,917 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2ca3d826{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om2_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:752)
om2_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.startAndCancelPrepare(OzoneManagerStarter.java:231)
om2_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.startOmUpgrade(OzoneManagerStarter.java:121)
om2_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm2_1   | 2023-05-31 01:06:24,021 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm2_1   | 2023-05-31 01:06:24,024 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
dn2_1    | 2023-05-31 01:06:15,246 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:16,241 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
om3_1    | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
om3_1    | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
om3_1    | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
om3_1    | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
dn5_1    | 2023-05-31 01:06:09,665 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3ca3648{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-13309613196848817003/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
om2_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
om2_1    | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
om2_1    | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:69)
recon_1  | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
om3_1    | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
om3_1    | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
dn5_1    | 2023-05-31 01:06:09,717 [main] INFO server.AbstractConnector: Started ServerConnector@70881123{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
scm3_1   | 2023-05-31 01:06:39,588 [d2fda184-2469-4fa2-af65-75268cb832ba-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm3_1   | 2023-05-31 01:06:39,685 [d2fda184-2469-4fa2-af65-75268cb832ba-groupManagement] INFO server.RaftServer$Division: d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm3_1   | 2023-05-31 01:06:39,692 [d2fda184-2469-4fa2-af65-75268cb832ba-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3_1   | 2023-05-31 01:06:39,723 [d2fda184-2469-4fa2-af65-75268cb832ba-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:38)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
dn4_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn4_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn1_1    | 2023-05-31 01:06:30,986 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-05-31 01:06:31,006 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-05-31 01:06:31,010 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-05-31 01:06:31,011 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-05-31 01:06:31,029 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-05-31 01:06:09,739 [main] INFO server.Server: Started @44487ms
scm1_1   | 2023-05-31 01:06:14,322 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm2_1   | 2023-05-31 01:06:24,029 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/in_use.lock acquired by nodename 7@2e348dbfef56
scm2_1   | 2023-05-31 01:06:24,042 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=5, votedFor=7b17d04a-0b5d-44ed-b487-1bf0d959b978} from /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/raft-meta
scm2_1   | 2023-05-31 01:06:24,098 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServer$Division: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E: set configuration 61: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-05-31 01:06:24,105 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn2_1    | 2023-05-31 01:06:16,242 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-05-31 01:06:45,748 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm2_1   | 2023-05-31 01:06:24,122 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
om3_1    | 	at picocli.CommandLine.execute(CommandLine.java:2078)
om3_1    | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
scm1_1   | 2023-05-31 01:06:14,389 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
recon_1  | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn2_1    | 2023-05-31 01:06:16,247 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 	at org.glassfish.jersey.process.internal.Stages.process(Stages.java:173)
s3g_1    | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:247)
s3g_1    | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
scm2_1   | 2023-05-31 01:06:24,123 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn3_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
om1_1    | 2023-05-31 01:06:48,054 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om1_1    | 2023-05-31 01:06:48,083 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om1_1    | 2023-05-31 01:06:48,083 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om1_1    | 2023-05-31 01:06:48,083 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn2_1    | 2023-05-31 01:06:17,107 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
scm3_1   | 2023-05-31 01:06:39,725 [d2fda184-2469-4fa2-af65-75268cb832ba-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm3_1   | 2023-05-31 01:06:39,851 [d2fda184-2469-4fa2-af65-75268cb832ba-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
om2_1    | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
om2_1    | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
scm1_1   | 2023-05-31 01:06:14,412 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
om3_1    | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
om3_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:58)
dn4_1    | 2023-05-31 01:06:17,787 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn2_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
s3g_1    | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1    | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1    | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
om2_1    | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
dn5_1    | 2023-05-31 01:06:09,754 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm1_1   | 2023-05-31 01:06:14,415 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
dn1_1    | 2023-05-31 01:06:31,043 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om3_1    | 2023-05-31 01:06:41,748 [main] WARN om.OzoneManager: Prepare marker file index 109 does not match DB prepare index 108. Writing DB index to prepare file and maintaining prepared state.
om3_1    | 2023-05-31 01:06:41,750 [main] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 108 to file /data/metadata/current/prepareMarker
dn4_1    | 2023-05-31 01:06:17,817 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
scm3_1   | 2023-05-31 01:06:39,879 [d2fda184-2469-4fa2-af65-75268cb832ba-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm3_1   | 2023-05-31 01:06:39,885 [d2fda184-2469-4fa2-af65-75268cb832ba-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm3_1   | 2023-05-31 01:06:39,885 [d2fda184-2469-4fa2-af65-75268cb832ba-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm3_1   | 2023-05-31 01:06:40,061 [d2fda184-2469-4fa2-af65-75268cb832ba-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn5_1    | 2023-05-31 01:06:09,754 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm1_1   | WARNING: An illegal reflective access operation has occurred
dn1_1    | 2023-05-31 01:06:31,044 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
om1_1    | 2023-05-31 01:06:48,088 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | 2023-05-31 01:06:48,107 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1    | 2023-05-31 01:06:48,130 [om1-impl-thread1] INFO server.RaftServer: om1: found a subdirectory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
scm3_1   | 2023-05-31 01:06:40,646 [d2fda184-2469-4fa2-af65-75268cb832ba-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
om2_1    | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
om2_1    | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
om2_1    | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
dn5_1    | 2023-05-31 01:06:09,759 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
scm1_1   | WARNING: Illegal reflective access by org.apache.hadoop.hdds.utils.MetricsUtil (file:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar) to method java.lang.Class.annotationData()
dn1_1    | 2023-05-31 01:06:31,048 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
om1_1    | 2023-05-31 01:06:48,177 [main] INFO server.RaftServer: om1: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@33d28f0a[Not completed]
dn4_1    | 2023-05-31 01:06:18,788 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-31 01:06:18,818 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
scm3_1   | 2023-05-31 01:06:40,669 [d2fda184-2469-4fa2-af65-75268cb832ba-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
s3g_1    | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1    | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1    | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
scm2_1   | 2023-05-31 01:06:24,126 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1_1   | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.hdds.utils.MetricsUtil
scm1_1   | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
scm1_1   | WARNING: All illegal access operations will be denied in a future release
om1_1    | 2023-05-31 01:06:48,178 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
dn4_1    | 2023-05-31 01:06:19,789 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-05-31 01:06:42,373 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
recon_1  | , while invoking $Proxy43.submitRequest over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
scm3_1   | 2023-05-31 01:06:40,669 [d2fda184-2469-4fa2-af65-75268cb832ba-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om2_1    | 	at picocli.CommandLine.execute(CommandLine.java:2078)
s3g_1    | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1    | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
scm2_1   | 2023-05-31 01:06:24,127 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1_1   | 2023-05-31 01:06:14,432 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
dn1_1    | 2023-05-31 01:06:31,050 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 2023-05-31 01:06:19,820 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-05-31 01:06:42,415 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
recon_1  | 2023-05-31 01:06:26,841 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2 is not the leader. Could not determine the leader node.
recon_1  | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
scm3_1   | 2023-05-31 01:06:40,670 [d2fda184-2469-4fa2-af65-75268cb832ba-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om2_1    | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
om2_1    | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
om2_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:58)
scm2_1   | 2023-05-31 01:06:24,133 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2_1   | 2023-05-31 01:06:24,142 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm2_1   | 2023-05-31 01:06:24,143 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 2023-05-31 01:06:48,183 [main] INFO om.OzoneManager: Creating RPC Server
dn4_1    | 2023-05-31 01:06:20,790 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-05-31 01:06:42,572 [main] INFO utils.RDBSnapshotProvider: Cleaning up the candidate dir: /data/metadata/snapshot/om.db.candidate
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
recon_1  | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
scm3_1   | 2023-05-31 01:06:40,670 [d2fda184-2469-4fa2-af65-75268cb832ba-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm3_1   | 2023-05-31 01:06:40,671 [d2fda184-2469-4fa2-af65-75268cb832ba-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om2_1    | 2023-05-31 01:06:42,022 [main] WARN om.OzoneManager: Prepare marker file index 109 does not match DB prepare index 108. Writing DB index to prepare file and maintaining prepared state.
om2_1    | 2023-05-31 01:06:42,035 [main] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 108 to file /data/metadata/current/prepareMarker
s3g_1    | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1    | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
dn1_1    | 2023-05-31 01:06:31,050 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1_1   | 2023-05-31 01:06:14,436 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 2, healthy pipeline threshold count is 1
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 2023-05-31 01:06:48,300 [om1-groupManagement] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
dn4_1    | 2023-05-31 01:06:20,809 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
om1_1    | 2023-05-31 01:06:48,327 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
recon_1  | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
scm3_1   | 2023-05-31 01:06:40,676 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm3_1   | 2023-05-31 01:06:40,679 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 2023-05-31 01:06:09,905 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn5_1    | 2023-05-31 01:06:10,062 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
dn5_1    | 2023-05-31 01:06:11,294 [Listener at 0.0.0.0/9864] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
dn5_1    | 2023-05-31 01:06:11,303 [Listener at 0.0.0.0/9864] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
scm2_1   | 2023-05-31 01:06:24,143 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-05-31 01:06:14,439 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | java.net.SocketTimeoutException: Call From 955d7ba09e69/10.9.0.20 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:40300 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
om3_1    | 2023-05-31 01:06:42,577 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
om1_1    | 2023-05-31 01:06:48,343 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
recon_1  | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
scm3_1   | 2023-05-31 01:06:40,682 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm3_1   | 2023-05-31 01:06:40,952 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
dn1_1    | 2023-05-31 01:06:31,051 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
s3g_1    | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
om2_1    | 2023-05-31 01:06:42,526 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm2_1   | 2023-05-31 01:06:24,148 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e
scm2_1   | 2023-05-31 01:06:24,149 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:35710 remote=recon/10.9.0.22:9891]
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
om3_1    | 2023-05-31 01:06:44,355 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om3_1    | 2023-05-31 01:06:44,435 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
scm3_1   | 2023-05-31 01:06:41,162 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
dn5_1    | 2023-05-31 01:06:11,303 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn1_1    | 2023-05-31 01:06:31,057 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
s3g_1    | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1459)
scm2_1   | 2023-05-31 01:06:24,150 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1_1   | 2023-05-31 01:06:14,554 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
om1_1    | 2023-05-31 01:06:48,345 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om1_1    | 2023-05-31 01:06:48,346 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:538)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | 2023-05-31 01:06:11,342 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
scm3_1   | 2023-05-31 01:06:41,599 [main] INFO node.SCMNodeManager: Entering startup safe mode.
dn1_1    | 2023-05-31 01:06:31,057 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1656)
scm2_1   | 2023-05-31 01:06:24,154 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1_1   | 2023-05-31 01:06:15,524 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
om1_1    | 2023-05-31 01:06:48,346 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | Caused by: java.util.concurrent.TimeoutException
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
dn5_1    | 2023-05-31 01:06:11,343 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
scm3_1   | 2023-05-31 01:06:41,673 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
dn1_1    | 2023-05-31 01:06:31,059 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
s3g_1    | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1    | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
scm2_1   | 2023-05-31 01:06:24,154 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om2_1    | 2023-05-31 01:06:42,529 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
om1_1    | 2023-05-31 01:06:48,346 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om1_1    | 2023-05-31 01:06:48,447 [om1-groupManagement] INFO server.RaftServer$Division: om1@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
recon_1  | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
dn5_1    | 2023-05-31 01:06:11,400 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn5_1    | 2023-05-31 01:06:11,407 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn1_1    | 2023-05-31 01:06:31,088 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1
scm2_1   | 2023-05-31 01:06:24,154 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | 2023-05-31 01:06:15,576 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1_1   | 2023-05-31 01:06:15,697 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
om1_1    | 2023-05-31 01:06:48,458 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-05-31 01:06:11,424 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5766a178] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm3_1   | 2023-05-31 01:06:41,678 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
dn1_1    | 2023-05-31 01:06:31,092 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm2_1   | 2023-05-31 01:06:24,155 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1_1   | 2023-05-31 01:06:15,858 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1_1   | 2023-05-31 01:06:15,868 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1_1   | 2023-05-31 01:06:15,880 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
om1_1    | 2023-05-31 01:06:48,531 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om1_1    | 2023-05-31 01:06:48,546 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om3_1    | 2023-05-31 01:06:44,762 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om3:9872, om1:9872, om2:9872
om3_1    | 2023-05-31 01:06:44,892 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:4, i:109)
dn5_1    | 2023-05-31 01:06:11,551 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
scm3_1   | 2023-05-31 01:06:42,050 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn1_1    | 2023-05-31 01:06:31,095 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm2_1   | 2023-05-31 01:06:24,155 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om2_1    | 2023-05-31 01:06:42,628 [main] INFO utils.RDBSnapshotProvider: Cleaning up the candidate dir: /data/metadata/snapshot/om.db.candidate
s3g_1    | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
scm1_1   | 2023-05-31 01:06:15,962 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
om3_1    | 2023-05-31 01:06:45,355 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn5_1    | 2023-05-31 01:06:11,579 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
scm3_1   | 2023-05-31 01:06:42,059 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm3_1   | 2023-05-31 01:06:42,106 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
dn1_1    | 2023-05-31 01:06:31,088 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444
scm2_1   | 2023-05-31 01:06:24,156 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm2_1   | 2023-05-31 01:06:24,168 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
s3g_1    | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
scm1_1   | 2023-05-31 01:06:15,982 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 2023-05-31 01:06:14,612 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-05-31 01:06:45,403 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 2023-05-31 01:06:42,109 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
dn1_1    | 2023-05-31 01:06:31,098 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm2_1   | 2023-05-31 01:06:24,168 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-05-31 01:06:24,347 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
scm1_1   | 2023-05-31 01:06:15,987 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
om1_1    | 2023-05-31 01:06:48,772 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn5_1    | 2023-05-31 01:06:14,618 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-05-31 01:06:45,414 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn1_1    | 2023-05-31 01:06:31,098 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
om2_1    | 2023-05-31 01:06:42,632 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
scm2_1   | 2023-05-31 01:06:24,347 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
s3g_1    | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1681)
scm1_1   | 2023-05-31 01:06:16,147 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
om1_1    | 2023-05-31 01:06:48,922 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om3_1    | 2023-05-31 01:06:45,416 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 2023-05-31 01:06:42,142 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm3_1   | 2023-05-31 01:06:42,157 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm3_1   | 2023-05-31 01:06:42,181 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm2_1   | 2023-05-31 01:06:24,353 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm2_1   | 2023-05-31 01:06:24,396 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServer$Division: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E: set configuration 0: peers:[7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-05-31 01:06:24,397 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/log_0-0
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 2023-05-31 01:06:14,623 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-05-31 01:06:49,087 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
recon_1  | , while invoking $Proxy43.submitRequest over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
scm3_1   | 2023-05-31 01:06:42,199 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
om2_1    | 2023-05-31 01:06:44,051 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn1_1    | 2023-05-31 01:06:31,100 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
s3g_1    | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
scm2_1   | 2023-05-31 01:06:24,399 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServer$Division: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E: set configuration 1: peers:[7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-05-31 01:06:24,405 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServer$Division: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E: set configuration 17: peers:[7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm2_1   | 2023-05-31 01:06:24,406 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServer$Division: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E: set configuration 19: peers:[7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn5_1    | 2023-05-31 01:06:15,453 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
om1_1    | 2023-05-31 01:06:49,093 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
recon_1  | 2023-05-31 01:06:28,842 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 46d31cd800d0/10.9.0.22 to scm3:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
scm3_1   | 2023-05-31 01:06:42,488 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm3_1   | 2023-05-31 01:06:42,633 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
dn1_1    | 2023-05-31 01:06:31,112 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1    | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1    | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
scm2_1   | 2023-05-31 01:06:24,407 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServer$Division: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E: set configuration 31: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn5_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
om1_1    | 2023-05-31 01:06:49,679 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
recon_1  | 2023-05-31 01:06:33,827 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 7 pipelines from SCM.
scm3_1   | 2023-05-31 01:06:43,134 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
om2_1    | 2023-05-31 01:06:44,175 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-05-31 01:06:31,113 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)
scm1_1   | 2023-05-31 01:06:16,148 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm2_1   | 2023-05-31 01:06:24,408 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServer$Division: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E: set configuration 33: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
om1_1    | 2023-05-31 01:06:49,887 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1    | 2023-05-31 01:06:49,943 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1    | 2023-05-31 01:06:49,951 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
recon_1  | 2023-05-31 01:06:33,835 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 7 pipelines in house.
scm3_1   | 2023-05-31 01:06:43,276 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm3_1   | 2023-05-31 01:06:43,283 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
om2_1    | 2023-05-31 01:06:44,642 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om2:9872, om1:9872, om3:9872
om2_1    | 2023-05-31 01:06:44,769 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:4, i:109)
om2_1    | 2023-05-31 01:06:45,257 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om2_1    | 2023-05-31 01:06:45,370 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om2_1    | 2023-05-31 01:06:45,386 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
om3_1    | 2023-05-31 01:06:45,417 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om3_1    | 2023-05-31 01:06:45,417 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
recon_1  | 2023-05-31 01:06:35,307 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
dn1_1    | 2023-05-31 01:06:31,120 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-05-31 01:06:31,121 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-05-31 01:06:31,126 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-05-31 01:06:31,122 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-05-31 01:06:31,134 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om2_1    | 2023-05-31 01:06:45,386 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm2_1   | 2023-05-31 01:06:24,421 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO segmented.LogSegment: Successfully read 48 entries from segment file /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/log_1-48
scm2_1   | 2023-05-31 01:06:24,424 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServer$Division: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E: set configuration 49: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn2_1    | 	... 1 more
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
om3_1    | 2023-05-31 01:06:45,417 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om3_1    | 2023-05-31 01:06:45,422 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om3_1    | 2023-05-31 01:06:45,439 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-05-31 01:06:45,441 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn1_1    | 2023-05-31 01:06:31,139 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om2_1    | 2023-05-31 01:06:45,399 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om2_1    | 2023-05-31 01:06:45,399 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 2023-05-31 01:06:17,243 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:17,243 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
scm1_1   | Container Balancer status:
scm1_1   | Key                            Value
scm1_1   | Running                        false
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
recon_1  | 2023-05-31 01:06:35,343 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
dn1_1    | 2023-05-31 01:06:31,145 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om2_1    | 2023-05-31 01:06:45,399 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om2_1    | 2023-05-31 01:06:45,416 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om2_1    | 2023-05-31 01:06:45,443 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-05-31 01:06:45,464 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om1_1    | 2023-05-31 01:06:49,952 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-05-31 01:06:18,244 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:18,245 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
om3_1    | 2023-05-31 01:06:45,449 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn3_1    | 2023-05-31 01:06:18,609 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-05-31 01:06:35,368 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1  | 2023-05-31 01:06:35,487 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1  | 2023-05-31 01:06:37,441 [IPC Server handler 18 on default port 9891] WARN ipc.Server: IPC Server handler 18 on default port 9891, call Call#9 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:48622: output error
recon_1  | 2023-05-31 01:06:37,456 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:35710: output error
recon_1  | 2023-05-31 01:06:37,455 [IPC Server handler 16 on default port 9891] WARN ipc.Server: IPC Server handler 16 on default port 9891, call Call#9 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:57342: output error
recon_1  | 2023-05-31 01:06:37,455 [IPC Server handler 17 on default port 9891] WARN ipc.Server: IPC Server handler 17 on default port 9891, call Call#9 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:50654: output error
om1_1    | 2023-05-31 01:06:49,952 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
s3g_1    | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:600)
s3g_1    | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1    | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
dn3_1    | 2023-05-31 01:06:18,610 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:19,610 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:19,611 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:20,611 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:20,612 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:21,612 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:21,613 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-05-31 01:06:49,956 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
s3g_1    | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1    | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
scm1_1   | Container Balancer Configuration values:
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
om2_1    | 2023-05-31 01:06:45,467 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1    | 2023-05-31 01:06:45,560 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-05-31 01:06:21,625 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From dcc4205663d5/10.9.0.19 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:38708 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
scm2_1   | 2023-05-31 01:06:24,425 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO segmented.LogSegment: Successfully read 12 entries from segment file /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/log_49-60
scm2_1   | 2023-05-31 01:06:24,426 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServer$Division: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E: set configuration 61: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-31 01:06:18,289 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
om3_1    | 2023-05-31 01:06:45,547 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
s3g_1    | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
s3g_1    | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:538)
om2_1    | 2023-05-31 01:06:45,583 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm3_1   | WARNING: An illegal reflective access operation has occurred
dn1_1    | 2023-05-31 01:06:31,152 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2_1   | 2023-05-31 01:06:24,475 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO segmented.LogSegment: Successfully read 20 entries from segment file /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/log_inprogress_61
scm2_1   | 2023-05-31 01:06:24,479 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 80
recon_1  | 2023-05-31 01:06:37,455 [IPC Server handler 3 on default port 9891] WARN ipc.Server: IPC Server handler 3 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:58668: output error
recon_1  | 2023-05-31 01:06:37,454 [IPC Server handler 21 on default port 9891] WARN ipc.Server: IPC Server handler 21 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:44290: output error
dn2_1    | java.net.SocketTimeoutException: Call From 86166c590ca2/10.9.0.18 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:59370 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
scm3_1   | WARNING: Illegal reflective access by org.apache.hadoop.hdds.utils.MetricsUtil (file:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar) to method java.lang.Class.annotationData()
scm3_1   | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.hdds.utils.MetricsUtil
scm3_1   | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
om1_1    | 2023-05-31 01:06:51,621 [main] INFO reflections.Reflections: Reflections took 3201 ms to scan 8 urls, producing 24 keys and 616 values [using 2 cores]
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
scm3_1   | WARNING: All illegal access operations will be denied in a future release
om1_1    | 2023-05-31 01:06:52,495 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om1_1    | 2023-05-31 01:06:52,562 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
recon_1  | 2023-05-31 01:06:37,454 [IPC Server handler 11 on default port 9891] WARN ipc.Server: IPC Server handler 11 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:42908: output error
recon_1  | 2023-05-31 01:06:37,453 [IPC Server handler 14 on default port 9891] WARN ipc.Server: IPC Server handler 14 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:59370: output error
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 2023-05-31 01:06:43,468 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm1_1   | Key                                                Value
scm2_1   | 2023-05-31 01:06:24,479 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 60
recon_1  | 2023-05-31 01:06:37,453 [IPC Server handler 12 on default port 9891] WARN ipc.Server: IPC Server handler 12 on default port 9891, call Call#11 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:52384: output error
recon_1  | 2023-05-31 01:06:37,449 [IPC Server handler 10 on default port 9891] WARN ipc.Server: IPC Server handler 10 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:58656: output error
dn4_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:40300 remote=scm1/10.9.0.14:9861]
dn5_1    | Caused by: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn1_1    | 2023-05-31 01:06:31,153 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om2_1    | 2023-05-31 01:06:45,591 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om2_1    | 2023-05-31 01:06:47,788 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm3_1   | 2023-05-31 01:06:43,507 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 2, healthy pipeline threshold count is 1
scm3_1   | 2023-05-31 01:06:43,520 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
scm2_1   | 2023-05-31 01:06:24,531 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServer$Division: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E: start as a follower, conf=61: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 2023-05-31 01:06:37,449 [IPC Server handler 9 on default port 9891] WARN ipc.Server: IPC Server handler 9 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:50640: output error
recon_1  | 2023-05-31 01:06:37,449 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:54816: output error
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
om3_1    | 2023-05-31 01:06:45,568 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 2023-05-31 01:06:31,187 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/6a499bf5-10af-4dab-a3fd-a4e2998a83c9
dn1_1    | 2023-05-31 01:06:31,187 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-05-31 01:06:31,187 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-05-31 01:06:31,187 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
om1_1    | 2023-05-31 01:06:55,020 [Listener at om1/9862] INFO om.OzoneManagerPrepareState: Deleted prepare marker file: /data/metadata/current/prepareMarker
scm1_1   | Threshold                                          10
scm2_1   | 2023-05-31 01:06:24,531 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServer$Division: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E: changes role from      null to FOLLOWER at term 5 for startAsFollower
recon_1  | 2023-05-31 01:06:37,442 [IPC Server handler 8 on default port 9891] WARN ipc.Server: IPC Server handler 8 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:59376: output error
recon_1  | 2023-05-31 01:06:37,442 [IPC Server handler 19 on default port 9891] WARN ipc.Server: IPC Server handler 19 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:35724: output error
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om3_1    | 2023-05-31 01:06:45,576 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn1_1    | 2023-05-31 01:06:31,187 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-05-31 01:06:31,188 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-05-31 01:06:31,188 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om1_1    | 2023-05-31 01:06:55,548 [Listener at om1/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om1_1    | 2023-05-31 01:06:55,611 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm2_1   | 2023-05-31 01:06:24,533 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO impl.RoleInfo: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2: start 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-FollowerState
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
recon_1  | 2023-05-31 01:06:37,442 [IPC Server handler 13 on default port 9891] WARN ipc.Server: IPC Server handler 13 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:57340: output error
recon_1  | 2023-05-31 01:06:37,442 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:42906: output error
dn5_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om3_1    | 2023-05-31 01:06:47,496 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om3_1    | 2023-05-31 01:06:47,512 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om3_1    | 2023-05-31 01:06:47,515 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om3_1    | 2023-05-31 01:06:47,515 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1    | 2023-05-31 01:06:47,516 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1_1   | Max Datanodes to Involve per Iteration(percent)    20
scm1_1   | Max Size to Move per Iteration                     500GB
scm1_1   | Max Size Entering Target per Iteration             26GB
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1    | 2023-05-31 01:06:47,525 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1    | 2023-05-31 01:06:47,534 [om3-impl-thread1] INFO server.RaftServer: om3: found a subdirectory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
s3g_1    | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
scm3_1   | 2023-05-31 01:06:43,955 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
scm3_1   | 2023-05-31 01:06:48,458 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
om1_1    | 2023-05-31 01:06:55,611 [Listener at om1/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om1_1    | 2023-05-31 01:06:55,960 [Listener at om1/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om1/10.9.0.11:9862
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
om2_1    | 2023-05-31 01:06:47,868 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om2_1    | 2023-05-31 01:06:47,873 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om2_1    | 2023-05-31 01:06:47,873 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1    | 2023-05-31 01:06:47,881 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1_1   | Max Size Leaving Source per Iteration              26GB
scm3_1   | 2023-05-31 01:06:48,671 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
recon_1  | 2023-05-31 01:06:37,442 [IPC Server handler 6 on default port 9891] WARN ipc.Server: IPC Server handler 6 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:53420: output error
recon_1  | 2023-05-31 01:06:37,482 [IPC Server handler 7 on default port 9891] WARN ipc.Server: IPC Server handler 7 on default port 9891, call Call#9 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:54822: output error
recon_1  | 2023-05-31 01:06:37,483 [IPC Server handler 5 on default port 9891] WARN ipc.Server: IPC Server handler 5 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:44286: output error
dn5_1    | 	... 1 more
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn2_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
scm1_1   | 
scm3_1   | 2023-05-31 01:06:48,943 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn4_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
scm2_1   | 2023-05-31 01:06:24,535 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6A492392CC3E,id=4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2
om1_1    | 2023-05-31 01:06:55,961 [Listener at om1/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
recon_1  | 2023-05-31 01:06:37,484 [IPC Server handler 18 on default port 9891] INFO ipc.Server: IPC Server handler 18 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
s3g_1    | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1    | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
s3g_1    | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
om3_1    | 2023-05-31 01:06:47,547 [main] INFO server.RaftServer: om3: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@5a592c70[Not completed]
scm1_1   | 2023-05-31 01:06:16,152 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm1_1   | 2023-05-31 01:06:16,188 [Listener at 0.0.0.0/9860] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn4_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
scm2_1   | 2023-05-31 01:06:24,538 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm2_1   | 2023-05-31 01:06:24,538 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
om2_1    | 2023-05-31 01:06:47,934 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1    | 2023-05-31 01:06:47,980 [om2-impl-thread1] INFO server.RaftServer: om2: found a subdirectory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om2_1    | 2023-05-31 01:06:48,011 [main] INFO server.RaftServer: om2: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@65cc5252[Not completed]
scm1_1   | 2023-05-31 01:06:16,196 [Listener at 0.0.0.0/9860] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn1_1    | 2023-05-31 01:06:31,188 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-05-31 01:06:31,188 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-05-31 01:06:31,224 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
om3_1    | 2023-05-31 01:06:47,547 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om3_1    | 2023-05-31 01:06:47,551 [main] INFO om.OzoneManager: Creating RPC Server
om3_1    | 2023-05-31 01:06:47,599 [om3-groupManagement] INFO server.RaftServer$Division: om3: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
scm1_1   | 2023-05-31 01:06:16,196 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm1_1   | 2023-05-31 01:06:16,205 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm1_1   | 2023-05-31 01:06:16,230 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/in_use.lock acquired by nodename 8@be0664a92293
scm1_1   | 2023-05-31 01:06:16,272 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=5, votedFor=7b17d04a-0b5d-44ed-b487-1bf0d959b978} from /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/raft-meta
om2_1    | 2023-05-31 01:06:48,014 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om2_1    | 2023-05-31 01:06:48,022 [main] INFO om.OzoneManager: Creating RPC Server
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
s3g_1    | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn3_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 2023-05-31 01:06:31,226 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-05-31 01:06:31,227 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-05-31 01:06:31,239 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
om1_1    | 2023-05-31 01:06:55,982 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@4c30cb93385d
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 2023-05-31 01:06:16,357 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServer$Division: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E: set configuration 61: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:06:16,365 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1    | 2023-05-31 01:06:48,111 [om2-groupManagement] INFO server.RaftServer$Division: om2: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om3_1    | 2023-05-31 01:06:47,610 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om3_1    | 2023-05-31 01:06:47,615 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
s3g_1    | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1    | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1    | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
s3g_1    | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
dn1_1    | 2023-05-31 01:06:31,253 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-05-31 01:06:56,000 [om1-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=4, votedFor=om1} from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/raft-meta
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
scm3_1   | 2023-05-31 01:06:49,256 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1    | 2023-05-31 01:06:48,170 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om2_1    | 2023-05-31 01:06:48,176 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om2_1    | 2023-05-31 01:06:48,176 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om2_1    | 2023-05-31 01:06:48,176 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1    | 2023-05-31 01:06:48,177 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | 2023-05-31 01:06:48,179 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
s3g_1    | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)
s3g_1    | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
dn1_1    | 2023-05-31 01:06:31,255 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-05-31 01:06:56,186 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-05-31 01:06:56,198 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm3_1   | 2023-05-31 01:06:49,294 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3_1   | 2023-05-31 01:06:49,299 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:38708 remote=scm1/10.9.0.14:9861]
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
s3g_1    | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1    | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1    | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn1_1    | 2023-05-31 01:06:32,018 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:32,063 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
scm2_1   | 2023-05-31 01:06:24,539 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm2_1   | 2023-05-31 01:06:24,539 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
om2_1    | 2023-05-31 01:06:48,254 [om2-groupManagement] INFO server.RaftServer$Division: om2@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om2_1    | 2023-05-31 01:06:48,268 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
s3g_1    | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn4_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 2023-05-31 01:06:32,076 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-05-31 01:06:32,079 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
om3_1    | 2023-05-31 01:06:47,615 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om3_1    | 2023-05-31 01:06:47,615 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1    | 2023-05-31 01:06:47,615 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1    | 2023-05-31 01:06:47,615 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1    | 2023-05-31 01:06:48,339 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
s3g_1    | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
om1_1    | 2023-05-31 01:06:56,256 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn1_1    | 2023-05-31 01:06:32,082 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3_1   | 2023-05-31 01:06:49,503 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
om3_1    | 2023-05-31 01:06:47,660 [om3-groupManagement] INFO server.RaftServer$Division: om3@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om3_1    | 2023-05-31 01:06:47,661 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1    | 2023-05-31 01:06:47,741 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn1_1    | 2023-05-31 01:06:32,088 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm3_1   | 2023-05-31 01:06:49,535 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3_1   | 2023-05-31 01:06:49,539 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
om3_1    | 2023-05-31 01:06:47,746 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om2_1    | 2023-05-31 01:06:48,341 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-05-31 01:06:15,613 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:15,620 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-05-31 01:06:56,256 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn1_1    | 2023-05-31 01:06:32,089 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-05-31 01:06:32,125 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3_1   | 2023-05-31 01:06:50,068 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
om3_1    | 2023-05-31 01:06:47,949 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
dn5_1    | 2023-05-31 01:06:15,624 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-05-31 01:06:56,265 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm1_1   | 2023-05-31 01:06:16,386 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1_1   | 2023-05-31 01:06:16,388 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-31 01:06:20,821 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-31 01:06:21,791 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-31 01:06:21,822 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-05-31 01:06:48,092 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om2_1    | 2023-05-31 01:06:48,557 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
dn5_1    | 2023-05-31 01:06:16,614 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-05-31 01:06:56,265 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:59370 remote=recon/10.9.0.22:9891]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
scm1_1   | 2023-05-31 01:06:16,392 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 2023-05-31 01:06:50,088 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
om3_1    | 2023-05-31 01:06:48,129 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om2_1    | 2023-05-31 01:06:48,649 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm2_1   | 2023-05-31 01:06:24,543 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2_1   | 2023-05-31 01:06:24,547 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-05-31 01:06:32,261 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-05-31 01:06:32,261 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm1_1   | 2023-05-31 01:06:16,395 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-05-31 01:06:22,792 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
scm3_1   | Container Balancer status:
om3_1    | 2023-05-31 01:06:48,132 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om3_1    | 2023-05-31 01:06:48,400 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om3_1    | 2023-05-31 01:06:49,095 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1    | 2023-05-31 01:06:49,226 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1    | 2023-05-31 01:06:49,226 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
s3g_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
s3g_1    | 2023-05-31 01:09:57,937 [qtp605052357-21] ERROR protocolPB.GrpcOmTransport: error unwrapping exception from OMResponse {}
dn1_1    | 2023-05-31 01:06:32,487 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9: set configuration 0: peers:[b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
scm1_1   | 2023-05-31 01:06:16,400 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
dn4_1    | 2023-05-31 01:06:22,823 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
scm3_1   | Key                            Value
scm2_1   | 2023-05-31 01:06:24,548 [Listener at 0.0.0.0/9860] INFO server.RaftServer: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2: start RPC server
om2_1    | 2023-05-31 01:06:48,782 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
dn5_1    | 2023-05-31 01:06:16,621 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-05-31 01:06:49,227 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
s3g_1    | 2023-05-31 01:09:58,514 [qtp605052357-21] WARN impl.MetricsSystemImpl: S3Gateway metrics system already initialized!
s3g_1    | 2023-05-31 01:11:04,543 [qtp605052357-24] INFO rpc.RpcClient: Creating Bucket: s3v/new2-bucket, with bucket layout OBJECT_STORE, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false, Replication Type set to server-side default replication type, Namespace Quota set to -1, Space Quota set to -1 
s3g_1    | 2023-05-31 01:11:05,866 [qtp605052357-19] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1_1   | 2023-05-31 01:06:16,408 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | 2023-05-31 01:06:16,410 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1    | 2023-05-31 01:06:56,281 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1    | 2023-05-31 01:06:56,313 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm3_1   | Running                        false
scm2_1   | 2023-05-31 01:06:24,593 [Listener at 0.0.0.0/9860] INFO server.GrpcService: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2: GrpcService started, listening on 9894
om2_1    | 2023-05-31 01:06:48,783 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om2_1    | 2023-05-31 01:06:49,103 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn5_1    | 2023-05-31 01:06:16,624 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-05-31 01:06:49,231 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-05-31 01:06:32,494 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: set configuration 0: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
om1_1    | 2023-05-31 01:06:56,313 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1    | 2023-05-31 01:06:56,314 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-05-31 01:06:56,352 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om1_1    | 2023-05-31 01:06:56,357 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om2_1    | 2023-05-31 01:06:49,634 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
dn5_1    | 2023-05-31 01:06:17,615 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-05-31 01:06:49,233 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om3_1    | 2023-05-31 01:06:50,177 [main] INFO reflections.Reflections: Reflections took 2466 ms to scan 8 urls, producing 24 keys and 616 values [using 2 cores]
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
scm1_1   | 2023-05-31 01:06:16,410 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-31 01:06:23,793 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 2023-05-31 01:06:56,357 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om1_1    | 2023-05-31 01:06:56,362 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1    | 2023-05-31 01:06:56,364 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm2_1   | 2023-05-31 01:06:24,607 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2: Started
om2_1    | 2023-05-31 01:06:49,675 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-05-31 01:06:17,621 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:18,616 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:18,624 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
scm1_1   | 2023-05-31 01:06:16,420 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e
dn4_1    | 2023-05-31 01:06:23,824 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-31 01:06:24,794 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
scm3_1   | Container Balancer Configuration values:
scm3_1   | Key                                                Value
scm3_1   | Threshold                                          10
om2_1    | 2023-05-31 01:06:49,683 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-05-31 01:06:18,664 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn1_1    | 2023-05-31 01:06:32,501 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/6a499bf5-10af-4dab-a3fd-a4e2998a83c9/current/log_0-0
dn1_1    | 2023-05-31 01:06:32,547 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: set configuration 0: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:06:16,421 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
dn4_1    | 2023-05-31 01:06:25,795 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn3_1    | 2023-05-31 01:06:22,613 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:22,613 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:23,614 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-05-31 01:06:49,686 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | java.net.SocketTimeoutException: Call From 51bd9aa00c0e/10.9.0.21 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:44286 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
scm1_1   | 2023-05-31 01:06:16,421 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
dn4_1    | 2023-05-31 01:06:26,796 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn3_1    | 2023-05-31 01:06:23,614 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:24,615 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-05-31 01:06:24,612 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm2_1   | 2023-05-31 01:06:24,612 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
om2_1    | 2023-05-31 01:06:49,688 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 2023-05-31 01:06:32,590 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9: set configuration 1: peers:[b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-31 01:06:32,676 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/6a499bf5-10af-4dab-a3fd-a4e2998a83c9/current/log_1-2
dn1_1    | 2023-05-31 01:06:32,678 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO segmented.LogSegment: Successfully read 5 entries from segment file /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/current/log_0-4
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
scm1_1   | 2023-05-31 01:06:16,422 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1_1   | 2023-05-31 01:06:16,423 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
dn4_1    | 2023-05-31 01:06:26,797 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn4_1    | java.net.ConnectException: Call From 955d7ba09e69/10.9.0.20 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn3_1    | 2023-05-31 01:06:24,615 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-05-31 01:06:24,614 [Listener at 0.0.0.0/9860] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm2_1   | 2023-05-31 01:06:24,614 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
om2_1    | 2023-05-31 01:06:49,756 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
om3_1    | 2023-05-31 01:06:50,953 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
scm1_1   | 2023-05-31 01:06:16,424 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | 2023-05-31 01:06:16,424 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm3_1   | Max Datanodes to Involve per Iteration(percent)    20
recon_1  | 2023-05-31 01:06:37,489 [IPC Server handler 5 on default port 9891] INFO ipc.Server: IPC Server handler 5 on default port 9891 caught an exception
dn3_1    | 2023-05-31 01:06:25,617 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-05-31 01:06:24,614 [Listener at 0.0.0.0/9860] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm2_1   | 2023-05-31 01:06:24,891 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om2_1    | 2023-05-31 01:06:51,488 [main] INFO reflections.Reflections: Reflections took 3175 ms to scan 8 urls, producing 24 keys and 616 values [using 2 cores]
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
om3_1    | 2023-05-31 01:06:51,024 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
scm1_1   | 2023-05-31 01:06:16,424 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3_1   | Max Size to Move per Iteration                     500GB
scm3_1   | Max Size Entering Target per Iteration             26GB
recon_1  | java.nio.channels.ClosedChannelException
dn3_1    | 2023-05-31 01:06:26,618 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-05-31 01:06:24,911 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm2_1   | 2023-05-31 01:06:24,911 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
om2_1    | 2023-05-31 01:06:52,019 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
om3_1    | 2023-05-31 01:06:53,173 [Listener at om3/9862] INFO om.OzoneManagerPrepareState: Deleted prepare marker file: /data/metadata/current/prepareMarker
dn1_1    | 2023-05-31 01:06:32,731 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: set configuration 5: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
scm1_1   | 2023-05-31 01:06:16,425 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3_1   | Max Size Leaving Source per Iteration              26GB
scm3_1   | 
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 2023-05-31 01:06:25,213 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm2_1   | 2023-05-31 01:06:25,224 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om2_1    | 2023-05-31 01:06:52,109 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
om3_1    | 2023-05-31 01:06:53,693 [Listener at om3/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om3_1    | 2023-05-31 01:06:53,814 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm1_1   | 2023-05-31 01:06:16,432 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm3_1   | 2023-05-31 01:06:50,089 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm3_1   | 2023-05-31 01:06:50,109 [Listener at 0.0.0.0/9860] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn3_1    | 2023-05-31 01:06:27,620 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:27,621 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
scm2_1   | 2023-05-31 01:06:25,228 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
om1_1    | 2023-05-31 01:06:56,365 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1    | 2023-05-31 01:06:56,368 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
om3_1    | 2023-05-31 01:06:53,815 [Listener at om3/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
dn1_1    | 2023-05-31 01:06:32,740 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO segmented.LogSegment: Successfully read 8 entries from segment file /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/current/log_0-7
scm1_1   | 2023-05-31 01:06:16,432 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 2023-05-31 01:06:50,116 [Listener at 0.0.0.0/9860] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
scm3_1   | 2023-05-31 01:06:50,116 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
dn3_1    | java.net.ConnectException: Call From dcc4205663d5/10.9.0.19 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
scm2_1   | 2023-05-31 01:06:25,419 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
om1_1    | 2023-05-31 01:06:56,368 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om2_1    | 2023-05-31 01:06:54,725 [Listener at om2/9862] INFO om.OzoneManagerPrepareState: Deleted prepare marker file: /data/metadata/current/prepareMarker
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
om3_1    | 2023-05-31 01:06:54,346 [Listener at om3/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om3/10.9.0.13:9862
om3_1    | 2023-05-31 01:06:54,353 [Listener at om3/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om3 at port 9872
dn1_1    | 2023-05-31 01:06:32,742 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO segmented.LogSegment: Successfully read 6 entries from segment file /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/current/log_5-10
dn1_1    | 2023-05-31 01:06:32,773 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: set configuration 8: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
om1_1    | 2023-05-31 01:06:56,368 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om2_1    | 2023-05-31 01:06:55,188 [Listener at om2/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn1_1    | 2023-05-31 01:06:32,787 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: set configuration 11: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:06:16,630 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
om3_1    | 2023-05-31 01:06:54,382 [om3-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@482910862efc
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
scm2_1   | 2023-05-31 01:06:25,421 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
om1_1    | 2023-05-31 01:06:56,407 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
om2_1    | 2023-05-31 01:06:55,261 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om2_1    | 2023-05-31 01:06:55,262 [Listener at om2/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn5_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 2023-05-31 01:06:19,108 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
om3_1    | 2023-05-31 01:06:54,430 [om3-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=4, votedFor=om3} from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/raft-meta
om3_1    | 2023-05-31 01:06:54,599 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-05-31 01:06:54,635 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om3_1    | 2023-05-31 01:06:54,684 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om3_1    | 2023-05-31 01:06:54,684 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-05-31 01:06:56,407 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
om2_1    | 2023-05-31 01:06:55,689 [Listener at om2/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om2/10.9.0.12:9862
dn1_1    | 2023-05-31 01:06:32,789 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9: set configuration 3: peers:[b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
scm2_1   | 2023-05-31 01:06:25,440 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2_1   | 2023-05-31 01:06:25,444 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm2_1   | 2023-05-31 01:06:25,751 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@38c460e8] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
om1_1    | 2023-05-31 01:06:56,472 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
om2_1    | 2023-05-31 01:06:55,690 [Listener at om2/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om2 at port 9872
dn1_1    | 2023-05-31 01:06:32,820 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/6a499bf5-10af-4dab-a3fd-a4e2998a83c9/current/log_inprogress_3
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
scm2_1   | 2023-05-31 01:06:25,754 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm2_1   | 2023-05-31 01:06:25,754 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm2_1   | 2023-05-31 01:06:25,782 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @7839ms to org.eclipse.jetty.util.log.Slf4jLog
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
om2_1    | 2023-05-31 01:06:55,730 [om2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 6@45ffe5496dee
om2_1    | 2023-05-31 01:06:55,776 [om2-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=4, votedFor=om1} from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/raft-meta
scm1_1   | 2023-05-31 01:06:16,631 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
scm2_1   | 2023-05-31 01:06:25,978 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm2_1   | 2023-05-31 01:06:25,990 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
om3_1    | 2023-05-31 01:06:54,689 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
om1_1    | 2023-05-31 01:06:56,476 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
om2_1    | 2023-05-31 01:06:55,917 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-31 01:06:32,864 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO segmented.LogSegment: Successfully read 14 entries from segment file /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/current/log_inprogress_11
scm1_1   | 2023-05-31 01:06:16,632 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm3_1   | 2023-05-31 01:06:50,127 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
scm2_1   | 2023-05-31 01:06:26,006 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
om3_1    | 2023-05-31 01:06:54,693 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
om1_1    | 2023-05-31 01:06:56,477 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om1_1    | 2023-05-31 01:06:56,511 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 109
om1_1    | 2023-05-31 01:06:56,511 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-05-31 01:06:32,895 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO segmented.LogSegment: Successfully read 21 entries from segment file /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/current/log_8-28
scm1_1   | 2023-05-31 01:06:16,664 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServer$Division: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E: set configuration 0: peers:[7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
scm3_1   | 2023-05-31 01:06:50,146 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/in_use.lock acquired by nodename 7@6bc445deac8a
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
scm2_1   | 2023-05-31 01:06:26,009 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
om1_1    | 2023-05-31 01:06:56,529 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: start as a follower, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-05-31 01:06:55,930 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1_1   | 2023-05-31 01:06:16,665 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/log_0-0
scm1_1   | 2023-05-31 01:06:16,666 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServer$Division: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E: set configuration 1: peers:[7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:06:16,682 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServer$Division: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E: set configuration 17: peers:[7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
om3_1    | 2023-05-31 01:06:54,699 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
scm2_1   | 2023-05-31 01:06:26,009 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm2_1   | 2023-05-31 01:06:26,010 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om1_1    | 2023-05-31 01:06:56,529 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from      null to FOLLOWER at term 4 for startAsFollower
om2_1    | 2023-05-31 01:06:55,966 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1_1   | 2023-05-31 01:06:16,687 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServer$Division: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E: set configuration 19: peers:[7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-31 01:06:32,898 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 24
scm3_1   | 2023-05-31 01:06:50,181 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=5, votedFor=} from /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/raft-meta
scm3_1   | 2023-05-31 01:06:50,431 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServer$Division: d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E: set configuration 61: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:538)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
scm2_1   | 2023-05-31 01:06:26,106 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm2_1   | 2023-05-31 01:06:26,112 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm2_1   | 2023-05-31 01:06:26,119 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
om2_1    | 2023-05-31 01:06:55,971 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-05-31 01:06:55,976 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-05-31 01:06:32,898 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 10
scm3_1   | 2023-05-31 01:06:50,449 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm3_1   | 2023-05-31 01:06:50,523 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om3_1    | 2023-05-31 01:06:54,726 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om3_1    | 2023-05-31 01:06:54,727 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1    | 2023-05-31 01:06:56,538 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
scm2_1   | 2023-05-31 01:06:26,239 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm2_1   | 2023-05-31 01:06:26,239 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm1_1   | 2023-05-31 01:06:16,688 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServer$Division: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E: set configuration 31: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
om2_1    | 2023-05-31 01:06:55,985 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-05-31 01:06:32,917 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
scm3_1   | 2023-05-31 01:06:50,523 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | Caused by: java.util.concurrent.TimeoutException
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn4_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
om1_1    | 2023-05-31 01:06:56,559 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm2_1   | 2023-05-31 01:06:26,241 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm2_1   | 2023-05-31 01:06:26,388 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@643f6179{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm1_1   | 2023-05-31 01:06:16,692 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServer$Division: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E: set configuration 33: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:06:16,698 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO segmented.LogSegment: Successfully read 48 entries from segment file /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/log_1-48
dn1_1    | 2023-05-31 01:06:32,921 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn1_1    | 2023-05-31 01:06:32,910 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: set configuration 29: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-31 01:06:50,530 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
om3_1    | 2023-05-31 01:06:54,730 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
scm2_1   | 2023-05-31 01:06:26,390 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@12fccff0{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn3_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
om2_1    | 2023-05-31 01:06:56,002 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 2023-05-31 01:06:50,546 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:44286 remote=recon/10.9.0.22:9891]
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
om2_1    | 2023-05-31 01:06:56,033 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om2_1    | 2023-05-31 01:06:56,033 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om2_1    | 2023-05-31 01:06:56,033 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-05-31 01:06:56,067 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om2@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
om1_1    | 2023-05-31 01:06:56,559 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 2023-05-31 01:06:56,564 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om1
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om2_1    | 2023-05-31 01:06:56,073 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn1_1    | 2023-05-31 01:06:32,968 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO segmented.LogSegment: Successfully read 14 entries from segment file /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/current/log_inprogress_29
dn1_1    | 2023-05-31 01:06:32,990 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 42
dn1_1    | 2023-05-31 01:06:32,990 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 28
dn1_1    | 2023-05-31 01:06:33,043 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:33,474 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9: start as a follower, conf=3: peers:[b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-31 01:06:33,475 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9: changes role from      null to FOLLOWER at term 3 for startAsFollower
om1_1    | 2023-05-31 01:06:56,570 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1   | 2023-05-31 01:06:16,700 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServer$Division: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E: set configuration 49: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn5_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 2023-05-31 01:06:33,476 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: start as a follower, conf=11: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-31 01:06:33,477 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: changes role from      null to FOLLOWER at term 11 for startAsFollower
dn1_1    | 2023-05-31 01:06:33,477 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: start as a follower, conf=29: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-31 01:06:33,504 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: changes role from      null to FOLLOWER at term 8 for startAsFollower
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
scm3_1   | 2023-05-31 01:06:50,593 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1    | 2023-05-31 01:06:54,742 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om3@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om3_1    | 2023-05-31 01:06:54,743 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om2_1    | 2023-05-31 01:06:56,075 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
dn2_1    | 	... 1 more
om1_1    | 2023-05-31 01:06:56,570 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
scm1_1   | 2023-05-31 01:06:16,702 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO segmented.LogSegment: Successfully read 12 entries from segment file /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/log_49-60
dn5_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm2_1   | 2023-05-31 01:06:26,607 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3313463c{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-12711300433662365222/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm2_1   | 2023-05-31 01:06:26,628 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@18d30e7{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm3_1   | 2023-05-31 01:06:50,651 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm3_1   | 2023-05-31 01:06:50,665 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm3_1   | 2023-05-31 01:06:50,690 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 2023-05-31 01:06:50,736 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e
dn2_1    | 2023-05-31 01:06:19,245 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:19,246 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-05-31 01:06:16,703 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServer$Division: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E: set configuration 61: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 2023-05-31 01:06:33,505 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2_1   | 2023-05-31 01:06:26,630 [Listener at 0.0.0.0/9860] INFO server.Server: Started @8687ms
scm2_1   | 2023-05-31 01:06:26,649 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
om3_1    | 2023-05-31 01:06:54,744 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om2_1    | 2023-05-31 01:06:56,080 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1    | 2023-05-31 01:06:56,092 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om2_1    | 2023-05-31 01:06:56,095 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-05-31 01:06:20,246 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-05-31 01:06:56,572 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om1_1    | 2023-05-31 01:06:56,576 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om1_1    | 2023-05-31 01:06:56,587 [Listener at om1/9862] INFO server.RaftServer: om1: start RPC server
dn1_1    | 2023-05-31 01:06:33,506 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState
om3_1    | 2023-05-31 01:06:54,746 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1    | 2023-05-31 01:06:54,747 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
recon_1  | 2023-05-31 01:06:37,490 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
om1_1    | 2023-05-31 01:06:57,070 [Listener at om1/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn1_1    | 2023-05-31 01:06:33,506 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-FollowerState
om2_1    | 2023-05-31 01:06:56,105 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om3_1    | 2023-05-31 01:06:54,747 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm3_1   | 2023-05-31 01:06:50,747 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm3_1   | 2023-05-31 01:06:50,751 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om1_1    | 2023-05-31 01:06:57,103 [Listener at om1/9862] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
om1_1    | 2023-05-31 01:06:57,114 [Listener at om1/9862] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
om1_1    | 2023-05-31 01:06:57,120 [Listener at om1/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om1_1    | 2023-05-31 01:06:57,133 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om1_1    | 2023-05-31 01:06:57,523 [Listener at om1/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
dn1_1    | 2023-05-31 01:06:33,545 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-DC0251522AF1,id=b20d0805-391e-447d-a646-35a5d1a5cbcf
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om2_1    | 2023-05-31 01:06:56,106 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om3_1    | 2023-05-31 01:06:54,750 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om3_1    | 2023-05-31 01:06:54,750 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om3_1    | 2023-05-31 01:06:54,751 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1_1   | 2023-05-31 01:06:16,746 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO segmented.LogSegment: Successfully read 20 entries from segment file /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/log_inprogress_61
scm1_1   | 2023-05-31 01:06:16,754 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 80
om1_1    | 2023-05-31 01:06:57,523 [Listener at om1/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om1_1    | 2023-05-31 01:06:57,756 [Listener at om1/9862] INFO util.log: Logging initialized @91947ms to org.eclipse.jetty.util.log.Slf4jLog
om1_1    | 2023-05-31 01:06:58,645 [Listener at om1/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn1_1    | 2023-05-31 01:06:33,552 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-05-31 01:06:33,567 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F1A4CF07E444,id=b20d0805-391e-447d-a646-35a5d1a5cbcf
dn1_1    | 2023-05-31 01:06:33,568 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-05-31 01:06:33,568 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
om2_1    | 2023-05-31 01:06:56,110 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om3_1    | 2023-05-31 01:06:54,780 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1_1   | 2023-05-31 01:06:16,754 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 60
dn2_1    | 2023-05-31 01:06:20,247 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:21,112 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn2_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
scm3_1   | 2023-05-31 01:06:50,756 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3_1   | 2023-05-31 01:06:50,764 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm3_1   | 2023-05-31 01:06:50,768 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-05-31 01:06:33,568 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1    | 2023-05-31 01:06:56,168 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om2_1    | 2023-05-31 01:06:56,173 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-05-31 01:06:56,213 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn1_1    | 2023-05-31 01:06:33,569 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | Caused by: java.net.ConnectException: Connection refused
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
om2_1    | 2023-05-31 01:06:56,240 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1_1   | 2023-05-31 01:06:16,836 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServer$Division: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E: start as a follower, conf=61: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-05-31 01:06:54,781 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 2023-05-31 01:06:50,772 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn1_1    | 2023-05-31 01:06:35,121 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@29a9edde] INFO util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1589ms
scm2_1   | 2023-05-31 01:06:26,650 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm2_1   | 2023-05-31 01:06:26,651 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm2_1   | 2023-05-31 01:06:28,023 [grpc-default-executor-0] INFO server.RaftServer$Division: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E: receive requestVote(PRE_VOTE, 7b17d04a-0b5d-44ed-b487-1bf0d959b978, group-6A492392CC3E, 5, (t:5, i:80))
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   | 2023-05-31 01:06:16,836 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServer$Division: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E: changes role from      null to FOLLOWER at term 5 for startAsFollower
scm1_1   | 2023-05-31 01:06:16,838 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO impl.RoleInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978: start 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-FollowerState
om1_1    | 2023-05-31 01:06:58,709 [Listener at om1/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | 2023-05-31 01:06:50,773 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3_1   | 2023-05-31 01:06:50,797 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3_1   | 2023-05-31 01:06:50,919 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm3_1   | 2023-05-31 01:06:50,923 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | GC pool 'ParNew' had collection(s): count=1 time=1527ms
scm2_1   | 2023-05-31 01:06:28,025 [grpc-default-executor-0] INFO impl.VoteContext: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-FOLLOWER: accept PRE_VOTE from 7b17d04a-0b5d-44ed-b487-1bf0d959b978: our priority 0 <= candidate's priority 0
scm2_1   | 2023-05-31 01:06:28,056 [grpc-default-executor-0] INFO server.RaftServer$Division: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E replies to PRE_VOTE vote request: 7b17d04a-0b5d-44ed-b487-1bf0d959b978<-4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2#0:OK-t5. Peer's state: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E:t5, leader=null, voted=7b17d04a-0b5d-44ed-b487-1bf0d959b978, raftlog=Memoized:4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-SegmentedRaftLog:OPENED:c80, conf=61: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-05-31 01:06:28,195 [grpc-default-executor-0] INFO server.RaftServer$Division: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E: receive requestVote(ELECTION, 7b17d04a-0b5d-44ed-b487-1bf0d959b978, group-6A492392CC3E, 6, (t:5, i:80))
om2_1    | 2023-05-31 01:06:56,245 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1_1   | 2023-05-31 01:06:16,840 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6A492392CC3E,id=7b17d04a-0b5d-44ed-b487-1bf0d959b978
scm1_1   | 2023-05-31 01:06:16,845 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1   | 2023-05-31 01:06:16,846 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | 2023-05-31 01:06:51,601 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3_1   | 2023-05-31 01:06:51,609 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-05-31 01:06:35,422 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm2_1   | 2023-05-31 01:06:28,195 [grpc-default-executor-0] INFO impl.VoteContext: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-FOLLOWER: accept ELECTION from 7b17d04a-0b5d-44ed-b487-1bf0d959b978: our priority 0 <= candidate's priority 0
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
scm1_1   | 2023-05-31 01:06:16,847 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om3_1    | 2023-05-31 01:06:54,814 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om1_1    | 2023-05-31 01:06:58,780 [Listener at om1/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn1_1    | 2023-05-31 01:06:35,368 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-A4E2998A83C9,id=b20d0805-391e-447d-a646-35a5d1a5cbcf
om2_1    | 2023-05-31 01:06:56,263 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 109
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:790)
om3_1    | 2023-05-31 01:06:54,815 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om1_1    | 2023-05-31 01:06:58,795 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om1_1    | 2023-05-31 01:06:58,795 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:538)
dn1_1    | 2023-05-31 01:06:35,247 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-05-31 01:06:56,263 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:363)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)
om3_1    | 2023-05-31 01:06:54,817 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om1_1    | 2023-05-31 01:06:58,795 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn5_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn5_1    | 2023-05-31 01:06:19,617 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-05-31 01:06:56,275 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: start as a follower, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-05-31 01:06:56,275 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from      null to FOLLOWER at term 4 for startAsFollower
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
dn4_1    | 	... 12 more
om3_1    | 2023-05-31 01:06:54,846 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 109
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn2_1    | Caused by: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm1_1   | 2023-05-31 01:06:16,848 [7b17d04a-0b5d-44ed-b487-1bf0d959b978-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1   | 2023-05-31 01:06:16,851 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-31 01:06:16,855 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-31 01:06:27,799 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-05-31 01:06:54,847 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn3_1    | Caused by: java.net.ConnectException: Connection refused
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
scm1_1   | 2023-05-31 01:06:16,856 [Listener at 0.0.0.0/9860] INFO server.RaftServer: 7b17d04a-0b5d-44ed-b487-1bf0d959b978: start RPC server
om2_1    | 2023-05-31 01:06:56,280 [om2-impl-thread1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
om2_1    | 2023-05-31 01:06:56,293 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-05-31 01:06:28,800 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-31 01:06:28,831 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
om3_1    | 2023-05-31 01:06:54,863 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: start as a follower, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-05-31 01:06:54,863 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from      null to FOLLOWER at term 4 for startAsFollower
om3_1    | 2023-05-31 01:06:54,868 [om3-impl-thread1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
om3_1    | 2023-05-31 01:06:54,879 [om3-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om3
scm2_1   | 2023-05-31 01:06:28,196 [grpc-default-executor-0] INFO server.RaftServer$Division: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E: changes role from  FOLLOWER to FOLLOWER at term 6 for candidate:7b17d04a-0b5d-44ed-b487-1bf0d959b978
scm1_1   | 2023-05-31 01:06:16,907 [Listener at 0.0.0.0/9860] INFO server.GrpcService: 7b17d04a-0b5d-44ed-b487-1bf0d959b978: GrpcService started, listening on 9894
scm1_1   | 2023-05-31 01:06:16,917 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-7b17d04a-0b5d-44ed-b487-1bf0d959b978: Started
scm1_1   | 2023-05-31 01:06:16,942 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn4_1    | java.net.SocketTimeoutException: Call From 955d7ba09e69/10.9.0.20 to scm2:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:54612 remote=scm2/10.9.0.15:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn3_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:790)
om3_1    | 2023-05-31 01:06:54,881 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm2_1   | 2023-05-31 01:06:28,196 [grpc-default-executor-0] INFO impl.RoleInfo: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2: shutdown 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-FollowerState
scm1_1   | 2023-05-31 01:06:16,943 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm1_1   | 2023-05-31 01:06:16,946 [Listener at 0.0.0.0/9860] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
dn1_1    | 2023-05-31 01:06:35,500 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
om3_1    | 2023-05-31 01:06:54,891 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm2_1   | 2023-05-31 01:06:28,197 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-FollowerState] INFO impl.FollowerState: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-FollowerState was interrupted
om2_1    | 2023-05-31 01:06:56,293 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-05-31 01:06:56,297 [om2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om2
om2_1    | 2023-05-31 01:06:56,301 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn5_1    | 2023-05-31 01:06:19,625 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:20,618 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:20,625 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
om3_1    | 2023-05-31 01:06:54,893 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2_1   | 2023-05-31 01:06:28,198 [grpc-default-executor-0] INFO impl.RoleInfo: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2: start 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-FollowerState
om2_1    | 2023-05-31 01:06:56,301 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om2_1    | 2023-05-31 01:06:56,304 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om2_1    | 2023-05-31 01:06:56,316 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om1_1    | 2023-05-31 01:06:59,169 [Listener at om1/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
scm3_1   | 2023-05-31 01:06:51,611 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn1_1    | 2023-05-31 01:06:35,500 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-05-31 01:06:35,246 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
om1_1    | 2023-05-31 01:06:59,201 [Listener at om1/9862] INFO http.HttpServer2: Jetty bound to port 9874
scm3_1   | 2023-05-31 01:06:51,717 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServer$Division: d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E: set configuration 0: peers:[7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 2023-05-31 01:06:37,499 [IPC Server handler 21 on default port 9891] INFO ipc.Server: IPC Server handler 21 on default port 9891 caught an exception
dn1_1    | 2023-05-31 01:06:35,246 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:35,246 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
scm1_1   | 2023-05-31 01:06:16,949 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
om1_1    | 2023-05-31 01:06:59,206 [Listener at om1/9862] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
scm3_1   | 2023-05-31 01:06:51,719 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/log_0-0
scm3_1   | 2023-05-31 01:06:51,732 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServer$Division: d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E: set configuration 1: peers:[7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | java.nio.channels.ClosedChannelException
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:363)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
dn3_1    | 	... 12 more
dn1_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
om1_1    | 2023-05-31 01:06:59,448 [Listener at om1/9862] INFO server.session: DefaultSessionIdManager workerName=node0
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
scm3_1   | 2023-05-31 01:06:51,787 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServer$Division: d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E: set configuration 17: peers:[7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
dn5_1    | 2023-05-31 01:06:21,619 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:21,626 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn3_1    | 2023-05-31 01:06:28,624 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:29,497 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn3_1    | 2023-05-31 01:06:29,518 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn2_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
om2_1    | 2023-05-31 01:06:56,357 [Listener at om2/9862] INFO server.RaftServer: om2: start RPC server
om2_1    | 2023-05-31 01:06:56,627 [Listener at om2/9862] INFO server.GrpcService: om2: GrpcService started, listening on 9872
om2_1    | 2023-05-31 01:06:56,635 [Listener at om2/9862] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
om2_1    | 2023-05-31 01:06:56,638 [Listener at om2/9862] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
scm1_1   | 2023-05-31 01:06:16,949 [Listener at 0.0.0.0/9860] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm3_1   | 2023-05-31 01:06:51,796 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServer$Division: d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E: set configuration 19: peers:[7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:06:21,635 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn5_1    | java.net.SocketTimeoutException: Call From 51bd9aa00c0e/10.9.0.21 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:44814 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 2023-05-31 01:06:28,229 [grpc-default-executor-0] INFO server.RaftServer$Division: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E replies to ELECTION vote request: 7b17d04a-0b5d-44ed-b487-1bf0d959b978<-4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2#0:OK-t6. Peer's state: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E:t6, leader=null, voted=7b17d04a-0b5d-44ed-b487-1bf0d959b978, raftlog=Memoized:4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-SegmentedRaftLog:OPENED:c80, conf=61: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-31 01:06:29,632 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:29,809 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
om1_1    | 2023-05-31 01:06:59,448 [Listener at om1/9862] INFO server.session: No SessionScavenger set, using defaults
om1_1    | 2023-05-31 01:06:59,457 [Listener at om1/9862] INFO server.session: node0 Scavenging every 660000ms
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm2_1   | 2023-05-31 01:06:28,631 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-server-thread1] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
dn3_1    | 2023-05-31 01:06:29,809 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 97734618-7966-42ac-b50e-e49ead6ce4d2
dn3_1    | 2023-05-31 01:06:29,886 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/9e886fff-1860-4674-bb59-7a8b4be06a04/in_use.lock acquired by nodename 7@dcc4205663d5
dn3_1    | 2023-05-31 01:06:29,892 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=97734618-7966-42ac-b50e-e49ead6ce4d2} from /data/metadata/ratis/9e886fff-1860-4674-bb59-7a8b4be06a04/current/raft-meta
scm3_1   | 2023-05-31 01:06:51,805 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServer$Division: d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E: set configuration 31: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1_1   | 2023-05-31 01:06:17,259 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm1_1   | 2023-05-31 01:06:17,309 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm1_1   | 2023-05-31 01:06:17,309 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
om2_1    | 2023-05-31 01:06:56,656 [Listener at om2/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om2_1    | 2023-05-31 01:06:56,675 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om2: Started
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm2_1   | 2023-05-31 01:06:28,631 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-server-thread1] INFO server.RaftServer$Division: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E: change Leader from null to 7b17d04a-0b5d-44ed-b487-1bf0d959b978 at term 6 for appendEntries, leader elected after 6384ms
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 2023-05-31 01:06:30,039 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04: set configuration 3: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-31 01:06:51,806 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServer$Division: d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E: set configuration 33: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:06:17,747 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
om2_1    | 2023-05-31 01:06:56,935 [Listener at om2/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om2_1    | 2023-05-31 01:06:56,944 [Listener at om2/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:538)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm2_1   | 2023-05-31 01:06:28,662 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-server-thread3] INFO server.RaftServer$Division: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E: set configuration 81: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-31 01:06:30,142 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO ratis.ContainerStateMachine: group-7A8B4BE06A04: Setting the last applied index to (t:3, i:4)
scm3_1   | 2023-05-31 01:06:51,818 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO segmented.LogSegment: Successfully read 48 entries from segment file /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/log_1-48
scm3_1   | 2023-05-31 01:06:51,831 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServer$Division: d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E: set configuration 49: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-31 01:06:51,840 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO segmented.LogSegment: Successfully read 12 entries from segment file /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/log_49-60
scm3_1   | 2023-05-31 01:06:51,842 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServer$Division: d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E: set configuration 61: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-31 01:06:52,367 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO segmented.LogSegment: Successfully read 20 entries from segment file /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/log_inprogress_61
scm3_1   | 2023-05-31 01:06:52,393 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO segmented.SegmentedRaftLogWorker: d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 80
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm2_1   | 2023-05-31 01:06:28,672 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2-server-thread3] INFO segmented.SegmentedRaftLogWorker: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-SegmentedRaftLogWorker: Rolling segment log-61_80 to index:80
om2_1    | 2023-05-31 01:06:57,068 [Listener at om2/9862] INFO util.log: Logging initialized @91922ms to org.eclipse.jetty.util.log.Slf4jLog
om2_1    | 2023-05-31 01:06:57,581 [Listener at om2/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om2_1    | 2023-05-31 01:06:57,610 [Listener at om2/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om2_1    | 2023-05-31 01:06:57,644 [Listener at om2/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm3_1   | 2023-05-31 01:06:52,393 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO segmented.SegmentedRaftLogWorker: d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 60
scm3_1   | 2023-05-31 01:06:52,975 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServer$Division: d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E: start as a follower, conf=61: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm2_1   | 2023-05-31 01:06:28,684 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/log_inprogress_61 to /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/log_61-80
dn3_1    | 2023-05-31 01:06:30,637 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:30,989 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-05-31 01:06:31,086 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1_1   | 2023-05-31 01:06:17,747 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-05-31 01:06:17,747 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm1_1   | 2023-05-31 01:06:18,006 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm3_1   | 2023-05-31 01:06:52,978 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServer$Division: d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E: changes role from      null to FOLLOWER at term 5 for startAsFollower
scm3_1   | 2023-05-31 01:06:52,984 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO impl.RoleInfo: d2fda184-2469-4fa2-af65-75268cb832ba: start d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm2_1   | 2023-05-31 01:06:28,798 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/log_inprogress_81
scm2_1   | 2023-05-31 01:06:28,892 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 2, healthy pipeline threshold count is 1
scm2_1   | 2023-05-31 01:06:28,898 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm2_1   | 2023-05-31 01:06:28,898 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
om1_1    | 2023-05-31 01:06:59,535 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@436a563f{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-05-31 01:06:31,088 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn4_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
scm3_1   | 2023-05-31 01:06:52,994 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6A492392CC3E,id=d2fda184-2469-4fa2-af65-75268cb832ba
scm3_1   | 2023-05-31 01:06:53,000 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn2_1    | 	... 1 more
scm2_1   | 2023-05-31 01:06:28,898 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm2_1   | 2023-05-31 01:06:28,914 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om3_1    | 2023-05-31 01:06:54,894 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om1_1    | 2023-05-31 01:06:59,541 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@772f3a3f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn1_1    | Caused by: java.util.concurrent.TimeoutException
dn3_1    | 2023-05-31 01:06:31,095 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
scm1_1   | 2023-05-31 01:06:18,051 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
om2_1    | 2023-05-31 01:06:57,656 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
scm3_1   | 2023-05-31 01:06:53,003 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:06:53,006 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm2_1   | 2023-05-31 01:06:28,919 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
dn2_1    | 2023-05-31 01:06:21,247 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-05-31 01:06:54,898 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om1_1    | 2023-05-31 01:07:00,429 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3e2c8ef{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-9886787402037626991/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn3_1    | 2023-05-31 01:06:31,102 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
scm3_1   | 2023-05-31 01:06:53,006 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm2_1   | 2023-05-31 01:06:29,417 [IPC Server handler 2 on default port 9861] WARN ipc.Server: IPC Server handler 2 on default port 9861, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:58198: output error
scm2_1   | 2023-05-31 01:06:29,424 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861 caught an exception
om1_1    | 2023-05-31 01:07:00,485 [Listener at om1/9862] INFO server.AbstractConnector: Started ServerConnector@57b711b6{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
dn3_1    | 2023-05-31 01:06:31,142 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm1_1   | 2023-05-31 01:06:18,054 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-05-31 01:06:18,059 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn2_1    | 2023-05-31 01:06:21,247 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn3_1    | 2023-05-31 01:06:31,184 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | 2023-05-31 01:06:18,530 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@38c460e8] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om2_1    | 2023-05-31 01:06:57,659 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm3_1   | 2023-05-31 01:06:53,007 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
scm2_1   | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn2_1    | 2023-05-31 01:06:21,295 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From 86166c590ca2/10.9.0.18 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:46874 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
om1_1    | 2023-05-31 01:07:00,486 [Listener at om1/9862] INFO server.Server: Started @94677ms
om1_1    | 2023-05-31 01:07:00,507 [Listener at om1/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn3_1    | 2023-05-31 01:06:31,184 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1_1   | 2023-05-31 01:06:18,555 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm1_1   | 2023-05-31 01:06:18,555 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 2023-05-31 01:06:53,009 [d2fda184-2469-4fa2-af65-75268cb832ba-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 2023-05-31 01:06:31,194 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-05-31 01:06:31,260 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/9e886fff-1860-4674-bb59-7a8b4be06a04
om2_1    | 2023-05-31 01:06:57,660 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn4_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:54612 remote=scm2/10.9.0.15:9861]
scm3_1   | 2023-05-31 01:06:53,023 [Listener at 0.0.0.0/9860] INFO server.RaftServer: d2fda184-2469-4fa2-af65-75268cb832ba: start RPC server
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn1_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm1_1   | 2023-05-31 01:06:18,639 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @53427ms to org.eclipse.jetty.util.log.Slf4jLog
scm1_1   | 2023-05-31 01:06:19,006 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om2_1    | 2023-05-31 01:06:58,095 [Listener at om2/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
scm3_1   | 2023-05-31 01:06:53,244 [Listener at 0.0.0.0/9860] INFO server.GrpcService: d2fda184-2469-4fa2-af65-75268cb832ba: GrpcService started, listening on 9894
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn3_1    | 2023-05-31 01:06:31,260 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
om1_1    | 2023-05-31 01:07:00,507 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om1_1    | 2023-05-31 01:07:00,509 [Listener at om1/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om1_1    | 2023-05-31 01:07:00,539 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-05-31 01:06:19,036 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
om2_1    | 2023-05-31 01:06:58,126 [Listener at om2/9862] INFO http.HttpServer2: Jetty bound to port 9874
scm3_1   | 2023-05-31 01:06:53,287 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-d2fda184-2469-4fa2-af65-75268cb832ba: Started
scm3_1   | 2023-05-31 01:06:53,330 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn3_1    | 2023-05-31 01:06:31,271 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
om1_1    | 2023-05-31 01:07:00,570 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om1_1    | 2023-05-31 01:07:00,640 [Listener at om1/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om1_1    | 2023-05-31 01:07:01,547 [Listener at om1/9862] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
scm1_1   | 2023-05-31 01:06:19,071 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
om2_1    | 2023-05-31 01:06:58,138 [Listener at om2/9862] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
scm3_1   | 2023-05-31 01:06:53,334 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
dn5_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 2023-05-31 01:06:37,499 [IPC Server handler 11 on default port 9891] INFO ipc.Server: IPC Server handler 11 on default port 9891 caught an exception
dn3_1    | 2023-05-31 01:06:31,280 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
om1_1    | 2023-05-31 01:07:01,600 [om1@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om1@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5062208650ns, electionTimeout:5033ms
om1_1    | 2023-05-31 01:07:01,617 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@4f2fdf3d] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om3_1    | 2023-05-31 01:06:54,901 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
scm1_1   | 2023-05-31 01:06:19,079 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm1_1   | 2023-05-31 01:06:19,079 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm3_1   | 2023-05-31 01:06:53,344 [Listener at 0.0.0.0/9860] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	... 1 more
recon_1  | java.nio.channels.ClosedChannelException
dn3_1    | 2023-05-31 01:06:31,280 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
om1_1    | 2023-05-31 01:07:01,607 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-FollowerState
om1_1    | 2023-05-31 01:07:01,625 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 4 for changeToCandidate
om3_1    | 2023-05-31 01:06:54,946 [Listener at om3/9862] INFO server.RaftServer: om3: start RPC server
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
scm1_1   | 2023-05-31 01:06:19,082 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om2_1    | 2023-05-31 01:06:58,501 [Listener at om2/9862] INFO server.session: DefaultSessionIdManager workerName=node0
scm3_1   | 2023-05-31 01:06:53,346 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm3_1   | 2023-05-31 01:06:53,347 [Listener at 0.0.0.0/9860] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn1_1    | 2023-05-31 01:06:35,142 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-05-31 01:06:35,652 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn3_1    | 2023-05-31 01:06:31,295 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1    | 2023-05-31 01:07:01,642 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om1_1    | 2023-05-31 01:07:01,648 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-LeaderElection1
om1_1    | 2023-05-31 01:07:01,731 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 4 for 71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn4_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
om2_1    | 2023-05-31 01:06:58,506 [Listener at om2/9862] INFO server.session: No SessionScavenger set, using defaults
scm3_1   | 2023-05-31 01:06:54,131 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-d2fda184-2469-4fa2-af65-75268cb832ba: Detected pause in JVM or host machine approximately 0.286s with 0.402s GC time.
scm3_1   | GC pool 'ParNew' had collection(s): count=1 time=402ms
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn1_1    | 2023-05-31 01:06:35,517 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-05-31 01:06:35,652 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn3_1    | 2023-05-31 01:06:31,305 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om1_1    | 2023-05-31 01:07:01,945 [om1@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om3
om3_1    | 2023-05-31 01:06:55,110 [Listener at om3/9862] INFO server.GrpcService: om3: GrpcService started, listening on 9872
om3_1    | 2023-05-31 01:06:55,119 [Listener at om3/9862] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
scm1_1   | 2023-05-31 01:06:19,183 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm1_1   | 2023-05-31 01:06:19,185 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm3_1   | 2023-05-31 01:06:55,212 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
om2_1    | 2023-05-31 01:06:58,516 [Listener at om2/9862] INFO server.session: node0 Scavenging every 600000ms
om2_1    | 2023-05-31 01:06:58,555 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6475e778{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om2_1    | 2023-05-31 01:06:58,560 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@51508029{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om2_1    | 2023-05-31 01:06:59,566 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@56499781{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-4293710475457113794/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn3_1    | 2023-05-31 01:06:31,305 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-05-31 01:06:31,315 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-05-31 01:06:31,402 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-05-31 01:06:31,403 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-05-31 01:06:19,188 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
scm3_1   | 2023-05-31 01:06:55,469 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn1_1    | 2023-05-31 01:06:35,500 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-05-31 01:06:35,652 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-05-31 01:06:35,653 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-05-31 01:06:35,653 [b20d0805-391e-447d-a646-35a5d1a5cbcf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn3_1    | 2023-05-31 01:06:31,623 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1_1   | 2023-05-31 01:06:19,265 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
scm3_1   | 2023-05-31 01:06:55,469 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm3_1   | 2023-05-31 01:06:56,614 [d2fda184-2469-4fa2-af65-75268cb832ba-server-thread1] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
om2_1    | 2023-05-31 01:06:59,670 [Listener at om2/9862] INFO server.AbstractConnector: Started ServerConnector@34eaf9c1{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om2_1    | 2023-05-31 01:06:59,671 [Listener at om2/9862] INFO server.Server: Started @94525ms
om2_1    | 2023-05-31 01:06:59,699 [Listener at om2/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn1_1    | 2023-05-31 01:06:35,733 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: b20d0805-391e-447d-a646-35a5d1a5cbcf: start RPC server
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn3_1    | 2023-05-31 01:06:31,624 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1_1   | 2023-05-31 01:06:19,265 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
scm3_1   | 2023-05-31 01:06:56,617 [d2fda184-2469-4fa2-af65-75268cb832ba-server-thread1] INFO server.RaftServer$Division: d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E: change Leader from null to 7b17d04a-0b5d-44ed-b487-1bf0d959b978 at term 6 for appendEntries, leader elected after 16771ms
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
om2_1    | 2023-05-31 01:06:59,700 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn1_1    | 2023-05-31 01:06:35,800 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: b20d0805-391e-447d-a646-35a5d1a5cbcf: GrpcService started, listening on 9858
dn1_1    | 2023-05-31 01:06:35,802 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: b20d0805-391e-447d-a646-35a5d1a5cbcf: GrpcService started, listening on 9856
dn1_1    | 2023-05-31 01:06:35,803 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: b20d0805-391e-447d-a646-35a5d1a5cbcf: GrpcService started, listening on 9857
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn3_1    | 2023-05-31 01:06:31,633 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
om3_1    | 2023-05-31 01:06:55,124 [Listener at om3/9862] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
om3_1    | 2023-05-31 01:06:55,129 [Listener at om3/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
scm1_1   | 2023-05-31 01:06:19,268 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
dn4_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
scm3_1   | 2023-05-31 01:06:56,660 [d2fda184-2469-4fa2-af65-75268cb832ba-server-thread2] INFO server.RaftServer$Division: d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E: Failed appendEntries as the first entry (index 0) already exists (snapshotIndex: 80, commitIndex: 80)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om2_1    | 2023-05-31 01:06:59,708 [Listener at om2/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om2_1    | 2023-05-31 01:06:59,714 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn3_1    | 2023-05-31 01:06:31,643 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-05-31 01:06:55,136 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om3: Started
scm1_1   | 2023-05-31 01:06:19,301 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@643f6179{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn4_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
scm3_1   | 2023-05-31 01:06:56,738 [d2fda184-2469-4fa2-af65-75268cb832ba-server-thread2] INFO server.RaftServer$Division: d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E: inconsistency entries. Reply:7b17d04a-0b5d-44ed-b487-1bf0d959b978<-d2fda184-2469-4fa2-af65-75268cb832ba#84:FAIL-t6,INCONSISTENCY,nextIndex=81,followerCommit=80,matchIndex=-1
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 2023-05-31 01:06:35,812 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis b20d0805-391e-447d-a646-35a5d1a5cbcf is started using port 9858 for RATIS
dn1_1    | 2023-05-31 01:06:35,812 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis b20d0805-391e-447d-a646-35a5d1a5cbcf is started using port 9857 for RATIS_ADMIN
dn1_1    | 2023-05-31 01:06:35,812 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis b20d0805-391e-447d-a646-35a5d1a5cbcf is started using port 9856 for RATIS_SERVER
dn1_1    | 2023-05-31 01:06:35,813 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-b20d0805-391e-447d-a646-35a5d1a5cbcf: Started
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn3_1    | 2023-05-31 01:06:32,399 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04: set configuration 0: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-05-31 01:07:01,957 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-31 01:06:19,304 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@12fccff0{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
scm3_1   | 2023-05-31 01:06:56,974 [d2fda184-2469-4fa2-af65-75268cb832ba-server-thread5] INFO server.RaftServer$Division: d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E: set configuration 81: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
om2_1    | 2023-05-31 01:06:59,766 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
dn1_1    | 2023-05-31 01:06:35,938 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn1_1    | 2023-05-31 01:06:35,948 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn3_1    | 2023-05-31 01:06:32,406 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/9e886fff-1860-4674-bb59-7a8b4be06a04/current/log_0-0
om1_1    | 2023-05-31 01:07:01,959 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 2023-05-31 01:07:01,970 [om1@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
om1_1    | 2023-05-31 01:07:03,094 [grpc-default-executor-2] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(PRE_VOTE, om2, group-D66704EFC61C, 4, (t:4, i:109))
scm1_1   | 2023-05-31 01:06:19,540 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3313463c{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-10466417513150314466/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
scm3_1   | 2023-05-31 01:06:56,984 [d2fda184-2469-4fa2-af65-75268cb832ba-server-thread5] INFO segmented.SegmentedRaftLogWorker: d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-SegmentedRaftLogWorker: Rolling segment log-61_80 to index:80
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
om2_1    | 2023-05-31 01:07:00,128 [Listener at om2/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
dn1_1    | 2023-05-31 01:06:36,517 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:37,518 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn3_1    | 2023-05-31 01:06:32,436 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04: set configuration 1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-05-31 01:06:55,286 [Listener at om3/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om3_1    | 2023-05-31 01:06:55,286 [Listener at om3/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
scm3_1   | 2023-05-31 01:06:57,020 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/log_inprogress_61 to /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/log_61-80
scm3_1   | 2023-05-31 01:06:57,185 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/log_inprogress_81
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn3_1    | 2023-05-31 01:06:32,452 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/9e886fff-1860-4674-bb59-7a8b4be06a04/current/log_1-2
om3_1    | 2023-05-31 01:06:55,363 [Listener at om3/9862] INFO util.log: Logging initialized @90526ms to org.eclipse.jetty.util.log.Slf4jLog
scm1_1   | 2023-05-31 01:06:19,552 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@18d30e7{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
om1_1    | 2023-05-31 01:07:03,111 [grpc-default-executor-3] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(PRE_VOTE, om3, group-D66704EFC61C, 4, (t:4, i:109))
om1_1    | 2023-05-31 01:07:03,145 [grpc-default-executor-2] INFO impl.VoteContext: om1@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om2: our priority 0 <= candidate's priority 0
dn2_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
om2_1    | 2023-05-31 01:07:00,725 [Listener at om2/9862] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om3_1    | 2023-05-31 01:06:55,917 [Listener at om3/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm1_1   | 2023-05-31 01:06:19,552 [Listener at 0.0.0.0/9860] INFO server.Server: Started @54340ms
dn5_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:44814 remote=scm1/10.9.0.14:9861]
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
om2_1    | 2023-05-31 01:07:00,767 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@d978ab9] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om3_1    | 2023-05-31 01:06:55,938 [Listener at om3/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
scm1_1   | 2023-05-31 01:06:19,556 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
scm3_1   | 2023-05-31 01:06:57,299 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 2, healthy pipeline threshold count is 1
scm2_1   | 2023-05-31 01:06:29,425 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:54612: output error
dn1_1    | 2023-05-31 01:06:37,639 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn3_1    | 2023-05-31 01:06:32,458 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04: set configuration 3: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-31 01:06:32,459 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/9e886fff-1860-4674-bb59-7a8b4be06a04/current/log_inprogress_3
om3_1    | 2023-05-31 01:06:55,972 [Listener at om3/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm1_1   | 2023-05-31 01:06:19,556 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
scm3_1   | 2023-05-31 01:06:57,323 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm3_1   | 2023-05-31 01:06:57,330 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
om1_1    | 2023-05-31 01:07:03,254 [grpc-default-executor-2] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to PRE_VOTE vote request: om2<-om1#0:OK-t4. Peer's state: om1@group-D66704EFC61C:t4, leader=null, voted=om1, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c109, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-05-31 01:06:29,446 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
dn1_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn3_1    | 2023-05-31 01:06:32,464 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn3_1    | 2023-05-31 01:06:32,464 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
om3_1    | 2023-05-31 01:06:55,979 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
scm1_1   | 2023-05-31 01:06:19,561 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn5_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
om2_1    | 2023-05-31 01:07:01,364 [om2@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om2@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5084074605ns, electionTimeout:5069ms
om2_1    | 2023-05-31 01:07:01,369 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-FollowerState
om2_1    | 2023-05-31 01:07:01,369 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 4 for changeToCandidate
om2_1    | 2023-05-31 01:07:01,379 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm2_1   | java.nio.channels.ClosedChannelException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
om3_1    | 2023-05-31 01:06:55,979 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm1_1   | 2023-05-31 01:06:21,999 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-FollowerState] INFO impl.FollowerState: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5160947606ns, electionTimeout:5142ms
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om2_1    | 2023-05-31 01:07:01,379 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-LeaderElection1
om1_1    | 2023-05-31 01:07:03,257 [grpc-default-executor-3] INFO impl.VoteContext: om1@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om3: our priority 0 <= candidate's priority 0
om1_1    | 2023-05-31 01:07:03,257 [grpc-default-executor-3] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to PRE_VOTE vote request: om3<-om1#0:OK-t4. Peer's state: om1@group-D66704EFC61C:t4, leader=null, voted=om1, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c109, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-05-31 01:07:03,280 [grpc-default-executor-3] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(ELECTION, om2, group-D66704EFC61C, 5, (t:4, i:109))
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
om3_1    | 2023-05-31 01:06:55,981 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om3_1    | 2023-05-31 01:06:56,158 [Listener at om3/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:46874 remote=scm1/10.9.0.14:9861]
scm1_1   | 2023-05-31 01:06:22,000 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-FollowerState] INFO impl.RoleInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978: shutdown 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-FollowerState
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn4_1    | 2023-05-31 01:06:29,454 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
om3_1    | 2023-05-31 01:06:56,171 [Listener at om3/9862] INFO http.HttpServer2: Jetty bound to port 9874
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn5_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
scm3_1   | 2023-05-31 01:06:57,332 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm3_1   | 2023-05-31 01:06:58,023 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1    | 2023-05-31 01:07:03,281 [grpc-default-executor-3] INFO impl.VoteContext: om1@group-D66704EFC61C-CANDIDATE: accept ELECTION from om2: our priority 0 <= candidate's priority 0
om2_1    | 2023-05-31 01:07:01,406 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 4 for 71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:06:22,000 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-FollowerState] INFO server.RaftServer$Division: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E: changes role from  FOLLOWER to CANDIDATE at term 5 for changeToCandidate
scm1_1   | 2023-05-31 01:06:22,003 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
dn4_1    | 2023-05-31 01:06:29,534 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
om3_1    | 2023-05-31 01:06:56,183 [Listener at om3/9862] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 2023-05-31 01:07:03,283 [grpc-default-executor-3] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from CANDIDATE to FOLLOWER at term 5 for candidate:om2
om1_1    | 2023-05-31 01:07:03,283 [grpc-default-executor-3] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-LeaderElection1
scm1_1   | 2023-05-31 01:06:22,003 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-FollowerState] INFO impl.RoleInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978: start 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection1
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:538)
om3_1    | 2023-05-31 01:06:56,618 [Listener at om3/9862] INFO server.session: DefaultSessionIdManager workerName=node0
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 2023-05-31 01:07:03,287 [grpc-default-executor-3] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
om2_1    | 2023-05-31 01:07:01,604 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-31 01:06:22,006 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection1] INFO impl.LeaderElection: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 5 for 61: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:06:22,086 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for d2fda184-2469-4fa2-af65-75268cb832ba
scm1_1   | 2023-05-31 01:06:22,086 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-05-31 01:06:29,802 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:32,652 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om1_1    | 2023-05-31 01:07:03,301 [grpc-default-executor-3] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to ELECTION vote request: om2<-om1#0:OK-t5. Peer's state: om1@group-D66704EFC61C:t5, leader=null, voted=om2, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c109, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-05-31 01:07:01,604 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn4_1    | 2023-05-31 01:06:29,885 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn3_1    | 2023-05-31 01:06:33,305 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04: start as a follower, conf=3: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
scm3_1   | 2023-05-31 01:06:58,159 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm3_1   | 2023-05-31 01:06:58,159 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn4_1    | 2023-05-31 01:06:29,885 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 3da1490a-d610-40de-95c1-bdfc259ba041
dn3_1    | 2023-05-31 01:06:33,307 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04: changes role from      null to FOLLOWER at term 3 for startAsFollower
recon_1  | 2023-05-31 01:06:37,499 [IPC Server handler 14 on default port 9891] INFO ipc.Server: IPC Server handler 14 on default port 9891 caught an exception
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn2_1    | 2023-05-31 01:06:22,247 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-05-31 01:06:30,031 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/in_use.lock acquired by nodename 6@955d7ba09e69
om1_1    | 2023-05-31 01:07:03,315 [grpc-default-executor-3] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(ELECTION, om3, group-D66704EFC61C, 5, (t:4, i:109))
om1_1    | 2023-05-31 01:07:03,315 [grpc-default-executor-3] INFO impl.VoteContext: om1@group-D66704EFC61C-FOLLOWER: reject ELECTION from om3: already has voted for om2 at current term 5
om1_1    | 2023-05-31 01:07:03,315 [grpc-default-executor-3] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to ELECTION vote request: om3<-om1#0:FAIL-t5. Peer's state: om1@group-D66704EFC61C:t5, leader=null, voted=om2, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c109, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-31 01:06:22,248 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:23,113 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn2_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn4_1    | 2023-05-31 01:06:30,057 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5509b2c4-975c-42f1-91be-39d463d01a51/in_use.lock acquired by nodename 6@955d7ba09e69
om1_1    | 2023-05-31 01:07:03,363 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1: PRE_VOTE DISCOVERED_A_NEW_TERM (term=5) received 1 response(s) and 0 exception(s):
om1_1    | 2023-05-31 01:07:03,364 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om1<-om2#0:OK-t5
dn3_1    | 2023-05-31 01:06:33,371 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO impl.RoleInfo: 97734618-7966-42ac-b50e-e49ead6ce4d2: start 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-FollowerState
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn1_1    | Caused by: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
om3_1    | 2023-05-31 01:06:56,618 [Listener at om3/9862] INFO server.session: No SessionScavenger set, using defaults
scm1_1   | 2023-05-31 01:06:22,088 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-31 01:06:30,059 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/in_use.lock acquired by nodename 6@955d7ba09e69
om1_1    | 2023-05-31 01:07:03,364 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result DISCOVERED_A_NEW_TERM (term=5)
dn3_1    | 2023-05-31 01:06:33,382 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-05-31 01:06:33,396 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-05-31 01:07:01,701 [om2@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
om2_1    | 2023-05-31 01:07:01,706 [om2@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om3
om2_1    | 2023-05-31 01:07:02,868 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(PRE_VOTE, om3, group-D66704EFC61C, 4, (t:4, i:109))
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
om3_1    | 2023-05-31 01:06:56,671 [Listener at om3/9862] INFO server.session: node0 Scavenging every 660000ms
om3_1    | 2023-05-31 01:06:56,852 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@356fa0d1{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om3_1    | 2023-05-31 01:06:56,877 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@76e00bdb{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn4_1    | 2023-05-31 01:06:30,090 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=8, votedFor=3da1490a-d610-40de-95c1-bdfc259ba041} from /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/current/raft-meta
om1_1    | 2023-05-31 01:07:03,785 [om1-server-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: change Leader from null to om2 at term 5 for appendEntries, leader elected after 15002ms
om1_1    | 2023-05-31 01:07:03,793 [om1-server-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 110: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-05-31 01:07:03,830 [om1-server-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:110
recon_1  | java.nio.channels.ClosedChannelException
scm3_1   | 2023-05-31 01:06:58,160 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:06:58,342 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm1_1   | 2023-05-31 01:06:22,096 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2
scm1_1   | 2023-05-31 01:06:22,563 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection1] INFO impl.LeaderElection: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om3_1    | 2023-05-31 01:06:57,721 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3ec7ad61{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-1702179737018690169/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
dn5_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn4_1    | 2023-05-31 01:06:30,092 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=3da1490a-d610-40de-95c1-bdfc259ba041} from /data/metadata/ratis/5509b2c4-975c-42f1-91be-39d463d01a51/current/raft-meta
dn3_1    | 2023-05-31 01:06:33,437 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-7A8B4BE06A04,id=97734618-7966-42ac-b50e-e49ead6ce4d2
om2_1    | 2023-05-31 01:07:02,893 [grpc-default-executor-1] INFO impl.VoteContext: om2@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om3: our priority 0 <= candidate's priority 0
om1_1    | 2023-05-31 01:07:04,111 [om1@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_110
om1_1    | 2023-05-31 01:07:04,562 [om1@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
scm3_1   | 2023-05-31 01:06:58,410 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3_1   | 2023-05-31 01:06:58,458 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 2023-05-31 01:06:22,563 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection1] INFO impl.LeaderElection: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om3_1    | 2023-05-31 01:06:57,790 [Listener at om3/9862] INFO server.AbstractConnector: Started ServerConnector@4b61e97{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
dn5_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn4_1    | 2023-05-31 01:06:30,128 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=11, votedFor=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd} from /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/current/raft-meta
dn3_1    | 2023-05-31 01:06:33,484 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om2_1    | 2023-05-31 01:07:02,933 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to PRE_VOTE vote request: om3<-om2#0:OK-t4. Peer's state: om2@group-D66704EFC61C:t4, leader=null, voted=om1, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c109, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-05-31 01:07:03,134 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
om2_1    | 2023-05-31 01:07:03,134 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om2<-om3#0:OK-t4
scm3_1   | 2023-05-31 01:06:58,875 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-d2fda184-2469-4fa2-af65-75268cb832ba: Detected pause in JVM or host machine approximately 0.109s without any GCs.
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn1_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm1_1   | 2023-05-31 01:06:22,564 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection1] INFO impl.LeaderElection: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection1: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
om3_1    | 2023-05-31 01:06:57,793 [Listener at om3/9862] INFO server.Server: Started @92954ms
dn4_1    | 2023-05-31 01:06:30,281 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1: set configuration 29: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
om1_1    | [id: "om1"
scm3_1   | 2023-05-31 01:06:59,936 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   | 2023-05-31 01:06:22,564 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection1] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om3_1    | 2023-05-31 01:06:57,817 [Listener at om3/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om2_1    | 2023-05-31 01:07:03,134 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result PASSED
om2_1    | 2023-05-31 01:07:03,147 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 ELECTION round 0: submit vote requests at term 5 for 71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-05-31 01:07:03,151 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-05-31 01:06:30,281 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51: set configuration 3: peers:[3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 2023-05-31 01:06:59,959 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn1_1    | 	... 1 more
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
om3_1    | 2023-05-31 01:06:57,817 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn5_1    | 2023-05-31 01:06:22,619 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn4_1    | 2023-05-31 01:06:30,281 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444: set configuration 11: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-31 01:06:59,965 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3_1   | 2023-05-31 01:06:59,976 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm3_1   | 2023-05-31 01:07:00,037 [IPC Server handler 9 on default port 9861] WARN ipc.Server: IPC Server handler 9 on default port 9861, call Call#23 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:44124: output error
scm1_1   | 2023-05-31 01:06:22,564 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection1] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
om3_1    | 2023-05-31 01:06:57,823 [Listener at om3/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
dn5_1    | 2023-05-31 01:06:22,626 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:23,620 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:23,627 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-05-31 01:07:03,151 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-05-31 01:07:03,192 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(ELECTION, om3, group-D66704EFC61C, 5, (t:4, i:109))
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om3_1    | 2023-05-31 01:06:57,832 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn5_1    | 2023-05-31 01:06:24,621 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:24,628 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:25,622 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-05-31 01:07:03,195 [grpc-default-executor-1] INFO impl.VoteContext: om2@group-D66704EFC61C-CANDIDATE: reject ELECTION from om3: already has voted for om2 at current term 5
om2_1    | 2023-05-31 01:07:03,197 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to ELECTION vote request: om3<-om2#0:FAIL-t5. Peer's state: om2@group-D66704EFC61C:t5, leader=null, voted=om2, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c109, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:06:22,565 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection1] INFO impl.LeaderElection: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection1 PRE_VOTE round 0: result REJECTED
scm3_1   | 2023-05-31 01:07:00,038 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#20 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:53032: output error
scm3_1   | 2023-05-31 01:07:00,038 [IPC Server handler 9 on default port 9861] INFO ipc.Server: IPC Server handler 9 on default port 9861 caught an exception
scm3_1   | java.nio.channels.ClosedChannelException
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
om3_1    | 2023-05-31 01:06:57,857 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
dn3_1    | 2023-05-31 01:06:33,487 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-05-31 01:06:33,488 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
om2_1    | 2023-05-31 01:07:03,270 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(PRE_VOTE, om1, group-D66704EFC61C, 4, (t:4, i:109))
om2_1    | 2023-05-31 01:07:03,270 [grpc-default-executor-1] INFO impl.VoteContext: om2@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om1: our priority 0 <= candidate's priority 0
om2_1    | 2023-05-31 01:07:03,270 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to PRE_VOTE vote request: om1<-om2#0:OK-t5. Peer's state: om2@group-D66704EFC61C:t5, leader=null, voted=om2, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c109, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:06:22,566 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection1] INFO server.RaftServer$Division: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E: changes role from CANDIDATE to FOLLOWER at term 5 for REJECTED
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn1_1    | 2023-05-31 01:06:38,519 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-31 01:06:30,383 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO ratis.ContainerStateMachine: group-39D463D01A51: Setting the last applied index to (t:3, i:4)
scm2_1   | 2023-05-31 01:06:29,471 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:42526: output error
scm2_1   | 2023-05-31 01:06:29,485 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
om3_1    | 2023-05-31 01:06:57,934 [Listener at om3/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
dn5_1    | 2023-05-31 01:06:26,623 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:27,624 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-05-31 01:07:03,339 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1: ELECTION PASSED received 2 response(s) and 0 exception(s):
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 2023-05-31 01:06:22,566 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection1] INFO impl.RoleInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978: shutdown 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection1
scm1_1   | 2023-05-31 01:06:22,567 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection1] INFO impl.RoleInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978: start 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-FollowerState
dn1_1    | 2023-05-31 01:06:39,521 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:40,522 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | java.nio.channels.ClosedChannelException
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:538)
om3_1    | 2023-05-31 01:06:58,834 [Listener at om3/9862] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om3_1    | 2023-05-31 01:06:58,870 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@86377d5] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn3_1    | 2023-05-31 01:06:33,490 [97734618-7966-42ac-b50e-e49ead6ce4d2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-05-31 01:06:33,534 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.RaftServer: 97734618-7966-42ac-b50e-e49ead6ce4d2: start RPC server
dn3_1    | 2023-05-31 01:06:33,566 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: 97734618-7966-42ac-b50e-e49ead6ce4d2: GrpcService started, listening on 9858
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm1_1   | 2023-05-31 01:06:27,757 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-FollowerState] INFO impl.FollowerState: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5190151197ns, electionTimeout:5175ms
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn1_1    | 2023-05-31 01:06:40,552 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO impl.FollowerState: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState: change to CANDIDATE, lastRpcElapsedTime:7046186161ns, electionTimeout:5051ms
dn1_1    | 2023-05-31 01:06:40,553 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState
om3_1    | 2023-05-31 01:06:59,948 [om3@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om3@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5083569099ns, electionTimeout:5054ms
om2_1    | 2023-05-31 01:07:03,340 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om2<-om1#0:OK-t5
om2_1    | 2023-05-31 01:07:03,340 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 1: om2<-om3#0:FAIL-t5
om2_1    | 2023-05-31 01:07:03,340 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 ELECTION round 0: result PASSED
om2_1    | 2023-05-31 01:07:03,340 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-LeaderElection1
scm1_1   | 2023-05-31 01:06:27,758 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-FollowerState] INFO impl.RoleInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978: shutdown 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-FollowerState
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn4_1    | 2023-05-31 01:06:30,389 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO ratis.ContainerStateMachine: group-F1A4CF07E444: Setting the last applied index to (t:11, i:24)
dn4_1    | 2023-05-31 01:06:30,400 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO ratis.ContainerStateMachine: group-DC0251522AF1: Setting the last applied index to (t:8, i:42)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
om3_1    | 2023-05-31 01:06:59,949 [om3@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-FollowerState
om3_1    | 2023-05-31 01:06:59,950 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 4 for changeToCandidate
om3_1    | 2023-05-31 01:06:59,953 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om3_1    | 2023-05-31 01:06:59,953 [om3@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-LeaderElection1
om3_1    | 2023-05-31 01:06:59,970 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 4 for 71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:06:27,758 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-FollowerState] INFO server.RaftServer$Division: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E: changes role from  FOLLOWER to CANDIDATE at term 5 for changeToCandidate
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn4_1    | 2023-05-31 01:06:30,806 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-31 01:06:31,344 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-05-31 01:06:40,554 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: changes role from  FOLLOWER to CANDIDATE at term 11 for changeToCandidate
om1_1    | address: "om1:9872"
om1_1    | startupRole: FOLLOWER
om1_1    | , id: "om3"
om1_1    | address: "om3:9872"
scm1_1   | 2023-05-31 01:06:27,758 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm1_1   | 2023-05-31 01:06:27,758 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-FollowerState] INFO impl.RoleInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978: start 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2
dn1_1    | 2023-05-31 01:06:40,559 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-05-31 01:06:40,559 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection1
om1_1    | startupRole: FOLLOWER
om1_1    | , id: "om2"
om1_1    | address: "om2:9872"
om1_1    | startupRole: FOLLOWER
om1_1    | ]
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn4_1    | 2023-05-31 01:06:31,379 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om2_1    | 2023-05-31 01:07:03,341 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from CANDIDATE to LEADER at term 5 for changeToLeader
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
om1_1    | 2023-05-31 01:09:25,348 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization started.
dn5_1    | 2023-05-31 01:06:28,625 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:28,626 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn1_1    | 2023-05-31 01:06:40,589 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-FollowerState] INFO impl.FollowerState: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-FollowerState: change to CANDIDATE, lastRpcElapsedTime:7083339086ns, electionTimeout:5087ms
dn4_1    | 2023-05-31 01:06:31,397 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om2_1    | 2023-05-31 01:07:03,344 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: change Leader from null to om2 at term 5 for becomeLeader, leader elected after 14784ms
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
om1_1    | 2023-05-31 01:09:25,354 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HSYNC.
om1_1    | 2023-05-31 01:09:25,362 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature HSYNC has been finalized.
dn5_1    | java.net.ConnectException: Call From 51bd9aa00c0e/10.9.0.21 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn1_1    | 2023-05-31 01:06:40,610 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-FollowerState
dn4_1    | 2023-05-31 01:06:31,407 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om2_1    | 2023-05-31 01:07:03,364 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm1_1   | 2023-05-31 01:06:27,759 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO impl.LeaderElection: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 5 for 61: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 2023-05-31 01:06:40,611 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-FollowerState] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn4_1    | 2023-05-31 01:06:31,407 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 2023-05-31 01:06:27,760 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO impl.LeaderElection: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
om1_1    | 2023-05-31 01:09:25,362 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: FILESYSTEM_SNAPSHOT.
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 2023-05-31 01:06:40,611 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-05-31 01:06:33,633 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: 97734618-7966-42ac-b50e-e49ead6ce4d2: GrpcService started, listening on 9856
dn3_1    | 2023-05-31 01:06:33,654 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:33,656 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: 97734618-7966-42ac-b50e-e49ead6ce4d2: GrpcService started, listening on 9857
dn3_1    | 2023-05-31 01:06:33,708 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-97734618-7966-42ac-b50e-e49ead6ce4d2: Started
dn4_1    | 2023-05-31 01:06:31,409 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 2023-05-31 01:06:28,122 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO impl.LeaderElection: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 1 exception(s):
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om1_1    | 2023-05-31 01:09:25,366 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature FILESYSTEM_SNAPSHOT has been finalized.
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 2023-05-31 01:06:40,611 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2
dn3_1    | 2023-05-31 01:06:33,725 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 97734618-7966-42ac-b50e-e49ead6ce4d2 is started using port 9858 for RATIS
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn4_1    | 2023-05-31 01:06:31,411 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | Caused by: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 2023-05-31 01:09:25,367 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 2023-05-31 01:06:40,639 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection1] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 11 for 11: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-31 01:06:40,647 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-31 01:06:40,667 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2 PRE_VOTE round 0: result PASSED (term=3)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn4_1    | 2023-05-31 01:06:31,411 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm1_1   | 2023-05-31 01:06:28,126 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO impl.LeaderElection:   Response 0: 7b17d04a-0b5d-44ed-b487-1bf0d959b978<-4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2#0:OK-t5
scm1_1   | 2023-05-31 01:06:28,126 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 2023-05-31 01:09:25,367 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn3_1    | 2023-05-31 01:06:33,732 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 97734618-7966-42ac-b50e-e49ead6ce4d2 is started using port 9857 for RATIS_ADMIN
dn3_1    | 2023-05-31 01:06:33,734 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 97734618-7966-42ac-b50e-e49ead6ce4d2 is started using port 9856 for RATIS_SERVER
om2_1    | 2023-05-31 01:07:03,373 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn4_1    | 2023-05-31 01:06:31,414 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm1_1   | 2023-05-31 01:06:28,127 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO impl.LeaderElection: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2 PRE_VOTE round 0: result PASSED
scm1_1   | 2023-05-31 01:06:28,168 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO impl.LeaderElection: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2 ELECTION round 0: submit vote requests at term 6 for 61: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om1_1    | 2023-05-31 01:09:25,374 [OMDoubleBufferFlushThread] INFO upgrade.OMFinalizeUpgradeResponse: Layout version to persist to DB : 5
dn3_1    | 2023-05-31 01:06:33,899 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
om2_1    | 2023-05-31 01:07:03,374 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om2_1    | 2023-05-31 01:07:03,414 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn4_1    | 2023-05-31 01:06:31,418 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	... 1 more
dn2_1    | 2023-05-31 01:06:23,248 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-05-31 01:06:28,176 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO impl.LeaderElection: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:28,238 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO impl.LeaderElection: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2: ELECTION PASSED received 1 response(s) and 1 exception(s):
recon_1  | 2023-05-31 01:06:37,499 [IPC Server handler 12 on default port 9891] INFO ipc.Server: IPC Server handler 12 on default port 9891 caught an exception
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
om1_1    | 2023-05-31 01:09:57,904 [grpc-default-executor-0] ERROR om.OzoneManagerServiceGrpc: Failed to submit request
dn3_1    | 2023-05-31 01:06:33,957 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-05-31 01:06:34,663 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:35,667 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn4_1    | 2023-05-31 01:06:31,415 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om2_1    | 2023-05-31 01:07:03,414 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om2_1    | 2023-05-31 01:07:03,425 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn4_1    | 2023-05-31 01:06:31,421 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1    | com.google.protobuf.ServiceException: org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Suggested leader is OM:om2[om2/10.9.0.12].
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:250)
om3_1    | 2023-05-31 01:07:00,029 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-05-31 01:07:00,029 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-05-31 01:07:00,040 [om3@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn3_1    | 2023-05-31 01:06:36,668 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-05-31 01:07:03,432 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om2_1    | 2023-05-31 01:07:03,438 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn4_1    | 2023-05-31 01:06:31,422 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-05-31 01:06:40,845 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:232)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:225)
om3_1    | 2023-05-31 01:07:00,044 [om3@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
recon_1  | java.nio.channels.ClosedChannelException
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn3_1    | 2023-05-31 01:06:37,441 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-97734618-7966-42ac-b50e-e49ead6ce4d2: Detected pause in JVM or host machine approximately 0.134s without any GCs.
dn3_1    | 2023-05-31 01:06:37,669 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:38,623 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-FollowerState] INFO impl.FollowerState: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5253095608ns, electionTimeout:5050ms
dn4_1    | 2023-05-31 01:06:31,426 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-05-31 01:06:40,880 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:177)
scm1_1   | 2023-05-31 01:06:28,240 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO impl.LeaderElection:   Response 0: 7b17d04a-0b5d-44ed-b487-1bf0d959b978<-4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2#0:OK-t6
om3_1    | 2023-05-31 01:07:02,999 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(PRE_VOTE, om2, group-D66704EFC61C, 4, (t:4, i:109))
om3_1    | 2023-05-31 01:07:03,015 [grpc-default-executor-0] INFO impl.VoteContext: om3@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om2: our priority 0 <= candidate's priority 0
om3_1    | 2023-05-31 01:07:03,056 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
om3_1    | 2023-05-31 01:07:03,074 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om3<-om2#0:OK-t4
om3_1    | 2023-05-31 01:07:03,074 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result PASSED
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn3_1    | 2023-05-31 01:06:38,645 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-97734618-7966-42ac-b50e-e49ead6ce4d2: Detected pause in JVM or host machine approximately 0.193s with 0.172s GC time.
dn3_1    | GC pool 'ParNew' had collection(s): count=1 time=172ms
dn3_1    | 2023-05-31 01:06:38,628 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-FollowerState] INFO impl.RoleInfo: 97734618-7966-42ac-b50e-e49ead6ce4d2: shutdown 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-FollowerState
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
om1_1    | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
om3_1    | 2023-05-31 01:07:03,097 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to PRE_VOTE vote request: om2<-om3#0:OK-t4. Peer's state: om3@group-D66704EFC61C:t4, leader=null, voted=om3, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c109, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-05-31 01:07:03,108 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 ELECTION round 0: submit vote requests at term 5 for 71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-05-31 01:07:03,136 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-05-31 01:06:23,249 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-05-31 01:07:03,466 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn4_1    | 2023-05-31 01:06:31,441 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-05-31 01:06:31,448 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om1_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerServiceGrpc.submitRequest(OzoneManagerServiceGrpc.java:87)
om1_1    | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerServiceGrpc$MethodHandlers.invoke(OzoneManagerServiceGrpc.java:237)
scm1_1   | 2023-05-31 01:06:28,240 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn3_1    | 2023-05-31 01:06:38,645 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-FollowerState] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn3_1    | 2023-05-31 01:06:38,676 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:24,249 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:24,249 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:25,116 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn2_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
scm3_1   | 2023-05-31 01:07:00,040 [IPC Server handler 10 on default port 9861] WARN ipc.Server: IPC Server handler 10 on default port 9861, call Call#19 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:40896: output error
scm3_1   | 2023-05-31 01:07:00,040 [IPC Server handler 10 on default port 9861] INFO ipc.Server: IPC Server handler 10 on default port 9861 caught an exception
scm1_1   | 2023-05-31 01:06:28,240 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO impl.LeaderElection: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2 ELECTION round 0: result PASSED
om3_1    | 2023-05-31 01:07:03,136 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-05-31 01:07:03,177 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(ELECTION, om2, group-D66704EFC61C, 5, (t:4, i:109))
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn3_1    | 2023-05-31 01:06:38,686 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-05-31 01:06:38,699 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-FollowerState] INFO impl.RoleInfo: 97734618-7966-42ac-b50e-e49ead6ce4d2: start 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1
scm3_1   | java.nio.channels.ClosedChannelException
om1_1    | 	at io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
scm1_1   | 2023-05-31 01:06:28,246 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO impl.RoleInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978: shutdown 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn3_1    | 2023-05-31 01:06:38,719 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1] INFO impl.LeaderElection: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-31 01:06:38,720 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1] INFO impl.LeaderElection: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1 PRE_VOTE round 0: result PASSED (term=3)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
om1_1    | 	at io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
scm1_1   | 2023-05-31 01:06:28,246 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO server.RaftServer$Division: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E: changes role from CANDIDATE to LEADER at term 6 for changeToLeader
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn4_1    | 2023-05-31 01:06:31,457 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-05-31 01:06:38,816 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1] INFO impl.LeaderElection: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1 ELECTION round 0: submit vote requests at term 4 for 3: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om1_1    | 	at io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
scm1_1   | 2023-05-31 01:06:28,247 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO ha.SCMStateMachine: current SCM becomes leader of term 6.
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
om2_1    | 2023-05-31 01:07:03,467 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-05-31 01:07:03,468 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om2_1    | 2023-05-31 01:07:03,471 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn4_1    | 2023-05-31 01:06:31,462 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-05-31 01:06:31,488 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
om1_1    | 	at io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)
scm1_1   | 2023-05-31 01:06:28,248 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,6>
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn4_1    | 2023-05-31 01:06:31,490 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-05-31 01:06:38,817 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1] INFO impl.LeaderElection: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1 ELECTION round 0: result PASSED (term=4)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn1_1    | 2023-05-31 01:06:40,883 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
scm1_1   | 2023-05-31 01:06:28,259 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO server.RaftServer$Division: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E: change Leader from null to 7b17d04a-0b5d-44ed-b487-1bf0d959b978 at term 6 for becomeLeader, leader elected after 15264ms
dn5_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
om2_1    | 2023-05-31 01:07:03,473 [om2@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om2_1    | 2023-05-31 01:07:03,478 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1    | 2023-05-31 01:07:03,479 [om2@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
om2_1    | 2023-05-31 01:07:03,479 [om2@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
om2_1    | 2023-05-31 01:07:03,479 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1    | 	at io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm1_1   | 2023-05-31 01:06:28,289 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn4_1    | 2023-05-31 01:06:31,491 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-05-31 01:06:38,818 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1] INFO impl.RoleInfo: 97734618-7966-42ac-b50e-e49ead6ce4d2: shutdown 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1
om1_1    | 	at io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm1_1   | 2023-05-31 01:06:28,305 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn4_1    | 2023-05-31 01:06:31,495 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-05-31 01:06:38,819 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
om1_1    | 	at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm2_1   | 2023-05-31 01:06:34,517 [IPC Server handler 0 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/97734618-7966-42ac-b50e-e49ead6ce4d2
scm2_1   | 2023-05-31 01:06:34,520 [IPC Server handler 0 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 97734618-7966-42ac-b50e-e49ead6ce4d2{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-05-31 01:06:34,576 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
om2_1    | 2023-05-31 01:07:03,480 [om2@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-05-31 01:06:31,495 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-05-31 01:06:38,819 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-7A8B4BE06A04 with new leaderId: 97734618-7966-42ac-b50e-e49ead6ce4d2
om1_1    | 	at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn1_1    | 2023-05-31 01:06:40,887 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO impl.FollowerState: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState: change to CANDIDATE, lastRpcElapsedTime:7382107812ns, electionTimeout:5195ms
scm1_1   | 2023-05-31 01:06:28,307 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om3_1    | 2023-05-31 01:07:03,178 [grpc-default-executor-0] INFO impl.VoteContext: om3@group-D66704EFC61C-CANDIDATE: reject ELECTION from om2: already has voted for om3 at current term 5
om3_1    | 2023-05-31 01:07:03,179 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to ELECTION vote request: om2<-om3#0:FAIL-t5. Peer's state: om3@group-D66704EFC61C:t5, leader=null, voted=om3, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c109, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-05-31 01:07:03,306 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(PRE_VOTE, om1, group-D66704EFC61C, 4, (t:4, i:109))
om3_1    | 2023-05-31 01:07:03,306 [grpc-default-executor-0] INFO impl.VoteContext: om3@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om1: our priority 0 <= candidate's priority 0
dn4_1    | 2023-05-31 01:06:31,494 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-05-31 01:06:34,583 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm2_1   | 2023-05-31 01:06:34,661 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm2_1   | 2023-05-31 01:06:34,638 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm2_1   | 2023-05-31 01:06:34,698 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-05-31 01:06:28,339 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm1_1   | 2023-05-31 01:06:28,341 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 2023-05-31 01:07:00,052 [IPC Server handler 5 on default port 9861] WARN ipc.Server: IPC Server handler 5 on default port 9861, call Call#20 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:44582: output error
scm3_1   | 2023-05-31 01:07:00,052 [IPC Server handler 5 on default port 9861] INFO ipc.Server: IPC Server handler 5 on default port 9861 caught an exception
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-05-31 01:06:31,504 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-05-31 01:06:40,897 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState
scm2_1   | 2023-05-31 01:06:35,189 [IPC Server handler 3 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/4bc93e86-8989-4282-b7d8-050f4a19b92f
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
dn5_1    | Caused by: java.net.ConnectException: Connection refused
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
om2_1    | 2023-05-31 01:07:03,485 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om2_1    | 2023-05-31 01:07:03,486 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-31 01:06:31,504 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-05-31 01:06:40,897 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: changes role from  FOLLOWER to CANDIDATE at term 8 for changeToCandidate
scm3_1   | java.nio.channels.ClosedChannelException
scm2_1   | 2023-05-31 01:06:35,233 [IPC Server handler 3 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 4bc93e86-8989-4282-b7d8-050f4a19b92f{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Suggested leader is OM:om2[om2/10.9.0.12].
dn3_1    | 2023-05-31 01:06:38,820 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04: change Leader from null to 97734618-7966-42ac-b50e-e49ead6ce4d2 at term 4 for becomeLeader, leader elected after 34353ms
scm1_1   | 2023-05-31 01:06:28,342 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn4_1    | 2023-05-31 01:06:31,504 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-05-31 01:06:40,897 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 2023-05-31 01:06:35,249 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:538)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:245)
dn3_1    | 2023-05-31 01:06:38,874 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1_1   | 2023-05-31 01:06:28,360 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om2_1    | 2023-05-31 01:07:03,486 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om2_1    | 2023-05-31 01:07:03,487 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om2_1    | 2023-05-31 01:07:03,489 [om2@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om3_1    | 2023-05-31 01:07:03,306 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to PRE_VOTE vote request: om1<-om3#0:OK-t5. Peer's state: om3@group-D66704EFC61C:t5, leader=null, voted=om3, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c109, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-05-31 01:07:03,330 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1: ELECTION REJECTED received 2 response(s) and 0 exception(s):
dn4_1    | 2023-05-31 01:06:31,541 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444
dn1_1    | 2023-05-31 01:06:40,898 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection3
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 2023-05-31 01:06:35,265 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | 	... 18 more
dn3_1    | 2023-05-31 01:06:38,883 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm1_1   | 2023-05-31 01:06:28,365 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
om2_1    | 2023-05-31 01:07:03,489 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1    | 2023-05-31 01:07:03,331 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om3<-om1#0:FAIL-t5
om3_1    | 2023-05-31 01:07:03,331 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 1: om3<-om2#0:FAIL-t5
dn4_1    | 2023-05-31 01:06:31,541 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm2_1   | 2023-05-31 01:06:35,271 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn2_1    | Caused by: java.util.concurrent.TimeoutException
om1_1    | 2023-05-31 01:10:38,509 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:new2-volume for user:hadoop
dn3_1    | 2023-05-31 01:06:38,900 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
scm1_1   | 2023-05-31 01:06:28,425 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)
dn4_1    | 2023-05-31 01:06:31,542 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm2_1   | 2023-05-31 01:06:35,270 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm2_1   | 2023-05-31 01:06:36,451 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
om1_1    | 2023-05-31 01:10:42,397 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: new2-volume
dn3_1    | 2023-05-31 01:06:38,906 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
scm1_1   | 2023-05-31 01:06:28,431 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-05-31 01:07:03,489 [om2@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
om2_1    | 2023-05-31 01:07:03,489 [om2@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
om2_1    | 2023-05-31 01:07:03,490 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn4_1    | 2023-05-31 01:06:31,543 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-05-31 01:06:40,888 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2 ELECTION round 0: submit vote requests at term 4 for 3: peers:[b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm2_1   | 2023-05-31 01:06:36,454 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-05-31 01:06:36,656 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
om1_1    | 2023-05-31 01:10:54,680 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: s3v
dn3_1    | 2023-05-31 01:06:38,923 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1_1   | 2023-05-31 01:06:28,436 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om2_1    | 2023-05-31 01:07:03,490 [om2@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om3_1    | 2023-05-31 01:07:03,331 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 ELECTION round 0: result REJECTED
om3_1    | 2023-05-31 01:07:03,332 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from CANDIDATE to FOLLOWER at term 5 for REJECTED
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn1_1    | 2023-05-31 01:06:40,912 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection3] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 8 for 29: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-31 01:06:40,883 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 3da1490a-d610-40de-95c1-bdfc259ba041
scm2_1   | 2023-05-31 01:06:36,656 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
om1_1    | 2023-05-31 01:11:04,614 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:new2-bucket in volume:s3v
dn3_1    | 2023-05-31 01:06:38,925 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1_1   | 2023-05-31 01:06:28,444 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm1_1   | 2023-05-31 01:06:28,445 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om2_1    | 2023-05-31 01:07:03,493 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-LeaderStateImpl
dn4_1    | 2023-05-31 01:06:31,549 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
om1_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om1_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:214)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:790)
scm1_1   | 2023-05-31 01:06:28,445 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
om2_1    | 2023-05-31 01:07:03,505 [om2@group-D66704EFC61C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:110
dn4_1    | 2023-05-31 01:06:31,549 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1
scm2_1   | 2023-05-31 01:06:36,701 [IPC Server handler 10 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:338)
dn3_1    | 2023-05-31 01:06:38,953 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-05-31 01:06:38,978 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn3_1    | 2023-05-31 01:06:38,980 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1] INFO impl.RoleInfo: 97734618-7966-42ac-b50e-e49ead6ce4d2: start 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderStateImpl
dn3_1    | 2023-05-31 01:06:39,013 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn3_1    | 2023-05-31 01:06:39,036 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/9e886fff-1860-4674-bb59-7a8b4be06a04/current/log_inprogress_3 to /data/metadata/ratis/9e886fff-1860-4674-bb59-7a8b4be06a04/current/log_3-4
dn3_1    | 2023-05-31 01:06:39,046 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderElection1] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04: set configuration 5: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-31 01:06:40,913 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2 ELECTION round 0: result PASSED (term=4)
om2_1    | 2023-05-31 01:07:03,575 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 110: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:06:31,555 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm2_1   | 2023-05-31 01:06:36,702 [IPC Server handler 10 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om1_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:567)
recon_1  | 2023-05-31 01:06:37,499 [IPC Server handler 10 on default port 9891] INFO ipc.Server: IPC Server handler 10 on default port 9891 caught an exception
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:363)
om3_1    | 2023-05-31 01:07:03,332 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-LeaderElection1
scm1_1   | 2023-05-31 01:06:28,446 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn3_1    | 2023-05-31 01:06:39,120 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9e886fff-1860-4674-bb59-7a8b4be06a04/current/log_inprogress_5
dn1_1    | 2023-05-31 01:06:40,942 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2
dn1_1    | 2023-05-31 01:06:40,949 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn1_1    | 2023-05-31 01:06:40,949 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-A4E2998A83C9 with new leaderId: b20d0805-391e-447d-a646-35a5d1a5cbcf
scm2_1   | 2023-05-31 01:06:36,703 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 2023-05-31 01:06:28,446 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
om2_1    | 2023-05-31 01:07:04,075 [om2@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_110
dn4_1    | 2023-05-31 01:06:31,555 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-05-31 01:06:39,680 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-05-31 01:06:36,735 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
om1_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:358)
scm1_1   | 2023-05-31 01:06:28,446 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-05-31 01:06:28,447 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 2023-05-31 01:06:40,994 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-05-31 01:07:04,460 [om2@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
dn4_1    | 2023-05-31 01:06:31,556 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-05-31 01:06:40,683 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-05-31 01:06:36,740 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 2, required at least one datanode reported per pipeline count is 2
scm2_1   | 2023-05-31 01:06:36,740 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	... 12 more
om3_1    | 2023-05-31 01:07:03,333 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
scm3_1   | 2023-05-31 01:07:00,065 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
scm1_1   | 2023-05-31 01:06:28,453 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om1_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
dn1_1    | 2023-05-31 01:06:40,994 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-05-31 01:06:41,021 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9: change Leader from null to b20d0805-391e-447d-a646-35a5d1a5cbcf at term 4 for becomeLeader, leader elected after 38934ms
dn1_1    | 2023-05-31 01:06:41,124 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn3_1    | 2023-05-31 01:06:41,683 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:42,684 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn5_1    | 2023-05-31 01:06:29,389 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
om3_1    | 2023-05-31 01:07:03,747 [om3-server-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: change Leader from null to om2 at term 5 for appendEntries, leader elected after 15787ms
scm3_1   | java.nio.channels.ClosedChannelException
scm1_1   | 2023-05-31 01:06:28,453 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-05-31 01:06:41,196 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 2023-05-31 01:06:43,686 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:29,409 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn5_1    | 2023-05-31 01:06:29,486 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
om3_1    | 2023-05-31 01:07:03,781 [om3-server-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 110: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-05-31 01:07:03,800 [om3-server-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:110
om3_1    | 2023-05-31 01:07:04,486 [om3@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_110
om3_1    | 2023-05-31 01:07:04,696 [om3@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
dn1_1    | 2023-05-31 01:06:41,205 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn1_1    | 2023-05-31 01:06:41,353 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 2023-05-31 01:06:44,688 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om3_1    | [id: "om1"
dn1_1    | 2023-05-31 01:06:41,367 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn1_1    | 2023-05-31 01:06:41,376 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn1_1    | 2023-05-31 01:06:41,434 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-05-31 01:06:41,517 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn1_1    | 2023-05-31 01:06:41,527 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-05-31 01:11:40,782 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:snapvolume-1 for user:hadoop
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
om3_1    | address: "om1:9872"
om3_1    | startupRole: FOLLOWER
om3_1    | , id: "om3"
om3_1    | address: "om3:9872"
om3_1    | startupRole: FOLLOWER
om3_1    | , id: "om2"
om1_1    | 2023-05-31 01:11:44,803 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: snapbucket-1 of layout FILE_SYSTEM_OPTIMIZED in volume: snapvolume-1
dn1_1    | 2023-05-31 01:06:41,570 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderStateImpl
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn2_1    | 	... 1 more
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
om3_1    | address: "om2:9872"
om3_1    | startupRole: FOLLOWER
om3_1    | ]
scm2_1   | 2023-05-31 01:06:36,740 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-05-31 01:06:36,767 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
dn1_1    | 2023-05-31 01:06:41,635 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn2_1    | 2023-05-31 01:06:25,250 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn3_1    | 2023-05-31 01:06:45,689 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | [id: "om1"
om2_1    | address: "om1:9872"
scm2_1   | 2023-05-31 01:06:36,767 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm2_1   | 2023-05-31 01:06:36,767 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm2_1   | 2023-05-31 01:06:36,767 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn2_1    | 2023-05-31 01:06:26,251 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:27,252 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
om2_1    | startupRole: FOLLOWER
om2_1    | , id: "om3"
scm2_1   | 2023-05-31 01:06:37,580 [IPC Server handler 0 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3da1490a-d610-40de-95c1-bdfc259ba041
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om1_1    | 2023-05-31 01:11:48,575 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotCreateRequest: Created snapshot 'snapshot1' under path 'snapvolume-1/snapbucket-1'
scm1_1   | 2023-05-31 01:06:28,454 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
dn2_1    | 2023-05-31 01:06:28,253 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:28,254 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
om2_1    | address: "om3:9872"
om2_1    | startupRole: FOLLOWER
recon_1  | 2023-05-31 01:06:37,499 [IPC Server handler 9 on default port 9891] INFO ipc.Server: IPC Server handler 9 on default port 9891 caught an exception
scm2_1   | 2023-05-31 01:06:37,581 [IPC Server handler 0 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3da1490a-d610-40de-95c1-bdfc259ba041{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn4_1    | 2023-05-31 01:06:31,556 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
om1_1    | 2023-05-31 01:11:48,695 [Thread-308] INFO rocksdiff.RocksDBCheckpointDiffer: Skipped the compaction entry. Compaction input files: [/data/metadata/om.db/000097.sst, /data/metadata/om.db/000077.sst, /data/metadata/om.db/000068.sst, /data/metadata/om.db/000046.sst] and output files: [/data/metadata/om.db/000097.sst, /data/metadata/om.db/000077.sst, /data/metadata/om.db/000068.sst, /data/metadata/om.db/000046.sst] are same.
scm1_1   | 2023-05-31 01:06:28,454 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn2_1    | java.net.ConnectException: Call From 86166c590ca2/10.9.0.18 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
om3_1    | 2023-05-31 01:09:25,333 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization started.
om3_1    | 2023-05-31 01:09:25,335 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HSYNC.
om2_1    | , id: "om2"
recon_1  | java.nio.channels.ClosedChannelException
dn1_1    | 2023-05-31 01:06:41,755 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderElection2] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9: set configuration 5: peers:[b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-31 01:06:41,775 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/6a499bf5-10af-4dab-a3fd-a4e2998a83c9/current/log_inprogress_3 to /data/metadata/ratis/6a499bf5-10af-4dab-a3fd-a4e2998a83c9/current/log_3-4
dn1_1    | 2023-05-31 01:06:41,860 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/6a499bf5-10af-4dab-a3fd-a4e2998a83c9/current/log_inprogress_5
om1_1    | 2023-05-31 01:11:48,714 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.snapshots/checkpointState/om.db-306729b7-ab36-446c-a060-48957daf2337 in 135 milliseconds
scm1_1   | 2023-05-31 01:06:28,454 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 2023-05-31 01:06:46,690 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-05-31 01:09:25,338 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature HSYNC has been finalized.
om3_1    | 2023-05-31 01:09:25,339 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: FILESYSTEM_SNAPSHOT.
om2_1    | address: "om2:9872"
om2_1    | startupRole: FOLLOWER
om2_1    | ]
om2_1    | 2023-05-31 01:09:25,324 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization started.
scm2_1   | 2023-05-31 01:06:37,582 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-05-31 01:06:37,584 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 2023-05-31 01:06:47,691 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-05-31 01:09:25,340 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature FILESYSTEM_SNAPSHOT has been finalized.
om3_1    | 2023-05-31 01:09:25,340 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
om3_1    | 2023-05-31 01:09:25,341 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization is done.
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm2_1   | 2023-05-31 01:06:37,771 [IPC Server handler 11 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/b20d0805-391e-447d-a646-35a5d1a5cbcf
scm2_1   | 2023-05-31 01:06:37,771 [IPC Server handler 11 on default port 9861] INFO node.SCMNodeManager: Registered Data node : b20d0805-391e-447d-a646-35a5d1a5cbcf{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 2023-05-31 01:06:48,698 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:06:49,698 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-05-31 01:09:25,327 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HSYNC.
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn1_1    | 2023-05-31 01:06:42,532 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm2_1   | 2023-05-31 01:06:37,772 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-05-31 01:06:37,774 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn3_1    | 2023-05-31 01:07:00,056 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn3_1    | 2023-05-31 01:07:33,958 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
om3_1    | 2023-05-31 01:09:25,349 [OMDoubleBufferFlushThread] INFO upgrade.OMFinalizeUpgradeResponse: Layout version to persist to DB : 5
dn1_1    | 2023-05-31 01:06:43,541 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:44,543 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn4_1    | 2023-05-31 01:06:31,556 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm2_1   | 2023-05-31 01:06:37,775 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
om1_1    | 2023-05-31 01:11:48,793 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointUtils: Waited for 76 milliseconds for checkpoint directory /data/metadata/db.snapshots/checkpointState/om.db-306729b7-ab36-446c-a060-48957daf2337 availability.
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:538)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
dn3_1    | 2023-05-31 01:08:09,820 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
om3_1    | 2023-05-31 01:10:38,553 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:new2-volume for user:hadoop
dn1_1    | 2023-05-31 01:06:44,997 [grpc-default-executor-0] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: receive requestVote(PRE_VOTE, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, group-F1A4CF07E444, 11, (t:11, i:24))
dn1_1    | 2023-05-31 01:06:45,017 [grpc-default-executor-2] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: receive requestVote(PRE_VOTE, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, group-DC0251522AF1, 8, (t:8, i:42))
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn4_1    | 2023-05-31 01:06:31,559 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om1_1    | 2023-05-31 01:11:48,803 [OMDoubleBufferFlushThread] INFO om.OmSnapshotManager: Created checkpoint : /data/metadata/db.snapshots/checkpointState/om.db-306729b7-ab36-446c-a060-48957daf2337 for snapshot snapshot1
dn5_1    | Caused by: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	... 1 more
dn5_1    | 2023-05-31 01:06:29,628 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:29,731 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
om2_1    | 2023-05-31 01:09:25,334 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature HSYNC has been finalized.
om2_1    | 2023-05-31 01:09:25,337 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: FILESYSTEM_SNAPSHOT.
om1_1    | 2023-05-31 01:11:57,464 [SstFilteringService#0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-05-31 01:11:57,484 [SstFilteringService#0] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn5_1    | 2023-05-31 01:06:29,733 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn2_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
om3_1    | 2023-05-31 01:10:42,383 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: new2-volume
om3_1    | 2023-05-31 01:10:54,673 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: s3v
om3_1    | 2023-05-31 01:11:04,605 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:new2-bucket in volume:s3v
om1_1    | 2023-05-31 01:11:59,739 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotCreateRequest: Created snapshot 'snapshot2' under path 'snapvolume-1/snapbucket-1'
scm1_1   | 2023-05-31 01:06:28,454 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
dn3_1    | 2023-05-31 01:08:09,822 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn1_1    | 2023-05-31 01:06:45,051 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection1] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection1: PRE_VOTE REJECTED received 1 response(s) and 0 exception(s):
dn1_1    | 2023-05-31 01:06:45,052 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection1] INFO impl.LeaderElection:   Response 0: b20d0805-391e-447d-a646-35a5d1a5cbcf<-3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#0:FAIL-t11
dn1_1    | 2023-05-31 01:06:45,063 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection1] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection1 PRE_VOTE round 0: result REJECTED
dn1_1    | 2023-05-31 01:06:45,071 [grpc-default-executor-2] INFO impl.VoteContext: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-CANDIDATE: accept PRE_VOTE from 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: our priority 0 <= candidate's priority 0
dn1_1    | 2023-05-31 01:06:45,071 [grpc-default-executor-2] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1 replies to PRE_VOTE vote request: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd<-b20d0805-391e-447d-a646-35a5d1a5cbcf#0:OK-t8. Peer's state: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1:t8, leader=null, voted=3da1490a-d610-40de-95c1-bdfc259ba041, raftlog=Memoized:b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-SegmentedRaftLog:OPENED:c42, conf=29: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-31 01:06:45,096 [grpc-default-executor-1] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: receive requestVote(PRE_VOTE, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, group-F1A4CF07E444, 11, (t:11, i:24))
om1_1    | 2023-05-31 01:11:59,815 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.snapshots/checkpointState/om.db-206b010f-499e-4ce3-95c4-cc737d6f1003 in 73 milliseconds
scm1_1   | 2023-05-31 01:06:28,454 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn3_1    | 2023-05-31 01:08:09,822 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn3_1    | 2023-05-31 01:08:09,824 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn1_1    | 2023-05-31 01:06:45,122 [grpc-default-executor-4] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: receive requestVote(PRE_VOTE, 3da1490a-d610-40de-95c1-bdfc259ba041, group-DC0251522AF1, 8, (t:8, i:42))
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
om1_1    | 2023-05-31 01:11:59,817 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointUtils: Waited for 1 milliseconds for checkpoint directory /data/metadata/db.snapshots/checkpointState/om.db-206b010f-499e-4ce3-95c4-cc737d6f1003 availability.
om1_1    | 2023-05-31 01:11:59,818 [OMDoubleBufferFlushThread] INFO om.OmSnapshotManager: Created checkpoint : /data/metadata/db.snapshots/checkpointState/om.db-206b010f-499e-4ce3-95c4-cc737d6f1003 for snapshot snapshot2
scm1_1   | 2023-05-31 01:06:28,455 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn3_1    | 2023-05-31 01:08:09,826 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
dn5_1    | 2023-05-31 01:06:29,894 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/437d8269-2a0d-4058-8be6-ed0a589991d2/in_use.lock acquired by nodename 7@51bd9aa00c0e
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 2023-05-31 01:06:45,072 [grpc-default-executor-0] INFO impl.VoteContext: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-CANDIDATE: accept PRE_VOTE from 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: our priority 0 <= candidate's priority 1
dn1_1    | 2023-05-31 01:06:45,097 [grpc-default-executor-3] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: receive requestVote(PRE_VOTE, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, group-DC0251522AF1, 8, (t:8, i:42))
dn1_1    | 2023-05-31 01:06:45,097 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection3] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection3: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
dn4_1    | 2023-05-31 01:06:31,561 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-05-31 01:06:31,565 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-05-31 01:06:31,555 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5509b2c4-975c-42f1-91be-39d463d01a51
om1_1    | 2023-05-31 01:12:24,715 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotDeleteRequest: Deleted snapshot 'snapshot1' under path 'snapvolume-1/snapbucket-1'
scm1_1   | 2023-05-31 01:06:28,455 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-05-31 01:08:09,826 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
om3_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om3_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:214)
om3_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:338)
dn1_1    | 2023-05-31 01:06:45,228 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection3] INFO impl.LeaderElection:   Response 0: b20d0805-391e-447d-a646-35a5d1a5cbcf<-3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#0:OK-t8
om2_1    | 2023-05-31 01:09:25,343 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature FILESYSTEM_SNAPSHOT has been finalized.
om2_1    | 2023-05-31 01:09:25,343 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
om2_1    | 2023-05-31 01:09:25,344 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn3_1    | 2023-05-31 01:08:09,827 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
scm2_1   | 2023-05-31 01:06:37,775 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm2_1   | 2023-05-31 01:06:37,775 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn1_1    | 2023-05-31 01:06:45,228 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection3] INFO impl.LeaderElection:   Response 1: b20d0805-391e-447d-a646-35a5d1a5cbcf<-3da1490a-d610-40de-95c1-bdfc259ba041#0:FAIL-t8
om2_1    | 2023-05-31 01:09:25,371 [OMDoubleBufferFlushThread] INFO upgrade.OMFinalizeUpgradeResponse: Layout version to persist to DB : 5
om2_1    | 2023-05-31 01:10:38,456 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:new2-volume for user:hadoop
om2_1    | 2023-05-31 01:10:42,352 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: new2-volume
dn3_1    | 2023-05-31 01:08:09,827 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HADOOP_PRC_PORTS_IN_DATANODEDETAILS.
om3_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:567)
om3_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:358)
om3_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1    | 2023-05-31 01:10:54,665 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: s3v
om2_1    | 2023-05-31 01:11:04,612 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:new2-bucket in volume:s3v
om2_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
dn3_1    | 2023-05-31 01:08:09,827 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature HADOOP_PRC_PORTS_IN_DATANODEDETAILS has been finalized.
dn5_1    | 2023-05-31 01:06:29,921 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd} from /data/metadata/ratis/437d8269-2a0d-4058-8be6-ed0a589991d2/current/raft-meta
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 2023-05-31 01:06:37,775 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm2_1   | 2023-05-31 01:06:37,775 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
dn4_1    | 2023-05-31 01:06:31,580 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-05-31 01:06:31,600 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-05-31 01:06:31,605 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-05-31 01:06:31,605 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 2023-05-31 01:07:24,625 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Stopping RatisPipelineUtilsThread.
scm2_1   | 2023-05-31 01:07:24,627 [RatisPipelineUtilsThread - 0] WARN pipeline.BackgroundPipelineCreator: RatisPipelineUtilsThread is interrupted.
scm1_1   | 2023-05-31 01:06:28,455 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:214)
om2_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:338)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1    | 2023-05-31 01:11:40,776 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:snapvolume-1 for user:hadoop
om3_1    | 2023-05-31 01:11:44,811 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: snapbucket-1 of layout FILE_SYSTEM_OPTIMIZED in volume: snapvolume-1
dn4_1    | 2023-05-31 01:06:31,606 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-05-31 01:06:31,607 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-05-31 01:06:31,608 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-05-31 01:06:31,609 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-05-31 01:06:31,609 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-05-31 01:06:31,614 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-05-31 01:06:29,935 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/in_use.lock acquired by nodename 7@51bd9aa00c0e
dn5_1    | 2023-05-31 01:06:29,952 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=8, votedFor=3da1490a-d610-40de-95c1-bdfc259ba041} from /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/current/raft-meta
dn5_1    | 2023-05-31 01:06:29,962 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/in_use.lock acquired by nodename 7@51bd9aa00c0e
dn5_1    | 2023-05-31 01:06:29,962 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=11, votedFor=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd} from /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/current/raft-meta
dn5_1    | 2023-05-31 01:06:30,177 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1: set configuration 29: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:06:28,459 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO impl.RoleInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978: start 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderStateImpl
scm1_1   | 2023-05-31 01:06:28,466 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-SegmentedRaftLogWorker: Rolling segment log-61_80 to index:80
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 2023-05-31 01:06:28,478 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/log_inprogress_61 to /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/log_61-80
scm3_1   | 2023-05-31 01:07:00,058 [IPC Server handler 0 on default port 9861] WARN ipc.Server: IPC Server handler 0 on default port 9861, call Call#19 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:54170: output error
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:567)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | 2023-05-31 01:06:30,177 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444: set configuration 11: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:06:31,609 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-05-31 01:06:31,618 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-05-31 01:06:37,499 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn1_1    | 2023-05-31 01:06:45,229 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection3] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection3 PRE_VOTE round 0: result REJECTED
dn1_1    | 2023-05-31 01:06:45,227 [grpc-default-executor-4] INFO impl.VoteContext: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-CANDIDATE: accept PRE_VOTE from 3da1490a-d610-40de-95c1-bdfc259ba041: our priority 0 <= candidate's priority 1
dn1_1    | 2023-05-31 01:06:45,181 [grpc-default-executor-6] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: receive requestVote(PRE_VOTE, 3da1490a-d610-40de-95c1-bdfc259ba041, group-F1A4CF07E444, 11, (t:11, i:24))
dn1_1    | 2023-05-31 01:06:45,236 [grpc-default-executor-4] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1 replies to PRE_VOTE vote request: 3da1490a-d610-40de-95c1-bdfc259ba041<-b20d0805-391e-447d-a646-35a5d1a5cbcf#0:OK-t8. Peer's state: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1:t8, leader=null, voted=3da1490a-d610-40de-95c1-bdfc259ba041, raftlog=Memoized:b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-SegmentedRaftLog:OPENED:c42, conf=29: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn4_1    | 2023-05-31 01:06:31,658 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-05-31 01:06:30,188 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2: set configuration 3: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:06:30,270 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO ratis.ContainerStateMachine: group-F1A4CF07E444: Setting the last applied index to (t:11, i:24)
dn5_1    | 2023-05-31 01:06:30,288 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO ratis.ContainerStateMachine: group-DC0251522AF1: Setting the last applied index to (t:8, i:42)
dn5_1    | 2023-05-31 01:06:30,299 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO ratis.ContainerStateMachine: group-ED0A589991D2: Setting the last applied index to (t:3, i:4)
dn5_1    | 2023-05-31 01:06:30,629 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:31,068 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-05-31 01:06:45,236 [grpc-default-executor-0] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444 replies to PRE_VOTE vote request: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd<-b20d0805-391e-447d-a646-35a5d1a5cbcf#0:OK-t11. Peer's state: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444:t11, leader=null, voted=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, raftlog=Memoized:b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-SegmentedRaftLog:OPENED:c24, conf=11: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-31 01:06:45,239 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection3] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: changes role from CANDIDATE to FOLLOWER at term 8 for REJECTED
dn1_1    | 2023-05-31 01:06:45,289 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection3] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection3
dn1_1    | 2023-05-31 01:06:45,260 [grpc-default-executor-6] INFO impl.VoteContext: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-CANDIDATE: accept PRE_VOTE from 3da1490a-d610-40de-95c1-bdfc259ba041: our priority 0 <= candidate's priority 0
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn5_1    | 2023-05-31 01:06:31,109 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-05-31 01:06:31,110 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-05-31 01:06:31,144 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-05-31 01:06:31,146 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:06:31,147 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-05-31 01:06:31,151 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-05-31 01:06:45,307 [grpc-default-executor-6] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444 replies to PRE_VOTE vote request: 3da1490a-d610-40de-95c1-bdfc259ba041<-b20d0805-391e-447d-a646-35a5d1a5cbcf#0:OK-t11. Peer's state: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444:t11, leader=null, voted=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, raftlog=Memoized:b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-SegmentedRaftLog:OPENED:c24, conf=11: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:06:28,502 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/log_inprogress_81
scm1_1   | 2023-05-31 01:06:28,510 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-LeaderElection2] INFO server.RaftServer$Division: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E: set configuration 81: peers:[d2fda184-2469-4fa2-af65-75268cb832ba|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:06:28,553 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-05-31 01:06:31,662 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn5_1    | 2023-05-31 01:06:31,151 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om3_1    | 2023-05-31 01:11:48,566 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotCreateRequest: Created snapshot 'snapshot1' under path 'snapvolume-1/snapbucket-1'
om3_1    | 2023-05-31 01:11:48,693 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.snapshots/checkpointState/om.db-306729b7-ab36-446c-a060-48957daf2337 in 125 milliseconds
om3_1    | 2023-05-31 01:11:48,761 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointUtils: Waited for 65 milliseconds for checkpoint directory /data/metadata/db.snapshots/checkpointState/om.db-306729b7-ab36-446c-a060-48957daf2337 availability.
om3_1    | 2023-05-31 01:11:48,793 [OMDoubleBufferFlushThread] INFO om.OmSnapshotManager: Created checkpoint : /data/metadata/db.snapshots/checkpointState/om.db-306729b7-ab36-446c-a060-48957daf2337 for snapshot snapshot1
om3_1    | 2023-05-31 01:11:55,258 [SstFilteringService#0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn1_1    | 2023-05-31 01:06:45,308 [grpc-default-executor-1] INFO impl.VoteContext: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-CANDIDATE: accept PRE_VOTE from 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: our priority 0 <= candidate's priority 1
dn1_1    | 2023-05-31 01:06:45,290 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection3] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState
dn1_1    | 2023-05-31 01:06:45,322 [grpc-default-executor-0] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: receive requestVote(ELECTION, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, group-F1A4CF07E444, 12, (t:11, i:24))
dn1_1    | 2023-05-31 01:06:45,309 [grpc-default-executor-1] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444 replies to PRE_VOTE vote request: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd<-b20d0805-391e-447d-a646-35a5d1a5cbcf#0:OK-t11. Peer's state: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444:t11, leader=null, voted=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, raftlog=Memoized:b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-SegmentedRaftLog:OPENED:c24, conf=11: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:06:31,668 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn5_1    | 2023-05-31 01:06:31,154 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:06:31,155 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-05-31 01:06:31,155 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-05-31 01:06:31,151 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-05-31 01:06:31,156 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:06:31,156 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm2_1   | 2023-05-31 01:07:25,002 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
scm2_1   | 2023-05-31 01:07:25,010 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
scm2_1   | 2023-05-31 01:07:25,029 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
scm2_1   | 2023-05-31 01:07:25,030 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
dn4_1    | 2023-05-31 01:06:31,674 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn5_1    | 2023-05-31 01:06:31,156 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-05-31 01:06:31,164 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-05-31 01:06:31,167 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-05-31 01:06:31,179 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-05-31 01:06:31,220 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-05-31 01:06:31,222 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | 2023-05-31 01:06:28,579 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
dn3_1    | 2023-05-31 01:08:09,827 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn3_1    | 2023-05-31 01:08:09,827 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn3_1    | 2023-05-31 01:08:33,959 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-05-31 01:08:39,821 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn5_1    | 2023-05-31 01:06:31,228 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-05-31 01:06:31,231 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-05-31 01:06:31,235 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:06:31,233 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:06:31,239 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | 2023-05-31 01:06:28,580 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:28,616 [grpc-default-executor-1] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:28,604 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:28,695 [grpc-default-executor-2] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-05-31 01:07:25,051 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HADOOP_PRC_PORTS_IN_DATANODEDETAILS.
dn1_1    | 2023-05-31 01:06:45,333 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: changes role from CANDIDATE to FOLLOWER at term 11 for REJECTED
dn1_1    | 2023-05-31 01:06:45,333 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection1] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection1
dn1_1    | 2023-05-31 01:06:45,335 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection1] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState
dn1_1    | 2023-05-31 01:06:45,350 [grpc-default-executor-1] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: receive requestVote(ELECTION, 3da1490a-d610-40de-95c1-bdfc259ba041, group-DC0251522AF1, 9, (t:8, i:42))
dn4_1    | 2023-05-31 01:06:31,692 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | Caused by: java.net.ConnectException: Connection refused
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:358)
scm1_1   | 2023-05-31 01:06:28,698 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-05-31 01:07:25,054 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature HADOOP_PRC_PORTS_IN_DATANODEDETAILS has been finalized.
om3_1    | 2023-05-31 01:11:55,268 [SstFilteringService#0] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
dn1_1    | 2023-05-31 01:06:45,355 [grpc-default-executor-3] INFO impl.VoteContext: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FOLLOWER: accept PRE_VOTE from 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: our priority 0 <= candidate's priority 0
scm3_1   | 2023-05-31 01:07:00,058 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#18 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:48578: output error
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn5_1    | 2023-05-31 01:06:31,239 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-05-31 01:06:31,695 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-31 01:06:32,994 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm2_1   | 2023-05-31 01:07:25,054 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
scm1_1   | 2023-05-31 01:06:28,700 [grpc-default-executor-1] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 82 -> 0
om3_1    | 2023-05-31 01:11:59,760 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotCreateRequest: Created snapshot 'snapshot2' under path 'snapvolume-1/snapbucket-1'
dn1_1    | 2023-05-31 01:06:45,374 [grpc-default-executor-0] INFO impl.VoteContext: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FOLLOWER: accept ELECTION from 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: our priority 0 <= candidate's priority 1
scm3_1   | 2023-05-31 01:07:00,073 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn5_1    | 2023-05-31 01:06:31,242 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-05-31 01:08:39,833 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 97734618-7966-42ac-b50e-e49ead6ce4d2: remove    LEADER 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04:t4, leader=97734618-7966-42ac-b50e-e49ead6ce4d2, voted=97734618-7966-42ac-b50e-e49ead6ce4d2, raftlog=Memoized:97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-SegmentedRaftLog:OPENED:c6, conf=5: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn4_1    | 2023-05-31 01:06:33,067 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-05-31 01:06:33,084 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-05-31 01:06:33,101 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2_1   | 2023-05-31 01:07:25,056 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm1_1   | 2023-05-31 01:06:28,792 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:28,793 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om3_1    | 2023-05-31 01:11:59,807 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.snapshots/checkpointState/om.db-206b010f-499e-4ce3-95c4-cc737d6f1003 in 41 milliseconds
om3_1    | 2023-05-31 01:11:59,809 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointUtils: Waited for 1 milliseconds for checkpoint directory /data/metadata/db.snapshots/checkpointState/om.db-206b010f-499e-4ce3-95c4-cc737d6f1003 availability.
om3_1    | 2023-05-31 01:11:59,809 [OMDoubleBufferFlushThread] INFO om.OmSnapshotManager: Created checkpoint : /data/metadata/db.snapshots/checkpointState/om.db-206b010f-499e-4ce3-95c4-cc737d6f1003 for snapshot snapshot2
om3_1    | 2023-05-31 01:12:24,717 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotDeleteRequest: Deleted snapshot 'snapshot1' under path 'snapvolume-1/snapbucket-1'
scm3_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn3_1    | 2023-05-31 01:08:39,835 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04: shutdown
dn1_1    | 2023-05-31 01:06:45,375 [grpc-default-executor-0] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: changes role from  FOLLOWER to FOLLOWER at term 12 for candidate:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
scm1_1   | 2023-05-31 01:06:28,799 [grpc-default-executor-3] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 82 -> 0
scm1_1   | 2023-05-31 01:06:28,836 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn3_1    | 2023-05-31 01:08:39,835 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-7A8B4BE06A04,id=97734618-7966-42ac-b50e-e49ead6ce4d2
dn1_1    | 2023-05-31 01:06:45,375 [grpc-default-executor-0] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState
scm1_1   | 2023-05-31 01:06:28,839 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn5_1    | 2023-05-31 01:06:31,270 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/437d8269-2a0d-4058-8be6-ed0a589991d2
dn1_1    | 2023-05-31 01:06:45,377 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO impl.FollowerState: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState: Stopping now (isRunning? false, role = FOLLOWER)
scm1_1   | 2023-05-31 01:06:28,841 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 2, healthy pipeline threshold count is 1
scm1_1   | 2023-05-31 01:06:28,845 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
om2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-05-31 01:06:37,499 [IPC Server handler 19 on default port 9891] INFO ipc.Server: IPC Server handler 19 on default port 9891 caught an exception
dn5_1    | 2023-05-31 01:06:31,272 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-05-31 01:06:45,384 [grpc-default-executor-0] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState
dn1_1    | 2023-05-31 01:06:45,385 [grpc-default-executor-3] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1 replies to PRE_VOTE vote request: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd<-b20d0805-391e-447d-a646-35a5d1a5cbcf#0:OK-t8. Peer's state: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1:t8, leader=null, voted=3da1490a-d610-40de-95c1-bdfc259ba041, raftlog=Memoized:b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-SegmentedRaftLog:OPENED:c42, conf=29: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-31 01:06:45,388 [grpc-default-executor-1] INFO impl.VoteContext: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FOLLOWER: accept ELECTION from 3da1490a-d610-40de-95c1-bdfc259ba041: our priority 0 <= candidate's priority 1
om2_1    | 2023-05-31 01:11:40,770 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:snapvolume-1 for user:hadoop
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:790)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:363)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)
recon_1  | java.nio.channels.ClosedChannelException
dn3_1    | 2023-05-31 01:08:39,836 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: 97734618-7966-42ac-b50e-e49ead6ce4d2: shutdown 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-LeaderStateImpl
dn1_1    | 2023-05-31 01:06:45,389 [grpc-default-executor-1] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: changes role from  FOLLOWER to FOLLOWER at term 9 for candidate:3da1490a-d610-40de-95c1-bdfc259ba041
dn1_1    | 2023-05-31 01:06:45,389 [grpc-default-executor-1] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState
dn1_1    | 2023-05-31 01:06:45,394 [grpc-default-executor-1] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState
scm1_1   | 2023-05-31 01:06:28,846 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
dn5_1    | 2023-05-31 01:06:31,272 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1
dn5_1    | 2023-05-31 01:06:31,275 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn3_1    | 2023-05-31 01:08:39,843 [PipelineCommandHandlerThread-0] INFO impl.PendingRequests: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-PendingRequests: sendNotLeaderResponses
dn3_1    | 2023-05-31 01:08:39,846 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-StateMachineUpdater: set stopIndex = 6
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn1_1    | 2023-05-31 01:06:45,395 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO impl.FollowerState: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState was interrupted
dn1_1    | 2023-05-31 01:06:45,408 [grpc-default-executor-0] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444 replies to ELECTION vote request: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd<-b20d0805-391e-447d-a646-35a5d1a5cbcf#0:OK-t12. Peer's state: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444:t12, leader=null, voted=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, raftlog=Memoized:b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-SegmentedRaftLog:OPENED:c24, conf=11: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
om2_1    | 2023-05-31 01:11:44,794 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: snapbucket-1 of layout FILE_SYSTEM_OPTIMIZED in volume: snapvolume-1
dn1_1    | 2023-05-31 01:06:45,474 [grpc-default-executor-1] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1 replies to ELECTION vote request: 3da1490a-d610-40de-95c1-bdfc259ba041<-b20d0805-391e-447d-a646-35a5d1a5cbcf#0:OK-t9. Peer's state: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1:t9, leader=null, voted=3da1490a-d610-40de-95c1-bdfc259ba041, raftlog=Memoized:b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-SegmentedRaftLog:OPENED:c42, conf=29: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-31 01:06:45,545 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:46,020 [b20d0805-391e-447d-a646-35a5d1a5cbcf-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-F1A4CF07E444 with new leaderId: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn1_1    | 2023-05-31 01:06:46,043 [b20d0805-391e-447d-a646-35a5d1a5cbcf-server-thread1] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: change Leader from null to 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd at term 12 for appendEntries, leader elected after 42486ms
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
dn2_1    | 	... 12 more
dn2_1    | 2023-05-31 01:06:29,256 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:29,263 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From 86166c590ca2/10.9.0.18 to scm2:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:58198 remote=scm2/10.9.0.15:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn4_1    | 2023-05-31 01:06:33,143 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-05-31 01:06:33,147 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-05-31 01:06:33,151 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-05-31 01:06:33,374 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-05-31 01:06:33,375 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-05-31 01:06:33,455 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-05-31 01:06:33,657 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51: set configuration 0: peers:[3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:06:33,658 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1: set configuration 0: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:06:33,682 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444: set configuration 0: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:06:33,688 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/5509b2c4-975c-42f1-91be-39d463d01a51/current/log_0-0
dn4_1    | 2023-05-31 01:06:33,800 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51: set configuration 1: peers:[3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:06:33,771 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO segmented.LogSegment: Successfully read 5 entries from segment file /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/current/log_0-4
dn4_1    | 2023-05-31 01:06:33,816 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/5509b2c4-975c-42f1-91be-39d463d01a51/current/log_1-2
dn4_1    | 2023-05-31 01:06:33,838 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO segmented.LogSegment: Successfully read 8 entries from segment file /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/current/log_0-7
dn4_1    | 2023-05-31 01:06:33,838 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444: set configuration 5: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:06:33,874 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO segmented.LogSegment: Successfully read 6 entries from segment file /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/current/log_5-10
scm2_1   | 2023-05-31 01:07:25,057 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) moved to HEALTHY READONLY state.
scm2_1   | 2023-05-31 01:07:25,057 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1 in state CLOSED which uses HEALTHY_READONLY datanode 3da1490a-d610-40de-95c1-bdfc259ba041. This will send close commands for its containers.
scm2_1   | 2023-05-31 01:07:25,057 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444 in state CLOSED which uses HEALTHY_READONLY datanode 3da1490a-d610-40de-95c1-bdfc259ba041. This will send close commands for its containers.
scm2_1   | 2023-05-31 01:07:25,058 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=5509b2c4-975c-42f1-91be-39d463d01a51 in state CLOSED which uses HEALTHY_READONLY datanode 3da1490a-d610-40de-95c1-bdfc259ba041. This will send close commands for its containers.
scm2_1   | 2023-05-31 01:07:25,058 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17) moved to HEALTHY READONLY state.
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 2023-05-31 01:07:00,058 [IPC Server handler 8 on default port 9861] WARN ipc.Server: IPC Server handler 8 on default port 9861, call Call#22 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:37844: output error
scm3_1   | 2023-05-31 01:07:00,076 [IPC Server handler 8 on default port 9861] INFO ipc.Server: IPC Server handler 8 on default port 9861 caught an exception
scm2_1   | 2023-05-31 01:07:25,058 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=6a499bf5-10af-4dab-a3fd-a4e2998a83c9 in state CLOSED which uses HEALTHY_READONLY datanode b20d0805-391e-447d-a646-35a5d1a5cbcf. This will send close commands for its containers.
scm2_1   | 2023-05-31 01:07:25,058 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1 in state CLOSED which uses HEALTHY_READONLY datanode b20d0805-391e-447d-a646-35a5d1a5cbcf. This will send close commands for its containers.
scm2_1   | 2023-05-31 01:07:25,058 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444 in state CLOSED which uses HEALTHY_READONLY datanode b20d0805-391e-447d-a646-35a5d1a5cbcf. This will send close commands for its containers.
scm2_1   | 2023-05-31 01:07:25,059 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18) moved to HEALTHY READONLY state.
scm2_1   | 2023-05-31 01:07:25,059 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=ccaf8839-8737-4f75-8b75-b011fdd1ea12 in state CLOSED which uses HEALTHY_READONLY datanode 4bc93e86-8989-4282-b7d8-050f4a19b92f. This will send close commands for its containers.
scm2_1   | 2023-05-31 01:07:25,059 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21) moved to HEALTHY READONLY state.
dn3_1    | 2023-05-31 01:08:39,846 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-7A8B4BE06A04: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/9e886fff-1860-4674-bb59-7a8b4be06a04/sm/snapshot.4_6
dn3_1    | 2023-05-31 01:08:39,856 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-7A8B4BE06A04: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/9e886fff-1860-4674-bb59-7a8b4be06a04/sm/snapshot.4_6 took: 10 ms
dn3_1    | 2023-05-31 01:08:39,858 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-StateMachineUpdater] INFO impl.StateMachineUpdater: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-StateMachineUpdater: Took a snapshot at index 6
dn3_1    | 2023-05-31 01:08:39,858 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-StateMachineUpdater] INFO impl.StateMachineUpdater: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
scm3_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn1_1    | 2023-05-31 01:06:46,179 [b20d0805-391e-447d-a646-35a5d1a5cbcf-server-thread1] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: set configuration 25: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-31 01:06:46,180 [b20d0805-391e-447d-a646-35a5d1a5cbcf-server-thread1] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-SegmentedRaftLogWorker: Rolling segment log-11_24 to index:24
scm2_1   | 2023-05-31 01:07:25,059 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=437d8269-2a0d-4058-8be6-ed0a589991d2 in state CLOSED which uses HEALTHY_READONLY datanode 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd. This will send close commands for its containers.
om2_1    | 2023-05-31 01:11:48,584 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotCreateRequest: Created snapshot 'snapshot1' under path 'snapvolume-1/snapbucket-1'
om2_1    | 2023-05-31 01:11:48,697 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.snapshots/checkpointState/om.db-306729b7-ab36-446c-a060-48957daf2337 in 116 milliseconds
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn2_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn3_1    | 2023-05-31 01:08:39,860 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04: closes. applyIndex: 6
scm2_1   | 2023-05-31 01:07:25,059 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1 in state CLOSED which uses HEALTHY_READONLY datanode 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd. This will send close commands for its containers.
om2_1    | 2023-05-31 01:11:48,704 [Thread-354] INFO rocksdiff.RocksDBCheckpointDiffer: Skipped the compaction entry. Compaction input files: [/data/metadata/om.db/000096.sst, /data/metadata/om.db/000077.sst, /data/metadata/om.db/000068.sst, /data/metadata/om.db/000046.sst] and output files: [/data/metadata/om.db/000096.sst, /data/metadata/om.db/000077.sst, /data/metadata/om.db/000068.sst, /data/metadata/om.db/000046.sst] are same.
om2_1    | 2023-05-31 01:11:48,791 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointUtils: Waited for 85 milliseconds for checkpoint directory /data/metadata/db.snapshots/checkpointState/om.db-306729b7-ab36-446c-a060-48957daf2337 availability.
dn1_1    | 2023-05-31 01:06:46,208 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/current/log_inprogress_11 to /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/current/log_11-24
dn1_1    | 2023-05-31 01:06:46,231 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/current/log_inprogress_25
scm2_1   | 2023-05-31 01:07:25,059 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444 in state CLOSED which uses HEALTHY_READONLY datanode 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd. This will send close commands for its containers.
om2_1    | 2023-05-31 01:11:48,794 [OMDoubleBufferFlushThread] INFO om.OmSnapshotManager: Created checkpoint : /data/metadata/db.snapshots/checkpointState/om.db-306729b7-ab36-446c-a060-48957daf2337 for snapshot snapshot1
om2_1    | 2023-05-31 01:11:56,889 [SstFilteringService#0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn4_1    | 2023-05-31 01:06:33,880 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51: set configuration 3: peers:[3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-31 01:06:46,556 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:08:40,256 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04-SegmentedRaftLogWorker close()
scm2_1   | 2023-05-31 01:07:25,059 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19) moved to HEALTHY READONLY state.
om2_1    | 2023-05-31 01:11:56,897 [SstFilteringService#0] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om2_1    | 2023-05-31 01:11:59,735 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotCreateRequest: Created snapshot 'snapshot2' under path 'snapvolume-1/snapbucket-1'
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn4_1    | 2023-05-31 01:06:33,903 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1: set configuration 8: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:06:33,905 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/5509b2c4-975c-42f1-91be-39d463d01a51/current/log_inprogress_3
dn4_1    | 2023-05-31 01:06:33,933 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444: set configuration 11: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:06:33,975 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn4_1    | 2023-05-31 01:06:33,975 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn1_1    | 2023-05-31 01:06:46,562 [b20d0805-391e-447d-a646-35a5d1a5cbcf-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-DC0251522AF1 with new leaderId: 3da1490a-d610-40de-95c1-bdfc259ba041
scm2_1   | 2023-05-31 01:07:25,060 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=9e886fff-1860-4674-bb59-7a8b4be06a04 in state CLOSED which uses HEALTHY_READONLY datanode 97734618-7966-42ac-b50e-e49ead6ce4d2. This will send close commands for its containers.
om2_1    | 2023-05-31 01:11:59,815 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.snapshots/checkpointState/om.db-206b010f-499e-4ce3-95c4-cc737d6f1003 in 76 milliseconds
om2_1    | 2023-05-31 01:11:59,818 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointUtils: Waited for 2 milliseconds for checkpoint directory /data/metadata/db.snapshots/checkpointState/om.db-206b010f-499e-4ce3-95c4-cc737d6f1003 availability.
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn3_1    | 2023-05-31 01:08:40,262 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-7A8B4BE06A04: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/9e886fff-1860-4674-bb59-7a8b4be06a04
dn1_1    | 2023-05-31 01:06:46,574 [b20d0805-391e-447d-a646-35a5d1a5cbcf-server-thread1] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: change Leader from null to 3da1490a-d610-40de-95c1-bdfc259ba041 at term 9 for appendEntries, leader elected after 43314ms
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn3_1    | 2023-05-31 01:08:40,264 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=9e886fff-1860-4674-bb59-7a8b4be06a04 command on datanode 97734618-7966-42ac-b50e-e49ead6ce4d2.
dn3_1    | 2023-05-31 01:09:09,847 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 97734618-7966-42ac-b50e-e49ead6ce4d2: addNew group-297089FABFC6:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] returns group-297089FABFC6:java.util.concurrent.CompletableFuture@4a2dafe0[Not completed]
om2_1    | 2023-05-31 01:11:59,827 [OMDoubleBufferFlushThread] INFO om.OmSnapshotManager: Created checkpoint : /data/metadata/db.snapshots/checkpointState/om.db-206b010f-499e-4ce3-95c4-cc737d6f1003 for snapshot snapshot2
om2_1    | 2023-05-31 01:12:08,106 [IPC Server handler 76 on default port 9862] INFO snapshot.SnapshotDiffManager: Submitting snap diff report generation request for volume: snapvolume-1, bucket: snapbucket-1, fromSnapshot: snapshot1 and toSnapshot: snapshot2
om2_1    | 2023-05-31 01:12:08,117 [snapshot-diff-job-thread-id-0] INFO snapshot.SnapshotDiffManager: Started snap diff report generation for volume: snapvolume-1 bucket: snapbucket-1, fromSnapshot: snapshot1 and toSnapshot: snapshot2
om2_1    | 2023-05-31 01:12:08,132 [snapshot-diff-job-thread-id-0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-05-31 01:12:08,133 [snapshot-diff-job-thread-id-0] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn3_1    | 2023-05-31 01:09:09,852 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2: new RaftServerImpl for group-297089FABFC6:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
om2_1    | 2023-05-31 01:12:08,270 [snapshot-diff-job-thread-id-0] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
scm1_1   | 2023-05-31 01:06:28,848 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm1_1   | 2023-05-31 01:06:28,850 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-05-31 01:06:28,851 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
dn1_1    | 2023-05-31 01:06:46,574 [b20d0805-391e-447d-a646-35a5d1a5cbcf-server-thread1] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: set configuration 43: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn4_1    | 2023-05-31 01:06:33,997 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-31 01:06:34,016 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO segmented.LogSegment: Successfully read 14 entries from segment file /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/current/log_inprogress_11
dn3_1    | 2023-05-31 01:09:09,853 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om2_1    | 2023-05-31 01:12:08,274 [snapshot-diff-job-thread-id-0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-05-31 01:06:28,904 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-05-31 01:07:38,508 [IPC Server handler 5 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-05-31 01:07:38,856 [IPC Server handler 16 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 2023-05-31 01:06:31,276 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-05-31 01:06:31,284 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-05-31 01:06:31,285 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-05-31 01:06:31,285 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1    | 2023-05-31 01:12:08,275 [snapshot-diff-job-thread-id-0] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
scm1_1   | 2023-05-31 01:06:28,906 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-05-31 01:07:40,982 [IPC Server handler 22 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-05-31 01:07:45,183 [IPC Server handler 3 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-05-31 01:07:45,355 [IPC Server handler 4 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-05-31 01:07:56,909 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for delTxnId, expected lastId is 0, actual lastId is 2000.
scm2_1   | 2023-05-31 01:08:08,480 [IPC Server handler 2 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn1_1    | 2023-05-31 01:06:46,582 [b20d0805-391e-447d-a646-35a5d1a5cbcf-server-thread1] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-SegmentedRaftLogWorker: Rolling segment log-29_42 to index:42
dn1_1    | 2023-05-31 01:06:46,584 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/current/log_inprogress_29 to /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/current/log_29-42
dn1_1    | 2023-05-31 01:06:46,595 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/current/log_inprogress_43
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1    | 2023-05-31 01:12:24,722 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotDeleteRequest: Deleted snapshot 'snapshot1' under path 'snapvolume-1/snapbucket-1'
scm1_1   | 2023-05-31 01:06:28,916 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861: skipped Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:40300
scm2_1   | 2023-05-31 01:08:08,840 [IPC Server handler 16 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-05-31 01:08:10,975 [IPC Server handler 18 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-05-31 01:08:14,278 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: ccaf8839-8737-4f75-8b75-b011fdd1ea12, Nodes: 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:4bc93e86-8989-4282-b7d8-050f4a19b92f, CreationTimestamp2023-05-31T01:06:22.725446Z[UTC]] removed.
scm2_1   | 2023-05-31 01:08:14,298 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 6a499bf5-10af-4dab-a3fd-a4e2998a83c9, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:b20d0805-391e-447d-a646-35a5d1a5cbcf, CreationTimestamp2023-05-31T01:06:22.724873Z[UTC]] removed.
scm2_1   | 2023-05-31 01:08:14,329 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 9e886fff-1860-4674-bb59-7a8b4be06a04, Nodes: 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:97734618-7966-42ac-b50e-e49ead6ce4d2, CreationTimestamp2023-05-31T01:06:22.725101Z[UTC]] removed.
dn1_1    | 2023-05-31 01:06:47,557 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:48,558 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:06:49,559 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | 2023-05-31 01:06:28,923 [grpc-default-executor-3] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 82 -> 0
dn1_1    | 2023-05-31 01:07:00,059 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn1_1    | 2023-05-31 01:07:35,949 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-05-31 01:08:11,950 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:58198 remote=scm2/10.9.0.15:9861]
scm1_1   | 2023-05-31 01:06:28,926 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861: skipped Call#2 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:48644
scm1_1   | 2023-05-31 01:06:28,946 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861: skipped Call#5 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:53120
scm1_1   | 2023-05-31 01:06:28,947 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861: skipped Call#2 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:46874
dn5_1    | 2023-05-31 01:06:31,285 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-05-31 01:06:31,287 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444
dn5_1    | 2023-05-31 01:06:31,288 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-05-31 01:08:11,952 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn1_1    | 2023-05-31 01:08:11,952 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn1_1    | 2023-05-31 01:08:11,953 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: OPEN
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
scm1_1   | 2023-05-31 01:06:28,948 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861: skipped Call#2 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:38708
dn5_1    | 2023-05-31 01:06:31,292 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-05-31 01:06:31,294 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-05-31 01:06:31,298 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-05-31 01:08:11,953 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn3_1    | 2023-05-31 01:09:09,853 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-05-31 01:09:09,853 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 2023-05-31 01:09:09,854 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-05-31 01:06:34,026 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 24
dn4_1    | 2023-05-31 01:06:34,026 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 10
scm1_1   | 2023-05-31 01:06:28,948 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861: skipped Call#5 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:48652
dn5_1    | 2023-05-31 01:06:31,300 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-05-31 01:06:31,288 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-05-31 01:06:31,293 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-05-31 01:09:09,854 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-05-31 01:09:09,854 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-05-31 01:09:09,855 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6: ConfigurationManager, init=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm1_1   | 2023-05-31 01:06:28,950 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861: skipped Call#5 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:53634
dn5_1    | 2023-05-31 01:06:31,307 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm2_1   | 2023-05-31 01:08:14,355 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 437d8269-2a0d-4058-8be6-ed0a589991d2, Nodes: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, CreationTimestamp2023-05-31T01:06:22.723150Z[UTC]] removed.
scm2_1   | 2023-05-31 01:08:14,388 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: a4fe2088-9059-47e2-9c65-dc0251522af1, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17)3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:3da1490a-d610-40de-95c1-bdfc259ba041, CreationTimestamp2023-05-31T01:06:22.725324Z[UTC]] removed.
dn3_1    | 2023-05-31 01:09:09,856 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-05-31 01:09:09,858 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-05-31 01:09:09,859 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn1_1    | 2023-05-31 01:08:11,953 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn4_1    | 2023-05-31 01:06:34,027 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO segmented.LogSegment: Successfully read 21 entries from segment file /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/current/log_8-28
dn4_1    | 2023-05-31 01:06:34,054 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1: set configuration 29: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:06:34,075 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO segmented.LogSegment: Successfully read 14 entries from segment file /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/current/log_inprogress_29
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn3_1    | 2023-05-31 01:09:09,859 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-05-31 01:09:09,861 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
scm2_1   | 2023-05-31 01:08:14,405 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 1f8ec4de-a531-4a7c-9881-f1a4cf07e444, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17)3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, CreationTimestamp2023-05-31T01:06:22.722438Z[UTC]] removed.
scm2_1   | 2023-05-31 01:08:14,428 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 5509b2c4-975c-42f1-91be-39d463d01a51, Nodes: 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:3da1490a-d610-40de-95c1-bdfc259ba041, CreationTimestamp2023-05-31T01:06:22.724378Z[UTC]] removed.
scm2_1   | 2023-05-31 01:08:15,188 [IPC Server handler 3 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-05-31 01:08:15,355 [IPC Server handler 4 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-05-31 01:08:16,773 [IPC Server handler 16 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-05-31 01:08:16,814 [IPC Server handler 17 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn3_1    | 2023-05-31 01:09:09,861 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
scm1_1   | 2023-05-31 01:06:28,950 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861: skipped Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:44814
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn4_1    | 2023-05-31 01:06:34,087 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 42
dn4_1    | 2023-05-31 01:06:34,087 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 28
dn4_1    | 2023-05-31 01:06:34,387 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1: start as a follower, conf=29: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:06:34,395 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1: changes role from      null to FOLLOWER at term 8 for startAsFollower
dn4_1    | 2023-05-31 01:06:34,452 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: start 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-FollowerState
dn4_1    | 2023-05-31 01:06:34,526 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-DC0251522AF1,id=3da1490a-d610-40de-95c1-bdfc259ba041
dn3_1    | 2023-05-31 01:09:09,862 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1_1   | 2023-05-31 01:06:28,951 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861: skipped Call#5 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:40302
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
scm2_1   | 2023-05-31 01:08:16,817 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to QUASI_CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported QUASI_CLOSED replica.
dn4_1    | 2023-05-31 01:06:34,572 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51: start as a follower, conf=3: peers:[3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 2023-05-31 01:07:00,057 [IPC Server handler 3 on default port 9861] WARN ipc.Server: IPC Server handler 3 on default port 9861, call Call#23 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:39602: output error
scm3_1   | 2023-05-31 01:07:00,057 [IPC Server handler 6 on default port 9861] WARN ipc.Server: IPC Server handler 6 on default port 9861, call Call#19 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:47908: output error
scm3_1   | 2023-05-31 01:07:00,080 [IPC Server handler 6 on default port 9861] INFO ipc.Server: IPC Server handler 6 on default port 9861 caught an exception
scm3_1   | java.nio.channels.ClosedChannelException
dn3_1    | 2023-05-31 01:09:09,862 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm1_1   | 2023-05-31 01:06:28,966 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861: skipped Call#5 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:59812
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
scm2_1   | 2023-05-31 01:08:16,890 [IPC Server handler 18 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-05-31 01:08:16,933 [FixedThreadPoolWithAffinityExecutor-9-0] WARN container.IncrementalContainerReportHandler: Failed to process QUASI_CLOSED Container #1 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E is not the leader 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn3_1    | 2023-05-31 01:09:09,871 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm1_1   | 2023-05-31 01:06:29,143 [grpc-default-executor-2] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 2023-05-31 01:06:35,374 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn4_1    | 2023-05-31 01:06:35,374 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: start 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-FollowerState
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn3_1    | 2023-05-31 01:09:09,872 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-05-31 01:06:29,148 [grpc-default-executor-2] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn4_1    | 2023-05-31 01:06:35,374 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
recon_1  | 2023-05-31 01:06:37,499 [IPC Server handler 13 on default port 9891] INFO ipc.Server: IPC Server handler 13 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn3_1    | 2023-05-31 01:09:09,872 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1_1   | 2023-05-31 01:06:29,172 [grpc-default-executor-2] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 2023-05-31 01:08:17,134 [IPC Server handler 3 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-05-31 01:08:17,157 [IPC Server handler 6 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-05-31 01:08:17,158 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to QUASI_CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported QUASI_CLOSED replica.
dn4_1    | 2023-05-31 01:06:35,374 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-05-31 01:06:35,375 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-05-31 01:09:09,872 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn2_1    | 2023-05-31 01:06:29,480 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn5_1    | 2023-05-31 01:06:31,324 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn2_1    | 2023-05-31 01:06:29,493 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn3_1    | 2023-05-31 01:09:09,873 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm2_1   | 2023-05-31 01:08:17,158 [FixedThreadPoolWithAffinityExecutor-9-0] WARN container.IncrementalContainerReportHandler: Failed to process QUASI_CLOSED Container #1001 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E is not the leader 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm2_1   | 2023-05-31 01:08:17,186 [IPC Server handler 4 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-05-31 01:08:17,232 [IPC Server handler 2 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-05-31 01:08:17,233 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to QUASI_CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported QUASI_CLOSED replica.
dn3_1    | 2023-05-31 01:09:09,873 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 2023-05-31 01:06:29,213 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-05-31 01:08:17,234 [FixedThreadPoolWithAffinityExecutor-9-0] WARN container.IncrementalContainerReportHandler: Failed to process QUASI_CLOSED Container #2 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E is not the leader 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm2_1   | 2023-05-31 01:08:17,892 [IPC Server handler 22 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn3_1    | 2023-05-31 01:09:09,873 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/b5c5c804-816d-41a2-83d8-297089fabfc6 does not exist. Creating ...
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn4_1    | 2023-05-31 01:06:35,375 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm2_1   | 2023-05-31 01:08:17,912 [IPC Server handler 20 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-05-31 01:08:17,947 [IPC Server handler 23 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-05-31 01:08:17,970 [IPC Server handler 21 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn3_1    | 2023-05-31 01:09:09,883 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/b5c5c804-816d-41a2-83d8-297089fabfc6/in_use.lock acquired by nodename 7@dcc4205663d5
dn3_1    | 2023-05-31 01:09:09,890 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/b5c5c804-816d-41a2-83d8-297089fabfc6 has been successfully formatted.
dn3_1    | 2023-05-31 01:09:09,936 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO ratis.ContainerStateMachine: group-297089FABFC6: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-05-31 01:09:09,940 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-05-31 01:06:34,581 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444: start as a follower, conf=11: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:06:35,386 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444: changes role from      null to FOLLOWER at term 11 for startAsFollower
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:733)
scm2_1   | 2023-05-31 01:08:18,417 [IPC Server handler 5 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn3_1    | 2023-05-31 01:09:09,940 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-05-31 01:09:09,941 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-05-31 01:09:09,942 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-05-31 01:09:09,942 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-05-31 01:06:35,386 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: start 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-FollowerState
dn5_1    | 2023-05-31 01:06:31,328 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-05-31 01:06:31,333 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm2_1   | 2023-05-31 01:08:18,434 [IPC Server handler 1 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm1_1   | 2023-05-31 01:06:29,215 [grpc-default-executor-3] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn3_1    | 2023-05-31 01:09:09,942 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-05-31 01:09:09,944 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-05-31 01:06:31,338 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-05-31 01:06:35,465 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-39D463D01A51,id=3da1490a-d610-40de-95c1-bdfc259ba041
scm2_1   | 2023-05-31 01:08:18,458 [IPC Server handler 7 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 2023-05-31 01:06:29,215 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn3_1    | 2023-05-31 01:09:09,945 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-05-31 01:09:09,945 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-05-31 01:08:18,475 [IPC Server handler 0 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn2_1    | 2023-05-31 01:06:29,879 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
scm1_1   | 2023-05-31 01:06:30,395 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:30,396 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-05-31 01:09:09,945 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/b5c5c804-816d-41a2-83d8-297089fabfc6
dn4_1    | 2023-05-31 01:06:35,466 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm2_1   | 2023-05-31 01:08:40,701 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18) moved to HEALTHY state.
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-05-31 01:06:37,499 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
scm1_1   | 2023-05-31 01:06:30,446 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-05-31 01:06:31,340 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:662)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-05-31 01:09:09,948 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-05-31 01:06:35,466 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
recon_1  | java.nio.channels.ClosedChannelException
dn2_1    | 2023-05-31 01:06:29,879 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 4bc93e86-8989-4282-b7d8-050f4a19b92f
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn3_1    | 2023-05-31 01:09:09,950 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-05-31 01:09:09,952 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-05-31 01:09:09,954 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-05-31 01:09:09,955 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | 2023-05-31 01:06:30,448 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:30,451 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
scm1_1   | 2023-05-31 01:06:30,540 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 2023-05-31 01:06:29,965 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ccaf8839-8737-4f75-8b75-b011fdd1ea12/in_use.lock acquired by nodename 7@86166c590ca2
dn5_1    | 2023-05-31 01:06:31,344 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-05-31 01:06:31,344 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-05-31 01:09:09,955 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-05-31 01:09:09,956 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-05-31 01:09:09,956 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-05-31 01:09:09,957 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
scm1_1   | 2023-05-31 01:06:30,543 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:30,548 [grpc-default-executor-3] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn2_1    | 2023-05-31 01:06:30,019 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=4bc93e86-8989-4282-b7d8-050f4a19b92f} from /data/metadata/ratis/ccaf8839-8737-4f75-8b75-b011fdd1ea12/current/raft-meta
dn1_1    | 2023-05-31 01:08:11,954 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn3_1    | 2023-05-31 01:09:09,961 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-05-31 01:09:10,007 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-05-31 01:09:10,008 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-05-31 01:09:10,008 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-05-31 01:09:10,008 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO segmented.SegmentedRaftLogWorker: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm2_1   | 2023-05-31 01:08:40,702 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-05-31 01:08:40,702 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19) moved to HEALTHY state.
dn5_1    | 2023-05-31 01:06:31,344 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-05-31 01:06:31,347 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-05-31 01:06:31,391 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-05-31 01:06:31,401 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-05-31 01:09:10,008 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO segmented.SegmentedRaftLogWorker: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-05-31 01:09:10,011 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6: start as a follower, conf=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-31 01:09:10,012 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn3_1    | 2023-05-31 01:09:10,012 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO impl.RoleInfo: 97734618-7966-42ac-b50e-e49ead6ce4d2: start 97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-FollowerState
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn5_1    | 2023-05-31 01:06:31,393 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-05-31 01:06:30,259 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn4_1    | 2023-05-31 01:06:35,466 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-05-31 01:06:35,466 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn2_1    | 2023-05-31 01:06:30,339 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12: set configuration 3: peers:[4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 2023-05-31 01:06:30,662 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-05-31 01:09:10,012 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-297089FABFC6,id=97734618-7966-42ac-b50e-e49ead6ce4d2
dn3_1    | 2023-05-31 01:09:10,012 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 2023-05-31 01:07:00,055 [IPC Server handler 7 on default port 9861] WARN ipc.Server: IPC Server handler 7 on default port 9861, call Call#24 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:56214: output error
scm3_1   | 2023-05-31 01:07:00,083 [IPC Server handler 7 on default port 9861] INFO ipc.Server: IPC Server handler 7 on default port 9861 caught an exception
dn2_1    | 2023-05-31 01:06:30,435 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO ratis.ContainerStateMachine: group-B011FDD1EA12: Setting the last applied index to (t:3, i:4)
dn2_1    | 2023-05-31 01:06:31,255 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 2023-05-31 01:06:30,669 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-05-31 01:09:10,012 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-05-31 01:06:35,472 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-31 01:06:35,513 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-05-31 01:06:35,513 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-31 01:06:35,514 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F1A4CF07E444,id=3da1490a-d610-40de-95c1-bdfc259ba041
dn2_1    | 2023-05-31 01:06:31,262 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:31,327 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn3_1    | 2023-05-31 01:09:10,012 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-05-31 01:06:35,514 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-05-31 01:06:35,514 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-05-31 01:06:35,514 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-05-31 01:06:35,515 [3da1490a-d610-40de-95c1-bdfc259ba041-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-05-31 01:06:31,335 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:733)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn4_1    | 2023-05-31 01:06:35,515 [3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-05-31 01:06:35,515 [3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-31 01:06:35,574 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-05-31 01:06:35,574 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-31 01:06:35,580 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 3da1490a-d610-40de-95c1-bdfc259ba041: start RPC server
dn2_1    | 2023-05-31 01:06:31,342 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-05-31 01:06:31,348 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm1_1   | 2023-05-31 01:06:30,674 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn3_1    | 2023-05-31 01:09:10,012 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-05-31 01:06:35,632 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 3da1490a-d610-40de-95c1-bdfc259ba041: GrpcService started, listening on 9858
dn2_1    | 2023-05-31 01:06:31,366 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn5_1    | 2023-05-31 01:06:31,408 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:06:31,403 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-05-31 01:06:35,684 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 3da1490a-d610-40de-95c1-bdfc259ba041: GrpcService started, listening on 9856
dn4_1    | 2023-05-31 01:06:35,697 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 3da1490a-d610-40de-95c1-bdfc259ba041: GrpcService started, listening on 9857
dn4_1    | 2023-05-31 01:06:35,863 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3da1490a-d610-40de-95c1-bdfc259ba041 is started using port 9858 for RATIS
dn4_1    | 2023-05-31 01:06:35,865 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3da1490a-d610-40de-95c1-bdfc259ba041 is started using port 9857 for RATIS_ADMIN
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm2_1   | 2023-05-31 01:08:40,702 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-05-31 01:08:40,972 [IPC Server handler 21 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn4_1    | 2023-05-31 01:06:35,865 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3da1490a-d610-40de-95c1-bdfc259ba041 is started using port 9856 for RATIS_SERVER
dn4_1    | 2023-05-31 01:06:35,868 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-3da1490a-d610-40de-95c1-bdfc259ba041: Started
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn2_1    | 2023-05-31 01:06:31,429 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-05-31 01:09:10,014 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm2_1   | 2023-05-31 01:08:41,138 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 46a9d8fb-17d8-4177-820b-4ef30e5fdeea, Nodes: 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:41.112Z[UTC]].
scm2_1   | 2023-05-31 01:08:41,158 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: b5c5c804-816d-41a2-83d8-297089fabfc6, Nodes: 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:41.141Z[UTC]].
dn5_1    | 2023-05-31 01:06:31,409 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:662)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-05-31 01:06:37,499 [IPC Server handler 8 on default port 9891] INFO ipc.Server: IPC Server handler 8 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
scm2_1   | 2023-05-31 01:08:42,916 [IPC Server handler 20 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-05-31 01:08:42,952 [IPC Server handler 21 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-05-31 01:08:42,963 [IPC Server handler 25 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn1_1    | 2023-05-31 01:08:11,954 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | 2023-05-31 01:06:31,433 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-05-31 01:06:31,433 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-05-31 01:06:31,479 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ccaf8839-8737-4f75-8b75-b011fdd1ea12
scm2_1   | 2023-05-31 01:08:42,981 [IPC Server handler 24 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-05-31 01:08:43,524 [IPC Server handler 10 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-05-31 01:08:43,535 [IPC Server handler 11 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn1_1    | 2023-05-31 01:08:11,954 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn2_1    | 2023-05-31 01:06:31,482 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-05-31 01:06:31,483 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-05-31 01:06:31,493 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-05-31 01:06:35,931 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn4_1    | 2023-05-31 01:06:35,943 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm2_1   | 2023-05-31 01:08:43,552 [IPC Server handler 16 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 2023-05-31 01:06:32,384 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:32,396 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-05-31 01:08:11,954 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
scm1_1   | 2023-05-31 01:06:30,747 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn2_1    | 2023-05-31 01:06:31,503 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-05-31 01:06:31,503 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-05-31 01:06:31,516 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-05-31 01:06:31,516 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-05-31 01:08:11,954 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: OPEN
scm1_1   | 2023-05-31 01:06:30,749 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-05-31 01:06:36,477 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-31 01:06:37,474 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn4_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm2_1   | 2023-05-31 01:08:43,571 [IPC Server handler 17 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn1_1    | 2023-05-31 01:08:11,954 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
scm1_1   | 2023-05-31 01:06:30,764 [grpc-default-executor-3] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
scm1_1   | 2023-05-31 01:06:30,907 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn5_1    | 2023-05-31 01:06:32,404 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-05-31 01:06:32,409 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm3_1   | java.nio.channels.ClosedChannelException
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm2_1   | 2023-05-31 01:08:49,702 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) moved to HEALTHY state.
dn1_1    | 2023-05-31 01:08:11,954 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn3_1    | 2023-05-31 01:09:10,029 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=b5c5c804-816d-41a2-83d8-297089fabfc6
scm1_1   | 2023-05-31 01:06:30,908 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn5_1    | 2023-05-31 01:06:32,495 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-05-31 01:06:32,495 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm2_1   | 2023-05-31 01:08:49,702 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn1_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
scm1_1   | 2023-05-31 01:06:30,912 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn5_1    | 2023-05-31 01:06:32,496 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-05-31 01:06:32,562 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-05-31 01:06:32,563 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn3_1    | 2023-05-31 01:09:10,030 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
scm1_1   | 2023-05-31 01:06:31,027 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-05-31 01:06:32,563 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-05-31 01:06:32,798 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444: set configuration 0: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
scm2_1   | 2023-05-31 01:08:49,703 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21) moved to HEALTHY state.
dn3_1    | 2023-05-31 01:09:10,030 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=b5c5c804-816d-41a2-83d8-297089fabfc6.
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
scm1_1   | 2023-05-31 01:06:31,032 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 2023-05-31 01:06:31,517 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-05-31 01:06:31,618 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-05-31 01:06:31,624 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
dn3_1    | 2023-05-31 01:09:10,032 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 97734618-7966-42ac-b50e-e49ead6ce4d2: addNew group-6F942E88FF83:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] returns group-6F942E88FF83:java.util.concurrent.CompletableFuture@13a01f1a[Not completed]
dn3_1    | 2023-05-31 01:09:10,035 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2: new RaftServerImpl for group-6F942E88FF83:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:733)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn2_1    | 2023-05-31 01:06:31,725 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn3_1    | 2023-05-31 01:09:10,036 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-05-31 01:09:10,036 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:662)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 2023-05-31 01:06:31,733 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-05-31 01:06:31,733 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn3_1    | 2023-05-31 01:09:10,036 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-05-31 01:09:10,036 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-05-31 01:09:10,037 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-05-31 01:09:10,038 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-05-31 01:08:11,954 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn3_1    | 2023-05-31 01:09:10,039 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83: ConfigurationManager, init=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-05-31 01:06:32,799 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1: set configuration 0: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:06:32,802 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2: set configuration 0: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:06:32,826 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/437d8269-2a0d-4058-8be6-ed0a589991d2/current/log_0-0
dn1_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn3_1    | 2023-05-31 01:09:10,039 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm2_1   | 2023-05-31 01:08:49,703 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-05-31 01:08:50,129 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e689613f-6a5f-45fa-9ee1-234a7250e2df, Nodes: 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:50.109Z[UTC]].
scm2_1   | 2023-05-31 01:08:50,152 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 9342e1de-b2fb-494f-91d9-6f942e88ff83, Nodes: 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19)3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21)3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:50.131Z[UTC]].
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn3_1    | 2023-05-31 01:09:10,039 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm2_1   | 2023-05-31 01:08:50,185 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: cabada01-83b4-406c-b6ca-8f00a57da29e, Nodes: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:50.144Z[UTC]].
scm2_1   | 2023-05-31 01:08:50,205 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 5d339866-97f6-4ef6-b080-a59219751d93, Nodes: 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18)97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:50.188Z[UTC]].
scm2_1   | 2023-05-31 01:09:09,532 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 46a9d8fb-17d8-4177-820b-4ef30e5fdeea, Nodes: 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:4bc93e86-8989-4282-b7d8-050f4a19b92f, CreationTimestamp2023-05-31T01:08:41.112Z[UTC]] moved to OPEN state
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:733)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 2023-05-31 01:09:10,039 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-05-31 01:06:32,890 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO segmented.LogSegment: Successfully read 5 entries from segment file /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/current/log_0-4
dn5_1    | 2023-05-31 01:06:32,890 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO segmented.LogSegment: Successfully read 8 entries from segment file /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/current/log_0-7
scm2_1   | 2023-05-31 01:09:09,906 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: b5c5c804-816d-41a2-83d8-297089fabfc6, Nodes: 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:97734618-7966-42ac-b50e-e49ead6ce4d2, CreationTimestamp2023-05-31T01:08:41.141Z[UTC]] moved to OPEN state
scm2_1   | 2023-05-31 01:09:13,704 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17) moved to HEALTHY state.
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
scm2_1   | 2023-05-31 01:09:13,705 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn2_1    | 2023-05-31 01:06:31,941 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12: set configuration 0: peers:[4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-31 01:09:10,040 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-05-31 01:06:32,942 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1: set configuration 8: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:538)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:662)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2_1   | 2023-05-31 01:09:14,130 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: ec1b01cd-6e71-4daf-8419-6155613624a8, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:09:14.112Z[UTC]].
dn2_1    | 2023-05-31 01:06:31,946 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/ccaf8839-8737-4f75-8b75-b011fdd1ea12/current/log_0-0
dn3_1    | 2023-05-31 01:09:10,040 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn5_1    | 2023-05-31 01:06:32,958 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444: set configuration 5: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:06:32,969 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO segmented.LogSegment: Successfully read 21 entries from segment file /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/current/log_8-28
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 2023-05-31 01:06:31,043 [grpc-default-executor-3] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn1_1    | 2023-05-31 01:08:16,381 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: b20d0805-391e-447d-a646-35a5d1a5cbcf: Completed APPEND_ENTRIES, lastRequest: 3da1490a-d610-40de-95c1-bdfc259ba041->b20d0805-391e-447d-a646-35a5d1a5cbcf#2-t9,previous=(t:9, i:43),leaderCommit=43,initializing? true,entries: size=1, first=(t:9, i:44), METADATAENTRY(c:43)
dn1_1    | 2023-05-31 01:08:16,381 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: b20d0805-391e-447d-a646-35a5d1a5cbcf: Completed APPEND_ENTRIES, lastReply: null
scm2_1   | 2023-05-31 01:09:14,152 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 46ae8d39-3c2f-48ac-8229-b2777f51f5a3, Nodes: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21)b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17)4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:09:14.136Z[UTC]].
dn2_1    | 2023-05-31 01:06:31,968 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12: set configuration 1: peers:[4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-31 01:09:10,041 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | 2023-05-31 01:06:33,016 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO segmented.LogSegment: Successfully read 6 entries from segment file /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/current/log_5-10
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn1_1    | 2023-05-31 01:08:16,387 [grpc-default-executor-7] INFO server.GrpcServerProtocolService: b20d0805-391e-447d-a646-35a5d1a5cbcf: Completed APPEND_ENTRIES, lastRequest: null
scm2_1   | 2023-05-31 01:09:15,771 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 5d339866-97f6-4ef6-b080-a59219751d93, Nodes: 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18)97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:3da1490a-d610-40de-95c1-bdfc259ba041, CreationTimestamp2023-05-31T01:08:50.188Z[UTC]] moved to OPEN state
dn2_1    | 2023-05-31 01:06:31,985 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/ccaf8839-8737-4f75-8b75-b011fdd1ea12/current/log_1-2
dn2_1    | 2023-05-31 01:06:32,003 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12: set configuration 3: peers:[4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-31 01:06:32,004 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/ccaf8839-8737-4f75-8b75-b011fdd1ea12/current/log_inprogress_3
dn5_1    | 2023-05-31 01:06:33,016 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2: set configuration 1: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | Caused by: java.util.concurrent.TimeoutException
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn1_1    | 2023-05-31 01:08:16,428 [grpc-default-executor-7] INFO server.GrpcServerProtocolService: b20d0805-391e-447d-a646-35a5d1a5cbcf: Completed APPEND_ENTRIES, lastReply: serverReply {
scm2_1   | 2023-05-31 01:09:16,723 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 9342e1de-b2fb-494f-91d9-6f942e88ff83, Nodes: 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19)3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21)3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, CreationTimestamp2023-05-31T01:08:50.131Z[UTC]] moved to OPEN state
scm2_1   | 2023-05-31 01:09:18,255 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e689613f-6a5f-45fa-9ee1-234a7250e2df, Nodes: 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3da1490a-d610-40de-95c1-bdfc259ba041, CreationTimestamp2023-05-31T01:08:50.109Z[UTC]] moved to OPEN state
dn3_1    | 2023-05-31 01:09:10,041 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-05-31 01:09:10,041 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn5_1    | 2023-05-31 01:06:33,019 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444: set configuration 11: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn1_1    |   requestorId: "3da1490a-d610-40de-95c1-bdfc259ba041"
dn2_1    | 2023-05-31 01:06:32,024 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
scm2_1   | 2023-05-31 01:09:19,498 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: cabada01-83b4-406c-b6ca-8f00a57da29e, Nodes: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, CreationTimestamp2023-05-31T01:08:50.144Z[UTC]] moved to OPEN state
dn3_1    | 2023-05-31 01:09:10,043 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-05-31 01:06:37,499 [IPC Server handler 6 on default port 9891] INFO ipc.Server: IPC Server handler 6 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn1_1    |   replyId: "b20d0805-391e-447d-a646-35a5d1a5cbcf"
dn2_1    | 2023-05-31 01:06:32,025 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
scm2_1   | 2023-05-31 01:09:25,559 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 46ae8d39-3c2f-48ac-8229-b2777f51f5a3, Nodes: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21)b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17)4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:b20d0805-391e-447d-a646-35a5d1a5cbcf, CreationTimestamp2023-05-31T01:09:14.136Z[UTC]] moved to OPEN state
dn3_1    | 2023-05-31 01:09:10,043 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn5_1    | 2023-05-31 01:06:33,019 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/437d8269-2a0d-4058-8be6-ed0a589991d2/current/log_1-2
scm1_1   | 2023-05-31 01:06:31,212 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn1_1    |   raftGroupId {
dn2_1    | 2023-05-31 01:06:32,273 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-05-31 01:09:44,861 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: ec1b01cd-6e71-4daf-8419-6155613624a8, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:b20d0805-391e-447d-a646-35a5d1a5cbcf, CreationTimestamp2023-05-31T01:09:14.112Z[UTC]] moved to OPEN state
dn3_1    | 2023-05-31 01:09:10,044 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn5_1    | 2023-05-31 01:06:33,017 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1: set configuration 29: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:06:31,219 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    |     id: "\244\376 \210\220YG\342\234e\334\002QR*\361"
dn2_1    | 2023-05-31 01:06:32,746 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12: start as a follower, conf=3: peers:[4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-05-31 01:10:20,059 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
dn3_1    | 2023-05-31 01:09:10,044 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 2023-05-31 01:06:33,033 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2: set configuration 3: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:06:31,230 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
scm1_1   | 2023-05-31 01:06:31,650 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 2023-05-31 01:10:23,420 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
dn3_1    | 2023-05-31 01:09:10,044 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn4_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn5_1    | 2023-05-31 01:06:33,056 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/437d8269-2a0d-4058-8be6-ed0a589991d2/current/log_inprogress_3
scm1_1   | 2023-05-31 01:06:31,650 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:31,735 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn1_1    |   }
dn1_1    |   callId: 41
dn3_1    | 2023-05-31 01:09:10,044 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 2023-05-31 01:06:33,021 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO segmented.LogSegment: Successfully read 14 entries from segment file /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/current/log_inprogress_11
scm1_1   | 2023-05-31 01:06:31,742 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:31,761 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 2023-05-31 01:07:00,081 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861 caught an exception
scm3_1   | java.nio.channels.ClosedChannelException
scm2_1   | 2023-05-31 01:10:24,994 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
dn3_1    | 2023-05-31 01:09:10,045 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/9342e1de-b2fb-494f-91d9-6f942e88ff83 does not exist. Creating ...
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn5_1    | 2023-05-31 01:06:33,060 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO segmented.LogSegment: Successfully read 14 entries from segment file /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/current/log_inprogress_29
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm1_1   | 2023-05-31 01:06:31,792 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-05-31 01:10:46,700 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for containerId, expected lastId is 0, actual lastId is 2000.
dn3_1    | 2023-05-31 01:09:10,047 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/9342e1de-b2fb-494f-91d9-6f942e88ff83/in_use.lock acquired by nodename 7@dcc4205663d5
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 2023-05-31 01:06:33,106 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn2_1    | 2023-05-31 01:06:32,746 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12: changes role from      null to FOLLOWER at term 3 for startAsFollower
scm1_1   | 2023-05-31 01:06:31,794 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:31,812 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
scm1_1   | 2023-05-31 01:06:31,898 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	... 1 more
dn5_1    | 2023-05-31 01:06:33,106 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn2_1    | 2023-05-31 01:06:32,785 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO impl.RoleInfo: 4bc93e86-8989-4282-b7d8-050f4a19b92f: start 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-FollowerState
dn2_1    | 2023-05-31 01:06:32,797 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm2_1   | 2023-05-31 01:10:46,774 [4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019203000.
dn3_1    | 2023-05-31 01:09:10,063 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/9342e1de-b2fb-494f-91d9-6f942e88ff83 has been successfully formatted.
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn4_1    | 2023-05-31 01:06:37,487 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:33,110 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 24
dn2_1    | 2023-05-31 01:06:32,815 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-05-31 01:06:32,827 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B011FDD1EA12,id=4bc93e86-8989-4282-b7d8-050f4a19b92f
scm1_1   | 2023-05-31 01:06:31,899 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:31,915 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
scm2_1   | 2023-05-31 01:10:49,496 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn4_1    | 2023-05-31 01:06:38,492 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-31 01:06:39,493 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:32,856 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-05-31 01:06:32,906 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm1_1   | 2023-05-31 01:06:31,978 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:31,983 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn4_1    | 2023-05-31 01:06:40,493 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-31 01:06:40,595 [3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-FollowerState] INFO impl.FollowerState: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5208691243ns, electionTimeout:5078ms
dn2_1    | 2023-05-31 01:06:32,912 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm1_1   | 2023-05-31 01:06:31,988 [grpc-default-executor-3] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn1_1    |   success: true
dn1_1    | }
dn1_1    | term: 9
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn4_1    | 2023-05-31 01:06:40,596 [3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-FollowerState] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: shutdown 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-FollowerState
dn4_1    | 2023-05-31 01:06:40,596 [3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-FollowerState] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444: changes role from  FOLLOWER to CANDIDATE at term 11 for changeToCandidate
dn2_1    | 2023-05-31 01:06:32,932 [4bc93e86-8989-4282-b7d8-050f4a19b92f-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1   | 2023-05-31 01:06:32,108 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:32,169 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:32,207 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
scm1_1   | 2023-05-31 01:06:32,607 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn5_1    | 2023-05-31 01:06:33,111 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 10
dn2_1    | 2023-05-31 01:06:33,064 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 4bc93e86-8989-4282-b7d8-050f4a19b92f: start RPC server
scm1_1   | 2023-05-31 01:06:32,611 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:32,644 [grpc-default-executor-3] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 50 -> 0
scm1_1   | 2023-05-31 01:06:32,907 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:32,908 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn4_1    | 2023-05-31 01:06:40,598 [3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-05-31 01:06:33,133 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 42
dn2_1    | 2023-05-31 01:06:33,140 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 4bc93e86-8989-4282-b7d8-050f4a19b92f: GrpcService started, listening on 9858
dn2_1    | 2023-05-31 01:06:33,167 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 4bc93e86-8989-4282-b7d8-050f4a19b92f: GrpcService started, listening on 9856
scm1_1   | 2023-05-31 01:06:33,068 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:33,075 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:33,107 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-05-31 01:06:37,499 [IPC Server handler 16 on default port 9891] INFO ipc.Server: IPC Server handler 16 on default port 9891 caught an exception
dn2_1    | 2023-05-31 01:06:33,176 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 4bc93e86-8989-4282-b7d8-050f4a19b92f: GrpcService started, listening on 9857
dn1_1    | nextIndex: 45
dn1_1    | followerCommit: 44
scm1_1   | 2023-05-31 01:06:33,237 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-05-31 01:06:33,160 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 28
scm2_1   | 2023-05-31 01:10:50,063 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
dn4_1    | 2023-05-31 01:06:40,598 [3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-FollowerState] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: start 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-LeaderElection1
dn4_1    | 2023-05-31 01:06:40,602 [3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-LeaderElection1] INFO impl.LeaderElection: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 11 for 11: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-31 01:06:33,201 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-4bc93e86-8989-4282-b7d8-050f4a19b92f: Started
dn1_1    | matchIndex: 18446744073709551615
scm1_1   | 2023-05-31 01:06:33,237 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-05-31 01:09:10,066 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO ratis.ContainerStateMachine: group-6F942E88FF83: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn5_1    | 2023-05-31 01:06:33,397 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-05-31 01:10:54,932 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
dn4_1    | 2023-05-31 01:06:40,673 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-FollowerState] INFO impl.FollowerState: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5299596494ns, electionTimeout:5160ms
dn4_1    | 2023-05-31 01:06:40,675 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-FollowerState] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: shutdown 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-FollowerState
dn2_1    | 2023-05-31 01:06:33,211 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 4bc93e86-8989-4282-b7d8-050f4a19b92f is started using port 9858 for RATIS
dn1_1    | isHearbeat: true
scm1_1   | 2023-05-31 01:06:33,250 [grpc-default-executor-3] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn3_1    | 2023-05-31 01:09:10,066 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-05-31 01:06:33,547 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444: start as a follower, conf=11: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-05-31 01:11:20,411 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn2_1    | 2023-05-31 01:06:33,211 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 4bc93e86-8989-4282-b7d8-050f4a19b92f is started using port 9857 for RATIS_ADMIN
dn1_1    | 
dn1_1    | 2023-05-31 01:08:17,982 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: b20d0805-391e-447d-a646-35a5d1a5cbcf: Completed APPEND_ENTRIES, lastRequest: null
dn1_1    | 2023-05-31 01:08:17,982 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: b20d0805-391e-447d-a646-35a5d1a5cbcf: Completed APPEND_ENTRIES, lastReply: serverReply {
dn1_1    |   requestorId: "3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd"
dn3_1    | 2023-05-31 01:09:10,067 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn4_1    | 2023-05-31 01:06:40,675 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-FollowerState] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
scm1_1   | 2023-05-31 01:06:33,357 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-05-31 01:11:20,419 [FixedThreadPoolWithAffinityExecutor-9-0] WARN container.IncrementalContainerReportHandler: Failed to process CLOSED Container #1 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E is not the leader 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn5_1    | 2023-05-31 01:06:33,556 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444: changes role from      null to FOLLOWER at term 11 for startAsFollower
dn1_1    |   replyId: "b20d0805-391e-447d-a646-35a5d1a5cbcf"
dn1_1    |   raftGroupId {
dn1_1    |     id: "\037\216\304\336\2451J|\230\201\361\244\317\a\344D"
dn4_1    | 2023-05-31 01:06:40,675 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | 2023-05-31 01:06:40,675 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-FollowerState] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: start 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm1_1   | 2023-05-31 01:06:33,364 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-05-31 01:11:20,435 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
dn5_1    | 2023-05-31 01:06:33,565 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: start 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-FollowerState
dn1_1    |   }
dn1_1    |   callId: 42
dn4_1    | 2023-05-31 01:06:40,714 [3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn2_1    | 2023-05-31 01:06:33,218 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 4bc93e86-8989-4282-b7d8-050f4a19b92f is started using port 9856 for RATIS_SERVER
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 2023-05-31 01:06:33,374 [grpc-default-executor-3] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
scm1_1   | 2023-05-31 01:06:33,567 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-05-31 01:06:33,578 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F1A4CF07E444,id=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn3_1    | 2023-05-31 01:09:10,067 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-31 01:06:40,788 [3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn2_1    | 2023-05-31 01:06:33,289 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:33,322 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 2023-05-31 01:06:33,602 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    |   success: true
dn5_1    | 2023-05-31 01:06:33,588 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-05-31 01:09:10,067 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-05-31 01:09:10,067 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn2_1    | 2023-05-31 01:06:33,359 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm1_1   | 2023-05-31 01:06:33,656 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn5_1    | 2023-05-31 01:06:33,590 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-05-31 01:09:10,089 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-05-31 01:06:40,741 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-FollowerState] INFO impl.FollowerState: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-FollowerState: change to CANDIDATE, lastRpcElapsedTime:6316934492ns, electionTimeout:5165ms
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm2_1   | 2023-05-31 01:11:20,469 [FixedThreadPoolWithAffinityExecutor-9-0] WARN container.IncrementalContainerReportHandler: Failed to process CLOSED Container #2 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E is not the leader 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn2_1    | 2023-05-31 01:06:34,289 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm1_1   | 2023-05-31 01:06:34,186 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-05-31 01:06:40,810 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-FollowerState] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: shutdown 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-FollowerState
scm2_1   | 2023-05-31 01:11:20,470 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
scm1_1   | 2023-05-31 01:06:34,188 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:34,335 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-05-31 01:06:33,592 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1: start as a follower, conf=29: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-31 01:09:10,089 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-05-31 01:09:10,092 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-05-31 01:09:10,094 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn2_1    | 2023-05-31 01:06:35,290 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:33,613 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1: changes role from      null to FOLLOWER at term 8 for startAsFollower
dn3_1    | 2023-05-31 01:09:10,094 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/9342e1de-b2fb-494f-91d9-6f942e88ff83
dn3_1    | 2023-05-31 01:09:10,094 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-05-31 01:09:10,094 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 2023-05-31 01:07:00,071 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861 caught an exception
dn5_1    | 2023-05-31 01:06:33,591 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2: start as a follower, conf=3: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-31 01:09:10,095 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-05-31 01:09:10,095 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-05-31 01:09:10,096 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm3_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn2_1    | 2023-05-31 01:06:36,323 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-05-31 01:11:20,475 [FixedThreadPoolWithAffinityExecutor-9-0] WARN container.IncrementalContainerReportHandler: Failed to process CLOSED Container #1001 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E is not the leader 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn5_1    | 2023-05-31 01:06:33,614 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn5_1    | 2023-05-31 01:06:33,614 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: start 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-FollowerState
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn4_1    | 2023-05-31 01:06:40,811 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-FollowerState] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1: changes role from  FOLLOWER to CANDIDATE at term 8 for changeToCandidate
dn4_1    | 2023-05-31 01:06:40,811 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn2_1    | 2023-05-31 01:06:37,327 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-05-31 01:06:34,339 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-05-31 01:11:20,475 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1002 to CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
scm2_1   | 2023-05-31 01:11:20,477 [FixedThreadPoolWithAffinityExecutor-9-0] WARN container.IncrementalContainerReportHandler: Failed to process CLOSED Container #1002 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server 4efa1e6c-ae38-4de8-83ba-fa8c46ff54c2@group-6A492392CC3E is not the leader 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn5_1    | 2023-05-31 01:06:33,590 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn2_1    | 2023-05-31 01:06:37,990 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-FollowerState] INFO impl.FollowerState: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5205429614ns, electionTimeout:5174ms
dn2_1    | 2023-05-31 01:06:37,991 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-FollowerState] INFO impl.RoleInfo: 4bc93e86-8989-4282-b7d8-050f4a19b92f: shutdown 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-FollowerState
scm2_1   | 2023-05-31 01:11:22,860 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
dn5_1    | 2023-05-31 01:06:33,613 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: start 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-FollowerState
dn5_1    | 2023-05-31 01:06:33,621 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-ED0A589991D2,id=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn1_1    | }
dn4_1    | 2023-05-31 01:06:40,811 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-FollowerState] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: start 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3
dn4_1    | 2023-05-31 01:06:40,811 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2] INFO impl.LeaderElection: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-31 01:06:37,991 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-FollowerState] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
scm1_1   | 2023-05-31 01:06:34,380 [IPC Server handler 3 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/97734618-7966-42ac-b50e-e49ead6ce4d2
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn1_1    | term: 12
dn1_1    | nextIndex: 27
dn2_1    | 2023-05-31 01:06:38,003 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn2_1    | 2023-05-31 01:06:38,004 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-FollowerState] INFO impl.RoleInfo: 4bc93e86-8989-4282-b7d8-050f4a19b92f: start 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 2023-05-31 01:06:34,409 [IPC Server handler 3 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 97734618-7966-42ac-b50e-e49ead6ce4d2{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn4_1    | 2023-05-31 01:06:40,815 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2] INFO impl.LeaderElection: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2 PRE_VOTE round 0: result PASSED (term=3)
dn4_1    | 2023-05-31 01:06:40,880 [3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for b20d0805-391e-447d-a646-35a5d1a5cbcf
dn4_1    | 2023-05-31 01:06:40,880 [3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn4_1    | 2023-05-31 01:06:40,910 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO impl.LeaderElection: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 8 for 29: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn3_1    | 2023-05-31 01:09:10,096 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-05-31 01:09:10,097 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-05-31 01:09:10,097 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm1_1   | 2023-05-31 01:06:34,653 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:34,653 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-05-31 01:06:40,945 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn3_1    | 2023-05-31 01:09:10,099 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn2_1    | 2023-05-31 01:06:38,109 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1] INFO impl.LeaderElection: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-31 01:06:38,142 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1] INFO impl.LeaderElection: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1 PRE_VOTE round 0: result PASSED (term=3)
scm1_1   | 2023-05-31 01:06:34,658 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:34,661 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn5_1    | 2023-05-31 01:06:33,622 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-05-31 01:09:10,105 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn1_1    | followerCommit: 26
dn1_1    | matchIndex: 18446744073709551615
scm1_1   | 2023-05-31 01:06:34,667 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:34,681 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn5_1    | 2023-05-31 01:06:33,622 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn1_1    | isHearbeat: true
dn1_1    | 
scm1_1   | 2023-05-31 01:06:34,652 [IPC Server handler 6 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/4bc93e86-8989-4282-b7d8-050f4a19b92f
scm1_1   | 2023-05-31 01:06:34,798 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn5_1    | 2023-05-31 01:06:33,622 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn1_1    | 2023-05-31 01:08:17,985 [grpc-default-executor-7] INFO server.GrpcServerProtocolService: b20d0805-391e-447d-a646-35a5d1a5cbcf: Completed APPEND_ENTRIES, lastRequest: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd->b20d0805-391e-447d-a646-35a5d1a5cbcf#2-t12,previous=(t:12, i:25),leaderCommit=25,initializing? true,entries: size=1, first=(t:12, i:26), METADATAENTRY(c:25)
dn1_1    | 2023-05-31 01:08:17,985 [grpc-default-executor-7] INFO server.GrpcServerProtocolService: b20d0805-391e-447d-a646-35a5d1a5cbcf: Completed APPEND_ENTRIES, lastReply: null
dn4_1    | 2023-05-31 01:06:40,949 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-31 01:06:41,095 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2] INFO impl.LeaderElection: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2 ELECTION round 0: submit vote requests at term 4 for 3: peers:[3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn5_1    | 2023-05-31 01:06:33,623 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn2_1    | 2023-05-31 01:06:38,331 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:38,409 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1] INFO impl.LeaderElection: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1 ELECTION round 0: submit vote requests at term 4 for 3: peers:[4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:06:34,801 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | 2023-05-31 01:08:22,296 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO impl.FollowerState: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5587174434ns, electionTimeout:5114ms
dn1_1    | 2023-05-31 01:08:22,296 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState
dn5_1    | 2023-05-31 01:06:33,639 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm3_1   | 2023-05-31 01:07:00,678 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@46cf8c07] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn2_1    | 2023-05-31 01:06:38,409 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1] INFO impl.LeaderElection: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1 ELECTION round 0: result PASSED (term=4)
dn4_1    | 2023-05-31 01:06:41,114 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2] INFO impl.LeaderElection: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2 ELECTION round 0: result PASSED (term=4)
scm1_1   | 2023-05-31 01:06:34,715 [IPC Server handler 6 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 4bc93e86-8989-4282-b7d8-050f4a19b92f{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-05-31 01:06:37,499 [IPC Server handler 3 on default port 9891] INFO ipc.Server: IPC Server handler 3 on default port 9891 caught an exception
dn1_1    | 2023-05-31 01:08:22,296 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: changes role from  FOLLOWER to CANDIDATE at term 12 for changeToCandidate
dn1_1    | 2023-05-31 01:08:22,297 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm3_1   | 2023-05-31 01:07:00,742 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm3_1   | 2023-05-31 01:07:00,743 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm3_1   | 2023-05-31 01:07:00,832 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @35029ms to org.eclipse.jetty.util.log.Slf4jLog
scm3_1   | 2023-05-31 01:07:01,267 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn5_1    | 2023-05-31 01:06:33,640 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1   | 2023-05-31 01:06:34,957 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-05-31 01:09:10,153 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-05-31 01:09:10,153 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-05-31 01:09:10,153 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-05-31 01:09:10,153 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO segmented.SegmentedRaftLogWorker: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm3_1   | 2023-05-31 01:07:01,283 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm3_1   | 2023-05-31 01:07:01,318 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn5_1    | 2023-05-31 01:06:34,300 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-05-31 01:06:41,115 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: shutdown 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2
scm1_1   | 2023-05-31 01:06:34,973 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn3_1    | 2023-05-31 01:09:10,153 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO segmented.SegmentedRaftLogWorker: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-05-31 01:09:10,163 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83: start as a follower, conf=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 2023-05-31 01:07:01,331 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm3_1   | 2023-05-31 01:07:01,331 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn5_1    | 2023-05-31 01:06:34,300 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-05-31 01:06:34,300 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-31 01:06:35,033 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
dn3_1    | 2023-05-31 01:09:10,163 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83: changes role from      null to FOLLOWER at term 0 for startAsFollower
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 2023-05-31 01:07:01,332 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn2_1    | 2023-05-31 01:06:38,410 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1] INFO impl.RoleInfo: 4bc93e86-8989-4282-b7d8-050f4a19b92f: shutdown 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1
dn4_1    | 2023-05-31 01:06:41,142 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn4_1    | 2023-05-31 01:06:41,144 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-39D463D01A51 with new leaderId: 3da1490a-d610-40de-95c1-bdfc259ba041
scm1_1   | 2023-05-31 01:06:35,034 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-05-31 01:09:10,163 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO impl.RoleInfo: 97734618-7966-42ac-b50e-e49ead6ce4d2: start 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-FollowerState
dn3_1    | 2023-05-31 01:09:10,165 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6F942E88FF83,id=97734618-7966-42ac-b50e-e49ead6ce4d2
dn1_1    | 2023-05-31 01:08:22,297 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection4
dn4_1    | 2023-05-31 01:06:41,148 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51: change Leader from null to 3da1490a-d610-40de-95c1-bdfc259ba041 at term 4 for becomeLeader, leader elected after 37449ms
dn4_1    | 2023-05-31 01:06:41,321 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1_1   | 2023-05-31 01:06:34,888 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
dn3_1    | 2023-05-31 01:09:10,165 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn1_1    | 2023-05-31 01:08:22,301 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection4] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: change Leader from 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd to null at term 12 for PRE_VOTE
dn2_1    | 2023-05-31 01:06:38,416 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn2_1    | 2023-05-31 01:06:38,424 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-B011FDD1EA12 with new leaderId: 4bc93e86-8989-4282-b7d8-050f4a19b92f
dn4_1    | 2023-05-31 01:06:41,375 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-05-31 01:06:41,382 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-05-31 01:09:10,165 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-05-31 01:06:38,520 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12: change Leader from null to 4bc93e86-8989-4282-b7d8-050f4a19b92f at term 4 for becomeLeader, leader elected after 33376ms
dn2_1    | 2023-05-31 01:06:38,854 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn2_1    | 2023-05-31 01:06:38,914 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-05-31 01:06:34,342 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-DC0251522AF1,id=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn5_1    | 2023-05-31 01:06:34,351 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1   | 2023-05-31 01:06:35,095 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1_1   | 2023-05-31 01:06:35,080 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
dn3_1    | 2023-05-31 01:09:10,166 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-05-31 01:06:41,432 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn2_1    | 2023-05-31 01:06:38,960 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn2_1    | 2023-05-31 01:06:38,977 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn5_1    | 2023-05-31 01:06:34,351 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-05-31 01:06:34,360 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm1_1   | 2023-05-31 01:06:35,273 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm1_1   | 2023-05-31 01:06:35,033 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
dn3_1    | 2023-05-31 01:09:10,166 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-05-31 01:06:41,435 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 2023-05-31 01:06:34,368 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-05-31 01:08:22,301 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection4] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection4 PRE_VOTE round 0: submit vote requests at term 12 for 25: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:06:35,274 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn3_1    | 2023-05-31 01:09:10,167 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-05-31 01:06:41,436 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn5_1    | 2023-05-31 01:06:34,400 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:08:22,349 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection4] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection4 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3da1490a-d610-40de-95c1-bdfc259ba041: group-F1A4CF07E444 not found.
scm1_1   | 2023-05-31 01:06:35,304 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn3_1    | 2023-05-31 01:09:10,175 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-31 01:06:41,494 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:34,462 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-05-31 01:06:34,462 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-05-31 01:06:34,549 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: start RPC server
dn1_1    | 2023-05-31 01:08:22,360 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection4] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection4 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: group-F1A4CF07E444 not found.
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn4_1    | 2023-05-31 01:06:41,571 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-05-31 01:06:41,588 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn2_1    | 2023-05-31 01:06:39,072 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn2_1    | 2023-05-31 01:06:39,076 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn5_1    | 2023-05-31 01:06:34,638 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: GrpcService started, listening on 9858
scm1_1   | 2023-05-31 01:06:35,308 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn1_1    | 2023-05-31 01:08:22,361 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection4] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection4: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
dn1_1    | 2023-05-31 01:08:22,361 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection4] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3da1490a-d610-40de-95c1-bdfc259ba041: group-F1A4CF07E444 not found.
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn4_1    | 2023-05-31 01:06:41,612 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: start 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderStateImpl
dn2_1    | 2023-05-31 01:06:39,176 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-05-31 01:06:39,209 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn5_1    | 2023-05-31 01:06:34,731 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: GrpcService started, listening on 9856
scm1_1   | 2023-05-31 01:06:36,051 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | 2023-05-31 01:08:22,361 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection4] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: group-F1A4CF07E444 not found.
dn1_1    | 2023-05-31 01:08:22,361 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection4] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection4 PRE_VOTE round 0: result REJECTED
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn4_1    | 2023-05-31 01:06:41,666 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn5_1    | 2023-05-31 01:06:34,733 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: GrpcService started, listening on 9857
scm1_1   | 2023-05-31 01:06:36,061 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | 2023-05-31 01:08:22,362 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection4] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: changes role from CANDIDATE to FOLLOWER at term 12 for REJECTED
dn1_1    | 2023-05-31 01:08:22,362 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection4] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection4
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn4_1    | 2023-05-31 01:06:41,733 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/5509b2c4-975c-42f1-91be-39d463d01a51/current/log_inprogress_3 to /data/metadata/ratis/5509b2c4-975c-42f1-91be-39d463d01a51/current/log_3-4
dn2_1    | 2023-05-31 01:06:39,239 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1] INFO impl.RoleInfo: 4bc93e86-8989-4282-b7d8-050f4a19b92f: start 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderStateImpl
scm3_1   | 2023-05-31 01:07:01,507 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
dn5_1    | 2023-05-31 01:06:34,736 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd is started using port 9858 for RATIS
dn1_1    | 2023-05-31 01:08:22,364 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection4] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn4_1    | 2023-05-31 01:06:42,124 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderElection2] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51: set configuration 5: peers:[3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-31 01:06:39,287 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn2_1    | 2023-05-31 01:06:39,315 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderElection1] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12: set configuration 5: peers:[4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-31 01:07:01,512 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
dn4_1    | 2023-05-31 01:06:42,132 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-3da1490a-d610-40de-95c1-bdfc259ba041: Detected pause in JVM or host machine approximately 0.136s without any GCs.
dn4_1    | 2023-05-31 01:06:42,222 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5509b2c4-975c-42f1-91be-39d463d01a51/current/log_inprogress_5
dn4_1    | 2023-05-31 01:06:42,551 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-31 01:06:43,553 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:08:22,864 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO impl.FollowerState: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState: change to CANDIDATE, lastRpcElapsedTime:7959825728ns, electionTimeout:5159ms
dn2_1    | 2023-05-31 01:06:39,333 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/ccaf8839-8737-4f75-8b75-b011fdd1ea12/current/log_inprogress_3 to /data/metadata/ratis/ccaf8839-8737-4f75-8b75-b011fdd1ea12/current/log_3-4
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 2023-05-31 01:07:01,520 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.14.1+1-LTS
scm3_1   | 2023-05-31 01:07:01,635 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm3_1   | 2023-05-31 01:07:01,639 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
dn1_1    | 2023-05-31 01:08:22,865 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState
dn1_1    | 2023-05-31 01:08:22,865 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: changes role from  FOLLOWER to CANDIDATE at term 9 for changeToCandidate
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 2023-05-31 01:07:01,645 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
scm3_1   | 2023-05-31 01:07:01,771 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@43588265{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm3_1   | 2023-05-31 01:07:01,781 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4baf997{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn1_1    | 2023-05-31 01:08:22,865 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-05-31 01:06:34,736 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd is started using port 9857 for RATIS_ADMIN
dn5_1    | 2023-05-31 01:06:34,736 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd is started using port 9856 for RATIS_SERVER
dn5_1    | 2023-05-31 01:06:34,739 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: Started
dn5_1    | 2023-05-31 01:06:34,894 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn5_1    | 2023-05-31 01:06:35,009 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-05-31 01:08:22,865 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection5
dn2_1    | 2023-05-31 01:06:39,335 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:09:10,180 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=9342e1de-b2fb-494f-91d9-6f942e88ff83
dn3_1    | 2023-05-31 01:09:11,419 [grpc-default-executor-0] INFO server.RaftServer: 97734618-7966-42ac-b50e-e49ead6ce4d2: addNew group-A59219751D93:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER] returns group-A59219751D93:java.util.concurrent.CompletableFuture@66632201[Not completed]
dn3_1    | 2023-05-31 01:09:11,431 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2: new RaftServerImpl for group-A59219751D93:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn3_1    | 2023-05-31 01:09:11,431 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm3_1   | 2023-05-31 01:07:02,828 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@595fed99{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-5936095925873386065/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm3_1   | 2023-05-31 01:07:02,875 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@45cd8607{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
dn1_1    | 2023-05-31 01:08:22,873 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection5] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: change Leader from 3da1490a-d610-40de-95c1-bdfc259ba041 to null at term 9 for PRE_VOTE
dn2_1    | 2023-05-31 01:06:39,404 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ccaf8839-8737-4f75-8b75-b011fdd1ea12/current/log_inprogress_5
dn3_1    | 2023-05-31 01:09:11,432 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-05-31 01:09:11,432 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-05-31 01:09:11,432 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-05-31 01:09:11,432 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3_1   | 2023-05-31 01:07:02,875 [Listener at 0.0.0.0/9860] INFO server.Server: Started @37072ms
scm1_1   | 2023-05-31 01:06:36,220 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 2023-05-31 01:06:40,367 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:09:11,432 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn4_1    | 2023-05-31 01:06:44,592 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-31 01:06:44,643 [grpc-default-executor-1] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1: receive requestVote(PRE_VOTE, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, group-DC0251522AF1, 8, (t:8, i:42))
scm3_1   | 2023-05-31 01:07:02,895 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm1_1   | 2023-05-31 01:06:36,230 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 2023-05-31 01:06:41,368 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:09:11,433 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93: ConfigurationManager, init=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-05-31 01:06:37,499 [IPC Server handler 17 on default port 9891] INFO ipc.Server: IPC Server handler 17 on default port 9891 caught an exception
scm3_1   | 2023-05-31 01:07:02,895 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm1_1   | 2023-05-31 01:06:36,242 [grpc-default-executor-3] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
scm1_1   | 2023-05-31 01:06:36,423 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
dn1_1    | 2023-05-31 01:08:22,873 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection5] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection5 PRE_VOTE round 0: submit vote requests at term 9 for 43: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-31 01:06:42,368 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:09:11,433 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 2023-05-31 01:07:02,901 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
dn1_1    | 2023-05-31 01:08:22,903 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection5] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection5 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: group-DC0251522AF1 not found.
dn2_1    | 2023-05-31 01:06:43,369 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:09:11,433 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-05-31 01:09:11,435 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-05-31 01:06:35,401 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:36,402 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-05-31 01:07:03,251 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-31 01:06:36,437 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn1_1    | 2023-05-31 01:08:22,910 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection5] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection5 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3da1490a-d610-40de-95c1-bdfc259ba041: group-DC0251522AF1 not found.
dn2_1    | 2023-05-31 01:06:44,371 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:45,372 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-31 01:06:44,646 [grpc-default-executor-2] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444: receive requestVote(PRE_VOTE, b20d0805-391e-447d-a646-35a5d1a5cbcf, group-F1A4CF07E444, 11, (t:11, i:24))
dn4_1    | 2023-05-31 01:06:44,653 [grpc-default-executor-0] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444: receive requestVote(PRE_VOTE, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, group-F1A4CF07E444, 11, (t:11, i:24))
dn3_1    | 2023-05-31 01:09:11,435 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
scm3_1   | 2023-05-31 01:07:03,251 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-31 01:06:36,627 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
dn2_1    | 2023-05-31 01:06:46,373 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:37,409 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:08:22,910 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection5] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection5: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
dn4_1    | 2023-05-31 01:06:44,660 [grpc-default-executor-3] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1: receive requestVote(PRE_VOTE, b20d0805-391e-447d-a646-35a5d1a5cbcf, group-DC0251522AF1, 8, (t:8, i:42))
dn4_1    | 2023-05-31 01:06:44,671 [grpc-default-executor-1] INFO impl.VoteContext: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-CANDIDATE: reject PRE_VOTE from 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: our priority 1 > candidate's priority 0
dn4_1    | 2023-05-31 01:06:44,671 [grpc-default-executor-2] INFO impl.VoteContext: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-CANDIDATE: accept PRE_VOTE from b20d0805-391e-447d-a646-35a5d1a5cbcf: our priority 0 <= candidate's priority 0
dn4_1    | 2023-05-31 01:06:44,720 [grpc-default-executor-2] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444 replies to PRE_VOTE vote request: b20d0805-391e-447d-a646-35a5d1a5cbcf<-3da1490a-d610-40de-95c1-bdfc259ba041#0:OK-t11. Peer's state: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444:t11, leader=null, voted=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, raftlog=Memoized:3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-SegmentedRaftLog:OPENED:c24, conf=11: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:06:44,728 [grpc-default-executor-0] INFO impl.VoteContext: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-CANDIDATE: accept PRE_VOTE from 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: our priority 0 <= candidate's priority 1
dn4_1    | 2023-05-31 01:06:44,737 [grpc-default-executor-0] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444 replies to PRE_VOTE vote request: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd<-3da1490a-d610-40de-95c1-bdfc259ba041#0:OK-t11. Peer's state: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444:t11, leader=null, voted=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, raftlog=Memoized:3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-SegmentedRaftLog:OPENED:c24, conf=11: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-31 01:06:47,374 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:38,432 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:39,301 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-FollowerState] INFO impl.FollowerState: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5740437848ns, electionTimeout:5000ms
scm1_1   | 2023-05-31 01:06:36,627 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn4_1    | 2023-05-31 01:06:44,747 [grpc-default-executor-1] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1 replies to PRE_VOTE vote request: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd<-3da1490a-d610-40de-95c1-bdfc259ba041#0:FAIL-t8. Peer's state: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1:t8, leader=null, voted=3da1490a-d610-40de-95c1-bdfc259ba041, raftlog=Memoized:3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-SegmentedRaftLog:OPENED:c42, conf=29: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:06:44,791 [grpc-default-executor-3] INFO impl.VoteContext: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-CANDIDATE: reject PRE_VOTE from b20d0805-391e-447d-a646-35a5d1a5cbcf: our priority 1 > candidate's priority 0
scm3_1   | 2023-05-31 01:07:08,341 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-05-31 01:06:48,375 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:06:49,376 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-05-31 01:06:36,638 [IPC Server handler 3 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 2023-05-31 01:07:08,342 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-05-31 01:07:00,049 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn5_1    | 2023-05-31 01:06:39,302 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-FollowerState] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: shutdown 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-FollowerState
dn1_1    | 2023-05-31 01:08:22,910 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection5] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: group-DC0251522AF1 not found.
dn1_1    | 2023-05-31 01:08:22,911 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection5] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3da1490a-d610-40de-95c1-bdfc259ba041: group-DC0251522AF1 not found.
scm1_1   | 2023-05-31 01:06:36,640 [IPC Server handler 3 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-05-31 01:07:08,499 [IPC Server handler 55 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/4bc93e86-8989-4282-b7d8-050f4a19b92f
dn5_1    | 2023-05-31 01:06:39,303 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-FollowerState] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444: changes role from  FOLLOWER to CANDIDATE at term 11 for changeToCandidate
dn4_1    | 2023-05-31 01:06:44,791 [grpc-default-executor-3] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1 replies to PRE_VOTE vote request: b20d0805-391e-447d-a646-35a5d1a5cbcf<-3da1490a-d610-40de-95c1-bdfc259ba041#0:FAIL-t8. Peer's state: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1:t8, leader=null, voted=3da1490a-d610-40de-95c1-bdfc259ba041, raftlog=Memoized:3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-SegmentedRaftLog:OPENED:c42, conf=29: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:06:45,004 [grpc-default-executor-3] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444: receive requestVote(PRE_VOTE, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, group-F1A4CF07E444, 11, (t:11, i:24))
scm1_1   | 2023-05-31 01:06:36,641 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
dn2_1    | 2023-05-31 01:07:33,360 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn2_1    | 2023-05-31 01:08:09,464 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 2023-05-31 01:07:08,503 [IPC Server handler 55 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 4bc93e86-8989-4282-b7d8-050f4a19b92f{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-05-31 01:07:08,518 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
dn4_1    | 2023-05-31 01:06:45,004 [grpc-default-executor-3] INFO impl.VoteContext: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-CANDIDATE: accept PRE_VOTE from 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: our priority 0 <= candidate's priority 1
dn4_1    | 2023-05-31 01:06:45,005 [grpc-default-executor-3] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444 replies to PRE_VOTE vote request: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd<-3da1490a-d610-40de-95c1-bdfc259ba041#0:OK-t11. Peer's state: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444:t11, leader=null, voted=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, raftlog=Memoized:3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-SegmentedRaftLog:OPENED:c24, conf=11: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:06:36,661 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
dn2_1    | 2023-05-31 01:08:09,466 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn5_1    | 2023-05-31 01:06:39,310 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-05-31 01:06:39,310 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-FollowerState] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: start 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1
dn4_1    | 2023-05-31 01:06:45,005 [grpc-default-executor-2] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1: receive requestVote(PRE_VOTE, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, group-DC0251522AF1, 8, (t:8, i:42))
dn4_1    | 2023-05-31 01:06:45,028 [grpc-default-executor-2] INFO impl.VoteContext: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-CANDIDATE: reject PRE_VOTE from 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: our priority 1 > candidate's priority 0
scm1_1   | 2023-05-31 01:06:36,661 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 2, required at least one datanode reported per pipeline count is 2
dn2_1    | 2023-05-31 01:08:09,467 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn3_1    | 2023-05-31 01:09:11,435 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn1_1    | 2023-05-31 01:08:22,911 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection5] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection5 PRE_VOTE round 0: result REJECTED
dn1_1    | 2023-05-31 01:08:22,915 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection5] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: changes role from CANDIDATE to FOLLOWER at term 9 for REJECTED
dn1_1    | 2023-05-31 01:08:22,915 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection5] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection5
dn1_1    | 2023-05-31 01:08:22,915 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection5] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn3_1    | 2023-05-31 01:09:11,436 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-05-31 01:08:27,565 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO impl.FollowerState: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5200700942ns, electionTimeout:5200ms
dn1_1    | 2023-05-31 01:08:27,566 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState
scm1_1   | 2023-05-31 01:06:36,662 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn2_1    | 2023-05-31 01:08:09,467 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn3_1    | 2023-05-31 01:09:11,436 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-05-31 01:08:27,566 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: changes role from  FOLLOWER to CANDIDATE at term 12 for changeToCandidate
dn1_1    | 2023-05-31 01:08:27,566 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm1_1   | 2023-05-31 01:06:36,680 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
dn2_1    | 2023-05-31 01:08:09,469 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn1_1    | 2023-05-31 01:08:27,566 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection6
dn1_1    | 2023-05-31 01:08:27,567 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection6] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection6 PRE_VOTE round 0: submit vote requests at term 12 for 25: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:06:36,702 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
dn2_1    | 2023-05-31 01:08:09,470 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
dn2_1    | 2023-05-31 01:08:09,470 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
dn2_1    | 2023-05-31 01:08:09,471 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HADOOP_PRC_PORTS_IN_DATANODEDETAILS.
dn4_1    | 2023-05-31 01:06:45,028 [grpc-default-executor-2] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1 replies to PRE_VOTE vote request: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd<-3da1490a-d610-40de-95c1-bdfc259ba041#0:FAIL-t8. Peer's state: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1:t8, leader=null, voted=3da1490a-d610-40de-95c1-bdfc259ba041, raftlog=Memoized:3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-SegmentedRaftLog:OPENED:c42, conf=29: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:06:45,071 [grpc-default-executor-2] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444: receive requestVote(ELECTION, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, group-F1A4CF07E444, 12, (t:11, i:24))
scm1_1   | 2023-05-31 01:06:36,702 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
dn2_1    | 2023-05-31 01:08:09,475 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature HADOOP_PRC_PORTS_IN_DATANODEDETAILS has been finalized.
dn3_1    | 2023-05-31 01:09:11,436 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn3_1    | 2023-05-31 01:09:11,437 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-05-31 01:06:45,072 [grpc-default-executor-2] INFO impl.VoteContext: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-CANDIDATE: accept ELECTION from 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: our priority 0 <= candidate's priority 1
dn4_1    | 2023-05-31 01:06:45,073 [grpc-default-executor-2] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444: changes role from CANDIDATE to FOLLOWER at term 12 for candidate:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
scm1_1   | 2023-05-31 01:06:36,703 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
dn2_1    | 2023-05-31 01:08:09,476 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn2_1    | 2023-05-31 01:08:09,476 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn2_1    | 2023-05-31 01:08:33,360 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn4_1    | 2023-05-31 01:06:45,076 [grpc-default-executor-2] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: shutdown 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-LeaderElection1
dn4_1    | 2023-05-31 01:06:45,076 [grpc-default-executor-2] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: start 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-FollowerState
scm1_1   | 2023-05-31 01:06:36,718 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
dn2_1    | 2023-05-31 01:08:39,466 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | 2023-05-31 01:08:39,478 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 4bc93e86-8989-4282-b7d8-050f4a19b92f: remove    LEADER 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12:t4, leader=4bc93e86-8989-4282-b7d8-050f4a19b92f, voted=4bc93e86-8989-4282-b7d8-050f4a19b92f, raftlog=Memoized:4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-SegmentedRaftLog:OPENED:c6, conf=5: peers:[4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn2_1    | 2023-05-31 01:08:39,481 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12: shutdown
dn4_1    | 2023-05-31 01:06:45,114 [3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-LeaderElection1] INFO impl.LeaderElection: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-LeaderElection1: PRE_VOTE REJECTED received 1 response(s) and 0 exception(s):
dn4_1    | 2023-05-31 01:06:45,144 [3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-LeaderElection1] INFO impl.LeaderElection:   Response 0: 3da1490a-d610-40de-95c1-bdfc259ba041<-3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#0:FAIL-t11
scm1_1   | 2023-05-31 01:06:36,744 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 1.
dn3_1    | 2023-05-31 01:09:11,441 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-05-31 01:09:11,447 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-05-31 01:09:11,447 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-05-31 01:06:39,360 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 11 for 11: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:06:39,442 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-31 01:06:45,149 [3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-LeaderElection1] INFO impl.LeaderElection: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-LeaderElection1 PRE_VOTE round 0: result REJECTED
dn4_1    | 2023-05-31 01:06:45,143 [grpc-default-executor-2] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444 replies to ELECTION vote request: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd<-3da1490a-d610-40de-95c1-bdfc259ba041#0:OK-t12. Peer's state: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444:t12, leader=null, voted=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, raftlog=Memoized:3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-SegmentedRaftLog:OPENED:c24, conf=11: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-31 01:09:11,447 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn5_1    | 2023-05-31 01:06:39,485 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 3da1490a-d610-40de-95c1-bdfc259ba041
dn5_1    | 2023-05-31 01:06:39,513 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-05-31 01:08:27,586 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection6] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection6 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3da1490a-d610-40de-95c1-bdfc259ba041: group-F1A4CF07E444 not found.
dn1_1    | 2023-05-31 01:08:27,589 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection6] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection6 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: group-F1A4CF07E444 not found.
dn3_1    | 2023-05-31 01:09:11,447 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-05-31 01:09:11,447 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5d339866-97f6-4ef6-b080-a59219751d93 does not exist. Creating ...
dn3_1    | 2023-05-31 01:09:11,448 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5d339866-97f6-4ef6-b080-a59219751d93/in_use.lock acquired by nodename 7@dcc4205663d5
dn5_1    | 2023-05-31 01:06:39,485 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-FollowerState] INFO impl.FollowerState: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5868378274ns, electionTimeout:5182ms
dn5_1    | 2023-05-31 01:06:39,525 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-FollowerState] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: shutdown 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-FollowerState
dn1_1    | 2023-05-31 01:08:27,590 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection6] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection6: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
dn1_1    | 2023-05-31 01:08:27,590 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection6] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3da1490a-d610-40de-95c1-bdfc259ba041: group-F1A4CF07E444 not found.
dn3_1    | 2023-05-31 01:09:11,450 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5d339866-97f6-4ef6-b080-a59219751d93 has been successfully formatted.
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 2023-05-31 01:07:08,530 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm3_1   | 2023-05-31 01:07:08,541 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
dn1_1    | 2023-05-31 01:08:27,590 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection6] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: group-F1A4CF07E444 not found.
dn1_1    | 2023-05-31 01:08:27,590 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection6] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection6 PRE_VOTE round 0: result REJECTED
dn3_1    | 2023-05-31 01:09:11,451 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO ratis.ContainerStateMachine: group-A59219751D93: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-05-31 01:09:11,451 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-05-31 01:09:11,451 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm3_1   | 2023-05-31 01:07:08,541 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-05-31 01:06:37,510 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
scm1_1   | 2023-05-31 01:06:37,303 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-05-31 01:09:11,451 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-05-31 01:08:39,482 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-B011FDD1EA12,id=4bc93e86-8989-4282-b7d8-050f4a19b92f
scm3_1   | 2023-05-31 01:07:08,542 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-05-31 01:07:08,843 [IPC Server handler 74 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/97734618-7966-42ac-b50e-e49ead6ce4d2
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm1_1   | 2023-05-31 01:06:37,305 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-05-31 01:09:11,451 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-05-31 01:09:11,451 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn1_1    | 2023-05-31 01:08:27,591 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection6] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: changes role from CANDIDATE to FOLLOWER at term 12 for REJECTED
scm1_1   | 2023-05-31 01:06:37,370 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-05-31 01:09:11,451 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-05-31 01:09:11,452 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-05-31 01:06:39,525 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-FollowerState] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn5_1    | 2023-05-31 01:06:39,525 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn1_1    | 2023-05-31 01:08:27,591 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection6] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection6
dn1_1    | 2023-05-31 01:08:27,594 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection6] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState
dn1_1    | 2023-05-31 01:08:28,088 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO impl.FollowerState: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5172245509ns, electionTimeout:5168ms
dn2_1    | 2023-05-31 01:08:39,483 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: 4bc93e86-8989-4282-b7d8-050f4a19b92f: shutdown 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-LeaderStateImpl
dn5_1    | 2023-05-31 01:06:39,525 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-FollowerState] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: start 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2
scm3_1   | 2023-05-31 01:07:08,844 [IPC Server handler 74 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 97734618-7966-42ac-b50e-e49ead6ce4d2{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn4_1    | 2023-05-31 01:06:45,171 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO impl.LeaderElection: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn4_1    | 2023-05-31 01:06:45,174 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO impl.LeaderElection:   Response 0: 3da1490a-d610-40de-95c1-bdfc259ba041<-3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#0:OK-t8
dn3_1    | 2023-05-31 01:09:11,452 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-05-31 01:09:11,452 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-05-31 01:06:37,383 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 2023-05-31 01:08:39,490 [PipelineCommandHandlerThread-0] INFO impl.PendingRequests: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-PendingRequests: sendNotLeaderResponses
dn5_1    | 2023-05-31 01:06:39,526 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:07:08,845 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn4_1    | 2023-05-31 01:06:45,174 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO impl.LeaderElection: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3 PRE_VOTE round 0: result PASSED
dn4_1    | 2023-05-31 01:06:45,183 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO impl.LeaderElection: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3 ELECTION round 0: submit vote requests at term 9 for 29: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:06:45,250 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-31 01:06:37,402 [grpc-default-executor-3] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn1_1    | 2023-05-31 01:08:28,088 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState
dn2_1    | 2023-05-31 01:08:39,493 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-B011FDD1EA12: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/ccaf8839-8737-4f75-8b75-b011fdd1ea12/sm/snapshot.4_6
dn2_1    | 2023-05-31 01:08:39,494 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-StateMachineUpdater: set stopIndex = 6
dn2_1    | 2023-05-31 01:08:39,495 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-B011FDD1EA12: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/ccaf8839-8737-4f75-8b75-b011fdd1ea12/sm/snapshot.4_6 took: 2 ms
scm3_1   | 2023-05-31 01:07:08,849 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
dn3_1    | 2023-05-31 01:09:11,452 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5d339866-97f6-4ef6-b080-a59219751d93
scm1_1   | 2023-05-31 01:06:37,658 [IPC Server handler 3 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3da1490a-d610-40de-95c1-bdfc259ba041
dn1_1    | 2023-05-31 01:08:28,088 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: changes role from  FOLLOWER to CANDIDATE at term 9 for changeToCandidate
scm3_1   | 2023-05-31 01:07:08,849 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
dn5_1    | 2023-05-31 01:06:39,553 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-FollowerState] INFO impl.FollowerState: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5939917194ns, electionTimeout:5088ms
scm1_1   | 2023-05-31 01:06:37,658 [IPC Server handler 3 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3da1490a-d610-40de-95c1-bdfc259ba041{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-05-31 01:06:37,658 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 2023-05-31 01:07:08,851 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn3_1    | 2023-05-31 01:09:11,452 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-05-31 01:09:11,452 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-05-31 01:06:39,554 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-FollowerState] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: shutdown 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-FollowerState
dn1_1    | 2023-05-31 01:08:28,089 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm1_1   | 2023-05-31 01:06:37,664 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 2.
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn4_1    | 2023-05-31 01:06:45,253 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-31 01:06:45,318 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO impl.LeaderElection: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn4_1    | 2023-05-31 01:06:45,321 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO impl.LeaderElection:   Response 0: 3da1490a-d610-40de-95c1-bdfc259ba041<-3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#0:OK-t9
dn4_1    | 2023-05-31 01:06:45,321 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO impl.LeaderElection: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3 ELECTION round 0: result PASSED
scm3_1   | 2023-05-31 01:07:11,018 [IPC Server handler 75 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/b20d0805-391e-447d-a646-35a5d1a5cbcf
dn5_1    | 2023-05-31 01:06:39,554 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-FollowerState] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1: changes role from  FOLLOWER to CANDIDATE at term 8 for changeToCandidate
dn1_1    | 2023-05-31 01:08:28,089 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection7
scm1_1   | 2023-05-31 01:06:37,676 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 2023-05-31 01:07:11,019 [IPC Server handler 75 on default port 9861] INFO node.SCMNodeManager: Registered Data node : b20d0805-391e-447d-a646-35a5d1a5cbcf{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-05-31 01:07:11,024 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn1_1    | 2023-05-31 01:08:28,089 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection7] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection7 PRE_VOTE round 0: submit vote requests at term 9 for 43: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:06:37,972 [IPC Server handler 8 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/b20d0805-391e-447d-a646-35a5d1a5cbcf
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-05-31 01:06:37,538 [IPC Server handler 15 on default port 9891] WARN ipc.Server: IPC Server handler 15 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:48618: output error
dn5_1    | 2023-05-31 01:06:39,554 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-05-31 01:08:28,098 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection7] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection7 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: group-DC0251522AF1 not found.
scm1_1   | 2023-05-31 01:06:37,978 [IPC Server handler 8 on default port 9861] INFO node.SCMNodeManager: Registered Data node : b20d0805-391e-447d-a646-35a5d1a5cbcf{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-05-31 01:07:11,024 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
recon_1  | 2023-05-31 01:06:37,543 [IPC Server handler 15 on default port 9891] INFO ipc.Server: IPC Server handler 15 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 2023-05-31 01:06:37,982 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-05-31 01:07:11,027 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
dn5_1    | 2023-05-31 01:06:39,554 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-FollowerState] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: start 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-LeaderElection3
dn5_1    | 2023-05-31 01:06:39,513 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for b20d0805-391e-447d-a646-35a5d1a5cbcf
dn5_1    | 2023-05-31 01:06:39,585 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:06:38,017 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm3_1   | 2023-05-31 01:07:11,027 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
dn3_1    | 2023-05-31 01:09:11,452 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-05-31 01:09:11,452 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-05-31 01:09:11,452 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | 2023-05-31 01:06:38,020 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
dn2_1    | 2023-05-31 01:08:39,496 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-StateMachineUpdater] INFO impl.StateMachineUpdater: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-StateMachineUpdater: Took a snapshot at index 6
dn2_1    | 2023-05-31 01:08:39,497 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-StateMachineUpdater] INFO impl.StateMachineUpdater: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
dn2_1    | 2023-05-31 01:08:39,499 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12: closes. applyIndex: 6
dn5_1    | 2023-05-31 01:06:39,609 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-LeaderElection3] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 8 for 29: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-31 01:09:11,452 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-05-31 01:09:11,452 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-05-31 01:09:11,452 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1_1   | 2023-05-31 01:06:38,020 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
dn4_1    | 2023-05-31 01:06:45,322 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: shutdown 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3
dn4_1    | 2023-05-31 01:06:45,322 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1: changes role from CANDIDATE to LEADER at term 9 for changeToLeader
scm3_1   | 2023-05-31 01:07:11,029 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
dn5_1    | 2023-05-31 01:06:39,610 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2 PRE_VOTE round 0: result PASSED (term=3)
dn1_1    | 2023-05-31 01:08:28,114 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection7] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection7 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3da1490a-d610-40de-95c1-bdfc259ba041: group-DC0251522AF1 not found.
dn1_1    | 2023-05-31 01:08:28,114 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection7] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection7: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
dn1_1    | 2023-05-31 01:08:28,114 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection7] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: group-DC0251522AF1 not found.
dn1_1    | 2023-05-31 01:08:28,114 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection7] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3da1490a-d610-40de-95c1-bdfc259ba041: group-DC0251522AF1 not found.
scm1_1   | 2023-05-31 01:06:38,016 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
dn2_1    | 2023-05-31 01:08:39,536 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12-SegmentedRaftLogWorker close()
dn2_1    | 2023-05-31 01:08:39,542 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B011FDD1EA12: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/ccaf8839-8737-4f75-8b75-b011fdd1ea12
scm3_1   | 2023-05-31 01:07:11,029 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn5_1    | 2023-05-31 01:06:39,659 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-05-31 01:08:28,114 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection7] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection7 PRE_VOTE round 0: result REJECTED
dn1_1    | 2023-05-31 01:08:28,115 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection7] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: changes role from CANDIDATE to FOLLOWER at term 9 for REJECTED
dn1_1    | 2023-05-31 01:08:28,115 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection7] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection7
dn1_1    | 2023-05-31 01:08:28,116 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection7] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState
dn1_1    | 2023-05-31 01:08:32,598 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO impl.FollowerState: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5006679103ns, electionTimeout:5000ms
dn1_1    | 2023-05-31 01:08:32,599 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState
dn1_1    | 2023-05-31 01:08:32,599 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: changes role from  FOLLOWER to CANDIDATE at term 12 for changeToCandidate
scm1_1   | 2023-05-31 01:06:38,025 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm1_1   | 2023-05-31 01:06:38,025 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm1_1   | 2023-05-31 01:06:38,027 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
dn1_1    | 2023-05-31 01:08:32,599 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm1_1   | 2023-05-31 01:06:38,028 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
dn5_1    | 2023-05-31 01:06:39,780 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-05-31 01:06:39,904 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2 ELECTION round 0: submit vote requests at term 4 for 3: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:06:38,028 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 2023-05-31 01:07:11,037 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn4_1    | 2023-05-31 01:06:45,322 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-DC0251522AF1 with new leaderId: 3da1490a-d610-40de-95c1-bdfc259ba041
dn4_1    | 2023-05-31 01:06:45,338 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1: change Leader from null to 3da1490a-d610-40de-95c1-bdfc259ba041 at term 9 for becomeLeader, leader elected after 40482ms
dn4_1    | 2023-05-31 01:06:45,341 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn4_1    | 2023-05-31 01:06:45,342 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm1_1   | 2023-05-31 01:06:38,033 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm1_1   | 2023-05-31 01:06:38,074 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm3_1   | 2023-05-31 01:07:11,042 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 2, required at least one datanode reported per pipeline count is 2
dn4_1    | 2023-05-31 01:06:45,343 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn4_1    | 2023-05-31 01:06:45,344 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn4_1    | 2023-05-31 01:06:45,345 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn4_1    | 2023-05-31 01:06:45,346 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn5_1    | 2023-05-31 01:06:39,905 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2 ELECTION round 0: result PASSED (term=4)
dn5_1    | 2023-05-31 01:06:39,905 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: shutdown 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2
scm3_1   | 2023-05-31 01:07:11,043 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
dn2_1    | 2023-05-31 01:08:39,544 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=ccaf8839-8737-4f75-8b75-b011fdd1ea12 command on datanode 4bc93e86-8989-4282-b7d8-050f4a19b92f.
dn2_1    | 2023-05-31 01:09:09,493 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 4bc93e86-8989-4282-b7d8-050f4a19b92f: addNew group-4EF30E5FDEEA:[4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] returns group-4EF30E5FDEEA:java.util.concurrent.CompletableFuture@2bba139a[Not completed]
dn2_1    | 2023-05-31 01:09:09,496 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f: new RaftServerImpl for group-4EF30E5FDEEA:[4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn2_1    | 2023-05-31 01:09:09,496 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-05-31 01:06:39,906 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn5_1    | 2023-05-31 01:06:39,911 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-ED0A589991D2 with new leaderId: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn2_1    | 2023-05-31 01:09:09,497 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-05-31 01:09:11,453 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-05-31 01:09:11,474 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-05-31 01:09:11,716 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1_1   | 2023-05-31 01:06:38,092 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO SCMHATransactionMonitor: Service SCMHATransactionMonitor transitions to RUNNING.
dn5_1    | 2023-05-31 01:06:39,928 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2: change Leader from null to 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd at term 4 for becomeLeader, leader elected after 33322ms
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 2023-05-31 01:07:13,513 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-05-31 01:09:09,497 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-05-31 01:09:11,716 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-05-31 01:09:11,716 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-05-31 01:09:11,717 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO segmented.SegmentedRaftLogWorker: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-05-31 01:09:11,717 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO segmented.SegmentedRaftLogWorker: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-05-31 01:06:40,032 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 2023-05-31 01:07:13,514 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-05-31 01:09:09,497 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-05-31 01:06:45,346 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-05-31 01:06:45,347 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn4_1    | 2023-05-31 01:06:45,559 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn5_1    | 2023-05-31 01:06:40,113 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm1_1   | 2023-05-31 01:06:38,554 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn1_1    | 2023-05-31 01:08:32,600 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection8
dn5_1    | 2023-05-31 01:06:40,114 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-05-31 01:09:11,738 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93: start as a follower, conf=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:06:38,555 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 2023-05-31 01:09:09,497 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-05-31 01:09:09,497 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-05-31 01:09:09,497 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA: ConfigurationManager, init=-1: peers:[4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-05-31 01:09:09,497 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-05-31 01:09:09,500 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-05-31 01:09:11,738 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm1_1   | 2023-05-31 01:06:38,687 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 2023-05-31 01:09:09,500 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-05-31 01:06:45,563 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-31 01:06:45,573 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn4_1    | 2023-05-31 01:06:45,594 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-31 01:06:45,640 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn5_1    | 2023-05-31 01:06:40,130 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-05-31 01:09:11,738 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO impl.RoleInfo: 97734618-7966-42ac-b50e-e49ead6ce4d2: start 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93-FollowerState
scm1_1   | 2023-05-31 01:06:38,692 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 2023-05-31 01:09:09,500 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-05-31 01:06:45,670 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn4_1    | 2023-05-31 01:06:45,678 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-05-31 01:06:45,686 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn4_1    | 2023-05-31 01:06:45,693 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn5_1    | 2023-05-31 01:06:40,193 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-05-31 01:09:11,739 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-A59219751D93,id=97734618-7966-42ac-b50e-e49ead6ce4d2
scm1_1   | 2023-05-31 01:06:38,694 [grpc-default-executor-3] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn4_1    | 2023-05-31 01:06:45,698 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn5_1    | 2023-05-31 01:06:40,218 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn3_1    | 2023-05-31 01:09:11,739 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1   | 2023-05-31 01:06:38,740 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-05-31 01:07:15,197 [IPC Server handler 38 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
scm3_1   | 2023-05-31 01:07:15,202 [IPC Server handler 38 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn4_1    | 2023-05-31 01:06:45,701 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-05-31 01:06:45,765 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn4_1    | 2023-05-31 01:06:45,777 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:06:40,300 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-05-31 01:09:11,739 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm1_1   | 2023-05-31 01:06:38,740 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-05-31 01:07:15,202 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-05-31 01:07:15,204 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-05-31 01:07:15,356 [IPC Server handler 99 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3da1490a-d610-40de-95c1-bdfc259ba041
scm3_1   | 2023-05-31 01:07:15,357 [IPC Server handler 99 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3da1490a-d610-40de-95c1-bdfc259ba041{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-05-31 01:07:15,358 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-05-31 01:07:15,359 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
dn5_1    | 2023-05-31 01:06:40,311 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn3_1    | 2023-05-31 01:09:11,739 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm1_1   | 2023-05-31 01:06:38,745 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
scm3_1   | 2023-05-31 01:07:15,359 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
dn5_1    | 2023-05-31 01:06:40,334 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: start 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderStateImpl
dn2_1    | 2023-05-31 01:09:09,500 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm1_1   | 2023-05-31 01:06:39,805 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-05-31 01:07:15,359 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn1_1    | 2023-05-31 01:08:32,600 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection8] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection8 PRE_VOTE round 0: submit vote requests at term 12 for 25: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-31 01:08:32,618 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection8] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection8 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: group-F1A4CF07E444 not found.
dn1_1    | 2023-05-31 01:08:32,622 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection8] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection8 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3da1490a-d610-40de-95c1-bdfc259ba041: group-F1A4CF07E444 not found.
dn5_1    | 2023-05-31 01:06:40,480 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-31 01:09:09,500 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
scm1_1   | 2023-05-31 01:06:39,805 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-05-31 01:07:15,359 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm3_1   | 2023-05-31 01:07:15,360 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
dn1_1    | 2023-05-31 01:08:32,622 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection8] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection8: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
dn1_1    | 2023-05-31 01:08:32,622 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection8] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: group-F1A4CF07E444 not found.
dn1_1    | 2023-05-31 01:08:32,623 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection8] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3da1490a-d610-40de-95c1-bdfc259ba041: group-F1A4CF07E444 not found.
dn5_1    | 2023-05-31 01:06:40,582 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn2_1    | 2023-05-31 01:09:09,500 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1_1   | 2023-05-31 01:06:39,862 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-05-31 01:07:18,626 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:07:18,626 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-05-31 01:08:32,623 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection8] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection8 PRE_VOTE round 0: result REJECTED
dn3_1    | 2023-05-31 01:09:11,739 [97734618-7966-42ac-b50e-e49ead6ce4d2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-05-31 01:09:11,740 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-05-31 01:06:40,747 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/437d8269-2a0d-4058-8be6-ed0a589991d2/current/log_inprogress_3 to /data/metadata/ratis/437d8269-2a0d-4058-8be6-ed0a589991d2/current/log_3-4
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn1_1    | 2023-05-31 01:08:32,623 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection8] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: changes role from CANDIDATE to FOLLOWER at term 12 for REJECTED
dn4_1    | 2023-05-31 01:06:45,781 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn4_1    | 2023-05-31 01:06:45,782 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn2_1    | 2023-05-31 01:09:09,501 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm1_1   | 2023-05-31 01:06:39,863 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
scm1_1   | 2023-05-31 01:06:39,862 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-05-31 01:06:40,758 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderElection2] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2: set configuration 5: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn3_1    | 2023-05-31 01:09:11,748 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-05-31 01:09:11,877 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=9342e1de-b2fb-494f-91d9-6f942e88ff83.
scm1_1   | 2023-05-31 01:06:41,058 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:41,064 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-05-31 01:06:40,833 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/437d8269-2a0d-4058-8be6-ed0a589991d2/current/log_inprogress_5
scm3_1   | 2023-05-31 01:07:23,773 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-05-31 01:08:32,623 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection8] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection8
dn4_1    | 2023-05-31 01:06:45,782 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn4_1    | 2023-05-31 01:06:45,782 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-05-31 01:06:45,786 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn5_1    | 2023-05-31 01:06:41,492 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:42,494 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-05-31 01:06:41,106 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | 2023-05-31 01:08:32,623 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection8] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState
dn4_1    | 2023-05-31 01:06:45,787 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn4_1    | 2023-05-31 01:06:45,788 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-05-31 01:06:45,792 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-05-31 01:06:43,499 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:44,503 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-05-31 01:07:23,773 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-31 01:06:41,117 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn1_1    | 2023-05-31 01:08:33,188 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO impl.FollowerState: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5072129254ns, electionTimeout:5068ms
dn3_1    | 2023-05-31 01:09:14,859 [grpc-default-executor-0] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93: receive requestVote(PRE_VOTE, 4bc93e86-8989-4282-b7d8-050f4a19b92f, group-A59219751D93, 0, (t:0, i:0))
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 2023-05-31 01:07:24,607 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Stopping RatisPipelineUtilsThread.
scm1_1   | 2023-05-31 01:06:41,120 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | 2023-05-31 01:08:33,188 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState
dn5_1    | 2023-05-31 01:06:44,647 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1: PRE_VOTE TIMEOUT received 0 response(s) and 0 exception(s):
dn3_1    | 2023-05-31 01:09:14,864 [grpc-default-executor-0] INFO impl.VoteContext: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93-FOLLOWER: accept PRE_VOTE from 4bc93e86-8989-4282-b7d8-050f4a19b92f: our priority 0 <= candidate's priority 0
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 2023-05-31 01:07:24,632 [RatisPipelineUtilsThread - 0] WARN pipeline.BackgroundPipelineCreator: RatisPipelineUtilsThread is interrupted.
scm1_1   | 2023-05-31 01:06:41,153 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-05-31 01:06:37,553 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#9 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:53422: output error
recon_1  | 2023-05-31 01:06:37,553 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
dn1_1    | 2023-05-31 01:08:33,188 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: changes role from  FOLLOWER to CANDIDATE at term 9 for changeToCandidate
dn1_1    | 2023-05-31 01:08:33,189 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-05-31 01:08:33,189 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection9
dn1_1    | 2023-05-31 01:08:33,193 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection9] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection9 PRE_VOTE round 0: submit vote requests at term 9 for 43: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-31 01:08:33,204 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection9] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection9 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: group-DC0251522AF1 not found.
dn5_1    | 2023-05-31 01:06:44,654 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1 PRE_VOTE round 0: result TIMEOUT
scm3_1   | 2023-05-31 01:07:24,959 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn1_1    | 2023-05-31 01:08:33,210 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection9] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection9 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3da1490a-d610-40de-95c1-bdfc259ba041: group-DC0251522AF1 not found.
dn5_1    | 2023-05-31 01:06:44,655 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1 PRE_VOTE round 1: submit vote requests at term 11 for 11: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-31 01:07:24,964 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn1_1    | 2023-05-31 01:08:33,210 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection9] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection9: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
dn1_1    | 2023-05-31 01:08:33,210 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection9] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: group-DC0251522AF1 not found.
dn1_1    | 2023-05-31 01:08:33,210 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection9] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3da1490a-d610-40de-95c1-bdfc259ba041: group-DC0251522AF1 not found.
dn1_1    | 2023-05-31 01:08:33,210 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection9] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection9 PRE_VOTE round 0: result REJECTED
dn1_1    | 2023-05-31 01:08:33,210 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection9] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: changes role from CANDIDATE to FOLLOWER at term 9 for REJECTED
dn5_1    | 2023-05-31 01:06:44,663 [grpc-default-executor-0] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1: receive requestVote(PRE_VOTE, b20d0805-391e-447d-a646-35a5d1a5cbcf, group-DC0251522AF1, 8, (t:8, i:42))
scm3_1   | 2023-05-31 01:07:24,983 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 2023-05-31 01:06:41,156 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-05-31 01:06:44,667 [grpc-default-executor-1] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444: receive requestVote(PRE_VOTE, 3da1490a-d610-40de-95c1-bdfc259ba041, group-F1A4CF07E444, 11, (t:11, i:24))
scm3_1   | 2023-05-31 01:07:24,991 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn2_1    | 2023-05-31 01:09:09,502 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-05-31 01:09:09,502 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-05-31 01:09:09,502 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-05-31 01:09:09,502 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-05-31 01:09:09,502 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm1_1   | 2023-05-31 01:06:41,169 [grpc-default-executor-3] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn3_1    | 2023-05-31 01:09:14,870 [grpc-default-executor-0] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93 replies to PRE_VOTE vote request: 4bc93e86-8989-4282-b7d8-050f4a19b92f<-97734618-7966-42ac-b50e-e49ead6ce4d2#0:OK-t0. Peer's state: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93:t0, leader=null, voted=, raftlog=Memoized:97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-31 01:09:15,142 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-FollowerState] INFO impl.FollowerState: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5130347260ns, electionTimeout:5112ms
dn3_1    | 2023-05-31 01:09:15,142 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-FollowerState] INFO impl.RoleInfo: 97734618-7966-42ac-b50e-e49ead6ce4d2: shutdown 97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-FollowerState
dn3_1    | 2023-05-31 01:09:15,143 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-FollowerState] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn3_1    | 2023-05-31 01:09:15,143 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-05-31 01:06:44,704 [grpc-default-executor-2] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444: receive requestVote(PRE_VOTE, b20d0805-391e-447d-a646-35a5d1a5cbcf, group-F1A4CF07E444, 11, (t:11, i:24))
dn5_1    | 2023-05-31 01:06:44,672 [grpc-default-executor-0] INFO impl.VoteContext: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-CANDIDATE: accept PRE_VOTE from b20d0805-391e-447d-a646-35a5d1a5cbcf: our priority 0 <= candidate's priority 0
scm1_1   | 2023-05-31 01:06:41,271 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-05-31 01:09:15,143 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-FollowerState] INFO impl.RoleInfo: 97734618-7966-42ac-b50e-e49ead6ce4d2: start 97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2
dn2_1    | 2023-05-31 01:09:09,502 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-05-31 01:06:45,838 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: start 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderStateImpl
dn4_1    | 2023-05-31 01:06:45,849 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-SegmentedRaftLogWorker: Rolling segment log-29_42 to index:42
dn4_1    | 2023-05-31 01:06:45,964 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderElection3] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1: set configuration 43: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:06:44,730 [grpc-default-executor-3] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1: receive requestVote(PRE_VOTE, 3da1490a-d610-40de-95c1-bdfc259ba041, group-DC0251522AF1, 8, (t:8, i:42))
dn5_1    | 2023-05-31 01:06:44,741 [grpc-default-executor-0] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1 replies to PRE_VOTE vote request: b20d0805-391e-447d-a646-35a5d1a5cbcf<-3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#0:OK-t8. Peer's state: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1:t8, leader=null, voted=3da1490a-d610-40de-95c1-bdfc259ba041, raftlog=Memoized:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-SegmentedRaftLog:OPENED:c42, conf=29: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:06:44,754 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-05-31 01:09:15,144 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2] INFO impl.LeaderElection: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-31 01:09:09,504 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/46a9d8fb-17d8-4177-820b-4ef30e5fdeea does not exist. Creating ...
dn4_1    | 2023-05-31 01:06:45,976 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/current/log_inprogress_29 to /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/current/log_29-42
dn4_1    | 2023-05-31 01:06:45,994 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/current/log_inprogress_43
dn4_1    | 2023-05-31 01:06:46,302 [3da1490a-d610-40de-95c1-bdfc259ba041-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-F1A4CF07E444 with new leaderId: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn5_1    | 2023-05-31 01:06:44,754 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-31 01:06:41,273 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:41,283 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn3_1    | 2023-05-31 01:09:15,144 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2] INFO impl.LeaderElection: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
dn3_1    | 2023-05-31 01:09:15,146 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2] INFO impl.LeaderElection: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:06:44,704 [grpc-default-executor-1] INFO impl.VoteContext: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-CANDIDATE: reject PRE_VOTE from 3da1490a-d610-40de-95c1-bdfc259ba041: our priority 1 > candidate's priority 0
dn5_1    | 2023-05-31 01:06:44,788 [grpc-default-executor-1] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444 replies to PRE_VOTE vote request: 3da1490a-d610-40de-95c1-bdfc259ba041<-3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#0:FAIL-t11. Peer's state: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444:t11, leader=null, voted=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, raftlog=Memoized:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-SegmentedRaftLog:OPENED:c24, conf=11: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn3_1    | 2023-05-31 01:09:15,146 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2] INFO impl.LeaderElection: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2 ELECTION round 0: result PASSED (term=1)
dn3_1    | 2023-05-31 01:09:15,146 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2] INFO impl.RoleInfo: 97734618-7966-42ac-b50e-e49ead6ce4d2: shutdown 97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2
dn3_1    | 2023-05-31 01:09:15,146 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn3_1    | 2023-05-31 01:09:15,146 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-297089FABFC6 with new leaderId: 97734618-7966-42ac-b50e-e49ead6ce4d2
dn3_1    | 2023-05-31 01:09:15,148 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6: change Leader from null to 97734618-7966-42ac-b50e-e49ead6ce4d2 at term 1 for becomeLeader, leader elected after 5287ms
scm1_1   | 2023-05-31 01:06:41,324 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-05-31 01:06:46,351 [3da1490a-d610-40de-95c1-bdfc259ba041-server-thread1] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444: change Leader from null to 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd at term 12 for appendEntries, leader elected after 41222ms
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn2_1    | 2023-05-31 01:09:09,508 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/46a9d8fb-17d8-4177-820b-4ef30e5fdeea/in_use.lock acquired by nodename 7@86166c590ca2
dn3_1    | 2023-05-31 01:09:15,148 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1_1   | 2023-05-31 01:06:41,327 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:41,338 [grpc-default-executor-3] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn2_1    | 2023-05-31 01:09:09,509 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/46a9d8fb-17d8-4177-820b-4ef30e5fdeea has been successfully formatted.
dn3_1    | 2023-05-31 01:09:15,149 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-05-31 01:09:15,149 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
scm1_1   | 2023-05-31 01:06:42,309 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:42,310 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:42,352 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:42,353 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 2023-05-31 01:09:09,510 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO ratis.ContainerStateMachine: group-4EF30E5FDEEA: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn2_1    | 2023-05-31 01:09:09,510 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn3_1    | 2023-05-31 01:09:15,156 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
scm3_1   | 2023-05-31 01:07:25,007 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HADOOP_PRC_PORTS_IN_DATANODEDETAILS.
scm3_1   | 2023-05-31 01:07:25,009 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature HADOOP_PRC_PORTS_IN_DATANODEDETAILS has been finalized.
scm3_1   | 2023-05-31 01:07:25,009 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn4_1    | 2023-05-31 01:06:46,599 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-31 01:08:33,211 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection9] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection9
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn3_1    | 2023-05-31 01:09:15,156 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm3_1   | 2023-05-31 01:07:25,024 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-05-31 01:06:38,131 [IPC Server handler 26 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for 2af98bbedaed
scm1_1   | 2023-05-31 01:06:42,366 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
scm1_1   | 2023-05-31 01:06:43,587 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-05-31 01:09:15,156 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm3_1   | 2023-05-31 01:07:25,024 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17) moved to HEALTHY READONLY state.
dn4_1    | 2023-05-31 01:06:46,605 [3da1490a-d610-40de-95c1-bdfc259ba041-server-thread2] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444: set configuration 25: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:06:46,626 [3da1490a-d610-40de-95c1-bdfc259ba041-server-thread2] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-SegmentedRaftLogWorker: Rolling segment log-11_24 to index:24
scm1_1   | 2023-05-31 01:06:43,704 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:43,728 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-05-31 01:09:15,157 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm3_1   | 2023-05-31 01:07:25,025 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=6a499bf5-10af-4dab-a3fd-a4e2998a83c9 in state CLOSED which uses HEALTHY_READONLY datanode b20d0805-391e-447d-a646-35a5d1a5cbcf. This will send close commands for its containers.
scm3_1   | 2023-05-31 01:07:25,025 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1 in state CLOSED which uses HEALTHY_READONLY datanode b20d0805-391e-447d-a646-35a5d1a5cbcf. This will send close commands for its containers.
dn2_1    | 2023-05-31 01:09:09,510 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-05-31 01:06:46,772 [3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/current/log_inprogress_11 to /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/current/log_11-24
recon_1  | 2023-05-31 01:06:38,516 [IPC Server handler 20 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn5_1.ha_net
dn5_1    | 2023-05-31 01:06:44,799 [grpc-default-executor-2] INFO impl.VoteContext: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-CANDIDATE: reject PRE_VOTE from b20d0805-391e-447d-a646-35a5d1a5cbcf: our priority 1 > candidate's priority 0
dn5_1    | 2023-05-31 01:06:44,799 [grpc-default-executor-2] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444 replies to PRE_VOTE vote request: b20d0805-391e-447d-a646-35a5d1a5cbcf<-3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#0:FAIL-t11. Peer's state: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444:t11, leader=null, voted=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, raftlog=Memoized:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-SegmentedRaftLog:OPENED:c24, conf=11: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-31 01:09:15,157 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn1_1    | 2023-05-31 01:08:33,211 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection9] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState
dn1_1    | 2023-05-31 01:08:35,949 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn2_1    | 2023-05-31 01:09:09,510 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-31 01:06:46,818 [3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/current/log_inprogress_25
recon_1  | 2023-05-31 01:06:38,852 [IPC Server handler 26 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn3_1.ha_net
recon_1  | 2023-05-31 01:06:38,876 [IPC Server handler 27 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn2_1.ha_net
recon_1  | 2023-05-31 01:06:39,501 [IPC Server handler 18 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn4_1.ha_net
dn4_1    | 2023-05-31 01:06:47,599 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-05-31 01:07:25,025 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444 in state CLOSED which uses HEALTHY_READONLY datanode b20d0805-391e-447d-a646-35a5d1a5cbcf. This will send close commands for its containers.
dn5_1    | 2023-05-31 01:06:44,746 [grpc-default-executor-3] INFO impl.VoteContext: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-CANDIDATE: accept PRE_VOTE from 3da1490a-d610-40de-95c1-bdfc259ba041: our priority 0 <= candidate's priority 1
dn5_1    | 2023-05-31 01:06:44,841 [grpc-default-executor-3] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1 replies to PRE_VOTE vote request: 3da1490a-d610-40de-95c1-bdfc259ba041<-3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#0:OK-t8. Peer's state: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1:t8, leader=null, voted=3da1490a-d610-40de-95c1-bdfc259ba041, raftlog=Memoized:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-SegmentedRaftLog:OPENED:c42, conf=29: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-31 01:09:09,511 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-05-31 01:09:09,511 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-05-31 01:09:09,511 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-05-31 01:09:09,511 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | 2023-05-31 01:06:43,740 [grpc-default-executor-2] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-05-31 01:06:44,850 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-LeaderElection3] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-LeaderElection3: PRE_VOTE TIMEOUT received 0 response(s) and 0 exception(s):
dn1_1    | 2023-05-31 01:08:37,787 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO impl.FollowerState: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5163862930ns, electionTimeout:5156ms
dn4_1    | 2023-05-31 01:06:48,600 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-31 01:09:15,157 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2] INFO impl.RoleInfo: 97734618-7966-42ac-b50e-e49ead6ce4d2: start 97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderStateImpl
scm1_1   | 2023-05-31 01:06:43,752 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
scm1_1   | 2023-05-31 01:06:43,825 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-05-31 01:06:44,853 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-LeaderElection3] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-LeaderElection3 PRE_VOTE round 0: result TIMEOUT
dn1_1    | 2023-05-31 01:08:37,788 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState
dn4_1    | 2023-05-31 01:06:49,601 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-31 01:07:00,049 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn4_1    | 2023-05-31 01:07:35,946 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm1_1   | 2023-05-31 01:06:43,832 [grpc-default-executor-2] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:43,860 [grpc-default-executor-3] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
scm1_1   | 2023-05-31 01:06:43,926 [grpc-default-executor-2] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-05-31 01:06:44,854 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-LeaderElection3] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-LeaderElection3 PRE_VOTE round 1: submit vote requests at term 8 for 29: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:06:44,869 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-31 01:06:43,949 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:43,967 [grpc-default-executor-2] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn5_1    | 2023-05-31 01:06:44,873 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-05-31 01:09:09,518 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm3_1   | 2023-05-31 01:07:25,025 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) moved to HEALTHY READONLY state.
scm3_1   | 2023-05-31 01:07:25,025 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1 in state CLOSED which uses HEALTHY_READONLY datanode 3da1490a-d610-40de-95c1-bdfc259ba041. This will send close commands for its containers.
dn1_1    | 2023-05-31 01:08:37,788 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: changes role from  FOLLOWER to CANDIDATE at term 12 for changeToCandidate
dn1_1    | 2023-05-31 01:08:37,788 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-05-31 01:08:37,788 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection10
scm1_1   | 2023-05-31 01:06:44,057 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:44,074 [grpc-default-executor-2] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-05-31 01:06:45,007 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn4_1    | 2023-05-31 01:08:16,334 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
recon_1  | 2023-05-31 01:07:05,537 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerSizeCountTask Thread.
recon_1  | 2023-05-31 01:07:05,540 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1  | 2023-05-31 01:07:05,542 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1  | 2023-05-31 01:07:05,763 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 7 pipelines in house.
recon_1  | 2023-05-31 01:07:05,845 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 302 milliseconds.
dn4_1    | 2023-05-31 01:08:16,335 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
recon_1  | 2023-05-31 01:07:06,043 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 501 milliseconds to process 0 existing database records.
recon_1  | 2023-05-31 01:07:06,241 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 197 milliseconds for processing 4 containers.
recon_1  | 2023-05-31 01:07:17,359 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1  | 2023-05-31 01:07:17,373 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
dn2_1    | 2023-05-31 01:09:09,518 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 2023-05-31 01:07:25,025 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444 in state CLOSED which uses HEALTHY_READONLY datanode 3da1490a-d610-40de-95c1-bdfc259ba041. This will send close commands for its containers.
scm3_1   | 2023-05-31 01:07:25,025 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=5509b2c4-975c-42f1-91be-39d463d01a51 in state CLOSED which uses HEALTHY_READONLY datanode 3da1490a-d610-40de-95c1-bdfc259ba041. This will send close commands for its containers.
dn5_1    | 2023-05-31 01:06:45,008 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO impl.LeaderElection:   Response 0: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd<-3da1490a-d610-40de-95c1-bdfc259ba041#0:OK-t11
dn4_1    | 2023-05-31 01:08:16,336 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn1_1    | 2023-05-31 01:08:37,794 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection10] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection10 PRE_VOTE round 0: submit vote requests at term 12 for 25: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-31 01:09:15,158 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-SegmentedRaftLogWorker: Starting segment from index:0
dn3_1    | 2023-05-31 01:09:15,159 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-LeaderElection2] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6: set configuration 0: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-31 01:09:09,518 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/46a9d8fb-17d8-4177-820b-4ef30e5fdeea
dn2_1    | 2023-05-31 01:09:09,518 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm3_1   | 2023-05-31 01:07:25,025 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18) moved to HEALTHY READONLY state.
dn5_1    | 2023-05-31 01:06:45,008 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1 PRE_VOTE round 1: result PASSED
dn4_1    | 2023-05-31 01:08:16,336 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: OPEN
dn1_1    | 2023-05-31 01:08:37,813 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection10] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection10 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: group-F1A4CF07E444 not found.
dn1_1    | 2023-05-31 01:08:37,814 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection10] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection10 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3da1490a-d610-40de-95c1-bdfc259ba041: group-F1A4CF07E444 not found.
dn1_1    | 2023-05-31 01:08:37,814 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection10] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection10: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
dn3_1    | 2023-05-31 01:09:15,161 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-297089FABFC6-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/b5c5c804-816d-41a2-83d8-297089fabfc6/current/log_inprogress_0
dn3_1    | 2023-05-31 01:09:15,358 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-FollowerState] INFO impl.FollowerState: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5194897679ns, electionTimeout:5183ms
scm1_1   | 2023-05-31 01:06:44,088 [grpc-default-executor-3] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
scm3_1   | 2023-05-31 01:07:25,026 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=ccaf8839-8737-4f75-8b75-b011fdd1ea12 in state CLOSED which uses HEALTHY_READONLY datanode 4bc93e86-8989-4282-b7d8-050f4a19b92f. This will send close commands for its containers.
scm3_1   | 2023-05-31 01:07:25,026 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21) moved to HEALTHY READONLY state.
dn4_1    | 2023-05-31 01:08:16,336 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
recon_1  | 2023-05-31 01:07:17,373 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
dn2_1    | 2023-05-31 01:09:09,518 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-05-31 01:09:15,359 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-FollowerState] INFO impl.RoleInfo: 97734618-7966-42ac-b50e-e49ead6ce4d2: shutdown 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-FollowerState
scm1_1   | 2023-05-31 01:06:44,174 [grpc-default-executor-2] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | 2023-05-31 01:08:37,814 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection10] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: group-F1A4CF07E444 not found.
scm3_1   | 2023-05-31 01:07:25,026 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=437d8269-2a0d-4058-8be6-ed0a589991d2 in state CLOSED which uses HEALTHY_READONLY datanode 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd. This will send close commands for its containers.
scm3_1   | 2023-05-31 01:07:25,026 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1 in state CLOSED which uses HEALTHY_READONLY datanode 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd. This will send close commands for its containers.
dn5_1    | 2023-05-31 01:06:45,010 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1 ELECTION round 1: submit vote requests at term 12 for 11: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 2023-05-31 01:07:17,373 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
dn2_1    | 2023-05-31 01:09:09,518 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-05-31 01:09:09,519 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm1_1   | 2023-05-31 01:06:44,180 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | 2023-05-31 01:08:37,814 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection10] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3da1490a-d610-40de-95c1-bdfc259ba041: group-F1A4CF07E444 not found.
scm3_1   | 2023-05-31 01:07:25,026 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444 in state CLOSED which uses HEALTHY_READONLY datanode 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd. This will send close commands for its containers.
dn5_1    | 2023-05-31 01:06:45,037 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-05-31 01:08:16,337 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn4_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
dn2_1    | 2023-05-31 01:09:09,569 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | 2023-05-31 01:06:44,197 [grpc-default-executor-2] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn1_1    | 2023-05-31 01:08:37,814 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection10] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection10 PRE_VOTE round 0: result REJECTED
dn1_1    | 2023-05-31 01:08:37,814 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection10] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: changes role from CANDIDATE to FOLLOWER at term 12 for REJECTED
dn1_1    | 2023-05-31 01:08:37,814 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection10] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection10
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
dn3_1    | 2023-05-31 01:09:15,359 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-FollowerState] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
recon_1  | 2023-05-31 01:07:17,374 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
dn2_1    | 2023-05-31 01:09:09,569 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-05-31 01:09:09,569 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-05-31 01:08:37,815 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-LeaderElection10] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState
dn1_1    | 2023-05-31 01:08:38,373 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO impl.FollowerState: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5161817414ns, electionTimeout:5157ms
dn1_1    | 2023-05-31 01:08:38,373 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState
scm3_1   | 2023-05-31 01:07:25,026 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19) moved to HEALTHY READONLY state.
dn3_1    | 2023-05-31 01:09:15,360 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-05-31 01:09:15,360 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-FollowerState] INFO impl.RoleInfo: 97734618-7966-42ac-b50e-e49ead6ce4d2: start 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-LeaderElection3
dn2_1    | 2023-05-31 01:09:09,569 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-05-31 01:09:09,571 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-05-31 01:06:45,078 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-05-31 01:08:38,374 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: changes role from  FOLLOWER to CANDIDATE at term 9 for changeToCandidate
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
scm3_1   | 2023-05-31 01:07:25,026 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=9e886fff-1860-4674-bb59-7a8b4be06a04 in state CLOSED which uses HEALTHY_READONLY datanode 97734618-7966-42ac-b50e-e49ead6ce4d2. This will send close commands for its containers.
recon_1  | 2023-05-31 01:07:17,374 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
recon_1  | 2023-05-31 01:07:17,374 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
scm1_1   | 2023-05-31 01:06:44,313 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:44,363 [grpc-default-executor-2] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-05-31 01:06:45,079 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-LeaderElection3] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-LeaderElection3: PRE_VOTE REJECTED received 1 response(s) and 0 exception(s):
dn1_1    | 2023-05-31 01:08:38,374 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
scm3_1   | 2023-05-31 01:07:28,880 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
recon_1  | 2023-05-31 01:07:17,374 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 131 
recon_1  | 2023-05-31 01:07:17,637 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 9, SequenceNumber diff: 25, SequenceNumber Lag from OM 0.
scm1_1   | 2023-05-31 01:06:44,379 [grpc-default-executor-3] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
scm1_1   | 2023-05-31 01:06:44,829 [grpc-default-executor-2] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:44,830 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | 2023-05-31 01:08:38,374 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection11
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:733)
scm3_1   | 2023-05-31 01:07:28,880 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-05-31 01:09:09,577 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-05-31 01:08:38,374 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection11] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection11 PRE_VOTE round 0: submit vote requests at term 9 for 43: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:662)
dn5_1    | 2023-05-31 01:06:45,089 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-LeaderElection3] INFO impl.LeaderElection:   Response 0: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd<-3da1490a-d610-40de-95c1-bdfc259ba041#0:FAIL-t8
dn5_1    | 2023-05-31 01:06:45,089 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-LeaderElection3] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-LeaderElection3 PRE_VOTE round 1: result REJECTED
dn1_1    | 2023-05-31 01:08:38,386 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection11] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection11 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3da1490a-d610-40de-95c1-bdfc259ba041: group-DC0251522AF1 not found.
scm3_1   | 2023-05-31 01:07:33,892 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:07:33,892 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-05-31 01:09:15,361 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-LeaderElection3] INFO impl.LeaderElection: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-31 01:09:15,364 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-LeaderElection3-1] INFO server.GrpcServerProtocolClient: Build channel for 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn3_1    | 2023-05-31 01:09:15,373 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-05-31 01:09:09,604 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-05-31 01:06:45,092 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-LeaderElection3] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1: changes role from CANDIDATE to FOLLOWER at term 8 for REJECTED
dn5_1    | 2023-05-31 01:06:45,092 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-LeaderElection3] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: shutdown 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-LeaderElection3
scm3_1   | 2023-05-31 01:07:38,525 [IPC Server handler 91 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-05-31 01:07:38,851 [IPC Server handler 74 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn4_1    | 2023-05-31 01:08:16,337 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
scm1_1   | 2023-05-31 01:06:44,923 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-05-31 01:07:17,637 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 25 records
dn3_1    | 2023-05-31 01:09:15,375 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-05-31 01:09:09,605 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-05-31 01:06:45,093 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-LeaderElection3] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: start 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-FollowerState
dn5_1    | 2023-05-31 01:06:45,158 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn5_1    | 2023-05-31 01:06:45,158 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO impl.LeaderElection:   Response 0: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd<-3da1490a-d610-40de-95c1-bdfc259ba041#0:OK-t12
dn4_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
recon_1  | 2023-05-31 01:07:17,644 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
dn3_1    | 2023-05-31 01:09:15,374 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-LeaderElection3-2] INFO server.GrpcServerProtocolClient: Build channel for 3da1490a-d610-40de-95c1-bdfc259ba041
dn2_1    | 2023-05-31 01:09:09,605 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-05-31 01:06:45,158 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1 ELECTION round 1: result PASSED
dn1_1    | 2023-05-31 01:08:38,392 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection11] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection11 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: group-DC0251522AF1 not found.
dn1_1    | 2023-05-31 01:08:38,392 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection11] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection11: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
scm1_1   | 2023-05-31 01:06:44,931 [grpc-default-executor-2] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
recon_1  | 2023-05-31 01:07:17,646 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
dn3_1    | 2023-05-31 01:09:15,425 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-LeaderElection3] INFO impl.LeaderElection: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-LeaderElection3: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
dn2_1    | 2023-05-31 01:09:09,605 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO segmented.SegmentedRaftLogWorker: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-05-31 01:09:09,605 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO segmented.SegmentedRaftLogWorker: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm3_1   | 2023-05-31 01:07:38,922 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-05-31 01:08:38,392 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection11] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3da1490a-d610-40de-95c1-bdfc259ba041: group-DC0251522AF1 not found.
scm1_1   | 2023-05-31 01:06:44,942 [grpc-default-executor-3] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:733)
dn3_1    | 2023-05-31 01:09:15,425 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-LeaderElection3] INFO impl.LeaderElection:   Response 0: 97734618-7966-42ac-b50e-e49ead6ce4d2<-3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#0:FAIL-t0
dn2_1    | 2023-05-31 01:09:09,606 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA: start as a follower, conf=-1: peers:[4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-31 01:09:09,607 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn1_1    | 2023-05-31 01:08:38,392 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection11] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: group-DC0251522AF1 not found.
dn1_1    | 2023-05-31 01:08:38,392 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection11] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection11 PRE_VOTE round 0: result REJECTED
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn3_1    | 2023-05-31 01:09:15,425 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-LeaderElection3] INFO impl.LeaderElection:   Response 1: 97734618-7966-42ac-b50e-e49ead6ce4d2<-3da1490a-d610-40de-95c1-bdfc259ba041#0:OK-t0
dn3_1    | 2023-05-31 01:09:15,425 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-LeaderElection3] INFO impl.LeaderElection: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-LeaderElection3 PRE_VOTE round 0: result REJECTED
scm3_1   | 2023-05-31 01:07:38,922 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-05-31 01:09:09,609 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO impl.RoleInfo: 4bc93e86-8989-4282-b7d8-050f4a19b92f: start 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-FollowerState
dn1_1    | 2023-05-31 01:08:38,392 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection11] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: changes role from CANDIDATE to FOLLOWER at term 9 for REJECTED
scm1_1   | 2023-05-31 01:06:46,083 [grpc-default-executor-2] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-05-31 01:07:17,997 [pool-49-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:662)
scm3_1   | 2023-05-31 01:07:40,991 [IPC Server handler 75 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn2_1    | 2023-05-31 01:09:09,610 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4EF30E5FDEEA,id=4bc93e86-8989-4282-b7d8-050f4a19b92f
dn1_1    | 2023-05-31 01:08:38,393 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection11] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection11
scm1_1   | 2023-05-31 01:06:46,083 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-05-31 01:07:18,020 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 3 OM DB update event(s).
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-05-31 01:08:16,348 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 3da1490a-d610-40de-95c1-bdfc259ba041: remove    LEADER 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1:t9, leader=3da1490a-d610-40de-95c1-bdfc259ba041, voted=3da1490a-d610-40de-95c1-bdfc259ba041, raftlog=Memoized:3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-SegmentedRaftLog:OPENED:c44, conf=43: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn3_1    | 2023-05-31 01:09:15,426 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-LeaderElection3] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
dn3_1    | 2023-05-31 01:09:15,426 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-LeaderElection3] INFO impl.RoleInfo: 97734618-7966-42ac-b50e-e49ead6ce4d2: shutdown 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-LeaderElection3
dn2_1    | 2023-05-31 01:09:09,611 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-05-31 01:08:38,393 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-LeaderElection11] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState
scm1_1   | 2023-05-31 01:06:46,098 [grpc-default-executor-2] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-05-31 01:07:18,076 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
dn5_1    | 2023-05-31 01:06:45,159 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: shutdown 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1
dn4_1    | 2023-05-31 01:08:16,355 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1: shutdown
dn3_1    | 2023-05-31 01:09:15,426 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-LeaderElection3] INFO impl.RoleInfo: 97734618-7966-42ac-b50e-e49ead6ce4d2: start 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-FollowerState
scm3_1   | 2023-05-31 01:07:43,988 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:07:43,989 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:07:45,215 [IPC Server handler 38 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn1_1    | 2023-05-31 01:08:41,952 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm1_1   | 2023-05-31 01:06:46,154 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-05-31 01:08:05,589 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Deleted 0 records from "CONTAINER_COUNT_BY_SIZE"
dn5_1    | 2023-05-31 01:06:45,159 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444: changes role from CANDIDATE to LEADER at term 12 for changeToLeader
dn4_1    | 2023-05-31 01:08:16,355 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-DC0251522AF1,id=3da1490a-d610-40de-95c1-bdfc259ba041
dn3_1    | 2023-05-31 01:09:15,730 [grpc-default-executor-0] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93: receive requestVote(PRE_VOTE, 3da1490a-d610-40de-95c1-bdfc259ba041, group-A59219751D93, 0, (t:0, i:0))
dn3_1    | 2023-05-31 01:09:15,730 [grpc-default-executor-0] INFO impl.VoteContext: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93-FOLLOWER: accept PRE_VOTE from 3da1490a-d610-40de-95c1-bdfc259ba041: our priority 0 <= candidate's priority 1
dn2_1    | 2023-05-31 01:09:09,611 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-05-31 01:08:41,952 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn1_1    | 2023-05-31 01:08:41,953 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn1_1    | 2023-05-31 01:08:41,953 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: OPEN
dn5_1    | 2023-05-31 01:06:45,159 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-F1A4CF07E444 with new leaderId: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn4_1    | 2023-05-31 01:08:16,355 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: shutdown 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-LeaderStateImpl
dn4_1    | 2023-05-31 01:08:16,357 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1->3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1->3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
dn4_1    | 2023-05-31 01:08:16,364 [grpc-default-executor-5] INFO server.GrpcLogAppender: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1->3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-AppendLogResponseHandler: follower responses appendEntries COMPLETED
recon_1  | 2023-05-31 01:08:05,681 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
dn1_1    | 2023-05-31 01:08:41,953 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn1_1    | 2023-05-31 01:08:41,953 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn5_1    | 2023-05-31 01:06:45,160 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444: change Leader from null to 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd at term 12 for becomeLeader, leader elected after 38486ms
dn4_1    | 2023-05-31 01:08:16,367 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1->b20d0805-391e-447d-a646-35a5d1a5cbcf-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1->b20d0805-391e-447d-a646-35a5d1a5cbcf-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
dn4_1    | 2023-05-31 01:08:16,383 [grpc-default-executor-2] INFO server.GrpcLogAppender: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1->b20d0805-391e-447d-a646-35a5d1a5cbcf-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn4_1    | 2023-05-31 01:08:16,386 [grpc-default-executor-2] INFO leader.FollowerInfo: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1->b20d0805-391e-447d-a646-35a5d1a5cbcf: decreaseNextIndex nextIndex: updateUnconditionally 45 -> 44
recon_1  | 2023-05-31 01:08:05,681 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 91
scm1_1   | 2023-05-31 01:06:46,163 [grpc-default-executor-2] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn5_1    | 2023-05-31 01:06:45,160 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn4_1    | 2023-05-31 01:08:16,387 [grpc-default-executor-5] INFO leader.FollowerInfo: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1->3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: decreaseNextIndex nextIndex: updateUnconditionally 45 -> 44
dn4_1    | 2023-05-31 01:08:16,388 [PipelineCommandHandlerThread-0] INFO impl.PendingRequests: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-PendingRequests: sendNotLeaderResponses
dn3_1    | 2023-05-31 01:09:15,731 [grpc-default-executor-0] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93 replies to PRE_VOTE vote request: 3da1490a-d610-40de-95c1-bdfc259ba041<-97734618-7966-42ac-b50e-e49ead6ce4d2#0:OK-t0. Peer's state: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93:t0, leader=null, voted=, raftlog=Memoized:97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-31 01:09:09,611 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
recon_1  | 2023-05-31 01:08:16,780 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: Container #1 has state OPEN, but given state is CLOSING.
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
dn5_1    | 2023-05-31 01:06:45,161 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm3_1   | 2023-05-31 01:07:45,362 [IPC Server handler 99 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-05-31 01:07:49,188 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:07:49,188 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-05-31 01:09:09,611 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
recon_1  | 2023-05-31 01:08:16,824 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to QUASI_CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported QUASI_CLOSED replica.
dn3_1    | 2023-05-31 01:09:15,749 [grpc-default-executor-0] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93: receive requestVote(ELECTION, 3da1490a-d610-40de-95c1-bdfc259ba041, group-A59219751D93, 1, (t:0, i:0))
dn3_1    | 2023-05-31 01:09:15,750 [grpc-default-executor-0] INFO impl.VoteContext: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93-FOLLOWER: accept ELECTION from 3da1490a-d610-40de-95c1-bdfc259ba041: our priority 0 <= candidate's priority 1
dn3_1    | 2023-05-31 01:09:15,750 [grpc-default-executor-0] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:3da1490a-d610-40de-95c1-bdfc259ba041
dn2_1    | 2023-05-31 01:09:09,612 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-05-31 01:09:09,612 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-05-31 01:06:45,161 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
scm1_1   | 2023-05-31 01:06:46,172 [grpc-default-executor-3] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn3_1    | 2023-05-31 01:09:15,750 [grpc-default-executor-0] INFO impl.RoleInfo: 97734618-7966-42ac-b50e-e49ead6ce4d2: shutdown 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93-FollowerState
scm3_1   | 2023-05-31 01:07:54,217 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:07:54,217 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-05-31 01:09:09,614 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=46a9d8fb-17d8-4177-820b-4ef30e5fdeea
recon_1  | 2023-05-31 01:08:16,860 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: Container #1002 has state OPEN, but given state is CLOSING.
dn5_1    | 2023-05-31 01:06:45,161 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
scm1_1   | 2023-05-31 01:06:46,249 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:46,333 [grpc-default-executor-1] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-05-31 01:07:56,900 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for delTxnId, expected lastId is 0, actual lastId is 2000.
scm3_1   | 2023-05-31 01:07:59,218 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-05-31 01:09:09,615 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=46a9d8fb-17d8-4177-820b-4ef30e5fdeea.
recon_1  | 2023-05-31 01:08:17,111 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1002 to QUASI_CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported QUASI_CLOSED replica.
recon_1  | 2023-05-31 01:08:17,117 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: Container #1001 has state OPEN, but given state is CLOSING.
recon_1  | 2023-05-31 01:08:17,159 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to QUASI_CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported QUASI_CLOSED replica.
dn5_1    | 2023-05-31 01:06:45,161 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 2023-05-31 01:06:45,161 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn3_1    | 2023-05-31 01:09:15,751 [grpc-default-executor-0] INFO impl.RoleInfo: 97734618-7966-42ac-b50e-e49ead6ce4d2: start 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93-FollowerState
dn3_1    | 2023-05-31 01:09:15,751 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93-FollowerState] INFO impl.FollowerState: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93-FollowerState was interrupted
dn3_1    | 2023-05-31 01:09:15,754 [grpc-default-executor-0] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93 replies to ELECTION vote request: 3da1490a-d610-40de-95c1-bdfc259ba041<-97734618-7966-42ac-b50e-e49ead6ce4d2#0:OK-t1. Peer's state: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93:t1, leader=null, voted=3da1490a-d610-40de-95c1-bdfc259ba041, raftlog=Memoized:97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-31 01:09:09,616 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 4bc93e86-8989-4282-b7d8-050f4a19b92f: addNew group-A59219751D93:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER] returns group-A59219751D93:java.util.concurrent.CompletableFuture@45014777[Not completed]
dn2_1    | 2023-05-31 01:09:09,624 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f: new RaftServerImpl for group-A59219751D93:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
recon_1  | 2023-05-31 01:08:17,177 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: Container #2 has state OPEN, but given state is CLOSING.
recon_1  | 2023-05-31 01:08:17,237 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to QUASI_CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported QUASI_CLOSED replica.
dn5_1    | 2023-05-31 01:06:45,162 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-05-31 01:06:45,162 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn5_1    | 2023-05-31 01:06:45,260 [grpc-default-executor-0] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1: receive requestVote(ELECTION, 3da1490a-d610-40de-95c1-bdfc259ba041, group-DC0251522AF1, 9, (t:8, i:42))
dn3_1    | 2023-05-31 01:09:15,863 [97734618-7966-42ac-b50e-e49ead6ce4d2-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-A59219751D93 with new leaderId: 3da1490a-d610-40de-95c1-bdfc259ba041
dn3_1    | 2023-05-31 01:09:15,864 [97734618-7966-42ac-b50e-e49ead6ce4d2-server-thread1] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93: change Leader from null to 3da1490a-d610-40de-95c1-bdfc259ba041 at term 1 for appendEntries, leader elected after 4428ms
recon_1  | 2023-05-31 01:08:18,096 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:733)
scm3_1   | 2023-05-31 01:07:59,218 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:08:04,223 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:08:04,223 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-05-31 01:09:15,864 [97734618-7966-42ac-b50e-e49ead6ce4d2-server-thread1] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93: set configuration 0: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-31 01:09:15,865 [97734618-7966-42ac-b50e-e49ead6ce4d2-server-thread1] INFO segmented.SegmentedRaftLogWorker: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93-SegmentedRaftLogWorker: Starting segment from index:0
dn3_1    | 2023-05-31 01:09:15,866 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-A59219751D93-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5d339866-97f6-4ef6-b080-a59219751d93/current/log_inprogress_0
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
scm1_1   | 2023-05-31 01:06:46,312 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-05-31 01:08:18,096 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1  | 2023-05-31 01:08:18,096 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 156 
recon_1  | 2023-05-31 01:08:18,110 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 1, SequenceNumber diff: 4, SequenceNumber Lag from OM 0.
recon_1  | 2023-05-31 01:08:18,110 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 4 records
recon_1  | 2023-05-31 01:08:18,114 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
recon_1  | 2023-05-31 01:08:18,114 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
dn2_1    | 2023-05-31 01:09:09,624 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm1_1   | 2023-05-31 01:06:46,387 [grpc-default-executor-3] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
recon_1  | 2023-05-31 01:08:18,206 [pool-49-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
dn2_1    | 2023-05-31 01:09:09,625 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1_1   | 2023-05-31 01:06:46,470 [grpc-default-executor-1] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:662)
recon_1  | 2023-05-31 01:08:18,206 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 0 OM DB update event(s).
recon_1  | 2023-05-31 01:08:18,206 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
dn4_1    | 2023-05-31 01:08:16,391 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-StateMachineUpdater: set stopIndex = 44
dn4_1    | 2023-05-31 01:08:16,397 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-DC0251522AF1: Taking a snapshot at:(t:9, i:44) file /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/sm/snapshot.9_44
dn4_1    | 2023-05-31 01:08:16,402 [grpc-default-executor-5] INFO server.GrpcLogAppender: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1->3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn2_1    | 2023-05-31 01:09:09,625 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1_1   | 2023-05-31 01:06:46,484 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:46,501 [grpc-default-executor-1] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn3_1    | 2023-05-31 01:09:16,670 [grpc-default-executor-1] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83: receive requestVote(PRE_VOTE, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, group-6F942E88FF83, 0, (t:0, i:0))
dn3_1    | 2023-05-31 01:09:16,671 [grpc-default-executor-1] INFO impl.VoteContext: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-FOLLOWER: accept PRE_VOTE from 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: our priority 0 <= candidate's priority 1
dn3_1    | 2023-05-31 01:09:16,671 [grpc-default-executor-1] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83 replies to PRE_VOTE vote request: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd<-97734618-7966-42ac-b50e-e49ead6ce4d2#0:OK-t0. Peer's state: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83:t0, leader=null, voted=, raftlog=Memoized:97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-31 01:09:16,685 [grpc-default-executor-1] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83: receive requestVote(ELECTION, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, group-6F942E88FF83, 1, (t:0, i:0))
recon_1  | 2023-05-31 01:09:05,682 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
dn2_1    | 2023-05-31 01:09:09,625 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | 2023-05-31 01:06:45,265 [grpc-default-executor-0] INFO impl.VoteContext: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-FOLLOWER: accept ELECTION from 3da1490a-d610-40de-95c1-bdfc259ba041: our priority 0 <= candidate's priority 1
dn3_1    | 2023-05-31 01:09:16,685 [grpc-default-executor-1] INFO impl.VoteContext: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-FOLLOWER: accept ELECTION from 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: our priority 0 <= candidate's priority 1
dn3_1    | 2023-05-31 01:09:16,686 [grpc-default-executor-1] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn3_1    | 2023-05-31 01:09:16,686 [grpc-default-executor-1] INFO impl.RoleInfo: 97734618-7966-42ac-b50e-e49ead6ce4d2: shutdown 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-FollowerState
scm1_1   | 2023-05-31 01:06:46,611 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-05-31 01:09:05,682 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
dn2_1    | 2023-05-31 01:09:09,625 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-05-31 01:08:41,953 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn5_1    | 2023-05-31 01:06:45,266 [grpc-default-executor-0] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1: changes role from  FOLLOWER to FOLLOWER at term 9 for candidate:3da1490a-d610-40de-95c1-bdfc259ba041
dn5_1    | 2023-05-31 01:06:45,267 [grpc-default-executor-0] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: shutdown 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-FollowerState
dn5_1    | 2023-05-31 01:06:45,267 [grpc-default-executor-0] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: start 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-FollowerState
dn5_1    | 2023-05-31 01:06:45,269 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-FollowerState] INFO impl.FollowerState: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-FollowerState was interrupted
dn4_1    | 2023-05-31 01:08:16,402 [grpc-default-executor-5] INFO leader.FollowerInfo: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1->3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: decreaseNextIndex nextIndex: updateUnconditionally 44 -> 43
scm1_1   | 2023-05-31 01:06:46,633 [grpc-default-executor-1] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:46,658 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn2_1    | 2023-05-31 01:09:09,626 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn2_1    | 2023-05-31 01:09:09,626 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93: ConfigurationManager, init=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-05-31 01:06:45,305 [grpc-default-executor-0] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1 replies to ELECTION vote request: 3da1490a-d610-40de-95c1-bdfc259ba041<-3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#0:OK-t9. Peer's state: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1:t9, leader=null, voted=3da1490a-d610-40de-95c1-bdfc259ba041, raftlog=Memoized:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-SegmentedRaftLog:OPENED:c42, conf=29: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 2023-05-31 01:09:09,521 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=46a9d8fb-17d8-4177-820b-4ef30e5fdeea. Trying to get from SCM.
dn3_1    | 2023-05-31 01:09:16,686 [grpc-default-executor-1] INFO impl.RoleInfo: 97734618-7966-42ac-b50e-e49ead6ce4d2: start 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-FollowerState
scm3_1   | 2023-05-31 01:08:08,477 [IPC Server handler 54 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn4_1    | 2023-05-31 01:08:16,410 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-DC0251522AF1: Finished taking a snapshot at:(t:9, i:44) file:/data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/sm/snapshot.9_44 took: 17 ms
scm1_1   | 2023-05-31 01:06:46,759 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 2023-05-31 01:09:09,626 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
dn5_1    | 2023-05-31 01:06:45,371 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn5_1    | 2023-05-31 01:06:45,384 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-05-31 01:09:16,686 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-FollowerState] INFO impl.FollowerState: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-FollowerState was interrupted
scm3_1   | 2023-05-31 01:08:08,838 [IPC Server handler 74 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn4_1    | 2023-05-31 01:08:16,414 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-StateMachineUpdater] INFO impl.StateMachineUpdater: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-StateMachineUpdater: Took a snapshot at index 44
scm1_1   | 2023-05-31 01:06:46,766 [grpc-default-executor-1] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 2023-05-31 01:09:09,627 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
dn2_1    | 2023-05-31 01:09:09,627 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-05-31 01:06:45,385 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn3_1    | 2023-05-31 01:09:16,688 [grpc-default-executor-1] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83 replies to ELECTION vote request: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd<-97734618-7966-42ac-b50e-e49ead6ce4d2#0:OK-t1. Peer's state: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83:t1, leader=null, voted=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, raftlog=Memoized:97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:08:16,415 [3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-StateMachineUpdater] INFO impl.StateMachineUpdater: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-StateMachineUpdater: snapshotIndex: updateIncreasingly 42 -> 44
scm1_1   | 2023-05-31 01:06:46,770 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
scm1_1   | 2023-05-31 01:06:46,922 [grpc-default-executor-1] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-05-31 01:09:09,546 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 46a9d8fb-17d8-4177-820b-4ef30e5fdeea, Nodes: 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:41.112Z[UTC]] to Recon pipeline metadata.
dn2_1    | 2023-05-31 01:09:09,627 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-05-31 01:06:45,412 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn3_1    | 2023-05-31 01:09:16,757 [97734618-7966-42ac-b50e-e49ead6ce4d2-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6F942E88FF83 with new leaderId: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn4_1    | 2023-05-31 01:08:16,417 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1: closes. applyIndex: 44
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
recon_1  | 2023-05-31 01:09:09,552 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 46a9d8fb-17d8-4177-820b-4ef30e5fdeea, Nodes: 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:41.112Z[UTC]].
dn2_1    | 2023-05-31 01:09:09,628 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn5_1    | 2023-05-31 01:06:45,425 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn3_1    | 2023-05-31 01:09:16,757 [97734618-7966-42ac-b50e-e49ead6ce4d2-server-thread1] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83: change Leader from null to 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd at term 1 for appendEntries, leader elected after 6717ms
dn4_1    | 2023-05-31 01:08:16,430 [grpc-default-executor-5] INFO server.GrpcLogAppender: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1->b20d0805-391e-447d-a646-35a5d1a5cbcf-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:733)
recon_1  | 2023-05-31 01:09:09,552 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=46a9d8fb-17d8-4177-820b-4ef30e5fdeea reported by 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18)
recon_1  | 2023-05-31 01:09:09,553 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 46a9d8fb-17d8-4177-820b-4ef30e5fdeea, Nodes: 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:4bc93e86-8989-4282-b7d8-050f4a19b92f, CreationTimestamp2023-05-31T01:08:41.112Z[UTC]] moved to OPEN state
dn2_1    | 2023-05-31 01:09:09,628 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-05-31 01:06:45,425 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-05-31 01:09:16,758 [97734618-7966-42ac-b50e-e49ead6ce4d2-server-thread2] INFO server.RaftServer$Division: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83: set configuration 0: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:08:16,431 [grpc-default-executor-5] INFO leader.FollowerInfo: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1->b20d0805-391e-447d-a646-35a5d1a5cbcf: decreaseNextIndex nextIndex: updateUnconditionally 44 -> 43
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
recon_1  | 2023-05-31 01:09:09,657 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=5d339866-97f6-4ef6-b080-a59219751d93. Trying to get from SCM.
recon_1  | 2023-05-31 01:09:09,665 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 5d339866-97f6-4ef6-b080-a59219751d93, Nodes: 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18)97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:50.188Z[UTC]] to Recon pipeline metadata.
dn2_1    | 2023-05-31 01:09:09,629 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-05-31 01:06:45,426 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn3_1    | 2023-05-31 01:09:16,759 [97734618-7966-42ac-b50e-e49ead6ce4d2-server-thread2] INFO segmented.SegmentedRaftLogWorker: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-SegmentedRaftLogWorker: Starting segment from index:0
dn3_1    | 2023-05-31 01:09:16,764 [97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 97734618-7966-42ac-b50e-e49ead6ce4d2@group-6F942E88FF83-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9342e1de-b2fb-494f-91d9-6f942e88ff83/current/log_inprogress_0
dn2_1    | 2023-05-31 01:09:09,629 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn5_1    | 2023-05-31 01:06:45,426 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn4_1    | 2023-05-31 01:08:16,734 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1-SegmentedRaftLogWorker close()
dn4_1    | 2023-05-31 01:08:16,771 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 27.
dn4_1    | 2023-05-31 01:08:16,771 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 27.
scm3_1   | 2023-05-31 01:08:09,338 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
recon_1  | 2023-05-31 01:09:09,670 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 5d339866-97f6-4ef6-b080-a59219751d93, Nodes: 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18)97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:50.188Z[UTC]].
dn3_1    | 2023-05-31 01:09:33,959 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm1_1   | 2023-05-31 01:06:46,925 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:46,939 [grpc-default-executor-1] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
scm1_1   | 2023-05-31 01:06:47,039 [grpc-default-executor-1] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-05-31 01:08:16,833 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 40.
dn4_1    | 2023-05-31 01:08:16,833 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 40.
scm3_1   | 2023-05-31 01:08:09,339 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 2023-05-31 01:09:09,670 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5d339866-97f6-4ef6-b080-a59219751d93 reported by 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18)
recon_1  | 2023-05-31 01:09:09,896 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=b5c5c804-816d-41a2-83d8-297089fabfc6. Trying to get from SCM.
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:662)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-05-31 01:08:16,906 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-DC0251522AF1: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1
dn4_1    | 2023-05-31 01:08:16,913 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1 command on datanode 3da1490a-d610-40de-95c1-bdfc259ba041.
scm3_1   | 2023-05-31 01:08:10,984 [IPC Server handler 75 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | 2023-05-31 01:09:09,900 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: b5c5c804-816d-41a2-83d8-297089fabfc6, Nodes: 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:41.141Z[UTC]] to Recon pipeline metadata.
dn3_1    | 2023-05-31 01:10:33,963 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-05-31 01:08:41,963 [PipelineCommandHandlerThread-0] INFO server.RaftServer: b20d0805-391e-447d-a646-35a5d1a5cbcf: remove    LEADER b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9:t4, leader=b20d0805-391e-447d-a646-35a5d1a5cbcf, voted=b20d0805-391e-447d-a646-35a5d1a5cbcf, raftlog=Memoized:b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-SegmentedRaftLog:OPENED:c6, conf=5: peers:[b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn1_1    | 2023-05-31 01:08:41,965 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9: shutdown
dn1_1    | 2023-05-31 01:08:41,965 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-A4E2998A83C9,id=b20d0805-391e-447d-a646-35a5d1a5cbcf
dn1_1    | 2023-05-31 01:08:41,966 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-LeaderStateImpl
dn2_1    | 2023-05-31 01:09:09,637 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm3_1   | 2023-05-31 01:08:14,276 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: ccaf8839-8737-4f75-8b75-b011fdd1ea12, Nodes: 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:4bc93e86-8989-4282-b7d8-050f4a19b92f, CreationTimestamp2023-05-31T01:06:41.936506Z[UTC]] removed.
recon_1  | 2023-05-31 01:09:09,902 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: b5c5c804-816d-41a2-83d8-297089fabfc6, Nodes: 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:41.141Z[UTC]].
dn3_1    | 2023-05-31 01:11:33,964 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm1_1   | 2023-05-31 01:06:47,051 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-05-31 01:06:45,426 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-05-31 01:06:45,426 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-05-31 01:09:09,639 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
recon_1  | 2023-05-31 01:09:09,902 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=b5c5c804-816d-41a2-83d8-297089fabfc6 reported by 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19)
recon_1  | 2023-05-31 01:09:09,902 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: b5c5c804-816d-41a2-83d8-297089fabfc6, Nodes: 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:97734618-7966-42ac-b50e-e49ead6ce4d2, CreationTimestamp2023-05-31T01:08:41.141Z[UTC]] moved to OPEN state
dn1_1    | 2023-05-31 01:08:41,973 [PipelineCommandHandlerThread-0] INFO impl.PendingRequests: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-PendingRequests: sendNotLeaderResponses
scm1_1   | 2023-05-31 01:06:47,062 [grpc-default-executor-1] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
scm3_1   | 2023-05-31 01:08:14,300 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 6a499bf5-10af-4dab-a3fd-a4e2998a83c9, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:b20d0805-391e-447d-a646-35a5d1a5cbcf, CreationTimestamp2023-05-31T01:06:41.931195Z[UTC]] removed.
scm3_1   | 2023-05-31 01:08:14,324 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 9e886fff-1860-4674-bb59-7a8b4be06a04, Nodes: 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:97734618-7966-42ac-b50e-e49ead6ce4d2, CreationTimestamp2023-05-31T01:06:41.931322Z[UTC]] removed.
dn2_1    | 2023-05-31 01:09:09,640 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-05-31 01:09:09,640 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-05-31 01:09:09,640 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-05-31 01:09:09,641 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-05-31 01:09:09,641 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5d339866-97f6-4ef6-b080-a59219751d93 does not exist. Creating ...
dn2_1    | 2023-05-31 01:09:09,643 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5d339866-97f6-4ef6-b080-a59219751d93/in_use.lock acquired by nodename 7@86166c590ca2
scm1_1   | 2023-05-31 01:06:47,146 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-05-31 01:08:14,354 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 437d8269-2a0d-4058-8be6-ed0a589991d2, Nodes: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, CreationTimestamp2023-05-31T01:06:41.924825Z[UTC]] removed.
scm3_1   | 2023-05-31 01:08:14,389 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: a4fe2088-9059-47e2-9c65-dc0251522af1, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17)3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:3da1490a-d610-40de-95c1-bdfc259ba041, CreationTimestamp2023-05-31T01:06:41.936363Z[UTC]] removed.
recon_1  | 2023-05-31 01:09:10,078 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=9342e1de-b2fb-494f-91d9-6f942e88ff83. Trying to get from SCM.
recon_1  | 2023-05-31 01:09:10,087 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 9342e1de-b2fb-494f-91d9-6f942e88ff83, Nodes: 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19)3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21)3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:50.131Z[UTC]] to Recon pipeline metadata.
dn1_1    | 2023-05-31 01:08:41,978 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-StateMachineUpdater: set stopIndex = 6
dn1_1    | 2023-05-31 01:08:41,979 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-A4E2998A83C9: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/6a499bf5-10af-4dab-a3fd-a4e2998a83c9/sm/snapshot.4_6
scm1_1   | 2023-05-31 01:06:47,158 [grpc-default-executor-1] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:47,166 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
scm1_1   | 2023-05-31 01:06:47,243 [grpc-default-executor-1] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-05-31 01:08:16,913 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 3da1490a-d610-40de-95c1-bdfc259ba041: remove  FOLLOWER 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444:t12, leader=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, voted=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, raftlog=Memoized:3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-SegmentedRaftLog:OPENED:c26, conf=25: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn4_1    | 2023-05-31 01:08:16,913 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444: shutdown
dn4_1    | 2023-05-31 01:08:16,914 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-F1A4CF07E444,id=3da1490a-d610-40de-95c1-bdfc259ba041
dn1_1    | 2023-05-31 01:08:41,981 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-A4E2998A83C9: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/6a499bf5-10af-4dab-a3fd-a4e2998a83c9/sm/snapshot.4_6 took: 3 ms
dn1_1    | 2023-05-31 01:08:41,983 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-StateMachineUpdater] INFO impl.StateMachineUpdater: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-StateMachineUpdater: Took a snapshot at index 6
scm3_1   | 2023-05-31 01:08:14,410 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 1f8ec4de-a531-4a7c-9881-f1a4cf07e444, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17)3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, CreationTimestamp2023-05-31T01:06:41.922557Z[UTC]] removed.
scm3_1   | 2023-05-31 01:08:14,417 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:08:14,418 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-05-31 01:12:33,964 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1  | 2023-05-31 01:09:10,091 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 9342e1de-b2fb-494f-91d9-6f942e88ff83, Nodes: 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19)3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21)3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:50.131Z[UTC]].
recon_1  | 2023-05-31 01:09:10,092 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9342e1de-b2fb-494f-91d9-6f942e88ff83 reported by 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19)
dn1_1    | 2023-05-31 01:08:41,983 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-StateMachineUpdater] INFO impl.StateMachineUpdater: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
dn2_1    | 2023-05-31 01:09:09,646 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5d339866-97f6-4ef6-b080-a59219751d93 has been successfully formatted.
dn1_1    | 2023-05-31 01:08:41,985 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9: closes. applyIndex: 6
dn4_1    | 2023-05-31 01:08:16,915 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: shutdown 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-FollowerState
dn5_1    | 2023-05-31 01:06:45,500 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn5_1    | 2023-05-31 01:06:45,506 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:45,530 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-05-31 01:08:42,255 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9-SegmentedRaftLogWorker close()
dn4_1    | 2023-05-31 01:08:16,915 [3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-FollowerState] INFO impl.FollowerState: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-FollowerState was interrupted
dn4_1    | 2023-05-31 01:08:16,915 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-StateMachineUpdater: set stopIndex = 26
scm3_1   | 2023-05-31 01:08:14,432 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 5509b2c4-975c-42f1-91be-39d463d01a51, Nodes: 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:3da1490a-d610-40de-95c1-bdfc259ba041, CreationTimestamp2023-05-31T01:06:41.931017Z[UTC]] removed.
dn5_1    | 2023-05-31 01:06:45,544 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn5_1    | 2023-05-31 01:06:45,545 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn5_1    | 2023-05-31 01:06:45,545 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn5_1    | 2023-05-31 01:06:45,546 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-05-31 01:08:16,917 [3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-F1A4CF07E444: Taking a snapshot at:(t:12, i:26) file /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/sm/snapshot.12_26
recon_1  | 2023-05-31 01:09:10,620 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5d339866-97f6-4ef6-b080-a59219751d93 reported by 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)
recon_1  | 2023-05-31 01:09:11,445 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9342e1de-b2fb-494f-91d9-6f942e88ff83 reported by 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21)
dn1_1    | 2023-05-31 01:08:42,262 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-A4E2998A83C9: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/6a499bf5-10af-4dab-a3fd-a4e2998a83c9
dn1_1    | 2023-05-31 01:08:42,263 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=6a499bf5-10af-4dab-a3fd-a4e2998a83c9 command on datanode b20d0805-391e-447d-a646-35a5d1a5cbcf.
dn1_1    | 2023-05-31 01:08:42,263 [PipelineCommandHandlerThread-0] INFO server.RaftServer: b20d0805-391e-447d-a646-35a5d1a5cbcf: remove  FOLLOWER b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1:t9, leader=null, voted=3da1490a-d610-40de-95c1-bdfc259ba041, raftlog=Memoized:b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-SegmentedRaftLog:OPENED:c44, conf=43: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn1_1    | 2023-05-31 01:08:42,264 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: shutdown
dn1_1    | 2023-05-31 01:08:42,264 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-DC0251522AF1,id=b20d0805-391e-447d-a646-35a5d1a5cbcf
scm3_1   | 2023-05-31 01:08:15,180 [IPC Server handler 36 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-05-31 01:08:15,355 [IPC Server handler 99 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-05-31 01:08:16,787 [IPC Server handler 74 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-05-31 01:08:16,805 [IPC Server handler 75 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-05-31 01:08:16,806 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to QUASI_CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported QUASI_CLOSED replica.
scm3_1   | 2023-05-31 01:08:16,868 [IPC Server handler 76 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-05-31 01:06:47,244 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:47,245 [grpc-default-executor-1] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn5_1    | 2023-05-31 01:06:45,550 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn1_1    | 2023-05-31 01:08:42,264 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState
scm3_1   | 2023-05-31 01:08:16,934 [FixedThreadPoolWithAffinityExecutor-9-0] WARN container.IncrementalContainerReportHandler: Failed to process QUASI_CLOSED Container #1 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E is not the leader 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3_1   | 2023-05-31 01:08:17,103 [IPC Server handler 36 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-05-31 01:08:17,161 [IPC Server handler 22 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn1_1    | 2023-05-31 01:08:42,264 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-StateMachineUpdater: set stopIndex = 44
dn2_1    | 2023-05-31 01:09:09,647 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO ratis.ContainerStateMachine: group-A59219751D93: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
recon_1  | 2023-05-31 01:09:11,732 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5d339866-97f6-4ef6-b080-a59219751d93 reported by 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19)
recon_1  | 2023-05-31 01:09:11,733 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9342e1de-b2fb-494f-91d9-6f942e88ff83 reported by 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19)
dn4_1    | 2023-05-31 01:08:16,920 [3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-F1A4CF07E444: Finished taking a snapshot at:(t:12, i:26) file:/data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/sm/snapshot.12_26 took: 2 ms
scm1_1   | 2023-05-31 01:06:47,293 [grpc-default-executor-1] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:47,293 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:47,349 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn2_1    | 2023-05-31 01:09:09,647 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-05-31 01:06:45,550 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
recon_1  | 2023-05-31 01:09:11,837 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5d339866-97f6-4ef6-b080-a59219751d93 reported by 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)
scm1_1   | 2023-05-31 01:06:47,464 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-05-31 01:08:17,164 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to QUASI_CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported QUASI_CLOSED replica.
dn1_1    | 2023-05-31 01:08:42,264 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState] INFO impl.FollowerState: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-FollowerState was interrupted
dn4_1    | 2023-05-31 01:08:16,921 [3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-StateMachineUpdater] INFO impl.StateMachineUpdater: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-StateMachineUpdater: Took a snapshot at index 26
dn2_1    | 2023-05-31 01:09:09,647 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-05-31 01:06:45,550 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
recon_1  | 2023-05-31 01:09:11,838 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9342e1de-b2fb-494f-91d9-6f942e88ff83 reported by 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)
scm1_1   | 2023-05-31 01:06:47,464 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
scm3_1   | 2023-05-31 01:08:17,164 [FixedThreadPoolWithAffinityExecutor-9-0] WARN container.IncrementalContainerReportHandler: Failed to process QUASI_CLOSED Container #1001 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E is not the leader 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3_1   | 2023-05-31 01:08:17,191 [IPC Server handler 16 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn4_1    | 2023-05-31 01:08:16,921 [3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-StateMachineUpdater] INFO impl.StateMachineUpdater: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-StateMachineUpdater: snapshotIndex: updateIncreasingly 24 -> 26
dn2_1    | 2023-05-31 01:09:09,661 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:06:45,551 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
recon_1  | 2023-05-31 01:09:14,645 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5d339866-97f6-4ef6-b080-a59219751d93 reported by 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18)
scm1_1   | 2023-05-31 01:06:47,495 [grpc-default-executor-1] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-05-31 01:08:17,225 [IPC Server handler 43 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn1_1    | 2023-05-31 01:08:42,265 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-DC0251522AF1: Taking a snapshot at:(t:9, i:44) file /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/sm/snapshot.9_44
dn4_1    | 2023-05-31 01:08:16,926 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444: closes. applyIndex: 26
dn4_1    | 2023-05-31 01:08:17,071 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444-SegmentedRaftLogWorker close()
dn5_1    | 2023-05-31 01:06:45,569 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: start 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderStateImpl
recon_1  | 2023-05-31 01:09:15,149 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5d339866-97f6-4ef6-b080-a59219751d93 reported by 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19)
scm1_1   | 2023-05-31 01:06:48,620 [grpc-default-executor-1] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-05-31 01:08:17,243 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to QUASI_CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported QUASI_CLOSED replica.
dn1_1    | 2023-05-31 01:08:42,268 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-DC0251522AF1: Finished taking a snapshot at:(t:9, i:44) file:/data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/sm/snapshot.9_44 took: 4 ms
dn4_1    | 2023-05-31 01:08:17,093 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 23.
dn2_1    | 2023-05-31 01:09:09,674 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-05-31 01:09:09,675 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-05-31 01:09:09,676 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
recon_1  | 2023-05-31 01:09:15,150 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9342e1de-b2fb-494f-91d9-6f942e88ff83 reported by 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19)
dn4_1    | 2023-05-31 01:08:17,093 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 23.
dn5_1    | 2023-05-31 01:06:45,572 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-SegmentedRaftLogWorker: Rolling segment log-11_24 to index:24
scm3_1   | 2023-05-31 01:08:17,245 [FixedThreadPoolWithAffinityExecutor-9-0] WARN container.IncrementalContainerReportHandler: Failed to process QUASI_CLOSED Container #2 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E is not the leader 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
recon_1  | 2023-05-31 01:09:15,772 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5d339866-97f6-4ef6-b080-a59219751d93 reported by 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)
dn1_1    | 2023-05-31 01:08:42,269 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-StateMachineUpdater] INFO impl.StateMachineUpdater: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-StateMachineUpdater: Took a snapshot at index 44
scm1_1   | 2023-05-31 01:06:48,620 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 2023-05-31 01:09:09,681 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-05-31 01:09:09,681 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-05-31 01:06:45,577 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/current/log_inprogress_11 to /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/current/log_11-24
scm3_1   | 2023-05-31 01:08:17,882 [IPC Server handler 77 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | 2023-05-31 01:09:15,773 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 5d339866-97f6-4ef6-b080-a59219751d93, Nodes: 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18)97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:3da1490a-d610-40de-95c1-bdfc259ba041, CreationTimestamp2023-05-31T01:08:50.188Z[UTC]] moved to OPEN state
scm1_1   | 2023-05-31 01:06:48,658 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | 2023-05-31 01:08:42,269 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-StateMachineUpdater] INFO impl.StateMachineUpdater: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-StateMachineUpdater: snapshotIndex: updateIncreasingly 42 -> 44
dn2_1    | 2023-05-31 01:09:09,681 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-31 01:08:17,167 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 9.
dn4_1    | 2023-05-31 01:08:17,168 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 9.
scm3_1   | 2023-05-31 01:08:17,924 [IPC Server handler 36 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | 2023-05-31 01:09:15,773 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9342e1de-b2fb-494f-91d9-6f942e88ff83 reported by 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)
scm1_1   | 2023-05-31 01:06:48,662 [grpc-default-executor-1] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:48,662 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn2_1    | 2023-05-31 01:09:09,681 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5d339866-97f6-4ef6-b080-a59219751d93
dn5_1    | 2023-05-31 01:06:45,604 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/current/log_inprogress_25
dn4_1    | 2023-05-31 01:08:17,225 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-F1A4CF07E444: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444
scm3_1   | 2023-05-31 01:08:17,937 [IPC Server handler 22 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | 2023-05-31 01:09:16,700 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9342e1de-b2fb-494f-91d9-6f942e88ff83 reported by 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21)
scm1_1   | 2023-05-31 01:06:48,713 [grpc-default-executor-1] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:48,731 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 2023-05-31 01:09:09,683 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-05-31 01:06:45,627 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderElection1] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444: set configuration 25: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:08:17,226 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444 command on datanode 3da1490a-d610-40de-95c1-bdfc259ba041.
scm3_1   | 2023-05-31 01:08:17,974 [IPC Server handler 28 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | 2023-05-31 01:09:16,702 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 9342e1de-b2fb-494f-91d9-6f942e88ff83, Nodes: 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19)3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21)3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, CreationTimestamp2023-05-31T01:08:50.131Z[UTC]] moved to OPEN state
scm1_1   | 2023-05-31 01:06:48,737 [grpc-default-executor-1] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn1_1    | 2023-05-31 01:08:42,271 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: closes. applyIndex: 44
dn2_1    | 2023-05-31 01:09:09,683 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-05-31 01:06:46,537 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:46,604 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-DC0251522AF1 with new leaderId: 3da1490a-d610-40de-95c1-bdfc259ba041
scm3_1   | 2023-05-31 01:08:18,414 [IPC Server handler 59 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | 2023-05-31 01:09:18,222 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
scm1_1   | 2023-05-31 01:06:49,871 [grpc-default-executor-1] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | 2023-05-31 01:08:42,891 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1-SegmentedRaftLogWorker close()
dn5_1    | 2023-05-31 01:06:46,608 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-server-thread1] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1: change Leader from null to 3da1490a-d610-40de-95c1-bdfc259ba041 at term 9 for appendEntries, leader elected after 41014ms
dn2_1    | 2023-05-31 01:09:09,683 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm3_1   | 2023-05-31 01:08:18,437 [IPC Server handler 97 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-05-31 01:08:18,464 [IPC Server handler 92 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-05-31 01:06:49,871 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | 2023-05-31 01:08:42,917 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 27.
dn4_1    | 2023-05-31 01:08:17,226 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 3da1490a-d610-40de-95c1-bdfc259ba041: remove    LEADER 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51:t4, leader=3da1490a-d610-40de-95c1-bdfc259ba041, voted=3da1490a-d610-40de-95c1-bdfc259ba041, raftlog=Memoized:3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-SegmentedRaftLog:OPENED:c6, conf=5: peers:[3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn4_1    | 2023-05-31 01:08:17,226 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51: shutdown
dn2_1    | 2023-05-31 01:09:09,683 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm3_1   | 2023-05-31 01:08:18,470 [IPC Server handler 9 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | 2023-05-31 01:09:18,223 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
scm1_1   | 2023-05-31 01:06:49,895 [grpc-default-executor-1] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | 2023-05-31 01:08:42,924 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 27.
dn4_1    | 2023-05-31 01:08:17,226 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-39D463D01A51,id=3da1490a-d610-40de-95c1-bdfc259ba041
dn5_1    | 2023-05-31 01:06:46,645 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-server-thread2] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1: set configuration 43: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:06:46,645 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-server-thread2] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-SegmentedRaftLogWorker: Rolling segment log-29_42 to index:42
scm3_1   | 2023-05-31 01:08:19,613 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
recon_1  | 2023-05-31 01:09:18,223 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 160 
scm1_1   | 2023-05-31 01:06:49,899 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | 2023-05-31 01:08:42,968 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 40.
dn1_1    | 2023-05-31 01:08:42,968 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 40.
dn5_1    | 2023-05-31 01:06:46,659 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/current/log_inprogress_29 to /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/current/log_29-42
dn2_1    | 2023-05-31 01:09:09,683 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm3_1   | 2023-05-31 01:08:19,614 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 2023-05-31 01:09:18,238 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 0, SequenceNumber diff: 0, SequenceNumber Lag from OM 0.
scm1_1   | 2023-05-31 01:06:49,912 [grpc-default-executor-1] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
dn1_1    | 2023-05-31 01:08:42,978 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-DC0251522AF1: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1
dn4_1    | 2023-05-31 01:08:17,227 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: shutdown 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-LeaderStateImpl
dn2_1    | 2023-05-31 01:09:09,684 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm3_1   | 2023-05-31 01:08:24,651 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
recon_1  | 2023-05-31 01:09:18,238 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 0 records
recon_1  | 2023-05-31 01:09:18,251 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=e689613f-6a5f-45fa-9ee1-234a7250e2df. Trying to get from SCM.
dn1_1    | 2023-05-31 01:08:42,978 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1 command on datanode b20d0805-391e-447d-a646-35a5d1a5cbcf.
dn4_1    | 2023-05-31 01:08:17,229 [PipelineCommandHandlerThread-0] INFO impl.PendingRequests: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-PendingRequests: sendNotLeaderResponses
dn5_1    | 2023-05-31 01:06:46,682 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/current/log_inprogress_43
dn2_1    | 2023-05-31 01:09:09,684 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3_1   | 2023-05-31 01:08:24,651 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:08:29,733 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:08:29,734 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-05-31 01:08:42,980 [PipelineCommandHandlerThread-0] INFO server.RaftServer: b20d0805-391e-447d-a646-35a5d1a5cbcf: remove  FOLLOWER b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444:t12, leader=null, voted=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, raftlog=Memoized:b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-SegmentedRaftLog:OPENED:c26, conf=25: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn4_1    | 2023-05-31 01:08:17,234 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-StateMachineUpdater: set stopIndex = 6
dn4_1    | 2023-05-31 01:08:17,239 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-39D463D01A51: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/5509b2c4-975c-42f1-91be-39d463d01a51/sm/snapshot.4_6
recon_1  | 2023-05-31 01:09:18,258 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: e689613f-6a5f-45fa-9ee1-234a7250e2df, Nodes: 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:50.109Z[UTC]] to Recon pipeline metadata.
recon_1  | 2023-05-31 01:09:18,265 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e689613f-6a5f-45fa-9ee1-234a7250e2df, Nodes: 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:50.109Z[UTC]].
dn1_1    | 2023-05-31 01:08:42,980 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: shutdown
dn5_1    | 2023-05-31 01:06:47,542 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:48,543 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-31 01:06:49,544 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-05-31 01:09:18,265 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=e689613f-6a5f-45fa-9ee1-234a7250e2df reported by 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)
recon_1  | 2023-05-31 01:09:18,266 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e689613f-6a5f-45fa-9ee1-234a7250e2df, Nodes: 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3da1490a-d610-40de-95c1-bdfc259ba041, CreationTimestamp2023-05-31T01:08:50.109Z[UTC]] moved to OPEN state
dn1_1    | 2023-05-31 01:08:42,981 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-F1A4CF07E444,id=b20d0805-391e-447d-a646-35a5d1a5cbcf
dn5_1    | 2023-05-31 01:07:00,042 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn5_1    | 2023-05-31 01:07:35,009 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm1_1   | 2023-05-31 01:06:51,123 [grpc-default-executor-1] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-05-31 01:08:34,889 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:08:34,890 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-05-31 01:08:42,981 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState
dn4_1    | 2023-05-31 01:08:17,252 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-39D463D01A51: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/5509b2c4-975c-42f1-91be-39d463d01a51/sm/snapshot.4_6 took: 13 ms
dn4_1    | 2023-05-31 01:08:17,254 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-StateMachineUpdater] INFO impl.StateMachineUpdater: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-StateMachineUpdater: Took a snapshot at index 6
scm1_1   | 2023-05-31 01:06:51,126 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-05-31 01:08:38,620 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18) moved to HEALTHY state.
recon_1  | 2023-05-31 01:09:19,516 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=cabada01-83b4-406c-b6ca-8f00a57da29e. Trying to get from SCM.
dn1_1    | 2023-05-31 01:08:42,982 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState] INFO impl.FollowerState: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-FollowerState was interrupted
dn4_1    | 2023-05-31 01:08:17,254 [3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-StateMachineUpdater] INFO impl.StateMachineUpdater: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
scm1_1   | 2023-05-31 01:06:51,141 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:51,143 [grpc-default-executor-3] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
recon_1  | 2023-05-31 01:09:19,525 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: cabada01-83b4-406c-b6ca-8f00a57da29e, Nodes: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, CreationTimestamp2023-05-31T01:08:50.144Z[UTC]] to Recon pipeline metadata.
recon_1  | 2023-05-31 01:09:19,526 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: cabada01-83b4-406c-b6ca-8f00a57da29e, Nodes: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, CreationTimestamp2023-05-31T01:08:50.144Z[UTC]].
recon_1  | 2023-05-31 01:09:19,735 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=46ae8d39-3c2f-48ac-8229-b2777f51f5a3. Trying to get from SCM.
dn5_1    | 2023-05-31 01:08:16,160 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    | 2023-05-31 01:08:16,161 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn5_1    | 2023-05-31 01:08:16,162 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn2_1    | 2023-05-31 01:09:09,684 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-05-31 01:09:09,685 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-05-31 01:09:09,686 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-05-31 01:09:09,714 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1_1   | 2023-05-31 01:06:51,147 [grpc-default-executor-1] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:52,392 [grpc-default-executor-1] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:52,394 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 2023-05-31 01:09:09,714 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-05-31 01:09:09,715 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-05-31 01:09:09,715 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO segmented.SegmentedRaftLogWorker: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm3_1   | 2023-05-31 01:08:38,622 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn1_1    | 2023-05-31 01:08:42,986 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-F1A4CF07E444: Taking a snapshot at:(t:12, i:26) file /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/sm/snapshot.12_26
dn2_1    | 2023-05-31 01:09:09,715 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO segmented.SegmentedRaftLogWorker: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-05-31 01:09:09,716 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93: start as a follower, conf=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-31 01:09:09,716 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn2_1    | 2023-05-31 01:09:09,716 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO impl.RoleInfo: 4bc93e86-8989-4282-b7d8-050f4a19b92f: start 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-FollowerState
dn2_1    | 2023-05-31 01:09:09,721 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-A59219751D93,id=4bc93e86-8989-4282-b7d8-050f4a19b92f
scm3_1   | 2023-05-31 01:08:40,077 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-05-31 01:08:42,982 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-StateMachineUpdater: set stopIndex = 26
scm1_1   | 2023-05-31 01:06:52,447 [grpc-default-executor-1] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:52,448 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-05-31 01:06:52,452 [grpc-default-executor-1] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: decreaseNextIndex nextIndex: updateUnconditionally 83 -> 0
scm1_1   | 2023-05-31 01:06:53,631 [grpc-default-executor-3] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-05-31 01:08:17,255 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51: closes. applyIndex: 6
scm3_1   | 2023-05-31 01:08:40,077 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-05-31 01:08:16,163 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: OPEN
dn5_1    | 2023-05-31 01:08:16,163 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn4_1    | 2023-05-31 01:08:17,520 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51-SegmentedRaftLogWorker close()
dn4_1    | 2023-05-31 01:08:17,522 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-39D463D01A51: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/5509b2c4-975c-42f1-91be-39d463d01a51
dn4_1    | 2023-05-31 01:08:17,523 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=5509b2c4-975c-42f1-91be-39d463d01a51 command on datanode 3da1490a-d610-40de-95c1-bdfc259ba041.
scm3_1   | 2023-05-31 01:08:40,972 [IPC Server handler 28 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-05-31 01:08:41,135 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 46a9d8fb-17d8-4177-820b-4ef30e5fdeea, Nodes: 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:41.112Z[UTC]].
scm3_1   | 2023-05-31 01:08:41,165 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: b5c5c804-816d-41a2-83d8-297089fabfc6, Nodes: 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:41.141Z[UTC]].
dn5_1    | 2023-05-31 01:08:16,163 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn5_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
recon_1  | 2023-05-31 01:09:19,739 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 46ae8d39-3c2f-48ac-8229-b2777f51f5a3, Nodes: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21)b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17)4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:09:14.136Z[UTC]] to Recon pipeline metadata.
recon_1  | 2023-05-31 01:09:19,744 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 46ae8d39-3c2f-48ac-8229-b2777f51f5a3, Nodes: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21)b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17)4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:09:14.136Z[UTC]].
recon_1  | 2023-05-31 01:09:19,756 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=46ae8d39-3c2f-48ac-8229-b2777f51f5a3 reported by 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21)
scm3_1   | 2023-05-31 01:08:41,620 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19) moved to HEALTHY state.
dn4_1    | 2023-05-31 01:08:17,989 [grpc-default-executor-5] INFO server.GrpcServerProtocolService: 3da1490a-d610-40de-95c1-bdfc259ba041: Completed APPEND_ENTRIES, lastRequest: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd->3da1490a-d610-40de-95c1-bdfc259ba041#5-t12,previous=(t:12, i:25),leaderCommit=26,initializing? true,entries: size=1, first=(t:12, i:26), METADATAENTRY(c:25)
dn4_1    | 2023-05-31 01:08:17,989 [grpc-default-executor-5] INFO server.GrpcServerProtocolService: 3da1490a-d610-40de-95c1-bdfc259ba041: Completed APPEND_ENTRIES, lastReply: null
dn4_1    | 2023-05-31 01:08:17,991 [grpc-default-executor-5] INFO server.GrpcServerProtocolService: 3da1490a-d610-40de-95c1-bdfc259ba041: Completed APPEND_ENTRIES, lastRequest: null
dn4_1    | 2023-05-31 01:08:22,309 [grpc-default-executor-5] WARN server.GrpcServerProtocolService: 3da1490a-d610-40de-95c1-bdfc259ba041: Failed requestVote b20d0805-391e-447d-a646-35a5d1a5cbcf->3da1490a-d610-40de-95c1-bdfc259ba041#0
scm1_1   | 2023-05-31 01:06:56,944 [grpc-default-executor-1] WARN server.GrpcLogAppender: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 81
recon_1  | 2023-05-31 01:09:20,221 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=46ae8d39-3c2f-48ac-8229-b2777f51f5a3 reported by b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17)
dn1_1    | 2023-05-31 01:08:42,989 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-F1A4CF07E444: Finished taking a snapshot at:(t:12, i:26) file:/data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/sm/snapshot.12_26 took: 3 ms
scm1_1   | 2023-05-31 01:06:56,953 [grpc-default-executor-1] INFO leader.FollowerInfo: 7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E->d2fda184-2469-4fa2-af65-75268cb832ba: setNextIndex nextIndex: updateUnconditionally 83 -> 81
scm1_1   | 2023-05-31 01:07:08,030 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
scm1_1   | 2023-05-31 01:07:24,453 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Finalization started.
scm3_1   | 2023-05-31 01:08:41,620 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
recon_1  | 2023-05-31 01:09:20,339 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=46ae8d39-3c2f-48ac-8229-b2777f51f5a3 reported by 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18)
recon_1  | 2023-05-31 01:09:24,937 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=46ae8d39-3c2f-48ac-8229-b2777f51f5a3 reported by 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21)
scm1_1   | 2023-05-31 01:07:24,572 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Stopping RatisPipelineUtilsThread.
scm1_1   | 2023-05-31 01:07:24,572 [RatisPipelineUtilsThread - 0] WARN pipeline.BackgroundPipelineCreator: RatisPipelineUtilsThread is interrupted.
scm1_1   | 2023-05-31 01:07:24,587 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: SCM Finalization has crossed checkpoint FINALIZATION_STARTED
scm3_1   | 2023-05-31 01:08:42,910 [IPC Server handler 36 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn4_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 3da1490a-d610-40de-95c1-bdfc259ba041: group-F1A4CF07E444 not found.
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn5_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
dn1_1    | 2023-05-31 01:08:42,990 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-StateMachineUpdater] INFO impl.StateMachineUpdater: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-StateMachineUpdater: Took a snapshot at index 26
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
recon_1  | 2023-05-31 01:09:25,562 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=46ae8d39-3c2f-48ac-8229-b2777f51f5a3 reported by b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17)
dn2_1    | 2023-05-31 01:09:09,722 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3_1   | 2023-05-31 01:08:42,940 [IPC Server handler 22 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
dn1_1    | 2023-05-31 01:08:42,990 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-StateMachineUpdater] INFO impl.StateMachineUpdater: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-StateMachineUpdater: snapshotIndex: updateIncreasingly 24 -> 26
dn1_1    | 2023-05-31 01:08:42,991 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: closes. applyIndex: 26
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
dn2_1    | 2023-05-31 01:09:09,722 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn1_1    | 2023-05-31 01:08:43,507 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444-SegmentedRaftLogWorker close()
scm1_1   | 2023-05-31 01:07:24,621 [IPC Server handler 24 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: ccaf8839-8737-4f75-8b75-b011fdd1ea12, Nodes: 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:4bc93e86-8989-4282-b7d8-050f4a19b92f, CreationTimestamp2023-05-31T01:06:14.149135Z[UTC]] moved to CLOSED state
recon_1  | 2023-05-31 01:09:25,562 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 46ae8d39-3c2f-48ac-8229-b2777f51f5a3, Nodes: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21)b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17)4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:b20d0805-391e-447d-a646-35a5d1a5cbcf, CreationTimestamp2023-05-31T01:09:14.136Z[UTC]] moved to OPEN state
recon_1  | 2023-05-31 01:09:44,862 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=ec1b01cd-6e71-4daf-8419-6155613624a8. Trying to get from SCM.
dn5_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn1_1    | 2023-05-31 01:08:43,525 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 23.
recon_1  | 2023-05-31 01:09:44,884 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: ec1b01cd-6e71-4daf-8419-6155613624a8, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:b20d0805-391e-447d-a646-35a5d1a5cbcf, CreationTimestamp2023-05-31T01:09:14.112Z[UTC]] to Recon pipeline metadata.
scm1_1   | 2023-05-31 01:07:24,648 [IPC Server handler 24 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6a499bf5-10af-4dab-a3fd-a4e2998a83c9, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:b20d0805-391e-447d-a646-35a5d1a5cbcf, CreationTimestamp2023-05-31T01:06:14.148590Z[UTC]] moved to CLOSED state
dn4_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:733)
dn1_1    | 2023-05-31 01:08:43,525 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 23.
recon_1  | 2023-05-31 01:09:44,885 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: ec1b01cd-6e71-4daf-8419-6155613624a8, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:b20d0805-391e-447d-a646-35a5d1a5cbcf, CreationTimestamp2023-05-31T01:09:14.112Z[UTC]].
scm1_1   | 2023-05-31 01:07:24,687 [IPC Server handler 24 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 9e886fff-1860-4674-bb59-7a8b4be06a04, Nodes: 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:97734618-7966-42ac-b50e-e49ead6ce4d2, CreationTimestamp2023-05-31T01:06:14.148725Z[UTC]] moved to CLOSED state
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
scm3_1   | 2023-05-31 01:08:42,965 [IPC Server handler 28 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-05-31 01:08:42,978 [IPC Server handler 30 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
recon_1  | 2023-05-31 01:09:44,885 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=ec1b01cd-6e71-4daf-8419-6155613624a8 reported by b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17)
scm1_1   | 2023-05-31 01:07:24,712 [IPC Server handler 24 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 437d8269-2a0d-4058-8be6-ed0a589991d2, Nodes: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, CreationTimestamp2023-05-31T01:06:14.148216Z[UTC]] moved to CLOSED state
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
dn2_1    | 2023-05-31 01:09:09,722 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm3_1   | 2023-05-31 01:08:43,518 [IPC Server handler 88 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:662)
dn1_1    | 2023-05-31 01:08:43,538 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 9.
recon_1  | 2023-05-31 01:09:44,886 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: ec1b01cd-6e71-4daf-8419-6155613624a8, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:b20d0805-391e-447d-a646-35a5d1a5cbcf, CreationTimestamp2023-05-31T01:09:14.112Z[UTC]] moved to OPEN state
scm1_1   | 2023-05-31 01:07:24,787 [IPC Server handler 24 on default port 9860] INFO pipeline.PipelineManagerImpl: Container #1 closed for pipeline=PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
dn2_1    | 2023-05-31 01:09:09,722 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm3_1   | 2023-05-31 01:08:43,544 [IPC Server handler 55 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-05-31 01:10:05,683 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1  | 2023-05-31 01:10:05,683 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
recon_1  | 2023-05-31 01:10:18,241 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
scm3_1   | 2023-05-31 01:08:43,554 [IPC Server handler 5 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 2023-05-31 01:08:16,164 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn2_1    | 2023-05-31 01:09:09,725 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=5d339866-97f6-4ef6-b080-a59219751d93
dn2_1    | 2023-05-31 01:09:09,735 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-05-31 01:09:09,735 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:08:43,562 [IPC Server handler 73 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn2_1    | 2023-05-31 01:09:11,785 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=5d339866-97f6-4ef6-b080-a59219751d93.
dn2_1    | 2023-05-31 01:09:14,625 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-FollowerState] INFO impl.FollowerState: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5015917385ns, electionTimeout:5012ms
scm3_1   | 2023-05-31 01:08:45,259 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:08:45,260 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-05-31 01:08:43,538 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 9.
dn1_1    | 2023-05-31 01:08:43,574 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-F1A4CF07E444: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
scm3_1   | 2023-05-31 01:08:47,620 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) moved to HEALTHY state.
dn1_1    | 2023-05-31 01:08:43,575 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444 command on datanode b20d0805-391e-447d-a646-35a5d1a5cbcf.
dn1_1    | 2023-05-31 01:09:11,951 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm3_1   | 2023-05-31 01:08:47,621 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
recon_1  | 2023-05-31 01:10:18,242 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
dn1_1    | 2023-05-31 01:09:11,951 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn5_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
dn2_1    | 2023-05-31 01:09:14,625 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-FollowerState] INFO impl.RoleInfo: 4bc93e86-8989-4282-b7d8-050f4a19b92f: shutdown 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-FollowerState
dn2_1    | 2023-05-31 01:09:14,625 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-FollowerState] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn2_1    | 2023-05-31 01:09:14,626 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm1_1   | 2023-05-31 01:07:24,788 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #1, current state: CLOSING
dn1_1    | 2023-05-31 01:09:11,952 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn5_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
dn5_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
scm3_1   | 2023-05-31 01:08:50,128 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e689613f-6a5f-45fa-9ee1-234a7250e2df, Nodes: 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:50.109Z[UTC]].
scm3_1   | 2023-05-31 01:08:50,148 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 9342e1de-b2fb-494f-91d9-6f942e88ff83, Nodes: 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19)3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21)3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:50.131Z[UTC]].
dn1_1    | 2023-05-31 01:09:11,952 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
dn2_1    | 2023-05-31 01:09:14,626 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-FollowerState] INFO impl.RoleInfo: 4bc93e86-8989-4282-b7d8-050f4a19b92f: start 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2
dn2_1    | 2023-05-31 01:09:14,632 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2] INFO impl.LeaderElection: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-31 01:09:14,632 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2] INFO impl.LeaderElection: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
scm1_1   | 2023-05-31 01:07:24,813 [IPC Server handler 24 on default port 9860] INFO pipeline.PipelineManagerImpl: Container #1002 closed for pipeline=PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1
scm1_1   | 2023-05-31 01:07:24,833 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #1002, current state: CLOSING
scm3_1   | 2023-05-31 01:08:50,184 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: cabada01-83b4-406c-b6ca-8f00a57da29e, Nodes: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:50.144Z[UTC]].
dn1_1    | 2023-05-31 01:09:11,955 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
dn2_1    | 2023-05-31 01:09:14,634 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2] INFO impl.LeaderElection: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-31 01:09:14,634 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2] INFO impl.LeaderElection: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2 ELECTION round 0: result PASSED (term=1)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm1_1   | 2023-05-31 01:07:24,841 [IPC Server handler 24 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: a4fe2088-9059-47e2-9c65-dc0251522af1, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17)3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:3da1490a-d610-40de-95c1-bdfc259ba041, CreationTimestamp2023-05-31T01:06:14.148990Z[UTC]] moved to CLOSED state
recon_1  | 2023-05-31 01:10:18,242 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 160 
scm3_1   | 2023-05-31 01:08:50,207 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 5d339866-97f6-4ef6-b080-a59219751d93, Nodes: 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18)97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:50.188Z[UTC]].
dn1_1    | 2023-05-31 01:09:11,955 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
dn5_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:733)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
scm1_1   | 2023-05-31 01:07:24,864 [IPC Server handler 24 on default port 9860] INFO pipeline.PipelineManagerImpl: Container #2 closed for pipeline=PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444
scm1_1   | 2023-05-31 01:07:24,867 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #2, current state: CLOSING
recon_1  | 2023-05-31 01:10:18,281 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 1, SequenceNumber diff: 2, SequenceNumber Lag from OM 0.
recon_1  | 2023-05-31 01:10:18,281 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 2 records
recon_1  | 2023-05-31 01:10:18,296 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
recon_1  | 2023-05-31 01:10:18,296 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
recon_1  | 2023-05-31 01:10:18,808 [pool-49-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
dn1_1    | 2023-05-31 01:09:11,957 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
dn2_1    | 2023-05-31 01:09:14,635 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2] INFO impl.RoleInfo: 4bc93e86-8989-4282-b7d8-050f4a19b92f: shutdown 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2
scm3_1   | 2023-05-31 01:08:50,433 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:662)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-05-31 01:10:18,809 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 0 OM DB update event(s).
recon_1  | 2023-05-31 01:10:18,810 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
dn2_1    | 2023-05-31 01:09:14,635 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn2_1    | 2023-05-31 01:09:14,635 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-4EF30E5FDEEA with new leaderId: 4bc93e86-8989-4282-b7d8-050f4a19b92f
scm1_1   | 2023-05-31 01:07:24,880 [IPC Server handler 24 on default port 9860] INFO pipeline.PipelineManagerImpl: Container #1001 closed for pipeline=PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444
scm1_1   | 2023-05-31 01:07:24,881 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #1001, current state: CLOSING
recon_1  | 2023-05-31 01:10:49,539 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: New container #2001 got from ha_dn4_1.ha_net.
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 2023-05-31 01:09:11,957 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HADOOP_PRC_PORTS_IN_DATANODEDETAILS.
dn2_1    | 2023-05-31 01:09:14,638 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA: change Leader from null to 4bc93e86-8989-4282-b7d8-050f4a19b92f at term 1 for becomeLeader, leader elected after 5134ms
dn2_1    | 2023-05-31 01:09:14,638 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1_1   | 2023-05-31 01:07:24,899 [IPC Server handler 24 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 1f8ec4de-a531-4a7c-9881-f1a4cf07e444, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17)3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, CreationTimestamp2023-05-31T01:06:14.144553Z[UTC]] moved to CLOSED state
dn5_1    | 2023-05-31 01:08:16,181 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: remove    LEADER 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2:t4, leader=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, voted=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, raftlog=Memoized:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-SegmentedRaftLog:OPENED:c6, conf=5: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
recon_1  | 2023-05-31 01:10:49,613 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: Successfully added container #2001 to Recon.
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 2023-05-31 01:09:14,638 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-05-31 01:09:14,639 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
scm1_1   | 2023-05-31 01:07:24,923 [IPC Server handler 24 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 5509b2c4-975c-42f1-91be-39d463d01a51, Nodes: 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:3da1490a-d610-40de-95c1-bdfc259ba041, CreationTimestamp2023-05-31T01:06:14.148442Z[UTC]] moved to CLOSED state
scm1_1   | 2023-05-31 01:07:24,923 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer:   Existing pipelines and containers will be closed during Upgrade.
recon_1  | 2023-05-31 01:11:05,721 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1  | 2023-05-31 01:11:05,722 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 38
dn2_1    | 2023-05-31 01:09:14,640 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
scm3_1   | 2023-05-31 01:08:50,434 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:08:50,621 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21) moved to HEALTHY state.
scm3_1   | 2023-05-31 01:08:50,622 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-05-31 01:08:55,444 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 2023-05-31 01:09:14,640 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1_1   |   New pipelines creation will remain frozen until Upgrade is finalized.
scm3_1   | 2023-05-31 01:08:55,444 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 2023-05-31 01:11:18,838 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
dn5_1    | 2023-05-31 01:08:16,184 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2: shutdown
dn4_1    | 2023-05-31 01:08:22,892 [grpc-default-executor-5] WARN server.GrpcServerProtocolService: 3da1490a-d610-40de-95c1-bdfc259ba041: Failed requestVote b20d0805-391e-447d-a646-35a5d1a5cbcf->3da1490a-d610-40de-95c1-bdfc259ba041#0
dn1_1    | 2023-05-31 01:09:11,958 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature HADOOP_PRC_PORTS_IN_DATANODEDETAILS has been finalized.
dn2_1    | 2023-05-31 01:09:14,640 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm3_1   | 2023-05-31 01:09:00,548 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
recon_1  | 2023-05-31 01:11:18,839 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1  | 2023-05-31 01:11:18,839 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 162 
dn4_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 3da1490a-d610-40de-95c1-bdfc259ba041: group-DC0251522AF1 not found.
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
scm1_1   | 2023-05-31 01:07:24,944 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
scm3_1   | 2023-05-31 01:09:00,548 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 2023-05-31 01:11:18,871 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 10, SequenceNumber diff: 31, SequenceNumber Lag from OM 0.
recon_1  | 2023-05-31 01:11:18,872 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 31 records
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
scm1_1   | 2023-05-31 01:07:24,963 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
dn2_1    | 2023-05-31 01:09:14,641 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
dn5_1    | 2023-05-31 01:08:16,185 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-ED0A589991D2,id=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn1_1    | 2023-05-31 01:09:11,958 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn1_1    | 2023-05-31 01:09:11,958 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
scm1_1   | 2023-05-31 01:07:24,975 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
dn1_1    | 2023-05-31 01:09:11,959 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn1_1    | 2023-05-31 01:09:11,959 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm1_1   | 2023-05-31 01:07:24,978 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
scm1_1   | 2023-05-31 01:07:25,001 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HADOOP_PRC_PORTS_IN_DATANODEDETAILS.
dn5_1    | 2023-05-31 01:08:16,185 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: shutdown 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-LeaderStateImpl
dn5_1    | 2023-05-31 01:08:16,185 [PipelineCommandHandlerThread-0] INFO impl.PendingRequests: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-PendingRequests: sendNotLeaderResponses
dn1_1    | 2023-05-31 01:09:11,959 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn1_1    | 2023-05-31 01:09:11,959 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm3_1   | 2023-05-31 01:09:05,656 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-31 01:07:25,005 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature HADOOP_PRC_PORTS_IN_DATANODEDETAILS has been finalized.
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
dn5_1    | 2023-05-31 01:08:16,189 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-ED0A589991D2: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/437d8269-2a0d-4058-8be6-ed0a589991d2/sm/snapshot.4_6
dn1_1    | 2023-05-31 01:09:11,959 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn1_1    | 2023-05-31 01:09:11,959 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm1_1   | 2023-05-31 01:07:25,005 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
scm1_1   | 2023-05-31 01:07:25,018 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm1_1   | 2023-05-31 01:07:25,019 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) moved to HEALTHY READONLY state.
recon_1  | 2023-05-31 01:11:18,911 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
dn2_1    | 2023-05-31 01:09:14,641 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn5_1    | 2023-05-31 01:08:16,190 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-StateMachineUpdater: set stopIndex = 6
dn1_1    | 2023-05-31 01:09:11,959 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn1_1    | 2023-05-31 01:09:20,174 [grpc-default-executor-8] INFO server.RaftServer: b20d0805-391e-447d-a646-35a5d1a5cbcf: addNew group-B2777F51F5A3:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] returns group-B2777F51F5A3:java.util.concurrent.CompletableFuture@212f3011[Not completed]
scm1_1   | 2023-05-31 01:07:25,019 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1 in state CLOSED which uses HEALTHY_READONLY datanode 3da1490a-d610-40de-95c1-bdfc259ba041. This will send close commands for its containers.
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
recon_1  | 2023-05-31 01:11:18,911 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
dn2_1    | 2023-05-31 01:09:14,641 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2] INFO impl.RoleInfo: 4bc93e86-8989-4282-b7d8-050f4a19b92f: start 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderStateImpl
dn5_1    | 2023-05-31 01:08:16,191 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-ED0A589991D2: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/437d8269-2a0d-4058-8be6-ed0a589991d2/sm/snapshot.4_6 took: 2 ms
dn5_1    | 2023-05-31 01:08:16,192 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-StateMachineUpdater] INFO impl.StateMachineUpdater: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-StateMachineUpdater: Took a snapshot at index 6
dn5_1    | 2023-05-31 01:08:16,193 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-StateMachineUpdater] INFO impl.StateMachineUpdater: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
scm1_1   | 2023-05-31 01:07:25,019 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444 in state CLOSED which uses HEALTHY_READONLY datanode 3da1490a-d610-40de-95c1-bdfc259ba041. This will send close commands for its containers.
scm1_1   | 2023-05-31 01:07:25,019 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=5509b2c4-975c-42f1-91be-39d463d01a51 in state CLOSED which uses HEALTHY_READONLY datanode 3da1490a-d610-40de-95c1-bdfc259ba041. This will send close commands for its containers.
scm1_1   | 2023-05-31 01:07:25,019 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17) moved to HEALTHY READONLY state.
recon_1  | 2023-05-31 01:11:19,071 [pool-49-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
dn5_1    | 2023-05-31 01:08:16,196 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2: closes. applyIndex: 6
scm3_1   | 2023-05-31 01:09:05,656 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:09:09,526 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 46a9d8fb-17d8-4177-820b-4ef30e5fdeea, Nodes: 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:4bc93e86-8989-4282-b7d8-050f4a19b92f, CreationTimestamp2023-05-31T01:08:41.112Z[UTC]] moved to OPEN state
scm1_1   | 2023-05-31 01:07:25,020 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=6a499bf5-10af-4dab-a3fd-a4e2998a83c9 in state CLOSED which uses HEALTHY_READONLY datanode b20d0805-391e-447d-a646-35a5d1a5cbcf. This will send close commands for its containers.
dn2_1    | 2023-05-31 01:09:14,643 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-SegmentedRaftLogWorker: Starting segment from index:0
dn2_1    | 2023-05-31 01:09:14,650 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-LeaderElection2] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA: set configuration 0: peers:[4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 2023-05-31 01:11:19,079 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 3 OM DB update event(s).
recon_1  | 2023-05-31 01:11:19,114 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
dn5_1    | 2023-05-31 01:08:16,360 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: Completed APPEND_ENTRIES, lastRequest: 3da1490a-d610-40de-95c1-bdfc259ba041->3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#3-t9,previous=(t:9, i:43),leaderCommit=43,initializing? true,entries: size=1, first=(t:9, i:44), METADATAENTRY(c:43)
scm3_1   | 2023-05-31 01:09:09,917 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: b5c5c804-816d-41a2-83d8-297089fabfc6, Nodes: 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:97734618-7966-42ac-b50e-e49ead6ce4d2, CreationTimestamp2023-05-31T01:08:41.141Z[UTC]] moved to OPEN state
scm3_1   | 2023-05-31 01:09:10,728 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:09:10,729 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:09:14,128 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: ec1b01cd-6e71-4daf-8419-6155613624a8, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:09:14.112Z[UTC]].
scm3_1   | 2023-05-31 01:09:14,158 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 46ae8d39-3c2f-48ac-8229-b2777f51f5a3, Nodes: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21)b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17)4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:09:14.136Z[UTC]].
scm3_1   | 2023-05-31 01:09:14,622 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17) moved to HEALTHY state.
dn5_1    | 2023-05-31 01:08:16,360 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: Completed APPEND_ENTRIES, lastReply: null
dn2_1    | 2023-05-31 01:09:14,651 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-4EF30E5FDEEA-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/46a9d8fb-17d8-4177-820b-4ef30e5fdeea/current/log_inprogress_0
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
recon_1  | 2023-05-31 01:11:20,406 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
scm3_1   | 2023-05-31 01:09:14,623 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-05-31 01:09:15,777 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 5d339866-97f6-4ef6-b080-a59219751d93, Nodes: 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18)97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:3da1490a-d610-40de-95c1-bdfc259ba041, CreationTimestamp2023-05-31T01:08:50.188Z[UTC]] moved to OPEN state
scm1_1   | 2023-05-31 01:07:25,020 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1 in state CLOSED which uses HEALTHY_READONLY datanode b20d0805-391e-447d-a646-35a5d1a5cbcf. This will send close commands for its containers.
dn5_1    | 2023-05-31 01:08:16,362 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: Completed APPEND_ENTRIES, lastRequest: null
dn2_1    | 2023-05-31 01:09:14,780 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-FollowerState] INFO impl.FollowerState: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5063770117ns, electionTimeout:5045ms
dn2_1    | 2023-05-31 01:09:14,781 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-FollowerState] INFO impl.RoleInfo: 4bc93e86-8989-4282-b7d8-050f4a19b92f: shutdown 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-FollowerState
dn2_1    | 2023-05-31 01:09:14,781 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-FollowerState] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
recon_1  | 2023-05-31 01:11:20,430 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
dn1_1    | 2023-05-31 01:09:20,179 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf: new RaftServerImpl for group-B2777F51F5A3:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
scm1_1   | 2023-05-31 01:07:25,020 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444 in state CLOSED which uses HEALTHY_READONLY datanode b20d0805-391e-447d-a646-35a5d1a5cbcf. This will send close commands for its containers.
dn5_1    | 2023-05-31 01:08:16,400 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: Completed APPEND_ENTRIES, lastReply: serverReply {
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 2023-05-31 01:09:14,781 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm3_1   | 2023-05-31 01:09:15,857 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-05-31 01:09:20,179 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm1_1   | 2023-05-31 01:07:25,020 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18) moved to HEALTHY READONLY state.
scm1_1   | 2023-05-31 01:07:25,020 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=ccaf8839-8737-4f75-8b75-b011fdd1ea12 in state CLOSED which uses HEALTHY_READONLY datanode 4bc93e86-8989-4282-b7d8-050f4a19b92f. This will send close commands for its containers.
scm1_1   | 2023-05-31 01:07:25,020 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21) moved to HEALTHY READONLY state.
scm1_1   | 2023-05-31 01:07:25,020 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=437d8269-2a0d-4058-8be6-ed0a589991d2 in state CLOSED which uses HEALTHY_READONLY datanode 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd. This will send close commands for its containers.
scm1_1   | 2023-05-31 01:07:25,021 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1 in state CLOSED which uses HEALTHY_READONLY datanode 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd. This will send close commands for its containers.
dn1_1    | 2023-05-31 01:09:20,179 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
recon_1  | 2023-05-31 01:11:20,442 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
dn2_1    | 2023-05-31 01:09:14,781 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-FollowerState] INFO impl.RoleInfo: 4bc93e86-8989-4282-b7d8-050f4a19b92f: start 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-LeaderElection3
dn5_1    |   requestorId: "3da1490a-d610-40de-95c1-bdfc259ba041"
dn5_1    |   replyId: "3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd"
scm1_1   | 2023-05-31 01:07:25,022 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444 in state CLOSED which uses HEALTHY_READONLY datanode 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd. This will send close commands for its containers.
scm1_1   | 2023-05-31 01:07:25,022 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19) moved to HEALTHY READONLY state.
scm1_1   | 2023-05-31 01:07:25,022 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=9e886fff-1860-4674-bb59-7a8b4be06a04 in state CLOSED which uses HEALTHY_READONLY datanode 97734618-7966-42ac-b50e-e49ead6ce4d2. This will send close commands for its containers.
recon_1  | 2023-05-31 01:11:20,464 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1002 to CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
dn2_1    | 2023-05-31 01:09:14,784 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-LeaderElection3] INFO impl.LeaderElection: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-31 01:09:15,857 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:09:16,700 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 9342e1de-b2fb-494f-91d9-6f942e88ff83, Nodes: 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19)3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21)3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, CreationTimestamp2023-05-31T01:08:50.131Z[UTC]] moved to OPEN state
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-05-31 01:08:27,571 [grpc-default-executor-5] WARN server.GrpcServerProtocolService: 3da1490a-d610-40de-95c1-bdfc259ba041: Failed requestVote b20d0805-391e-447d-a646-35a5d1a5cbcf->3da1490a-d610-40de-95c1-bdfc259ba041#0
dn4_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 3da1490a-d610-40de-95c1-bdfc259ba041: group-F1A4CF07E444 not found.
recon_1  | 2023-05-31 01:12:05,722 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
dn2_1    | 2023-05-31 01:09:14,798 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-LeaderElection3-1] INFO server.GrpcServerProtocolClient: Build channel for 97734618-7966-42ac-b50e-e49ead6ce4d2
dn5_1    |   raftGroupId {
dn5_1    |     id: "\244\376 \210\220YG\342\234e\334\002QR*\361"
dn5_1    |   }
dn5_1    |   callId: 42
recon_1  | 2023-05-31 01:12:05,723 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
dn2_1    | 2023-05-31 01:09:14,802 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
dn2_1    | 2023-05-31 01:09:14,804 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 2023-05-31 01:12:06,244 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 1 milliseconds to process 0 existing database records.
dn2_1    | 2023-05-31 01:09:14,805 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-LeaderElection3-2] INFO server.GrpcServerProtocolClient: Build channel for 3da1490a-d610-40de-95c1-bdfc259ba041
dn2_1    | 2023-05-31 01:09:14,858 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-LeaderElection3] INFO impl.LeaderElection: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-LeaderElection3: PRE_VOTE REJECTED received 1 response(s) and 0 exception(s):
dn2_1    | 2023-05-31 01:09:14,859 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-LeaderElection3] INFO impl.LeaderElection:   Response 0: 4bc93e86-8989-4282-b7d8-050f4a19b92f<-3da1490a-d610-40de-95c1-bdfc259ba041#0:FAIL-t0
dn2_1    | 2023-05-31 01:09:14,859 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-LeaderElection3] INFO impl.LeaderElection: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-LeaderElection3 PRE_VOTE round 0: result REJECTED
recon_1  | 2023-05-31 01:12:06,255 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 10 milliseconds for processing 5 containers.
recon_1  | 2023-05-31 01:12:06,273 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 15 pipelines in house.
recon_1  | 2023-05-31 01:12:06,275 [PipelineSyncTask] INFO scm.ReconPipelineManager: Removing invalid pipeline PipelineID=6a499bf5-10af-4dab-a3fd-a4e2998a83c9 from Recon.
recon_1  | 2023-05-31 01:12:06,275 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6a499bf5-10af-4dab-a3fd-a4e2998a83c9, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:b20d0805-391e-447d-a646-35a5d1a5cbcf, CreationTimestamp2023-05-31T01:06:14.148Z[UTC]] moved to CLOSED state
recon_1  | 2023-05-31 01:12:06,281 [PipelineSyncTask] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 6a499bf5-10af-4dab-a3fd-a4e2998a83c9, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:b20d0805-391e-447d-a646-35a5d1a5cbcf, CreationTimestamp2023-05-31T01:06:14.148Z[UTC]] removed.
recon_1  | 2023-05-31 01:12:06,283 [PipelineSyncTask] INFO scm.ReconPipelineManager: Removing invalid pipeline PipelineID=437d8269-2a0d-4058-8be6-ed0a589991d2 from Recon.
recon_1  | 2023-05-31 01:12:06,285 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 437d8269-2a0d-4058-8be6-ed0a589991d2, Nodes: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, CreationTimestamp2023-05-31T01:06:14.148Z[UTC]] moved to CLOSED state
recon_1  | 2023-05-31 01:12:06,285 [PipelineSyncTask] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 437d8269-2a0d-4058-8be6-ed0a589991d2, Nodes: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, CreationTimestamp2023-05-31T01:06:14.148Z[UTC]] removed.
dn2_1    | 2023-05-31 01:09:14,860 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-LeaderElection3] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
dn2_1    | 2023-05-31 01:09:14,860 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-LeaderElection3] INFO impl.RoleInfo: 4bc93e86-8989-4282-b7d8-050f4a19b92f: shutdown 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-LeaderElection3
dn2_1    | 2023-05-31 01:09:14,861 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-LeaderElection3] INFO impl.RoleInfo: 4bc93e86-8989-4282-b7d8-050f4a19b92f: start 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-FollowerState
dn2_1    | 2023-05-31 01:09:15,911 [grpc-default-executor-0] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93: receive requestVote(PRE_VOTE, 3da1490a-d610-40de-95c1-bdfc259ba041, group-A59219751D93, 0, (t:0, i:0))
dn2_1    | 2023-05-31 01:09:15,912 [grpc-default-executor-1] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93: receive requestVote(ELECTION, 3da1490a-d610-40de-95c1-bdfc259ba041, group-A59219751D93, 1, (t:0, i:0))
dn2_1    | 2023-05-31 01:09:15,916 [grpc-default-executor-0] INFO impl.VoteContext: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-FOLLOWER: accept PRE_VOTE from 3da1490a-d610-40de-95c1-bdfc259ba041: our priority 0 <= candidate's priority 1
recon_1  | 2023-05-31 01:12:06,285 [PipelineSyncTask] INFO scm.ReconPipelineManager: Removing invalid pipeline PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1 from Recon.
recon_1  | 2023-05-31 01:12:06,287 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: a4fe2088-9059-47e2-9c65-dc0251522af1, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17)3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:3da1490a-d610-40de-95c1-bdfc259ba041, CreationTimestamp2023-05-31T01:06:14.148Z[UTC]] moved to CLOSED state
recon_1  | 2023-05-31 01:12:06,287 [PipelineSyncTask] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: a4fe2088-9059-47e2-9c65-dc0251522af1, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17)3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:3da1490a-d610-40de-95c1-bdfc259ba041, CreationTimestamp2023-05-31T01:06:14.148Z[UTC]] removed.
recon_1  | 2023-05-31 01:12:06,288 [PipelineSyncTask] INFO scm.ReconPipelineManager: Removing invalid pipeline PipelineID=5509b2c4-975c-42f1-91be-39d463d01a51 from Recon.
recon_1  | 2023-05-31 01:12:06,289 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 5509b2c4-975c-42f1-91be-39d463d01a51, Nodes: 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:3da1490a-d610-40de-95c1-bdfc259ba041, CreationTimestamp2023-05-31T01:06:14.148Z[UTC]] moved to CLOSED state
recon_1  | 2023-05-31 01:12:06,290 [PipelineSyncTask] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 5509b2c4-975c-42f1-91be-39d463d01a51, Nodes: 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:3da1490a-d610-40de-95c1-bdfc259ba041, CreationTimestamp2023-05-31T01:06:14.148Z[UTC]] removed.
dn2_1    | 2023-05-31 01:09:15,920 [grpc-default-executor-0] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93 replies to PRE_VOTE vote request: 3da1490a-d610-40de-95c1-bdfc259ba041<-4bc93e86-8989-4282-b7d8-050f4a19b92f#0:OK-t0. Peer's state: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93:t0, leader=null, voted=, raftlog=Memoized:4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-31 01:09:15,920 [grpc-default-executor-1] INFO impl.VoteContext: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-FOLLOWER: accept ELECTION from 3da1490a-d610-40de-95c1-bdfc259ba041: our priority 0 <= candidate's priority 1
dn2_1    | 2023-05-31 01:09:15,937 [grpc-default-executor-1] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:3da1490a-d610-40de-95c1-bdfc259ba041
dn2_1    | 2023-05-31 01:09:15,937 [grpc-default-executor-1] INFO impl.RoleInfo: 4bc93e86-8989-4282-b7d8-050f4a19b92f: shutdown 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-FollowerState
dn2_1    | 2023-05-31 01:09:15,937 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-FollowerState] INFO impl.FollowerState: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-FollowerState was interrupted
dn2_1    | 2023-05-31 01:09:15,937 [grpc-default-executor-1] INFO impl.RoleInfo: 4bc93e86-8989-4282-b7d8-050f4a19b92f: start 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-FollowerState
recon_1  | 2023-05-31 01:12:06,291 [PipelineSyncTask] INFO scm.ReconPipelineManager: Removing invalid pipeline PipelineID=ccaf8839-8737-4f75-8b75-b011fdd1ea12 from Recon.
recon_1  | 2023-05-31 01:12:06,296 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: ccaf8839-8737-4f75-8b75-b011fdd1ea12, Nodes: 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:4bc93e86-8989-4282-b7d8-050f4a19b92f, CreationTimestamp2023-05-31T01:06:14.149Z[UTC]] moved to CLOSED state
recon_1  | 2023-05-31 01:12:06,297 [PipelineSyncTask] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: ccaf8839-8737-4f75-8b75-b011fdd1ea12, Nodes: 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:4bc93e86-8989-4282-b7d8-050f4a19b92f, CreationTimestamp2023-05-31T01:06:14.149Z[UTC]] removed.
recon_1  | 2023-05-31 01:12:06,298 [PipelineSyncTask] INFO scm.ReconPipelineManager: Removing invalid pipeline PipelineID=9e886fff-1860-4674-bb59-7a8b4be06a04 from Recon.
recon_1  | 2023-05-31 01:12:06,301 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 9e886fff-1860-4674-bb59-7a8b4be06a04, Nodes: 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:97734618-7966-42ac-b50e-e49ead6ce4d2, CreationTimestamp2023-05-31T01:06:14.148Z[UTC]] moved to CLOSED state
recon_1  | 2023-05-31 01:12:06,301 [PipelineSyncTask] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 9e886fff-1860-4674-bb59-7a8b4be06a04, Nodes: 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:97734618-7966-42ac-b50e-e49ead6ce4d2, CreationTimestamp2023-05-31T01:06:14.148Z[UTC]] removed.
dn2_1    | 2023-05-31 01:09:15,939 [grpc-default-executor-1] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93 replies to ELECTION vote request: 3da1490a-d610-40de-95c1-bdfc259ba041<-4bc93e86-8989-4282-b7d8-050f4a19b92f#0:OK-t1. Peer's state: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93:t1, leader=null, voted=3da1490a-d610-40de-95c1-bdfc259ba041, raftlog=Memoized:4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-31 01:09:15,973 [4bc93e86-8989-4282-b7d8-050f4a19b92f-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-A59219751D93 with new leaderId: 3da1490a-d610-40de-95c1-bdfc259ba041
dn2_1    | 2023-05-31 01:09:15,974 [4bc93e86-8989-4282-b7d8-050f4a19b92f-server-thread1] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93: change Leader from null to 3da1490a-d610-40de-95c1-bdfc259ba041 at term 1 for appendEntries, leader elected after 6346ms
dn2_1    | 2023-05-31 01:09:15,976 [4bc93e86-8989-4282-b7d8-050f4a19b92f-server-thread1] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93: set configuration 0: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-31 01:09:15,977 [4bc93e86-8989-4282-b7d8-050f4a19b92f-server-thread1] INFO segmented.SegmentedRaftLogWorker: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-SegmentedRaftLogWorker: Starting segment from index:0
dn2_1    | 2023-05-31 01:09:15,979 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-A59219751D93-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5d339866-97f6-4ef6-b080-a59219751d93/current/log_inprogress_0
dn2_1    | 2023-05-31 01:09:20,319 [grpc-default-executor-3] INFO server.RaftServer: 4bc93e86-8989-4282-b7d8-050f4a19b92f: addNew group-B2777F51F5A3:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] returns group-B2777F51F5A3:java.util.concurrent.CompletableFuture@585e817d[Not completed]
dn2_1    | 2023-05-31 01:09:20,321 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f: new RaftServerImpl for group-B2777F51F5A3:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn2_1    | 2023-05-31 01:09:20,321 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-05-31 01:09:20,322 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-05-31 01:09:20,322 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-05-31 01:09:20,322 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
recon_1  | 2023-05-31 01:12:06,304 [PipelineSyncTask] INFO scm.ReconPipelineManager: Removing invalid pipeline PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444 from Recon.
recon_1  | 2023-05-31 01:12:06,305 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 1f8ec4de-a531-4a7c-9881-f1a4cf07e444, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17)3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, CreationTimestamp2023-05-31T01:06:14.144Z[UTC]] moved to CLOSED state
recon_1  | 2023-05-31 01:12:06,306 [PipelineSyncTask] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 1f8ec4de-a531-4a7c-9881-f1a4cf07e444, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17)3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, CreationTimestamp2023-05-31T01:06:14.144Z[UTC]] removed.
recon_1  | 2023-05-31 01:12:06,323 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 83 milliseconds.
recon_1  | 2023-05-31 01:12:19,122 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1  | 2023-05-31 01:12:19,123 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1  | 2023-05-31 01:12:19,123 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 193 
recon_1  | 2023-05-31 01:12:19,137 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 6, SequenceNumber diff: 17, SequenceNumber Lag from OM 0.
recon_1  | 2023-05-31 01:12:19,137 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 17 records
recon_1  | 2023-05-31 01:12:19,142 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
recon_1  | 2023-05-31 01:12:19,142 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
recon_1  | 2023-05-31 01:12:19,326 [pool-49-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
recon_1  | 2023-05-31 01:12:19,329 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 1 OM DB update event(s).
dn1_1    | 2023-05-31 01:09:20,180 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-05-31 01:09:20,180 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-05-31 01:09:20,180 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-05-31 01:09:20,180 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-05-31 01:09:20,181 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3: ConfigurationManager, init=-1: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-05-31 01:09:20,182 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
scm3_1   | 2023-05-31 01:09:18,251 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e689613f-6a5f-45fa-9ee1-234a7250e2df, Nodes: 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3da1490a-d610-40de-95c1-bdfc259ba041, CreationTimestamp2023-05-31T01:08:50.109Z[UTC]] moved to OPEN state
scm3_1   | 2023-05-31 01:09:19,501 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: cabada01-83b4-406c-b6ca-8f00a57da29e, Nodes: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, CreationTimestamp2023-05-31T01:08:50.144Z[UTC]] moved to OPEN state
dn2_1    | 2023-05-31 01:09:20,322 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-05-31 01:09:20,322 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-05-31 01:09:20,322 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3: ConfigurationManager, init=-1: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-05-31 01:09:20,322 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    |   success: true
dn1_1    | 2023-05-31 01:09:20,189 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1_1   | 2023-05-31 01:07:25,027 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: SCM Finalization has crossed checkpoint MLV_EQUALS_SLV
scm3_1   | 2023-05-31 01:09:21,057 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:09:21,058 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:09:25,574 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 46ae8d39-3c2f-48ac-8229-b2777f51f5a3, Nodes: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21)b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17)4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:b20d0805-391e-447d-a646-35a5d1a5cbcf, CreationTimestamp2023-05-31T01:09:14.136Z[UTC]] moved to OPEN state
scm3_1   | 2023-05-31 01:09:26,113 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:09:26,113 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | }
dn1_1    | 2023-05-31 01:09:20,189 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1_1   | 2023-05-31 01:07:25,034 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-05-31 01:07:25,035 [RatisPipelineUtilsThread - 0] ERROR pipeline.PipelinePlacementPolicy: No healthy node found to allocate container.
scm1_1   | 2023-05-31 01:07:30,034 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-05-31 01:07:35,037 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn5_1    | term: 9
dn1_1    | 2023-05-31 01:09:20,192 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
recon_1  | 2023-05-31 01:12:19,338 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 2023-05-31 01:09:20,323 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1_1   | 2023-05-31 01:07:38,499 [IPC Server handler 5 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 2023-05-31 01:09:20,323 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm3_1   | 2023-05-31 01:09:31,296 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:09:31,296 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | nextIndex: 45
dn1_1    | 2023-05-31 01:09:20,192 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn1_1    | 2023-05-31 01:09:20,192 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-05-31 01:09:20,193 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-05-31 01:09:20,323 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-05-31 01:09:20,323 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn2_1    | 2023-05-31 01:09:20,323 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | followerCommit: 44
scm1_1   | 2023-05-31 01:07:38,840 [IPC Server handler 10 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn2_1    | 2023-05-31 01:09:20,323 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | matchIndex: 18446744073709551615
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | isHearbeat: true
dn1_1    | 2023-05-31 01:09:20,195 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn1_1    | 2023-05-31 01:09:20,199 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-05-31 01:09:20,201 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-05-31 01:09:20,201 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-05-31 01:09:20,201 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 
scm1_1   | 2023-05-31 01:07:40,037 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn4_1    | 2023-05-31 01:08:28,102 [grpc-default-executor-5] WARN server.GrpcServerProtocolService: 3da1490a-d610-40de-95c1-bdfc259ba041: Failed requestVote b20d0805-391e-447d-a646-35a5d1a5cbcf->3da1490a-d610-40de-95c1-bdfc259ba041#0
dn4_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 3da1490a-d610-40de-95c1-bdfc259ba041: group-DC0251522AF1 not found.
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn5_1    | 2023-05-31 01:08:17,114 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2-SegmentedRaftLogWorker close()
scm1_1   | 2023-05-31 01:07:40,988 [IPC Server handler 0 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
dn4_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
scm1_1   | 2023-05-31 01:07:45,038 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn5_1    | 2023-05-31 01:08:17,132 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-ED0A589991D2: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/437d8269-2a0d-4058-8be6-ed0a589991d2
dn1_1    | 2023-05-31 01:09:20,201 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-05-31 01:09:20,202 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm3_1   | 2023-05-31 01:09:36,346 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:09:36,346 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:09:41,384 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-05-31 01:08:17,136 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=437d8269-2a0d-4058-8be6-ed0a589991d2 command on datanode 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd.
scm1_1   | 2023-05-31 01:07:45,194 [IPC Server handler 83 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
dn5_1    | 2023-05-31 01:08:17,138 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: remove  FOLLOWER 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1:t9, leader=3da1490a-d610-40de-95c1-bdfc259ba041, voted=3da1490a-d610-40de-95c1-bdfc259ba041, raftlog=Memoized:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-SegmentedRaftLog:OPENED:c44, conf=43: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn2_1    | 2023-05-31 01:09:20,324 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn2_1    | 2023-05-31 01:09:20,324 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-05-31 01:09:20,325 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-05-31 01:09:20,325 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-05-31 01:09:20,325 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-05-31 01:09:20,325 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-05-31 01:08:17,138 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1: shutdown
dn2_1    | 2023-05-31 01:09:20,325 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-05-31 01:09:20,325 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/46ae8d39-3c2f-48ac-8229-b2777f51f5a3 does not exist. Creating ...
scm1_1   | 2023-05-31 01:07:45,392 [IPC Server handler 4 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-05-31 01:07:50,038 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-05-31 01:07:55,036 [RatisPipelineUtilsThread - 0] ERROR pipeline.PipelinePlacementPolicy: No healthy node found to allocate container.
scm3_1   | 2023-05-31 01:09:41,385 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-05-31 01:08:17,139 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-DC0251522AF1,id=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn2_1    | 2023-05-31 01:09:20,327 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/46ae8d39-3c2f-48ac-8229-b2777f51f5a3/in_use.lock acquired by nodename 7@86166c590ca2
scm1_1   | 2023-05-31 01:07:55,038 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm3_1   | 2023-05-31 01:09:44,854 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: ec1b01cd-6e71-4daf-8419-6155613624a8, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:b20d0805-391e-447d-a646-35a5d1a5cbcf, CreationTimestamp2023-05-31T01:09:14.112Z[UTC]] moved to OPEN state
scm3_1   | 2023-05-31 01:09:46,435 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:09:46,435 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:09:51,548 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:09:51,548 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:09:56,638 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:09:56,639 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-31 01:07:56,890 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for delTxnId, expected lastId is 0, actual lastId is 2000.
scm1_1   | 2023-05-31 01:07:56,942 [IPC Server handler 7 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for delTxnId, change lastId from 2000 to 3000.
scm1_1   | 2023-05-31 01:08:00,038 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn5_1    | 2023-05-31 01:08:17,139 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: shutdown 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-FollowerState
dn2_1    | 2023-05-31 01:09:20,329 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/46ae8d39-3c2f-48ac-8229-b2777f51f5a3 has been successfully formatted.
dn2_1    | 2023-05-31 01:09:20,329 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO ratis.ContainerStateMachine: group-B2777F51F5A3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn5_1    | 2023-05-31 01:08:17,139 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-FollowerState] INFO impl.FollowerState: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-FollowerState was interrupted
dn5_1    | 2023-05-31 01:08:17,141 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-StateMachineUpdater: set stopIndex = 44
dn1_1    | 2023-05-31 01:09:20,202 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/46ae8d39-3c2f-48ac-8229-b2777f51f5a3 does not exist. Creating ...
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
dn5_1    | 2023-05-31 01:08:17,142 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-DC0251522AF1: Taking a snapshot at:(t:9, i:44) file /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/sm/snapshot.9_44
dn1_1    | 2023-05-31 01:09:20,206 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/46ae8d39-3c2f-48ac-8229-b2777f51f5a3/in_use.lock acquired by nodename 6@2af98bbedaed
dn1_1    | 2023-05-31 01:09:20,208 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/46ae8d39-3c2f-48ac-8229-b2777f51f5a3 has been successfully formatted.
scm1_1   | 2023-05-31 01:08:05,039 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-05-31 01:08:08,486 [IPC Server handler 5 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-05-31 01:08:08,852 [IPC Server handler 0 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 2023-05-31 01:08:17,148 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-DC0251522AF1: Finished taking a snapshot at:(t:9, i:44) file:/data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1/sm/snapshot.9_44 took: 5 ms
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn5_1    | 2023-05-31 01:08:17,148 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-StateMachineUpdater] INFO impl.StateMachineUpdater: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-StateMachineUpdater: Took a snapshot at index 44
scm3_1   | 2023-05-31 01:10:01,715 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:10:01,715 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-05-31 01:09:20,330 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-05-31 01:09:20,209 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO ratis.ContainerStateMachine: group-B2777F51F5A3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn1_1    | 2023-05-31 01:09:20,210 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-05-31 01:08:17,148 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-StateMachineUpdater] INFO impl.StateMachineUpdater: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-StateMachineUpdater: snapshotIndex: updateIncreasingly 42 -> 44
dn5_1    | 2023-05-31 01:08:17,154 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1: closes. applyIndex: 44
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 2023-05-31 01:09:20,330 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-05-31 01:09:20,226 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-05-31 01:08:17,860 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1-SegmentedRaftLogWorker close()
scm1_1   | 2023-05-31 01:08:10,039 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm3_1   | 2023-05-31 01:10:06,723 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:10:06,724 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-05-31 01:09:20,331 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-05-31 01:09:20,226 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:08:17,883 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 27.
scm1_1   | 2023-05-31 01:08:10,971 [IPC Server handler 13 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-05-31 01:08:14,256 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=ccaf8839-8737-4f75-8b75-b011fdd1ea12 since it stays at CLOSED stage.
scm3_1   | 2023-05-31 01:10:11,846 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-05-31 01:09:20,331 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-05-31 01:09:20,331 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-05-31 01:08:17,884 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 27.
scm3_1   | 2023-05-31 01:10:11,847 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:10:16,932 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-05-31 01:09:20,227 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-05-31 01:09:20,227 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-05-31 01:08:17,943 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 40.
scm1_1   | 2023-05-31 01:08:14,257 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=ccaf8839-8737-4f75-8b75-b011fdd1ea12 close command to datanode 4bc93e86-8989-4282-b7d8-050f4a19b92f
scm1_1   | 2023-05-31 01:08:14,269 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: ccaf8839-8737-4f75-8b75-b011fdd1ea12, Nodes: 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:4bc93e86-8989-4282-b7d8-050f4a19b92f, CreationTimestamp2023-05-31T01:06:14.149135Z[UTC]] removed.
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | 2023-05-31 01:09:20,228 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-05-31 01:08:17,943 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 40.
scm3_1   | 2023-05-31 01:10:16,933 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-31 01:08:14,280 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=6a499bf5-10af-4dab-a3fd-a4e2998a83c9 since it stays at CLOSED stage.
dn4_1    | 2023-05-31 01:08:32,609 [grpc-default-executor-5] WARN server.GrpcServerProtocolService: 3da1490a-d610-40de-95c1-bdfc259ba041: Failed requestVote b20d0805-391e-447d-a646-35a5d1a5cbcf->3da1490a-d610-40de-95c1-bdfc259ba041#0
dn4_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 3da1490a-d610-40de-95c1-bdfc259ba041: group-F1A4CF07E444 not found.
dn1_1    | 2023-05-31 01:09:20,229 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-05-31 01:08:17,978 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-DC0251522AF1: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/a4fe2088-9059-47e2-9c65-dc0251522af1
scm3_1   | 2023-05-31 01:10:20,051 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
scm1_1   | 2023-05-31 01:08:14,281 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=6a499bf5-10af-4dab-a3fd-a4e2998a83c9 close command to datanode b20d0805-391e-447d-a646-35a5d1a5cbcf
scm1_1   | 2023-05-31 01:08:14,291 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 6a499bf5-10af-4dab-a3fd-a4e2998a83c9, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:b20d0805-391e-447d-a646-35a5d1a5cbcf, CreationTimestamp2023-05-31T01:06:14.148590Z[UTC]] removed.
scm1_1   | 2023-05-31 01:08:14,293 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=9e886fff-1860-4674-bb59-7a8b4be06a04 since it stays at CLOSED stage.
dn2_1    | 2023-05-31 01:09:20,331 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
scm1_1   | 2023-05-31 01:08:14,293 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=9e886fff-1860-4674-bb59-7a8b4be06a04 close command to datanode 97734618-7966-42ac-b50e-e49ead6ce4d2
dn2_1    | 2023-05-31 01:09:20,331 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | 2023-05-31 01:08:14,320 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 9e886fff-1860-4674-bb59-7a8b4be06a04, Nodes: 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:97734618-7966-42ac-b50e-e49ead6ce4d2, CreationTimestamp2023-05-31T01:06:14.148725Z[UTC]] removed.
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
dn2_1    | 2023-05-31 01:09:20,332 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-05-31 01:09:20,229 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm3_1   | 2023-05-31 01:10:22,007 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-05-31 01:08:17,979 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1 command on datanode 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd.
dn5_1    | 2023-05-31 01:08:17,979 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: remove    LEADER 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444:t12, leader=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, voted=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, raftlog=Memoized:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-SegmentedRaftLog:OPENED:c26, conf=25: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn5_1    | 2023-05-31 01:08:17,979 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444: shutdown
dn2_1    | 2023-05-31 01:09:20,332 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-05-31 01:09:20,332 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/46ae8d39-3c2f-48ac-8229-b2777f51f5a3
dn2_1    | 2023-05-31 01:09:20,332 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-05-31 01:09:20,229 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:08:17,979 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-F1A4CF07E444,id=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
scm1_1   | 2023-05-31 01:08:14,324 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=437d8269-2a0d-4058-8be6-ed0a589991d2 since it stays at CLOSED stage.
scm3_1   | 2023-05-31 01:10:22,007 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
dn5_1    | 2023-05-31 01:08:17,979 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: shutdown 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-LeaderStateImpl
scm1_1   | 2023-05-31 01:08:14,325 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=437d8269-2a0d-4058-8be6-ed0a589991d2 close command to datanode 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn5_1    | 2023-05-31 01:08:17,980 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444->b20d0805-391e-447d-a646-35a5d1a5cbcf-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444->b20d0805-391e-447d-a646-35a5d1a5cbcf-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
scm3_1   | 2023-05-31 01:10:23,414 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
dn1_1    | 2023-05-31 01:09:20,230 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO segmented.SegmentedRaftLogWorker: new b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/46ae8d39-3c2f-48ac-8229-b2777f51f5a3
dn1_1    | 2023-05-31 01:09:20,230 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-05-31 01:09:20,230 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-05-31 01:08:17,980 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444->3da1490a-d610-40de-95c1-bdfc259ba041-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444->3da1490a-d610-40de-95c1-bdfc259ba041-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
scm1_1   | 2023-05-31 01:08:14,346 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 437d8269-2a0d-4058-8be6-ed0a589991d2, Nodes: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, CreationTimestamp2023-05-31T01:06:14.148216Z[UTC]] removed.
dn5_1    | 2023-05-31 01:08:17,983 [PipelineCommandHandlerThread-0] INFO impl.PendingRequests: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-PendingRequests: sendNotLeaderResponses
dn4_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
scm3_1   | 2023-05-31 01:10:24,982 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
scm3_1   | 2023-05-31 01:10:27,101 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:10:27,101 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:10:32,147 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:10:32,147 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-31 01:08:14,346 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1 since it stays at CLOSED stage.
scm1_1   | 2023-05-31 01:08:14,346 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1 close command to datanode b20d0805-391e-447d-a646-35a5d1a5cbcf
scm3_1   | 2023-05-31 01:10:37,307 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:10:37,308 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-05-31 01:09:20,332 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
scm1_1   | 2023-05-31 01:08:14,347 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1 close command to datanode 3da1490a-d610-40de-95c1-bdfc259ba041
scm1_1   | 2023-05-31 01:08:14,347 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1 close command to datanode 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn1_1    | 2023-05-31 01:09:20,230 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-05-31 01:09:20,230 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-05-31 01:09:20,332 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-05-31 01:09:20,332 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm1_1   | 2023-05-31 01:08:14,377 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: a4fe2088-9059-47e2-9c65-dc0251522af1, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17)3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:3da1490a-d610-40de-95c1-bdfc259ba041, CreationTimestamp2023-05-31T01:06:14.148990Z[UTC]] removed.
scm1_1   | 2023-05-31 01:08:14,379 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444 since it stays at CLOSED stage.
dn1_1    | 2023-05-31 01:09:20,230 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-05-31 01:09:20,230 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm1_1   | 2023-05-31 01:08:14,380 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444 close command to datanode b20d0805-391e-447d-a646-35a5d1a5cbcf
dn2_1    | 2023-05-31 01:09:20,332 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-05-31 01:08:17,984 [grpc-default-executor-0] INFO server.GrpcLogAppender: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444->b20d0805-391e-447d-a646-35a5d1a5cbcf-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn1_1    | 2023-05-31 01:09:20,231 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm3_1   | 2023-05-31 01:10:42,466 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:10:42,467 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-05-31 01:08:17,987 [grpc-default-executor-1] INFO server.GrpcLogAppender: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444->b20d0805-391e-447d-a646-35a5d1a5cbcf-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn1_1    | 2023-05-31 01:09:20,231 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-05-31 01:09:20,332 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-05-31 01:09:20,332 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-05-31 01:09:20,332 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | 2023-05-31 01:08:17,997 [grpc-default-executor-0] INFO leader.FollowerInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444->b20d0805-391e-447d-a646-35a5d1a5cbcf: decreaseNextIndex nextIndex: updateUnconditionally 27 -> 26
scm1_1   | 2023-05-31 01:08:14,380 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444 close command to datanode 3da1490a-d610-40de-95c1-bdfc259ba041
scm3_1   | 2023-05-31 01:10:46,702 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for containerId, expected lastId is 0, actual lastId is 2000.
dn4_1    | 2023-05-31 01:08:33,205 [grpc-default-executor-5] WARN server.GrpcServerProtocolService: 3da1490a-d610-40de-95c1-bdfc259ba041: Failed requestVote b20d0805-391e-447d-a646-35a5d1a5cbcf->3da1490a-d610-40de-95c1-bdfc259ba041#0
dn4_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 3da1490a-d610-40de-95c1-bdfc259ba041: group-DC0251522AF1 not found.
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
scm1_1   | 2023-05-31 01:08:14,380 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444 close command to datanode 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
scm3_1   | 2023-05-31 01:10:46,801 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019203000.
dn5_1    | 2023-05-31 01:08:17,998 [grpc-default-executor-1] INFO leader.FollowerInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444->b20d0805-391e-447d-a646-35a5d1a5cbcf: decreaseNextIndex nextIndex: updateUnconditionally 26 -> 25
dn5_1    | 2023-05-31 01:08:18,000 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-StateMachineUpdater: set stopIndex = 26
dn2_1    | 2023-05-31 01:09:20,333 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-05-31 01:09:20,339 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-05-31 01:08:14,401 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 1f8ec4de-a531-4a7c-9881-f1a4cf07e444, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17)3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, CreationTimestamp2023-05-31T01:06:14.144553Z[UTC]] removed.
dn5_1    | 2023-05-31 01:08:18,001 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-F1A4CF07E444: Taking a snapshot at:(t:12, i:26) file /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/sm/snapshot.12_26
dn1_1    | 2023-05-31 01:09:20,232 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-05-31 01:09:20,234 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn5_1    | 2023-05-31 01:08:18,002 [grpc-default-executor-4] INFO server.GrpcLogAppender: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444->3da1490a-d610-40de-95c1-bdfc259ba041-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn5_1    | 2023-05-31 01:08:18,002 [grpc-default-executor-4] INFO leader.FollowerInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444->3da1490a-d610-40de-95c1-bdfc259ba041: decreaseNextIndex nextIndex: updateUnconditionally 27 -> 26
dn5_1    | 2023-05-31 01:08:18,008 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-F1A4CF07E444: Finished taking a snapshot at:(t:12, i:26) file:/data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444/sm/snapshot.12_26 took: 8 ms
dn1_1    | 2023-05-31 01:09:20,253 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-05-31 01:09:20,253 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1_1   | 2023-05-31 01:08:14,407 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=5509b2c4-975c-42f1-91be-39d463d01a51 since it stays at CLOSED stage.
scm3_1   | 2023-05-31 01:10:47,580 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:10:47,580 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-31 01:08:14,408 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=5509b2c4-975c-42f1-91be-39d463d01a51 close command to datanode 3da1490a-d610-40de-95c1-bdfc259ba041
scm1_1   | 2023-05-31 01:08:14,423 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 5509b2c4-975c-42f1-91be-39d463d01a51, Nodes: 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:3da1490a-d610-40de-95c1-bdfc259ba041, CreationTimestamp2023-05-31T01:06:14.148442Z[UTC]] removed.
dn2_1    | 2023-05-31 01:09:20,344 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
dn1_1    | 2023-05-31 01:09:20,253 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm1_1   | 2023-05-31 01:08:15,040 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn2_1    | 2023-05-31 01:09:20,344 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
dn1_1    | 2023-05-31 01:09:20,254 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-05-31 01:09:20,254 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
dn5_1    | 2023-05-31 01:08:18,009 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-StateMachineUpdater] INFO impl.StateMachineUpdater: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-StateMachineUpdater: Took a snapshot at index 26
dn2_1    | 2023-05-31 01:09:20,344 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-05-31 01:09:20,344 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO segmented.SegmentedRaftLogWorker: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-05-31 01:08:15,189 [IPC Server handler 83 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn1_1    | 2023-05-31 01:09:20,255 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3: start as a follower, conf=-1: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
scm1_1   | 2023-05-31 01:08:15,366 [IPC Server handler 6 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
scm3_1   | 2023-05-31 01:10:49,571 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
scm3_1   | 2023-05-31 01:10:50,073 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
scm3_1   | 2023-05-31 01:10:52,716 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-31 01:08:15,367 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1 is not found
dn2_1    | 2023-05-31 01:09:20,344 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO segmented.SegmentedRaftLogWorker: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
scm3_1   | 2023-05-31 01:10:52,717 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:10:54,962 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
dn5_1    | 2023-05-31 01:08:18,009 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-StateMachineUpdater] INFO impl.StateMachineUpdater: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-StateMachineUpdater: snapshotIndex: updateIncreasingly 24 -> 26
dn5_1    | 2023-05-31 01:08:18,010 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444: closes. applyIndex: 26
dn5_1    | 2023-05-31 01:08:18,396 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444-SegmentedRaftLogWorker close()
dn2_1    | 2023-05-31 01:09:20,345 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3: start as a follower, conf=-1: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
dn1_1    | 2023-05-31 01:09:20,255 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn1_1    | 2023-05-31 01:09:20,255 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-FollowerState
dn5_1    | 2023-05-31 01:08:18,405 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 23.
scm1_1   | 2023-05-31 01:08:15,367 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=5509b2c4-975c-42f1-91be-39d463d01a51 is not found
dn2_1    | 2023-05-31 01:09:20,345 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn2_1    | 2023-05-31 01:09:20,345 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO impl.RoleInfo: 4bc93e86-8989-4282-b7d8-050f4a19b92f: start 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3-FollowerState
dn1_1    | 2023-05-31 01:09:20,258 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:10:57,757 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:10:57,757 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-31 01:08:15,367 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444 is not found
dn1_1    | 2023-05-31 01:09:20,259 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-05-31 01:09:20,260 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B2777F51F5A3,id=b20d0805-391e-447d-a646-35a5d1a5cbcf
dn1_1    | 2023-05-31 01:09:20,260 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3_1   | 2023-05-31 01:11:02,807 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-05-31 01:09:20,346 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B2777F51F5A3,id=4bc93e86-8989-4282-b7d8-050f4a19b92f
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
scm3_1   | 2023-05-31 01:11:02,807 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:11:08,007 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:11:08,007 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-05-31 01:09:20,260 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-05-31 01:09:20,260 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-05-31 01:09:20,346 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn5_1    | 2023-05-31 01:08:18,405 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 23.
dn5_1    | 2023-05-31 01:08:18,460 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 9.
dn5_1    | 2023-05-31 01:08:18,460 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 9.
dn1_1    | 2023-05-31 01:09:20,260 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
scm3_1   | 2023-05-31 01:11:13,025 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-05-31 01:09:20,346 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm1_1   | 2023-05-31 01:08:16,783 [IPC Server handler 10 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-05-31 01:08:16,851 [IPC Server handler 0 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 2023-05-31 01:08:18,487 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-F1A4CF07E444: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/1f8ec4de-a531-4a7c-9881-f1a4cf07e444
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3_1   | 2023-05-31 01:11:13,025 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-05-31 01:09:20,346 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm1_1   | 2023-05-31 01:08:16,853 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to QUASI_CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported QUASI_CLOSED replica.
dn1_1    | 2023-05-31 01:09:24,940 [grpc-default-executor-8] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3: receive requestVote(PRE_VOTE, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, group-B2777F51F5A3, 0, (t:0, i:0))
dn1_1    | 2023-05-31 01:09:24,941 [grpc-default-executor-8] INFO impl.VoteContext: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-FOLLOWER: reject PRE_VOTE from 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: our priority 1 > candidate's priority 0
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 2023-05-31 01:09:20,346 [4bc93e86-8989-4282-b7d8-050f4a19b92f-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 2023-05-31 01:11:18,106 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:11:18,107 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:11:20,416 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
dn2_1    | 2023-05-31 01:09:20,349 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-05-31 01:09:24,942 [grpc-default-executor-8] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3 replies to PRE_VOTE vote request: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd<-b20d0805-391e-447d-a646-35a5d1a5cbcf#0:FAIL-t0. Peer's state: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3:t0, leader=null, voted=, raftlog=Memoized:b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:08:18,487 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444 command on datanode 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd.
dn5_1    | 2023-05-31 01:08:22,307 [grpc-default-executor-4] WARN server.GrpcServerProtocolService: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: Failed requestVote b20d0805-391e-447d-a646-35a5d1a5cbcf->3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#0
dn2_1    | 2023-05-31 01:09:20,350 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-05-31 01:09:25,434 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-FollowerState] INFO impl.FollowerState: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5178363565ns, electionTimeout:5173ms
scm1_1   | 2023-05-31 01:08:16,880 [IPC Server handler 13 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-05-31 01:08:16,886 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1002 to QUASI_CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported QUASI_CLOSED replica.
dn5_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: group-F1A4CF07E444 not found.
dn4_1    | 2023-05-31 01:08:35,947 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-05-31 01:09:25,434 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-FollowerState
dn1_1    | 2023-05-31 01:09:25,434 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-FollowerState] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn1_1    | 2023-05-31 01:09:25,435 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn2_1    | 2023-05-31 01:09:24,980 [grpc-default-executor-3] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3: receive requestVote(PRE_VOTE, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, group-B2777F51F5A3, 0, (t:0, i:0))
dn4_1    | 2023-05-31 01:08:37,798 [grpc-default-executor-5] WARN server.GrpcServerProtocolService: 3da1490a-d610-40de-95c1-bdfc259ba041: Failed requestVote b20d0805-391e-447d-a646-35a5d1a5cbcf->3da1490a-d610-40de-95c1-bdfc259ba041#0
dn1_1    | 2023-05-31 01:09:25,435 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12
dn1_1    | 2023-05-31 01:09:25,436 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-31 01:11:20,419 [FixedThreadPoolWithAffinityExecutor-9-0] WARN container.IncrementalContainerReportHandler: Failed to process CLOSED Container #1 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E is not the leader 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn2_1    | 2023-05-31 01:09:24,981 [grpc-default-executor-3] INFO impl.VoteContext: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3-FOLLOWER: accept PRE_VOTE from 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: our priority 0 <= candidate's priority 0
dn4_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 3da1490a-d610-40de-95c1-bdfc259ba041: group-F1A4CF07E444 not found.
dn1_1    | 2023-05-31 01:09:25,450 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-05-31 01:09:25,463 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-05-31 01:09:24,981 [grpc-default-executor-3] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3 replies to PRE_VOTE vote request: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd<-4bc93e86-8989-4282-b7d8-050f4a19b92f#0:OK-t0. Peer's state: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3:t0, leader=null, voted=, raftlog=Memoized:4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn1_1    | 2023-05-31 01:09:25,464 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn1_1    | 2023-05-31 01:09:25,464 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO impl.LeaderElection:   Response 0: b20d0805-391e-447d-a646-35a5d1a5cbcf<-3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#0:OK-t0
dn2_1    | 2023-05-31 01:09:25,458 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn1_1    | 2023-05-31 01:09:25,464 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12 PRE_VOTE round 0: result PASSED
scm3_1   | 2023-05-31 01:11:20,425 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
dn2_1    | 2023-05-31 01:09:25,458 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-31 01:08:17,096 [IPC Server handler 28 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
dn1_1    | 2023-05-31 01:09:25,467 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12-2] INFO server.GrpcServerProtocolClient: Build channel for 4bc93e86-8989-4282-b7d8-050f4a19b92f
dn2_1    | 2023-05-31 01:09:25,544 [grpc-default-executor-3] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3: receive requestVote(ELECTION, b20d0805-391e-447d-a646-35a5d1a5cbcf, group-B2777F51F5A3, 1, (t:0, i:0))
scm1_1   | 2023-05-31 01:08:17,160 [IPC Server handler 83 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
scm3_1   | 2023-05-31 01:11:20,425 [FixedThreadPoolWithAffinityExecutor-9-0] WARN container.IncrementalContainerReportHandler: Failed to process CLOSED Container #2 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E is not the leader 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn1_1    | 2023-05-31 01:09:25,471 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12 ELECTION round 0: submit vote requests at term 1 for -1: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-31 01:09:25,545 [grpc-default-executor-3] INFO impl.VoteContext: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3-FOLLOWER: accept ELECTION from b20d0805-391e-447d-a646-35a5d1a5cbcf: our priority 0 <= candidate's priority 1
scm1_1   | 2023-05-31 01:08:17,162 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to QUASI_CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported QUASI_CLOSED replica.
dn5_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
scm3_1   | 2023-05-31 01:11:20,443 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
dn1_1    | 2023-05-31 01:09:25,493 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-05-31 01:09:25,546 [grpc-default-executor-3] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:b20d0805-391e-447d-a646-35a5d1a5cbcf
scm1_1   | 2023-05-31 01:08:17,192 [IPC Server handler 95 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
dn1_1    | 2023-05-31 01:09:25,494 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-05-31 01:09:25,546 [grpc-default-executor-3] INFO impl.RoleInfo: 4bc93e86-8989-4282-b7d8-050f4a19b92f: shutdown 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3-FollowerState
scm1_1   | 2023-05-31 01:08:17,215 [IPC Server handler 94 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
scm3_1   | 2023-05-31 01:11:20,444 [FixedThreadPoolWithAffinityExecutor-9-0] WARN container.IncrementalContainerReportHandler: Failed to process CLOSED Container #1001 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E is not the leader 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
dn1_1    | 2023-05-31 01:09:25,530 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn1_1    | 2023-05-31 01:09:25,530 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO impl.LeaderElection:   Response 0: b20d0805-391e-447d-a646-35a5d1a5cbcf<-3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#0:OK-t1
scm1_1   | 2023-05-31 01:08:17,217 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to QUASI_CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported QUASI_CLOSED replica.
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
scm3_1   | 2023-05-31 01:11:20,468 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1002 to CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
dn4_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
scm1_1   | 2023-05-31 01:08:17,887 [IPC Server handler 28 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
scm3_1   | 2023-05-31 01:11:20,474 [FixedThreadPoolWithAffinityExecutor-9-0] WARN container.IncrementalContainerReportHandler: Failed to process CLOSED Container #1002 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E is not the leader 7b17d04a-0b5d-44ed-b487-1bf0d959b978|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
dn2_1    | 2023-05-31 01:09:25,546 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3-FollowerState] INFO impl.FollowerState: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3-FollowerState was interrupted
dn2_1    | 2023-05-31 01:09:25,547 [grpc-default-executor-3] INFO impl.RoleInfo: 4bc93e86-8989-4282-b7d8-050f4a19b92f: start 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3-FollowerState
scm1_1   | 2023-05-31 01:08:17,918 [IPC Server handler 12 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-05-31 01:08:17,938 [IPC Server handler 33 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
dn2_1    | 2023-05-31 01:09:25,576 [grpc-default-executor-3] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3 replies to ELECTION vote request: b20d0805-391e-447d-a646-35a5d1a5cbcf<-4bc93e86-8989-4282-b7d8-050f4a19b92f#0:OK-t1. Peer's state: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3:t1, leader=null, voted=b20d0805-391e-447d-a646-35a5d1a5cbcf, raftlog=Memoized:4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-31 01:09:25,531 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12 ELECTION round 0: result PASSED
scm1_1   | 2023-05-31 01:08:17,968 [IPC Server handler 27 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-05-31 01:08:18,409 [IPC Server handler 5 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
dn2_1    | 2023-05-31 01:09:25,650 [grpc-default-executor-3] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3: receive requestVote(PRE_VOTE, b20d0805-391e-447d-a646-35a5d1a5cbcf, group-B2777F51F5A3, 0, (t:0, i:0))
dn2_1    | 2023-05-31 01:09:25,651 [grpc-default-executor-3] INFO impl.VoteContext: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3-FOLLOWER: accept PRE_VOTE from b20d0805-391e-447d-a646-35a5d1a5cbcf: our priority 0 <= candidate's priority 1
scm1_1   | 2023-05-31 01:08:18,439 [IPC Server handler 1 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-05-31 01:11:23,171 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:11:23,171 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn1_1    | 2023-05-31 01:09:25,549 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12
dn1_1    | 2023-05-31 01:09:25,549 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm1_1   | 2023-05-31 01:08:18,459 [IPC Server handler 2 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-05-31 01:11:28,312 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 2023-05-31 01:09:25,550 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-B2777F51F5A3 with new leaderId: b20d0805-391e-447d-a646-35a5d1a5cbcf
dn2_1    | 2023-05-31 01:09:25,653 [grpc-default-executor-3] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3 replies to PRE_VOTE vote request: b20d0805-391e-447d-a646-35a5d1a5cbcf<-4bc93e86-8989-4282-b7d8-050f4a19b92f#0:OK-t1. Peer's state: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3:t1, leader=null, voted=b20d0805-391e-447d-a646-35a5d1a5cbcf, raftlog=Memoized:4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:08:18,480 [IPC Server handler 7 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-05-31 01:08:20,040 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm3_1   | 2023-05-31 01:11:28,312 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-05-31 01:09:25,564 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3: change Leader from null to b20d0805-391e-447d-a646-35a5d1a5cbcf at term 1 for becomeLeader, leader elected after 5357ms
dn1_1    | 2023-05-31 01:09:25,565 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
scm1_1   | 2023-05-31 01:08:25,038 [RatisPipelineUtilsThread - 0] ERROR pipeline.PipelinePlacementPolicy: No healthy node found to allocate container.
scm1_1   | 2023-05-31 01:08:25,040 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-05-31 01:08:30,040 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn1_1    | 2023-05-31 01:09:25,567 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-05-31 01:09:25,568 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm1_1   | 2023-05-31 01:08:35,041 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm3_1   | 2023-05-31 01:11:33,434 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:11:33,435 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-05-31 01:09:25,883 [4bc93e86-8989-4282-b7d8-050f4a19b92f-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-B2777F51F5A3 with new leaderId: b20d0805-391e-447d-a646-35a5d1a5cbcf
dn2_1    | 2023-05-31 01:09:25,886 [4bc93e86-8989-4282-b7d8-050f4a19b92f-server-thread1] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3: change Leader from null to b20d0805-391e-447d-a646-35a5d1a5cbcf at term 1 for appendEntries, leader elected after 5560ms
scm1_1   | 2023-05-31 01:08:40,041 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 2023-05-31 01:09:25,894 [4bc93e86-8989-4282-b7d8-050f4a19b92f-server-thread1] INFO server.RaftServer$Division: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3: set configuration 0: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-31 01:09:25,896 [4bc93e86-8989-4282-b7d8-050f4a19b92f-server-thread1] INFO segmented.SegmentedRaftLogWorker: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3-SegmentedRaftLogWorker: Starting segment from index:0
scm3_1   | 2023-05-31 01:11:38,453 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:11:38,454 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-31 01:08:40,979 [IPC Server handler 23 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn1_1    | 2023-05-31 01:09:25,570 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
scm3_1   | 2023-05-31 01:11:43,303 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 2023-05-31 01:09:25,902 [4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4bc93e86-8989-4282-b7d8-050f4a19b92f@group-B2777F51F5A3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/46ae8d39-3c2f-48ac-8229-b2777f51f5a3/current/log_inprogress_0
dn4_1    | 2023-05-31 01:08:38,378 [grpc-default-executor-5] WARN server.GrpcServerProtocolService: 3da1490a-d610-40de-95c1-bdfc259ba041: Failed requestVote b20d0805-391e-447d-a646-35a5d1a5cbcf->3da1490a-d610-40de-95c1-bdfc259ba041#0
dn1_1    | 2023-05-31 01:09:25,601 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm3_1   | 2023-05-31 01:11:43,592 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-05-31 01:08:22,877 [grpc-default-executor-4] WARN server.GrpcServerProtocolService: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: Failed requestVote b20d0805-391e-447d-a646-35a5d1a5cbcf->3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#0
dn5_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: group-DC0251522AF1 not found.
dn2_1    | 2023-05-31 01:09:33,361 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-05-31 01:09:25,628 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm3_1   | 2023-05-31 01:11:43,592 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn2_1    | 2023-05-31 01:10:33,362 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-05-31 01:09:25,629 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
dn2_1    | 2023-05-31 01:11:33,362 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-05-31 01:09:25,629 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm3_1   | 2023-05-31 01:11:48,636 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
dn2_1    | 2023-05-31 01:12:33,362 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn4_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 3da1490a-d610-40de-95c1-bdfc259ba041: group-DC0251522AF1 not found.
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn1_1    | 2023-05-31 01:09:25,687 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn1_1    | 2023-05-31 01:09:25,700 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-05-31 01:08:41,109 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18) moved to HEALTHY state.
scm1_1   | 2023-05-31 01:08:41,110 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1_1   | 2023-05-31 01:08:41,112 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19) moved to HEALTHY state.
scm1_1   | 2023-05-31 01:08:41,112 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1_1   | 2023-05-31 01:08:41,113 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=46a9d8fb-17d8-4177-820b-4ef30e5fdeea to datanode:4bc93e86-8989-4282-b7d8-050f4a19b92f
dn1_1    | 2023-05-31 01:09:25,703 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
scm3_1   | 2023-05-31 01:11:48,636 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:11:53,821 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-31 01:08:41,130 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 46a9d8fb-17d8-4177-820b-4ef30e5fdeea, Nodes: 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:41.112Z[UTC]].
dn1_1    | 2023-05-31 01:09:25,714 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn1_1    | 2023-05-31 01:09:25,718 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn1_1    | 2023-05-31 01:09:25,718 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-05-31 01:09:25,718 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
scm1_1   | 2023-05-31 01:08:41,141 [RatisPipelineUtilsThread - 0] ERROR scm.SCMCommonPlacementPolicy: Unable to find enough nodes that meet the space requirement of 1073741824 bytes for metadata and 1073741824 bytes for data in healthy node set. Required 3. Found 2.
dn5_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
dn5_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
dn1_1    | 2023-05-31 01:09:25,718 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
scm1_1   | 2023-05-31 01:08:41,142 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=b5c5c804-816d-41a2-83d8-297089fabfc6 to datanode:97734618-7966-42ac-b50e-e49ead6ce4d2
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
dn1_1    | 2023-05-31 01:09:25,718 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
scm1_1   | 2023-05-31 01:08:41,152 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: b5c5c804-816d-41a2-83d8-297089fabfc6, Nodes: 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:41.141Z[UTC]].
dn1_1    | 2023-05-31 01:09:25,718 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm1_1   | 2023-05-31 01:08:41,153 [RatisPipelineUtilsThread - 0] ERROR scm.SCMCommonPlacementPolicy: Unable to find enough nodes that meet the space requirement of 1073741824 bytes for metadata and 1073741824 bytes for data in healthy node set. Required 3. Found 2.
dn1_1    | 2023-05-31 01:09:25,727 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
scm1_1   | 2023-05-31 01:08:42,914 [IPC Server handler 12 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn1_1    | 2023-05-31 01:09:25,728 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-05-31 01:09:25,730 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
dn4_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
scm1_1   | 2023-05-31 01:08:42,934 [IPC Server handler 33 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn1_1    | 2023-05-31 01:09:25,730 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn1_1    | 2023-05-31 01:09:25,730 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
scm1_1   | 2023-05-31 01:08:42,963 [IPC Server handler 27 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
scm3_1   | 2023-05-31 01:11:53,822 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-31 01:08:42,981 [IPC Server handler 23 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 2023-05-31 01:08:27,574 [grpc-default-executor-4] WARN server.GrpcServerProtocolService: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: Failed requestVote b20d0805-391e-447d-a646-35a5d1a5cbcf->3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#0
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
scm3_1   | 2023-05-31 01:11:58,914 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-31 01:08:43,517 [IPC Server handler 3 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: group-F1A4CF07E444 not found.
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm1_1   | 2023-05-31 01:08:43,535 [IPC Server handler 10 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-05-31 01:08:43,541 [IPC Server handler 0 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn1_1    | 2023-05-31 01:09:25,730 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
scm3_1   | 2023-05-31 01:11:58,915 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-31 01:08:43,562 [IPC Server handler 13 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-05-31 01:08:45,041 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3_1   | 2023-05-31 01:12:03,919 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-31 01:08:50,041 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn1_1    | 2023-05-31 01:09:25,730 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | 2023-05-31 01:08:50,108 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) moved to HEALTHY state.
dn1_1    | 2023-05-31 01:09:25,730 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
scm3_1   | 2023-05-31 01:12:03,920 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:12:08,964 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-05-31 01:09:25,730 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
scm3_1   | 2023-05-31 01:12:08,964 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:12:14,047 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-05-31 01:09:25,730 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
scm3_1   | 2023-05-31 01:12:14,047 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:12:19,178 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-31 01:08:50,109 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
dn1_1    | 2023-05-31 01:09:25,734 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderStateImpl
scm3_1   | 2023-05-31 01:12:19,179 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-05-31 01:12:24,232 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-31 01:08:50,109 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21) moved to HEALTHY state.
dn1_1    | 2023-05-31 01:09:25,736 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
scm3_1   | 2023-05-31 01:12:24,232 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-31 01:08:46,333 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn1_1    | 2023-05-31 01:09:25,746 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-LeaderElection12] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3: set configuration 0: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
scm3_1   | 2023-05-31 01:12:29,301 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-05-31 01:08:46,333 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
scm1_1   | 2023-05-31 01:08:50,109 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
dn1_1    | 2023-05-31 01:09:25,746 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-B2777F51F5A3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/46ae8d39-3c2f-48ac-8229-b2777f51f5a3/current/log_inprogress_0
scm3_1   | 2023-05-31 01:12:29,301 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-31 01:08:46,334 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
scm1_1   | 2023-05-31 01:08:50,109 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=e689613f-6a5f-45fa-9ee1-234a7250e2df to datanode:3da1490a-d610-40de-95c1-bdfc259ba041
dn1_1    | 2023-05-31 01:09:35,950 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
scm3_1   | 2023-05-31 01:12:34,480 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-05-31 01:08:46,334 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
scm1_1   | 2023-05-31 01:08:50,122 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e689613f-6a5f-45fa-9ee1-234a7250e2df, Nodes: 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:50.109Z[UTC]].
dn1_1    | 2023-05-31 01:09:44,640 [PipelineCommandHandlerThread-0] INFO server.RaftServer: b20d0805-391e-447d-a646-35a5d1a5cbcf: addNew group-6155613624A8:[b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] returns group-6155613624A8:java.util.concurrent.CompletableFuture@5844c0a5[Not completed]
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
scm3_1   | 2023-05-31 01:12:34,481 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-31 01:08:46,335 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
dn1_1    | 2023-05-31 01:09:44,644 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf: new RaftServerImpl for group-6155613624A8:[b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
scm1_1   | 2023-05-31 01:08:50,131 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9342e1de-b2fb-494f-91d9-6f942e88ff83 to datanode:97734618-7966-42ac-b50e-e49ead6ce4d2
scm3_1   | 2023-05-31 01:12:39,642 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-05-31 01:12:39,643 [d2fda184-2469-4fa2-af65-75268cb832ba@group-6A492392CC3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
dn1_1    | 2023-05-31 01:09:44,644 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-05-31 01:08:46,336 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
dn4_1    | 2023-05-31 01:08:46,336 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
dn1_1    | 2023-05-31 01:09:44,644 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-05-31 01:08:46,336 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HADOOP_PRC_PORTS_IN_DATANODEDETAILS.
dn4_1    | 2023-05-31 01:08:46,337 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature HADOOP_PRC_PORTS_IN_DATANODEDETAILS has been finalized.
dn4_1    | 2023-05-31 01:08:46,337 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn4_1    | 2023-05-31 01:08:46,337 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
scm1_1   | 2023-05-31 01:08:50,131 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9342e1de-b2fb-494f-91d9-6f942e88ff83 to datanode:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
dn1_1    | 2023-05-31 01:09:44,644 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-05-31 01:08:46,337 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn1_1    | 2023-05-31 01:09:44,644 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn4_1    | 2023-05-31 01:08:46,337 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-05-31 01:08:46,337 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn1_1    | 2023-05-31 01:09:44,644 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-05-31 01:09:44,644 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1_1   | 2023-05-31 01:08:50,131 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9342e1de-b2fb-494f-91d9-6f942e88ff83 to datanode:3da1490a-d610-40de-95c1-bdfc259ba041
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 2023-05-31 01:09:44,645 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8: ConfigurationManager, init=-1: peers:[b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-05-31 01:08:46,337 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm1_1   | 2023-05-31 01:08:50,141 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 9342e1de-b2fb-494f-91d9-6f942e88ff83, Nodes: 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19)3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21)3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:50.131Z[UTC]].
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 2023-05-31 01:09:44,645 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-05-31 01:09:44,645 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1_1   | 2023-05-31 01:08:50,144 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=cabada01-83b4-406c-b6ca-8f00a57da29e to datanode:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
scm1_1   | 2023-05-31 01:08:50,178 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: cabada01-83b4-406c-b6ca-8f00a57da29e, Nodes: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:50.144Z[UTC]].
dn1_1    | 2023-05-31 01:09:44,645 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-05-31 01:08:46,337 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm1_1   | 2023-05-31 01:08:50,188 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5d339866-97f6-4ef6-b080-a59219751d93 to datanode:3da1490a-d610-40de-95c1-bdfc259ba041
scm1_1   | 2023-05-31 01:08:50,190 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5d339866-97f6-4ef6-b080-a59219751d93 to datanode:4bc93e86-8989-4282-b7d8-050f4a19b92f
dn1_1    | 2023-05-31 01:09:44,645 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | 2023-05-31 01:08:50,190 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5d339866-97f6-4ef6-b080-a59219751d93 to datanode:97734618-7966-42ac-b50e-e49ead6ce4d2
dn1_1    | 2023-05-31 01:09:44,645 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn4_1    | 2023-05-31 01:08:46,337 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    | 2023-05-31 01:08:28,094 [grpc-default-executor-4] WARN server.GrpcServerProtocolService: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: Failed requestVote b20d0805-391e-447d-a646-35a5d1a5cbcf->3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#0
scm1_1   | 2023-05-31 01:08:50,197 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 5d339866-97f6-4ef6-b080-a59219751d93, Nodes: 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18)97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:08:50.188Z[UTC]].
dn1_1    | 2023-05-31 01:09:44,646 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-05-31 01:08:46,337 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: group-DC0251522AF1 not found.
dn1_1    | 2023-05-31 01:09:44,646 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1_1   | 2023-05-31 01:08:50,198 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 2.
dn4_1    | 2023-05-31 01:09:10,507 [grpc-default-executor-5] INFO server.RaftServer: 3da1490a-d610-40de-95c1-bdfc259ba041: addNew group-A59219751D93:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER] returns group-A59219751D93:java.util.concurrent.CompletableFuture@3a78fc66[Not completed]
dn4_1    | 2023-05-31 01:09:10,516 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041: new RaftServerImpl for group-A59219751D93:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn1_1    | 2023-05-31 01:09:44,646 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn1_1    | 2023-05-31 01:09:44,647 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-05-31 01:09:44,647 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
scm1_1   | 2023-05-31 01:08:55,042 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn1_1    | 2023-05-31 01:09:44,647 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-05-31 01:09:10,517 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
scm1_1   | 2023-05-31 01:09:00,043 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn4_1    | 2023-05-31 01:09:10,517 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-05-31 01:09:44,647 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1_1   | 2023-05-31 01:09:05,043 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
dn4_1    | 2023-05-31 01:09:10,517 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-05-31 01:09:44,647 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1_1   | 2023-05-31 01:09:09,546 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 46a9d8fb-17d8-4177-820b-4ef30e5fdeea, Nodes: 4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:4bc93e86-8989-4282-b7d8-050f4a19b92f, CreationTimestamp2023-05-31T01:08:41.112Z[UTC]] moved to OPEN state
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
dn4_1    | 2023-05-31 01:09:10,518 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-05-31 01:09:44,647 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm1_1   | 2023-05-31 01:09:09,919 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: b5c5c804-816d-41a2-83d8-297089fabfc6, Nodes: 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:97734618-7966-42ac-b50e-e49ead6ce4d2, CreationTimestamp2023-05-31T01:08:41.141Z[UTC]] moved to OPEN state
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
dn4_1    | 2023-05-31 01:09:10,518 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-05-31 01:09:44,647 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ec1b01cd-6e71-4daf-8419-6155613624a8 does not exist. Creating ...
scm1_1   | 2023-05-31 01:09:10,043 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn5_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
dn4_1    | 2023-05-31 01:09:10,518 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-05-31 01:09:44,649 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ec1b01cd-6e71-4daf-8419-6155613624a8/in_use.lock acquired by nodename 6@2af98bbedaed
scm1_1   | 2023-05-31 01:09:14,110 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17) moved to HEALTHY state.
dn5_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
dn4_1    | 2023-05-31 01:09:10,518 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93: ConfigurationManager, init=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-05-31 01:09:44,653 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ec1b01cd-6e71-4daf-8419-6155613624a8 has been successfully formatted.
scm1_1   | 2023-05-31 01:09:14,111 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
dn1_1    | 2023-05-31 01:09:44,655 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO ratis.ContainerStateMachine: group-6155613624A8: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
scm1_1   | 2023-05-31 01:09:14,112 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ec1b01cd-6e71-4daf-8419-6155613624a8 to datanode:b20d0805-391e-447d-a646-35a5d1a5cbcf
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
dn4_1    | 2023-05-31 01:09:10,518 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-05-31 01:09:44,655 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1_1   | 2023-05-31 01:09:14,124 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: ec1b01cd-6e71-4daf-8419-6155613624a8, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:09:14.112Z[UTC]].
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
dn4_1    | 2023-05-31 01:09:10,523 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-05-31 01:09:10,523 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-05-31 01:09:44,657 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1_1   | 2023-05-31 01:09:14,136 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=46ae8d39-3c2f-48ac-8229-b2777f51f5a3 to datanode:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
dn4_1    | 2023-05-31 01:09:10,524 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-05-31 01:09:44,658 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-05-31 01:09:14,136 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=46ae8d39-3c2f-48ac-8229-b2777f51f5a3 to datanode:b20d0805-391e-447d-a646-35a5d1a5cbcf
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
dn4_1    | 2023-05-31 01:09:10,524 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn1_1    | 2023-05-31 01:09:44,658 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1_1   | 2023-05-31 01:09:14,138 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=46ae8d39-3c2f-48ac-8229-b2777f51f5a3 to datanode:4bc93e86-8989-4282-b7d8-050f4a19b92f
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn4_1    | 2023-05-31 01:09:10,524 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-05-31 01:09:44,658 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-05-31 01:09:44,659 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn4_1    | 2023-05-31 01:09:10,524 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1_1   | 2023-05-31 01:09:14,144 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 46ae8d39-3c2f-48ac-8229-b2777f51f5a3, Nodes: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21)b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17)4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-31T01:09:14.136Z[UTC]].
scm1_1   | 2023-05-31 01:09:14,149 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | 2023-05-31 01:08:32,603 [grpc-default-executor-4] WARN server.GrpcServerProtocolService: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: Failed requestVote b20d0805-391e-447d-a646-35a5d1a5cbcf->3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#0
dn1_1    | 2023-05-31 01:09:44,660 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | 2023-05-31 01:09:15,044 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn4_1    | 2023-05-31 01:09:10,524 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn4_1    | 2023-05-31 01:09:10,529 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm1_1   | 2023-05-31 01:09:15,779 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 5d339866-97f6-4ef6-b080-a59219751d93, Nodes: 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20)4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18)97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:3da1490a-d610-40de-95c1-bdfc259ba041, CreationTimestamp2023-05-31T01:08:50.188Z[UTC]] moved to OPEN state
dn5_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: group-F1A4CF07E444 not found.
dn4_1    | 2023-05-31 01:09:10,531 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-05-31 01:09:44,660 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-05-31 01:09:44,661 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-31 01:09:10,531 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-05-31 01:09:44,662 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO segmented.SegmentedRaftLogWorker: new b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ec1b01cd-6e71-4daf-8419-6155613624a8
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
scm1_1   | 2023-05-31 01:09:16,709 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 9342e1de-b2fb-494f-91d9-6f942e88ff83, Nodes: 97734618-7966-42ac-b50e-e49ead6ce4d2(ha_dn3_1.ha_net/10.9.0.19)3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21)3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, CreationTimestamp2023-05-31T01:08:50.131Z[UTC]] moved to OPEN state
dn4_1    | 2023-05-31 01:09:10,532 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-05-31 01:09:44,662 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
scm1_1   | 2023-05-31 01:09:18,257 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e689613f-6a5f-45fa-9ee1-234a7250e2df, Nodes: 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3da1490a-d610-40de-95c1-bdfc259ba041, CreationTimestamp2023-05-31T01:08:50.109Z[UTC]] moved to OPEN state
dn4_1    | 2023-05-31 01:09:10,532 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-05-31 01:09:44,662 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
scm1_1   | 2023-05-31 01:09:18,492 [SCMBlockDeletingService#0] INFO block.SCMBlockDeletingService: Totally added 18 blocks to be deleted for 3 datanodes, task elapsed time: 3ms
dn4_1    | 2023-05-31 01:09:10,532 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-05-31 01:09:44,662 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
scm1_1   | 2023-05-31 01:09:19,503 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: cabada01-83b4-406c-b6ca-8f00a57da29e, Nodes: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, CreationTimestamp2023-05-31T01:08:50.144Z[UTC]] moved to OPEN state
dn4_1    | 2023-05-31 01:09:10,533 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5d339866-97f6-4ef6-b080-a59219751d93 does not exist. Creating ...
dn1_1    | 2023-05-31 01:09:44,663 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
scm1_1   | 2023-05-31 01:09:20,045 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Open pipeline found after SCM finalization
dn4_1    | 2023-05-31 01:09:10,539 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5d339866-97f6-4ef6-b080-a59219751d93/in_use.lock acquired by nodename 6@955d7ba09e69
dn1_1    | 2023-05-31 01:09:44,663 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
scm1_1   | 2023-05-31 01:09:20,051 [IPC Server handler 24 on default port 9860] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn4_1    | 2023-05-31 01:09:10,542 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5d339866-97f6-4ef6-b080-a59219751d93 has been successfully formatted.
dn1_1    | 2023-05-31 01:09:44,664 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
scm1_1   | 2023-05-31 01:09:25,569 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 46ae8d39-3c2f-48ac-8229-b2777f51f5a3, Nodes: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21)b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17)4bc93e86-8989-4282-b7d8-050f4a19b92f(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:b20d0805-391e-447d-a646-35a5d1a5cbcf, CreationTimestamp2023-05-31T01:09:14.136Z[UTC]] moved to OPEN state
dn4_1    | 2023-05-31 01:09:10,544 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO ratis.ContainerStateMachine: group-A59219751D93: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn1_1    | 2023-05-31 01:09:44,664 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
scm1_1   | 2023-05-31 01:09:44,152 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
dn4_1    | 2023-05-31 01:09:10,550 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-05-31 01:09:44,664 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
scm1_1   | 2023-05-31 01:09:44,856 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: ec1b01cd-6e71-4daf-8419-6155613624a8, Nodes: b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:b20d0805-391e-447d-a646-35a5d1a5cbcf, CreationTimestamp2023-05-31T01:09:14.112Z[UTC]] moved to OPEN state
dn1_1    | 2023-05-31 01:09:44,688 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-05-31 01:09:44,691 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-31 01:09:10,550 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-05-31 01:09:44,828 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
scm1_1   | 2023-05-31 01:09:48,648 [IPC Server handler 73 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.14
dn4_1    | 2023-05-31 01:09:10,551 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-05-31 01:09:44,828 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
scm1_1   | 2023-05-31 01:10:08,730 [IPC Server handler 80 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.14
dn4_1    | 2023-05-31 01:09:10,551 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-05-31 01:09:44,828 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
scm1_1   | 2023-05-31 01:10:14,154 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
dn4_1    | 2023-05-31 01:09:10,551 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn1_1    | 2023-05-31 01:09:44,829 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-05-31 01:09:44,829 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-05-31 01:09:10,551 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn1_1    | 2023-05-31 01:09:44,846 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8: start as a follower, conf=-1: peers:[b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:10:18,508 [SCMBlockDeletingService#0] INFO block.SCMBlockDeletingService: Totally added 18 blocks to be deleted for 3 datanodes, task elapsed time: 12ms
dn4_1    | 2023-05-31 01:09:10,552 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 2023-05-31 01:09:44,857 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm1_1   | 2023-05-31 01:10:23,934 [IPC Server handler 7 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.14
dn4_1    | 2023-05-31 01:09:10,552 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 2023-05-31 01:09:44,857 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-FollowerState
scm1_1   | 2023-05-31 01:10:44,155 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
dn1_1    | 2023-05-31 01:09:44,881 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6155613624A8,id=b20d0805-391e-447d-a646-35a5d1a5cbcf
dn4_1    | 2023-05-31 01:09:10,554 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | 2023-05-31 01:10:46,695 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for containerId, expected lastId is 0, actual lastId is 2000.
dn1_1    | 2023-05-31 01:09:44,881 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-05-31 01:08:33,197 [grpc-default-executor-4] WARN server.GrpcServerProtocolService: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: Failed requestVote b20d0805-391e-447d-a646-35a5d1a5cbcf->3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#0
scm1_1   | 2023-05-31 01:10:46,713 [IPC Server handler 80 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 2000 to 3000.
dn4_1    | 2023-05-31 01:09:10,554 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5d339866-97f6-4ef6-b080-a59219751d93
dn1_1    | 2023-05-31 01:09:44,881 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: group-DC0251522AF1 not found.
scm1_1   | 2023-05-31 01:10:46,768 [7b17d04a-0b5d-44ed-b487-1bf0d959b978@group-6A492392CC3E-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019203000.
dn4_1    | 2023-05-31 01:09:10,555 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-05-31 01:09:44,881 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
scm1_1   | 2023-05-31 01:10:46,794 [IPC Server handler 80 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019203000 to 111677748019204000.
scm1_1   | 2023-05-31 01:11:13,885 [IPC Server handler 7 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.14
dn1_1    | 2023-05-31 01:09:44,881 [b20d0805-391e-447d-a646-35a5d1a5cbcf-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
scm1_1   | 2023-05-31 01:11:14,158 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
scm1_1   | 2023-05-31 01:11:14,443 [ReplicationMonitor] INFO health.QuasiClosedContainerHandler: Force closing container #1 with BCSID 27, which is in QUASI_CLOSED state.
dn1_1    | 2023-05-31 01:09:44,896 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=ec1b01cd-6e71-4daf-8419-6155613624a8
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
scm1_1   | 2023-05-31 01:11:14,448 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1, force: true] for container ContainerInfo{id=#1, state=QUASI_CLOSED, pipelineID=PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1, stateEnterTime=2023-05-31T00:55:57.277Z, owner=om3} to 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21) with datanode deadline 1685496044444 and scm deadline 1685496074444
dn4_1    | 2023-05-31 01:09:10,556 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-05-31 01:09:44,900 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=ec1b01cd-6e71-4daf-8419-6155613624a8.
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
scm1_1   | 2023-05-31 01:11:14,448 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1, force: true] for container ContainerInfo{id=#1, state=QUASI_CLOSED, pipelineID=PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1, stateEnterTime=2023-05-31T00:55:57.277Z, owner=om3} to b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17) with datanode deadline 1685496044448 and scm deadline 1685496074448
dn4_1    | 2023-05-31 01:09:10,559 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-05-31 01:09:44,936 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
scm1_1   | 2023-05-31 01:11:14,449 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1, force: true] for container ContainerInfo{id=#1, state=QUASI_CLOSED, pipelineID=PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1, stateEnterTime=2023-05-31T00:55:57.277Z, owner=om3} to 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) with datanode deadline 1685496044449 and scm deadline 1685496074449
dn4_1    | 2023-05-31 01:09:10,559 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-05-31 01:09:44,938 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
scm1_1   | 2023-05-31 01:11:14,449 [ReplicationMonitor] INFO health.QuasiClosedContainerHandler: Force closing container #2 with BCSID 9, which is in QUASI_CLOSED state.
dn4_1    | 2023-05-31 01:09:10,559 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-05-31 01:09:45,397 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@467045c4] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(0),2(0),3(0),1001(0)], numOfContainers=2, numOfBlocks=6
dn5_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
scm1_1   | 2023-05-31 01:11:14,449 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444, force: true] for container ContainerInfo{id=#2, state=QUASI_CLOSED, pipelineID=PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444, stateEnterTime=2023-05-31T00:56:08.038Z, owner=om3} to b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17) with datanode deadline 1685496044449 and scm deadline 1685496074449
dn4_1    | 2023-05-31 01:09:10,559 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-05-31 01:09:49,965 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-FollowerState] INFO impl.FollowerState: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5107596049ns, electionTimeout:5026ms
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
scm1_1   | 2023-05-31 01:11:14,450 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444, force: true] for container ContainerInfo{id=#2, state=QUASI_CLOSED, pipelineID=PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444, stateEnterTime=2023-05-31T00:56:08.038Z, owner=om3} to 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) with datanode deadline 1685496044449 and scm deadline 1685496074449
dn4_1    | 2023-05-31 01:09:10,560 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-05-31 01:09:49,966 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-FollowerState
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
scm1_1   | 2023-05-31 01:11:14,450 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444, force: true] for container ContainerInfo{id=#2, state=QUASI_CLOSED, pipelineID=PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444, stateEnterTime=2023-05-31T00:56:08.038Z, owner=om3} to 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21) with datanode deadline 1685496044450 and scm deadline 1685496074450
dn4_1    | 2023-05-31 01:09:10,560 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-05-31 01:09:49,966 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-FollowerState] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
scm1_1   | 2023-05-31 01:11:14,450 [ReplicationMonitor] INFO health.QuasiClosedContainerHandler: Force closing container #1001 with BCSID 23, which is in QUASI_CLOSED state.
scm1_1   | 2023-05-31 01:11:14,450 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1001, pipelineID: PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444, force: true] for container ContainerInfo{id=#1001, state=QUASI_CLOSED, pipelineID=PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444, stateEnterTime=2023-05-31T01:04:08.013Z, owner=om1} to b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17) with datanode deadline 1685496044450 and scm deadline 1685496074450
dn1_1    | 2023-05-31 01:09:49,966 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
scm1_1   | 2023-05-31 01:11:14,450 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1001, pipelineID: PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444, force: true] for container ContainerInfo{id=#1001, state=QUASI_CLOSED, pipelineID=PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444, stateEnterTime=2023-05-31T01:04:08.013Z, owner=om1} to 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) with datanode deadline 1685496044450 and scm deadline 1685496074450
dn4_1    | 2023-05-31 01:09:10,563 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-05-31 01:09:49,967 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-FollowerState] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13
scm1_1   | 2023-05-31 01:11:14,451 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1001, pipelineID: PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444, force: true] for container ContainerInfo{id=#1001, state=QUASI_CLOSED, pipelineID=PipelineID=1f8ec4de-a531-4a7c-9881-f1a4cf07e444, stateEnterTime=2023-05-31T01:04:08.013Z, owner=om1} to 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21) with datanode deadline 1685496044451 and scm deadline 1685496074451
dn4_1    | 2023-05-31 01:09:10,567 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
dn1_1    | 2023-05-31 01:09:49,968 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:11:14,451 [ReplicationMonitor] INFO health.QuasiClosedContainerHandler: Force closing container #1002 with BCSID 40, which is in QUASI_CLOSED state.
dn4_1    | 2023-05-31 01:09:10,591 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn1_1    | 2023-05-31 01:09:49,968 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13 PRE_VOTE round 0: result PASSED (term=0)
scm1_1   | 2023-05-31 01:11:14,451 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1002, pipelineID: PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1, force: true] for container ContainerInfo{id=#1002, state=QUASI_CLOSED, pipelineID=PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1, stateEnterTime=2023-05-31T01:04:21.663Z, owner=om1} to 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd(ha_dn5_1.ha_net/10.9.0.21) with datanode deadline 1685496044451 and scm deadline 1685496074451
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn4_1    | 2023-05-31 01:09:10,591 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-05-31 01:09:10,592 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm1_1   | 2023-05-31 01:11:14,451 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1002, pipelineID: PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1, force: true] for container ContainerInfo{id=#1002, state=QUASI_CLOSED, pipelineID=PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1, stateEnterTime=2023-05-31T01:04:21.663Z, owner=om1} to b20d0805-391e-447d-a646-35a5d1a5cbcf(ha_dn1_1.ha_net/10.9.0.17) with datanode deadline 1685496044451 and scm deadline 1685496074451
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 2023-05-31 01:09:49,969 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13 ELECTION round 0: submit vote requests at term 1 for -1: peers:[b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:09:10,592 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-05-31 01:11:14,452 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1002, pipelineID: PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1, force: true] for container ContainerInfo{id=#1002, state=QUASI_CLOSED, pipelineID=PipelineID=a4fe2088-9059-47e2-9c65-dc0251522af1, stateEnterTime=2023-05-31T01:04:21.663Z, owner=om1} to 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) with datanode deadline 1685496044452 and scm deadline 1685496074452
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | 2023-05-31 01:08:35,010 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm1_1   | 2023-05-31 01:11:14,454 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 31 milliseconds for processing 5 containers.
dn5_1    | 2023-05-31 01:08:37,803 [grpc-default-executor-4] WARN server.GrpcServerProtocolService: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: Failed requestVote b20d0805-391e-447d-a646-35a5d1a5cbcf->3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#0
dn1_1    | 2023-05-31 01:09:49,970 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13] INFO impl.LeaderElection: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13 ELECTION round 0: result PASSED (term=1)
dn4_1    | 2023-05-31 01:09:10,594 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-05-31 01:11:20,414 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
dn5_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: group-F1A4CF07E444 not found.
dn1_1    | 2023-05-31 01:09:49,970 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: shutdown b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13
dn1_1    | 2023-05-31 01:09:49,970 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm1_1   | 2023-05-31 01:11:20,447 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn1_1    | 2023-05-31 01:09:49,970 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6155613624A8 with new leaderId: b20d0805-391e-447d-a646-35a5d1a5cbcf
dn4_1    | 2023-05-31 01:09:10,600 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93: start as a follower, conf=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-31 01:11:20,484 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn1_1    | 2023-05-31 01:09:49,970 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8: change Leader from null to b20d0805-391e-447d-a646-35a5d1a5cbcf at term 1 for becomeLeader, leader elected after 5324ms
dn4_1    | 2023-05-31 01:09:10,600 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm1_1   | 2023-05-31 01:11:20,501 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1002 to CLOSED state, datanode 3da1490a-d610-40de-95c1-bdfc259ba041(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
dn1_1    | 2023-05-31 01:09:49,971 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1_1   | 2023-05-31 01:11:44,161 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
dn4_1    | 2023-05-31 01:09:10,600 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: start 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-FollowerState
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
dn1_1    | 2023-05-31 01:09:49,972 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm1_1   | 2023-05-31 01:12:14,162 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
dn4_1    | 2023-05-31 01:09:10,601 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-A59219751D93,id=3da1490a-d610-40de-95c1-bdfc259ba041
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
dn1_1    | 2023-05-31 01:09:49,972 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn4_1    | 2023-05-31 01:09:10,602 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
dn1_1    | 2023-05-31 01:09:49,975 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn5_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
dn4_1    | 2023-05-31 01:09:10,604 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
dn4_1    | 2023-05-31 01:09:10,604 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-05-31 01:09:49,975 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
dn4_1    | 2023-05-31 01:09:10,604 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-05-31 01:09:49,976 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn4_1    | 2023-05-31 01:09:10,614 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
dn1_1    | 2023-05-31 01:09:49,976 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-05-31 01:09:10,631 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
dn1_1    | 2023-05-31 01:09:49,976 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn1_1    | 2023-05-31 01:09:49,976 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13] INFO impl.RoleInfo: b20d0805-391e-447d-a646-35a5d1a5cbcf: start b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderStateImpl
dn1_1    | 2023-05-31 01:09:49,977 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-05-31 01:09:11,031 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-3da1490a-d610-40de-95c1-bdfc259ba041: Detected pause in JVM or host machine approximately 0.255s with 0.348s GC time.
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
dn1_1    | 2023-05-31 01:09:49,979 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ec1b01cd-6e71-4daf-8419-6155613624a8/current/log_inprogress_0
dn4_1    | GC pool 'ParNew' had collection(s): count=1 time=348ms
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn1_1    | 2023-05-31 01:09:50,003 [b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8-LeaderElection13] INFO server.RaftServer$Division: b20d0805-391e-447d-a646-35a5d1a5cbcf@group-6155613624A8: set configuration 0: peers:[b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:09:11,809 [grpc-default-executor-5] INFO server.RaftServer: 3da1490a-d610-40de-95c1-bdfc259ba041: addNew group-6F942E88FF83:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] returns group-6F942E88FF83:java.util.concurrent.CompletableFuture@2631dc24[Not completed]
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn1_1    | 2023-05-31 01:10:21,457 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@467045c4] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(1),2(1),3(1),1001(1)], numOfContainers=2, numOfBlocks=6
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 2023-05-31 01:09:11,810 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041: new RaftServerImpl for group-6F942E88FF83:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn4_1    | 2023-05-31 01:09:11,811 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | 2023-05-31 01:08:38,380 [grpc-default-executor-4] WARN server.GrpcServerProtocolService: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: Failed requestVote b20d0805-391e-447d-a646-35a5d1a5cbcf->3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#0
dn1_1    | 2023-05-31 01:10:21,458 [DeleteBlocksCommandHandlerThread-0] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 1 is either received out of order or retried, 2 <= 1001
dn1_1    | 2023-05-31 01:10:21,458 [DeleteBlocksCommandHandlerThread-1] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 2 is either received out of order or retried, 3 <= 3
dn5_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: group-DC0251522AF1 not found.
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
dn1_1    | 2023-05-31 01:10:21,460 [DeleteBlocksCommandHandlerThread-4] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 1 is either received out of order or retried, 1 <= 1001
dn4_1    | 2023-05-31 01:09:11,811 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
dn5_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
dn4_1    | 2023-05-31 01:09:11,811 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-05-31 01:09:11,811 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
dn4_1    | 2023-05-31 01:09:11,812 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-05-31 01:09:11,812 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
dn1_1    | 2023-05-31 01:10:21,460 [DeleteBlocksCommandHandlerThread-2] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 1 is either received out of order or retried, 1001 <= 1001
dn1_1    | 2023-05-31 01:10:35,951 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-05-31 01:11:20,971 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 27.
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 2023-05-31 01:09:11,812 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83: ConfigurationManager, init=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-05-31 01:11:20,973 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 27.
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | 2023-05-31 01:08:46,161 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-05-31 01:09:11,812 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-05-31 01:09:11,813 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-05-31 01:08:46,161 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn5_1    | 2023-05-31 01:08:46,161 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn5_1    | 2023-05-31 01:08:46,161 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
dn5_1    | 2023-05-31 01:08:46,163 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
dn5_1    | 2023-05-31 01:08:46,163 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
dn5_1    | 2023-05-31 01:08:46,164 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
dn5_1    | 2023-05-31 01:08:46,164 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HADOOP_PRC_PORTS_IN_DATANODEDETAILS.
dn1_1    | 2023-05-31 01:11:20,989 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is closed with bcsId 27.
dn1_1    | 2023-05-31 01:11:20,991 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 9.
dn5_1    | 2023-05-31 01:08:46,165 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature HADOOP_PRC_PORTS_IN_DATANODEDETAILS has been finalized.
dn5_1    | 2023-05-31 01:08:46,165 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn4_1    | 2023-05-31 01:09:11,813 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-05-31 01:11:20,991 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 9.
dn5_1    | 2023-05-31 01:08:46,165 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn1_1    | 2023-05-31 01:11:21,024 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is closed with bcsId 9.
dn1_1    | 2023-05-31 01:11:21,037 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 23.
dn4_1    | 2023-05-31 01:09:11,813 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-05-31 01:09:11,813 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn4_1    | 2023-05-31 01:09:11,813 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-05-31 01:09:11,813 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-05-31 01:09:11,813 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn4_1    | 2023-05-31 01:09:11,814 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-05-31 01:09:11,814 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-05-31 01:11:21,038 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 23.
dn1_1    | 2023-05-31 01:11:21,043 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1001 is closed with bcsId 23.
dn1_1    | 2023-05-31 01:11:21,043 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 40.
dn1_1    | 2023-05-31 01:11:21,043 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 40.
dn1_1    | 2023-05-31 01:11:21,047 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1002 is closed with bcsId 40.
dn1_1    | 2023-05-31 01:11:35,956 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 12/4988 blocks from 2 candidate containers.
dn1_1    | 2023-05-31 01:11:35,977 [BlockDeletingService#0] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/containerDir0/2/chunks/111677748019200002.block
dn1_1    | 2023-05-31 01:11:35,977 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/containerDir0/1/chunks/111677748019200001.block
dn1_1    | 2023-05-31 01:11:35,979 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/containerDir0/1/chunks/111677748019201001.block
dn1_1    | 2023-05-31 01:11:35,980 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/containerDir0/1/chunks/111677748019201002.block
dn5_1    | 2023-05-31 01:08:46,165 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    | 2023-05-31 01:08:46,165 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    | 2023-05-31 01:08:46,165 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-05-31 01:09:11,817 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-05-31 01:09:11,817 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-05-31 01:09:11,817 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-05-31 01:09:11,817 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-05-31 01:09:11,818 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/9342e1de-b2fb-494f-91d9-6f942e88ff83 does not exist. Creating ...
dn5_1    | 2023-05-31 01:08:46,165 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    | 2023-05-31 01:08:46,166 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    | 2023-05-31 01:08:46,166 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    | 2023-05-31 01:08:46,166 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    | 2023-05-31 01:08:46,166 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    | 2023-05-31 01:09:11,246 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: new RaftServerImpl for group-6F942E88FF83:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn5_1    | 2023-05-31 01:09:11,247 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-05-31 01:09:11,247 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-05-31 01:09:11,247 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-05-31 01:09:11,819 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/9342e1de-b2fb-494f-91d9-6f942e88ff83/in_use.lock acquired by nodename 6@955d7ba09e69
dn1_1    | 2023-05-31 01:11:35,981 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/containerDir0/1/chunks/111677748019201003.block
dn1_1    | 2023-05-31 01:11:35,983 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/containerDir0/1/chunks/111677748019200003.block
dn5_1    | 2023-05-31 01:09:11,247 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-05-31 01:09:11,821 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/9342e1de-b2fb-494f-91d9-6f942e88ff83 has been successfully formatted.
dn5_1    | 2023-05-31 01:09:11,247 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-05-31 01:12:35,971 [BlockDeletingService#4] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 6/4994 blocks from 2 candidate containers.
dn1_1    | 2023-05-31 01:12:35,972 [BlockDeletingService#2] INFO background.BlockDeletingService: No transaction found in container 1 with pending delete block count 5
dn1_1    | 2023-05-31 01:12:35,972 [BlockDeletingService#4] INFO background.BlockDeletingService: No transaction found in container 2 with pending delete block count 1
dn4_1    | 2023-05-31 01:09:11,821 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO ratis.ContainerStateMachine: group-6F942E88FF83: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn4_1    | 2023-05-31 01:09:11,821 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-05-31 01:09:11,247 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-05-31 01:09:11,821 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-05-31 01:09:11,247 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83: ConfigurationManager, init=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-05-31 01:09:11,248 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-05-31 01:09:11,821 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:09:11,250 [grpc-default-executor-4] INFO server.RaftServer: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: addNew group-6F942E88FF83:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] returns group-6F942E88FF83:java.util.concurrent.CompletableFuture@3063b765[Not completed]
dn4_1    | 2023-05-31 01:09:11,822 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-05-31 01:09:11,258 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-05-31 01:09:11,822 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-05-31 01:09:11,258 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-05-31 01:09:11,839 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-05-31 01:09:11,258 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-05-31 01:09:11,839 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-05-31 01:09:11,258 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn4_1    | 2023-05-31 01:09:11,839 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-05-31 01:09:11,260 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-05-31 01:09:11,839 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:09:11,261 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-05-31 01:09:11,840 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/9342e1de-b2fb-494f-91d9-6f942e88ff83
dn5_1    | 2023-05-31 01:09:11,262 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn4_1    | 2023-05-31 01:09:11,841 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-05-31 01:09:11,268 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-05-31 01:09:11,841 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-05-31 01:09:11,268 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-05-31 01:09:11,842 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-05-31 01:09:11,842 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-05-31 01:09:11,268 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-05-31 01:09:11,842 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-05-31 01:09:11,268 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-05-31 01:09:11,842 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-05-31 01:09:11,268 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-05-31 01:09:11,843 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-05-31 01:09:11,268 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-05-31 01:09:11,843 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-05-31 01:09:11,269 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/9342e1de-b2fb-494f-91d9-6f942e88ff83 does not exist. Creating ...
dn4_1    | 2023-05-31 01:09:11,843 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-05-31 01:09:11,845 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:09:11,272 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/9342e1de-b2fb-494f-91d9-6f942e88ff83/in_use.lock acquired by nodename 7@51bd9aa00c0e
dn4_1    | 2023-05-31 01:09:11,849 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-05-31 01:09:11,849 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-05-31 01:09:11,276 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/9342e1de-b2fb-494f-91d9-6f942e88ff83 has been successfully formatted.
dn4_1    | 2023-05-31 01:09:11,849 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-05-31 01:09:11,289 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO ratis.ContainerStateMachine: group-6F942E88FF83: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn4_1    | 2023-05-31 01:09:11,849 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-05-31 01:09:11,293 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-05-31 01:09:11,294 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-05-31 01:09:11,298 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:09:11,298 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-05-31 01:09:11,298 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-05-31 01:09:11,299 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-05-31 01:09:11,304 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-05-31 01:09:11,304 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-05-31 01:09:11,304 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:09:11,305 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/9342e1de-b2fb-494f-91d9-6f942e88ff83
dn5_1    | 2023-05-31 01:09:11,305 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-05-31 01:09:11,306 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-05-31 01:09:11,306 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-05-31 01:09:11,306 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-05-31 01:09:11,307 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-05-31 01:09:11,307 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-05-31 01:09:11,307 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-05-31 01:09:11,308 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-05-31 01:09:11,329 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-05-31 01:09:11,334 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:09:11,427 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-05-31 01:09:11,427 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-05-31 01:09:11,427 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-05-31 01:09:11,428 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-05-31 01:09:11,428 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-05-31 01:09:11,442 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83: start as a follower, conf=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:09:11,475 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn5_1    | 2023-05-31 01:09:11,476 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: start 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-FollowerState
dn5_1    | 2023-05-31 01:09:11,491 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6F942E88FF83,id=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn5_1    | 2023-05-31 01:09:11,492 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-05-31 01:09:11,492 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-05-31 01:09:11,492 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-05-31 01:09:11,492 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-05-31 01:09:11,498 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-05-31 01:09:11,508 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-05-31 01:09:15,422 [grpc-default-executor-4] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83: receive requestVote(PRE_VOTE, 97734618-7966-42ac-b50e-e49ead6ce4d2, group-6F942E88FF83, 0, (t:0, i:0))
dn5_1    | 2023-05-31 01:09:15,422 [grpc-default-executor-4] INFO impl.VoteContext: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-FOLLOWER: reject PRE_VOTE from 97734618-7966-42ac-b50e-e49ead6ce4d2: our priority 1 > candidate's priority 0
dn5_1    | 2023-05-31 01:09:15,423 [grpc-default-executor-4] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83 replies to PRE_VOTE vote request: 97734618-7966-42ac-b50e-e49ead6ce4d2<-3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#0:FAIL-t0. Peer's state: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83:t0, leader=null, voted=, raftlog=Memoized:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:09:16,651 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-FollowerState] INFO impl.FollowerState: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5175494653ns, electionTimeout:5143ms
dn5_1    | 2023-05-31 01:09:16,652 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-FollowerState] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: shutdown 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-FollowerState
dn5_1    | 2023-05-31 01:09:16,652 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-FollowerState] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn5_1    | 2023-05-31 01:09:16,652 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-05-31 01:09:16,652 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-FollowerState] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: start 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4
dn5_1    | 2023-05-31 01:09:16,653 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:09:16,654 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-05-31 01:09:16,654 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4-1] INFO server.GrpcServerProtocolClient: Build channel for 97734618-7966-42ac-b50e-e49ead6ce4d2
dn5_1    | 2023-05-31 01:09:16,654 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-05-31 01:09:16,677 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn5_1    | 2023-05-31 01:09:16,677 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO impl.LeaderElection:   Response 0: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd<-3da1490a-d610-40de-95c1-bdfc259ba041#0:OK-t0
dn5_1    | 2023-05-31 01:09:16,677 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4 PRE_VOTE round 0: result PASSED
dn5_1    | 2023-05-31 01:09:16,681 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4 ELECTION round 0: submit vote requests at term 1 for -1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:09:16,682 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-05-31 01:09:16,682 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-05-31 01:09:16,696 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn5_1    | 2023-05-31 01:09:16,696 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO impl.LeaderElection:   Response 0: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd<-3da1490a-d610-40de-95c1-bdfc259ba041#0:OK-t1
dn5_1    | 2023-05-31 01:09:16,696 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4 ELECTION round 0: result PASSED
dn5_1    | 2023-05-31 01:09:16,696 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: shutdown 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4
dn5_1    | 2023-05-31 01:09:16,696 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn5_1    | 2023-05-31 01:09:16,696 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6F942E88FF83 with new leaderId: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn5_1    | 2023-05-31 01:09:16,696 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83: change Leader from null to 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd at term 1 for becomeLeader, leader elected after 5437ms
dn5_1    | 2023-05-31 01:09:16,696 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn5_1    | 2023-05-31 01:09:16,697 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-05-31 01:09:16,697 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn5_1    | 2023-05-31 01:09:16,697 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn5_1    | 2023-05-31 01:09:16,697 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 2023-05-31 01:09:16,697 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn5_1    | 2023-05-31 01:09:16,697 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-05-31 01:09:16,697 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn5_1    | 2023-05-31 01:09:16,705 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn5_1    | 2023-05-31 01:09:16,705 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:09:16,705 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn5_1    | 2023-05-31 01:09:16,705 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn5_1    | 2023-05-31 01:09:16,705 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn5_1    | 2023-05-31 01:09:16,715 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-05-31 01:09:16,715 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn5_1    | 2023-05-31 01:09:16,715 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn5_1    | 2023-05-31 01:09:16,715 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-05-31 01:09:16,715 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-05-31 01:09:16,722 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn4_1    | 2023-05-31 01:09:11,849 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-05-31 01:09:11,850 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83: start as a follower, conf=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:09:11,850 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn4_1    | 2023-05-31 01:09:11,850 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: start 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83-FollowerState
dn4_1    | 2023-05-31 01:09:11,851 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6F942E88FF83,id=3da1490a-d610-40de-95c1-bdfc259ba041
dn4_1    | 2023-05-31 01:09:11,851 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-05-31 01:09:11,851 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-05-31 01:09:11,851 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-05-31 01:09:11,851 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-05-31 01:09:11,852 [3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-05-31 01:09:11,859 [3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-31 01:09:14,839 [grpc-default-executor-5] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93: receive requestVote(PRE_VOTE, 4bc93e86-8989-4282-b7d8-050f4a19b92f, group-A59219751D93, 0, (t:0, i:0))
dn4_1    | 2023-05-31 01:09:14,840 [grpc-default-executor-5] INFO impl.VoteContext: 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-FOLLOWER: reject PRE_VOTE from 4bc93e86-8989-4282-b7d8-050f4a19b92f: our priority 1 > candidate's priority 0
dn4_1    | 2023-05-31 01:09:14,840 [grpc-default-executor-5] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93 replies to PRE_VOTE vote request: 4bc93e86-8989-4282-b7d8-050f4a19b92f<-3da1490a-d610-40de-95c1-bdfc259ba041#0:FAIL-t0. Peer's state: 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93:t0, leader=null, voted=, raftlog=Memoized:3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:09:15,406 [grpc-default-executor-5] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83: receive requestVote(PRE_VOTE, 97734618-7966-42ac-b50e-e49ead6ce4d2, group-6F942E88FF83, 0, (t:0, i:0))
dn4_1    | 2023-05-31 01:09:15,407 [grpc-default-executor-5] INFO impl.VoteContext: 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83-FOLLOWER: accept PRE_VOTE from 97734618-7966-42ac-b50e-e49ead6ce4d2: our priority 0 <= candidate's priority 0
dn4_1    | 2023-05-31 01:09:15,407 [grpc-default-executor-5] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83 replies to PRE_VOTE vote request: 97734618-7966-42ac-b50e-e49ead6ce4d2<-3da1490a-d610-40de-95c1-bdfc259ba041#0:OK-t0. Peer's state: 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83:t0, leader=null, voted=, raftlog=Memoized:3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:09:15,714 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-FollowerState] INFO impl.FollowerState: 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5113662627ns, electionTimeout:5074ms
dn4_1    | 2023-05-31 01:09:15,714 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-FollowerState] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: shutdown 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-FollowerState
dn4_1    | 2023-05-31 01:09:15,714 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-FollowerState] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn4_1    | 2023-05-31 01:09:15,715 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | 2023-05-31 01:09:15,715 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-FollowerState] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: start 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4
dn4_1    | 2023-05-31 01:09:15,715 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO impl.LeaderElection: 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:09:15,717 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4-1] INFO server.GrpcServerProtocolClient: Build channel for 97734618-7966-42ac-b50e-e49ead6ce4d2
dn4_1    | 2023-05-31 01:09:15,724 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-05-31 01:09:15,730 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-31 01:09:15,731 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4-2] INFO server.GrpcServerProtocolClient: Build channel for 4bc93e86-8989-4282-b7d8-050f4a19b92f
dn4_1    | 2023-05-31 01:09:15,739 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO impl.LeaderElection: 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn4_1    | 2023-05-31 01:09:15,739 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO impl.LeaderElection:   Response 0: 3da1490a-d610-40de-95c1-bdfc259ba041<-97734618-7966-42ac-b50e-e49ead6ce4d2#0:OK-t0
dn4_1    | 2023-05-31 01:09:15,739 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO impl.LeaderElection: 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4 PRE_VOTE round 0: result PASSED
dn4_1    | 2023-05-31 01:09:15,742 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO impl.LeaderElection: 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4 ELECTION round 0: submit vote requests at term 1 for -1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:09:15,743 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-05-31 01:09:15,743 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-31 01:09:15,756 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO impl.LeaderElection: 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn4_1    | 2023-05-31 01:09:15,756 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO impl.LeaderElection:   Response 0: 3da1490a-d610-40de-95c1-bdfc259ba041<-97734618-7966-42ac-b50e-e49ead6ce4d2#0:OK-t1
dn4_1    | 2023-05-31 01:09:15,756 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO impl.LeaderElection: 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4 ELECTION round 0: result PASSED
dn4_1    | 2023-05-31 01:09:15,757 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: shutdown 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4
dn4_1    | 2023-05-31 01:09:15,757 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn4_1    | 2023-05-31 01:09:15,757 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-A59219751D93 with new leaderId: 3da1490a-d610-40de-95c1-bdfc259ba041
dn4_1    | 2023-05-31 01:09:15,757 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93: change Leader from null to 3da1490a-d610-40de-95c1-bdfc259ba041 at term 1 for becomeLeader, leader elected after 5233ms
dn4_1    | 2023-05-31 01:09:15,758 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn4_1    | 2023-05-31 01:09:15,758 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-05-31 01:09:15,758 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn4_1    | 2023-05-31 01:09:15,758 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn4_1    | 2023-05-31 01:09:15,758 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn4_1    | 2023-05-31 01:09:15,758 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn4_1    | 2023-05-31 01:09:15,759 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-05-31 01:09:15,759 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn4_1    | 2023-05-31 01:09:15,759 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn4_1    | 2023-05-31 01:09:15,759 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-31 01:09:15,759 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn4_1    | 2023-05-31 01:09:15,760 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn4_1    | 2023-05-31 01:09:15,760 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn4_1    | 2023-05-31 01:09:15,769 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-05-31 01:09:15,769 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn4_1    | 2023-05-31 01:09:15,769 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn4_1    | 2023-05-31 01:09:15,769 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-05-31 01:09:15,769 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-05-31 01:09:15,796 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn4_1    | 2023-05-31 01:09:15,797 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-31 01:09:15,797 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn4_1    | 2023-05-31 01:09:15,797 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn4_1    | 2023-05-31 01:09:15,797 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn4_1    | 2023-05-31 01:09:15,797 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-05-31 01:09:15,797 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn4_1    | 2023-05-31 01:09:15,798 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn4_1    | 2023-05-31 01:09:15,798 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-05-31 01:09:15,798 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-05-31 01:09:15,805 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: start 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderStateImpl
dn4_1    | 2023-05-31 01:09:15,806 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-05-31 01:09:15,808 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5d339866-97f6-4ef6-b080-a59219751d93/current/log_inprogress_0
dn4_1    | 2023-05-31 01:09:15,832 [3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93-LeaderElection4] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-A59219751D93: set configuration 0: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:09:16,658 [grpc-default-executor-5] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83: receive requestVote(PRE_VOTE, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, group-6F942E88FF83, 0, (t:0, i:0))
dn4_1    | 2023-05-31 01:09:16,659 [grpc-default-executor-5] INFO impl.VoteContext: 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83-FOLLOWER: accept PRE_VOTE from 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: our priority 0 <= candidate's priority 1
dn4_1    | 2023-05-31 01:09:16,659 [grpc-default-executor-5] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83 replies to PRE_VOTE vote request: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd<-3da1490a-d610-40de-95c1-bdfc259ba041#0:OK-t0. Peer's state: 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83:t0, leader=null, voted=, raftlog=Memoized:3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:09:16,686 [grpc-default-executor-5] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83: receive requestVote(ELECTION, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, group-6F942E88FF83, 1, (t:0, i:0))
dn4_1    | 2023-05-31 01:09:16,686 [grpc-default-executor-5] INFO impl.VoteContext: 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83-FOLLOWER: accept ELECTION from 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: our priority 0 <= candidate's priority 1
dn4_1    | 2023-05-31 01:09:16,687 [grpc-default-executor-5] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn4_1    | 2023-05-31 01:09:16,687 [grpc-default-executor-5] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: shutdown 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83-FollowerState
dn4_1    | 2023-05-31 01:09:16,688 [3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83-FollowerState] INFO impl.FollowerState: 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83-FollowerState was interrupted
dn4_1    | 2023-05-31 01:09:16,688 [grpc-default-executor-5] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: start 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83-FollowerState
dn4_1    | 2023-05-31 01:09:16,693 [grpc-default-executor-5] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83 replies to ELECTION vote request: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd<-3da1490a-d610-40de-95c1-bdfc259ba041#0:OK-t1. Peer's state: 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83:t1, leader=null, voted=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd, raftlog=Memoized:3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:09:16,765 [3da1490a-d610-40de-95c1-bdfc259ba041-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6F942E88FF83 with new leaderId: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn4_1    | 2023-05-31 01:09:16,765 [3da1490a-d610-40de-95c1-bdfc259ba041-server-thread1] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83: change Leader from null to 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd at term 1 for appendEntries, leader elected after 4952ms
dn4_1    | 2023-05-31 01:09:16,772 [3da1490a-d610-40de-95c1-bdfc259ba041-server-thread2] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83: set configuration 0: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:09:16,773 [3da1490a-d610-40de-95c1-bdfc259ba041-server-thread2] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-05-31 01:09:16,776 [3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-6F942E88FF83-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9342e1de-b2fb-494f-91d9-6f942e88ff83/current/log_inprogress_0
dn4_1    | 2023-05-31 01:09:18,232 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 3da1490a-d610-40de-95c1-bdfc259ba041: addNew group-234A7250E2DF:[3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] returns group-234A7250E2DF:java.util.concurrent.CompletableFuture@361013fd[Not completed]
dn4_1    | 2023-05-31 01:09:18,236 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041: new RaftServerImpl for group-234A7250E2DF:[3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn4_1    | 2023-05-31 01:09:18,236 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-05-31 01:09:18,236 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-05-31 01:09:18,236 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-05-31 01:09:18,236 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-05-31 01:09:18,236 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-05-31 01:09:18,236 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-05-31 01:09:18,236 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF: ConfigurationManager, init=-1: peers:[3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-05-31 01:09:18,236 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-05-31 01:09:18,237 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-05-31 01:09:18,237 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-05-31 01:09:18,237 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-05-31 01:09:18,237 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn4_1    | 2023-05-31 01:09:18,237 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-05-31 01:09:18,237 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-05-31 01:09:18,237 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn4_1    | 2023-05-31 01:09:18,240 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-05-31 01:09:18,240 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-05-31 01:09:18,240 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-05-31 01:09:18,240 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-05-31 01:09:18,240 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-05-31 01:09:18,240 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-05-31 01:09:18,240 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/e689613f-6a5f-45fa-9ee1-234a7250e2df does not exist. Creating ...
dn4_1    | 2023-05-31 01:09:18,245 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e689613f-6a5f-45fa-9ee1-234a7250e2df/in_use.lock acquired by nodename 6@955d7ba09e69
dn4_1    | 2023-05-31 01:09:18,247 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/e689613f-6a5f-45fa-9ee1-234a7250e2df has been successfully formatted.
dn4_1    | 2023-05-31 01:09:18,248 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO ratis.ContainerStateMachine: group-234A7250E2DF: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn4_1    | 2023-05-31 01:09:18,248 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-05-31 01:09:18,248 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-05-31 01:09:18,249 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-31 01:09:18,249 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-05-31 01:09:18,249 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-05-31 01:09:18,249 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-05-31 01:09:18,269 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-05-31 01:09:18,269 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-05-31 01:09:18,278 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-31 01:09:18,278 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/e689613f-6a5f-45fa-9ee1-234a7250e2df
dn4_1    | 2023-05-31 01:09:18,279 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-05-31 01:09:18,279 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-05-31 01:09:18,279 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-05-31 01:09:18,280 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-05-31 01:09:18,280 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-05-31 01:09:18,281 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-05-31 01:09:18,281 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-05-31 01:09:18,281 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-05-31 01:09:18,281 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-05-31 01:09:18,289 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-31 01:09:18,359 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-05-31 01:09:18,359 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-05-31 01:09:18,359 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-05-31 01:09:16,722 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:09:16,722 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn5_1    | 2023-05-31 01:09:16,724 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn5_1    | 2023-05-31 01:09:16,724 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn5_1    | 2023-05-31 01:09:16,725 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-05-31 01:09:16,725 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn5_1    | 2023-05-31 01:09:16,725 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn5_1    | 2023-05-31 01:09:16,725 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-05-31 01:09:16,725 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-05-31 01:09:16,725 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: start 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderStateImpl
dn5_1    | 2023-05-31 01:09:16,725 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 2023-05-31 01:09:16,735 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9342e1de-b2fb-494f-91d9-6f942e88ff83/current/log_inprogress_0
dn5_1    | 2023-05-31 01:09:16,749 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83-LeaderElection4] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-6F942E88FF83: set configuration 0: peers:[97734618-7966-42ac-b50e-e49ead6ce4d2|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:09:19,489 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: new RaftServerImpl for group-8F00A57DA29E:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn5_1    | 2023-05-31 01:09:19,489 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-05-31 01:09:19,489 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-05-31 01:09:19,489 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-05-31 01:09:19,489 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-05-31 01:09:19,489 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-05-31 01:09:19,489 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-05-31 01:09:19,490 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E: ConfigurationManager, init=-1: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-05-31 01:09:19,490 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-05-31 01:09:19,490 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-05-31 01:09:19,490 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-05-31 01:09:19,490 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-05-31 01:09:19,490 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn5_1    | 2023-05-31 01:09:19,490 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-05-31 01:09:19,490 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-05-31 01:09:19,491 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn5_1    | 2023-05-31 01:09:19,491 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-05-31 01:09:19,491 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-05-31 01:09:19,491 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-05-31 01:09:19,491 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-05-31 01:09:19,492 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-05-31 01:09:19,492 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-05-31 01:09:19,492 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: addNew group-8F00A57DA29E:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER] returns      null 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E:t0, leader=null, voted=null, raftlog=UNINITIALIZED, conf=-1: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null NEW
dn5_1    | 2023-05-31 01:09:19,492 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/cabada01-83b4-406c-b6ca-8f00a57da29e does not exist. Creating ...
dn5_1    | 2023-05-31 01:09:19,494 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/cabada01-83b4-406c-b6ca-8f00a57da29e/in_use.lock acquired by nodename 7@51bd9aa00c0e
dn5_1    | 2023-05-31 01:09:19,495 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/cabada01-83b4-406c-b6ca-8f00a57da29e has been successfully formatted.
dn5_1    | 2023-05-31 01:09:19,496 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO ratis.ContainerStateMachine: group-8F00A57DA29E: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn5_1    | 2023-05-31 01:09:19,496 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-05-31 01:09:19,496 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-05-31 01:09:19,497 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:09:19,497 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-05-31 01:09:19,497 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-05-31 01:09:19,497 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-05-31 01:09:19,519 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-05-31 01:09:19,519 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-05-31 01:09:19,520 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:09:19,520 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/cabada01-83b4-406c-b6ca-8f00a57da29e
dn5_1    | 2023-05-31 01:09:19,520 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-05-31 01:09:19,520 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-05-31 01:09:19,520 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-05-31 01:09:19,520 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-05-31 01:09:19,520 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-05-31 01:09:19,520 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-05-31 01:09:19,520 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-05-31 01:09:19,520 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-05-31 01:09:19,521 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-05-31 01:09:19,528 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:09:19,706 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-05-31 01:09:19,706 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-05-31 01:09:19,706 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-05-31 01:09:19,708 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-05-31 01:09:19,708 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-05-31 01:09:19,709 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E: start as a follower, conf=-1: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:09:19,709 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn5_1    | 2023-05-31 01:09:19,710 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: start 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-FollowerState
dn5_1    | 2023-05-31 01:09:19,710 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8F00A57DA29E,id=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn5_1    | 2023-05-31 01:09:19,710 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-05-31 01:09:19,710 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-05-31 01:09:19,711 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-05-31 01:09:19,711 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-05-31 01:09:19,711 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-05-31 01:09:19,712 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=cabada01-83b4-406c-b6ca-8f00a57da29e
dn5_1    | 2023-05-31 01:09:19,715 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=cabada01-83b4-406c-b6ca-8f00a57da29e.
dn5_1    | 2023-05-31 01:09:19,716 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: addNew group-B2777F51F5A3:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] returns group-B2777F51F5A3:java.util.concurrent.CompletableFuture@28d31fd9[Not completed]
dn5_1    | 2023-05-31 01:09:19,718 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: new RaftServerImpl for group-B2777F51F5A3:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn5_1    | 2023-05-31 01:09:19,719 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-05-31 01:09:19,719 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-05-31 01:09:19,719 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-05-31 01:09:19,719 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-05-31 01:09:19,719 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-05-31 01:09:19,719 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-05-31 01:09:19,720 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3: ConfigurationManager, init=-1: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-05-31 01:09:19,720 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-05-31 01:09:19,720 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-05-31 01:09:19,722 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-05-31 01:09:19,722 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-05-31 01:09:19,722 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-05-31 01:09:19,722 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn5_1    | 2023-05-31 01:09:19,723 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-05-31 01:09:19,723 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-05-31 01:09:19,723 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn5_1    | 2023-05-31 01:09:19,725 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-05-31 01:09:19,725 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-05-31 01:09:19,725 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-05-31 01:09:19,725 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-05-31 01:09:19,726 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-05-31 01:09:19,726 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-05-31 01:09:19,726 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/46ae8d39-3c2f-48ac-8229-b2777f51f5a3 does not exist. Creating ...
dn5_1    | 2023-05-31 01:09:19,729 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/46ae8d39-3c2f-48ac-8229-b2777f51f5a3/in_use.lock acquired by nodename 7@51bd9aa00c0e
dn5_1    | 2023-05-31 01:09:19,730 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/46ae8d39-3c2f-48ac-8229-b2777f51f5a3 has been successfully formatted.
dn5_1    | 2023-05-31 01:09:19,731 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO ratis.ContainerStateMachine: group-B2777F51F5A3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn5_1    | 2023-05-31 01:09:19,731 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-05-31 01:09:19,731 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-05-31 01:09:19,731 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:09:19,731 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-05-31 01:09:19,731 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-05-31 01:09:18,359 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-05-31 01:09:18,359 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-05-31 01:09:18,363 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF: start as a follower, conf=-1: peers:[3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:09:18,363 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn4_1    | 2023-05-31 01:09:18,363 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: start 3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-FollowerState
dn4_1    | 2023-05-31 01:09:18,368 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-234A7250E2DF,id=3da1490a-d610-40de-95c1-bdfc259ba041
dn4_1    | 2023-05-31 01:09:18,368 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-05-31 01:09:18,368 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-05-31 01:09:18,368 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-05-31 01:09:18,368 [3da1490a-d610-40de-95c1-bdfc259ba041-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-05-31 01:09:18,369 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-05-31 01:09:18,369 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-31 01:09:18,371 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=e689613f-6a5f-45fa-9ee1-234a7250e2df
dn4_1    | 2023-05-31 01:09:18,372 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=e689613f-6a5f-45fa-9ee1-234a7250e2df.
dn4_1    | 2023-05-31 01:09:23,381 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-FollowerState] INFO impl.FollowerState: 3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5018005637ns, electionTimeout:5012ms
dn4_1    | 2023-05-31 01:09:23,382 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-FollowerState] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: shutdown 3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-FollowerState
dn4_1    | 2023-05-31 01:09:23,382 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-FollowerState] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn4_1    | 2023-05-31 01:09:23,382 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | 2023-05-31 01:09:23,382 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-FollowerState] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: start 3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5
dn4_1    | 2023-05-31 01:09:23,384 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5] INFO impl.LeaderElection: 3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:09:23,384 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5] INFO impl.LeaderElection: 3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5 PRE_VOTE round 0: result PASSED (term=0)
dn4_1    | 2023-05-31 01:09:23,388 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5] INFO impl.LeaderElection: 3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5 ELECTION round 0: submit vote requests at term 1 for -1: peers:[3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:09:23,388 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5] INFO impl.LeaderElection: 3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5 ELECTION round 0: result PASSED (term=1)
dn4_1    | 2023-05-31 01:09:23,389 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: shutdown 3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5
dn4_1    | 2023-05-31 01:09:23,389 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn4_1    | 2023-05-31 01:09:23,389 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-234A7250E2DF with new leaderId: 3da1490a-d610-40de-95c1-bdfc259ba041
dn4_1    | 2023-05-31 01:09:23,390 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF: change Leader from null to 3da1490a-d610-40de-95c1-bdfc259ba041 at term 1 for becomeLeader, leader elected after 5152ms
dn4_1    | 2023-05-31 01:09:23,399 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn4_1    | 2023-05-31 01:09:23,400 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-05-31 01:09:23,401 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn4_1    | 2023-05-31 01:09:23,404 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn4_1    | 2023-05-31 01:09:23,404 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn4_1    | 2023-05-31 01:09:23,413 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn4_1    | 2023-05-31 01:09:23,414 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-05-31 01:09:23,415 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn4_1    | 2023-05-31 01:09:23,416 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5] INFO impl.RoleInfo: 3da1490a-d610-40de-95c1-bdfc259ba041: start 3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderStateImpl
dn4_1    | 2023-05-31 01:09:23,416 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-05-31 01:09:23,418 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/e689613f-6a5f-45fa-9ee1-234a7250e2df/current/log_inprogress_0
dn4_1    | 2023-05-31 01:09:23,430 [3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF-LeaderElection5] INFO server.RaftServer$Division: 3da1490a-d610-40de-95c1-bdfc259ba041@group-234A7250E2DF: set configuration 0: peers:[3da1490a-d610-40de-95c1-bdfc259ba041|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-31 01:09:35,948 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn4_1    | 2023-05-31 01:09:47,498 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@467045c4] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(0),2(0),3(0),1001(0)], numOfContainers=2, numOfBlocks=6
dn4_1    | 2023-05-31 01:10:25,544 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@467045c4] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(1),2(1),3(1),1001(1)], numOfContainers=2, numOfBlocks=6
dn4_1    | 2023-05-31 01:10:25,545 [DeleteBlocksCommandHandlerThread-2] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 1 is either received out of order or retried, 2 <= 1001
dn4_1    | 2023-05-31 01:10:25,546 [DeleteBlocksCommandHandlerThread-1] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 2 is either received out of order or retried, 3 <= 3
dn4_1    | 2023-05-31 01:10:25,546 [DeleteBlocksCommandHandlerThread-4] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 1 is either received out of order or retried, 1 <= 1001
dn4_1    | 2023-05-31 01:10:25,547 [DeleteBlocksCommandHandlerThread-0] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 1 is either received out of order or retried, 1001 <= 1001
dn4_1    | 2023-05-31 01:10:35,949 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn4_1    | 2023-05-31 01:11:20,397 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 27.
dn4_1    | 2023-05-31 01:11:20,398 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 27.
dn4_1    | 2023-05-31 01:11:20,402 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is closed with bcsId 27.
dn4_1    | 2023-05-31 01:11:20,403 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 9.
dn4_1    | 2023-05-31 01:11:20,414 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 9.
dn4_1    | 2023-05-31 01:11:20,421 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is closed with bcsId 9.
dn4_1    | 2023-05-31 01:11:20,424 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 23.
dn4_1    | 2023-05-31 01:11:20,424 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 23.
dn4_1    | 2023-05-31 01:11:20,426 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1001 is closed with bcsId 23.
dn4_1    | 2023-05-31 01:11:20,431 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 40.
dn4_1    | 2023-05-31 01:11:20,431 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 40.
dn4_1    | 2023-05-31 01:11:20,460 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1002 is closed with bcsId 40.
dn4_1    | 2023-05-31 01:11:35,950 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 12/4988 blocks from 2 candidate containers.
dn4_1    | 2023-05-31 01:11:36,011 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/containerDir0/1/chunks/111677748019200001.block
dn4_1    | 2023-05-31 01:11:36,011 [BlockDeletingService#0] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/containerDir0/2/chunks/111677748019200002.block
dn4_1    | 2023-05-31 01:11:36,015 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/containerDir0/1/chunks/111677748019201001.block
dn4_1    | 2023-05-31 01:11:36,015 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/containerDir0/1/chunks/111677748019201002.block
dn4_1    | 2023-05-31 01:11:36,015 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/containerDir0/1/chunks/111677748019201003.block
dn4_1    | 2023-05-31 01:11:36,015 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/containerDir0/1/chunks/111677748019200003.block
dn4_1    | 2023-05-31 01:12:35,966 [BlockDeletingService#7] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 6/4994 blocks from 2 candidate containers.
dn4_1    | 2023-05-31 01:12:35,967 [BlockDeletingService#2] INFO background.BlockDeletingService: No transaction found in container 1 with pending delete block count 5
dn4_1    | 2023-05-31 01:12:35,968 [BlockDeletingService#7] INFO background.BlockDeletingService: No transaction found in container 2 with pending delete block count 1
dn5_1    | 2023-05-31 01:09:19,741 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-05-31 01:09:19,742 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-05-31 01:09:19,742 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-05-31 01:09:19,742 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:09:19,742 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/46ae8d39-3c2f-48ac-8229-b2777f51f5a3
dn5_1    | 2023-05-31 01:09:19,743 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-05-31 01:09:19,743 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-05-31 01:09:19,743 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-05-31 01:09:19,743 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-05-31 01:09:19,743 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-05-31 01:09:19,743 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-05-31 01:09:19,745 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-05-31 01:09:19,745 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-05-31 01:09:19,749 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-05-31 01:09:19,752 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-31 01:09:19,774 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-05-31 01:09:19,776 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-05-31 01:09:19,776 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-05-31 01:09:19,777 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-05-31 01:09:19,777 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-05-31 01:09:19,783 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3: start as a follower, conf=-1: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:09:19,783 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn5_1    | 2023-05-31 01:09:19,783 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: start 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-FollowerState
dn5_1    | 2023-05-31 01:09:19,787 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B2777F51F5A3,id=3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn5_1    | 2023-05-31 01:09:19,788 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-05-31 01:09:19,788 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-05-31 01:09:19,788 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-05-31 01:09:19,788 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-05-31 01:09:19,789 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-05-31 01:09:19,791 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=46ae8d39-3c2f-48ac-8229-b2777f51f5a3
dn5_1    | 2023-05-31 01:09:19,800 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-05-31 01:09:20,370 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=46ae8d39-3c2f-48ac-8229-b2777f51f5a3.
dn5_1    | 2023-05-31 01:09:24,878 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-FollowerState] INFO impl.FollowerState: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5095209918ns, electionTimeout:5078ms
dn5_1    | 2023-05-31 01:09:24,879 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-FollowerState] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: shutdown 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-FollowerState
dn5_1    | 2023-05-31 01:09:24,879 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-FollowerState] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn5_1    | 2023-05-31 01:09:24,880 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-05-31 01:09:24,880 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-FollowerState] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: start 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-LeaderElection5
dn5_1    | 2023-05-31 01:09:24,887 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-FollowerState] INFO impl.FollowerState: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5177409755ns, electionTimeout:5165ms
dn5_1    | 2023-05-31 01:09:24,887 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-FollowerState] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: shutdown 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-FollowerState
dn5_1    | 2023-05-31 01:09:24,887 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-FollowerState] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn5_1    | 2023-05-31 01:09:24,889 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-LeaderElection5] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-LeaderElection5 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:09:24,890 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-05-31 01:09:24,890 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-FollowerState] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: start 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6
dn5_1    | 2023-05-31 01:09:24,896 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-05-31 01:09:24,897 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-05-31 01:09:24,897 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:09:24,897 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6 PRE_VOTE round 0: result PASSED (term=0)
dn5_1    | 2023-05-31 01:09:24,896 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-LeaderElection5-1] INFO server.GrpcServerProtocolClient: Build channel for 4bc93e86-8989-4282-b7d8-050f4a19b92f
dn5_1    | 2023-05-31 01:09:24,900 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6 ELECTION round 0: submit vote requests at term 1 for -1: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:09:24,900 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6 ELECTION round 0: result PASSED (term=1)
dn5_1    | 2023-05-31 01:09:24,900 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: shutdown 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6
dn5_1    | 2023-05-31 01:09:24,900 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn5_1    | 2023-05-31 01:09:24,900 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8F00A57DA29E with new leaderId: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd
dn5_1    | 2023-05-31 01:09:24,924 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E: change Leader from null to 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd at term 1 for becomeLeader, leader elected after 5410ms
dn5_1    | 2023-05-31 01:09:24,964 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-LeaderElection5] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-LeaderElection5: PRE_VOTE REJECTED received 1 response(s) and 0 exception(s):
dn5_1    | 2023-05-31 01:09:24,965 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-LeaderElection5] INFO impl.LeaderElection:   Response 0: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd<-b20d0805-391e-447d-a646-35a5d1a5cbcf#0:FAIL-t0
dn5_1    | 2023-05-31 01:09:24,965 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-LeaderElection5] INFO impl.LeaderElection: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-LeaderElection5 PRE_VOTE round 0: result REJECTED
dn5_1    | 2023-05-31 01:09:24,965 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-LeaderElection5] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
dn5_1    | 2023-05-31 01:09:24,965 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-LeaderElection5] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: shutdown 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-LeaderElection5
dn5_1    | 2023-05-31 01:09:24,965 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-LeaderElection5] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: start 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-FollowerState
dn5_1    | 2023-05-31 01:09:24,966 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn5_1    | 2023-05-31 01:09:24,968 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-05-31 01:09:24,968 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn5_1    | 2023-05-31 01:09:24,968 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn5_1    | 2023-05-31 01:09:24,969 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 2023-05-31 01:09:24,969 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn5_1    | 2023-05-31 01:09:24,969 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-05-31 01:09:24,969 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn5_1    | 2023-05-31 01:09:24,969 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: start 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderStateImpl
dn5_1    | 2023-05-31 01:09:24,982 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 2023-05-31 01:09:24,989 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/cabada01-83b4-406c-b6ca-8f00a57da29e/current/log_inprogress_0
dn5_1    | 2023-05-31 01:09:24,992 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E-LeaderElection6] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-8F00A57DA29E: set configuration 0: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:09:25,440 [grpc-default-executor-5] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3: receive requestVote(PRE_VOTE, b20d0805-391e-447d-a646-35a5d1a5cbcf, group-B2777F51F5A3, 0, (t:0, i:0))
dn5_1    | 2023-05-31 01:09:25,440 [grpc-default-executor-5] INFO impl.VoteContext: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-FOLLOWER: accept PRE_VOTE from b20d0805-391e-447d-a646-35a5d1a5cbcf: our priority 0 <= candidate's priority 1
dn5_1    | 2023-05-31 01:09:25,441 [grpc-default-executor-5] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3 replies to PRE_VOTE vote request: b20d0805-391e-447d-a646-35a5d1a5cbcf<-3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#0:OK-t0. Peer's state: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3:t0, leader=null, voted=, raftlog=Memoized:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:09:25,513 [grpc-default-executor-5] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3: receive requestVote(ELECTION, b20d0805-391e-447d-a646-35a5d1a5cbcf, group-B2777F51F5A3, 1, (t:0, i:0))
dn5_1    | 2023-05-31 01:09:25,514 [grpc-default-executor-5] INFO impl.VoteContext: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-FOLLOWER: accept ELECTION from b20d0805-391e-447d-a646-35a5d1a5cbcf: our priority 0 <= candidate's priority 1
dn5_1    | 2023-05-31 01:09:25,514 [grpc-default-executor-5] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:b20d0805-391e-447d-a646-35a5d1a5cbcf
dn5_1    | 2023-05-31 01:09:25,514 [grpc-default-executor-5] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: shutdown 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-FollowerState
dn5_1    | 2023-05-31 01:09:25,514 [grpc-default-executor-5] INFO impl.RoleInfo: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd: start 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-FollowerState
dn5_1    | 2023-05-31 01:09:25,515 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-FollowerState] INFO impl.FollowerState: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-FollowerState was interrupted
dn5_1    | 2023-05-31 01:09:25,517 [grpc-default-executor-5] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3 replies to ELECTION vote request: b20d0805-391e-447d-a646-35a5d1a5cbcf<-3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd#0:OK-t1. Peer's state: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3:t1, leader=null, voted=b20d0805-391e-447d-a646-35a5d1a5cbcf, raftlog=Memoized:3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:09:25,837 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-B2777F51F5A3 with new leaderId: b20d0805-391e-447d-a646-35a5d1a5cbcf
dn5_1    | 2023-05-31 01:09:25,837 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-server-thread1] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3: change Leader from null to b20d0805-391e-447d-a646-35a5d1a5cbcf at term 1 for appendEntries, leader elected after 6114ms
dn5_1    | 2023-05-31 01:09:25,876 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-server-thread2] INFO server.RaftServer$Division: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3: set configuration 0: peers:[3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 4bc93e86-8989-4282-b7d8-050f4a19b92f|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, b20d0805-391e-447d-a646-35a5d1a5cbcf|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-31 01:09:25,877 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd-server-thread2] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 2023-05-31 01:09:25,880 [3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3c26eb5e-f324-4aaa-9e30-cf0076a2f1cd@group-B2777F51F5A3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/46ae8d39-3c2f-48ac-8229-b2777f51f5a3/current/log_inprogress_0
dn5_1    | 2023-05-31 01:09:35,011 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 2023-05-31 01:09:48,504 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@467045c4] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(0),2(0),3(0),1001(0)], numOfContainers=2, numOfBlocks=6
dn5_1    | 2023-05-31 01:10:26,703 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@467045c4] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(1),2(1),3(1),1001(1)], numOfContainers=2, numOfBlocks=6
dn5_1    | 2023-05-31 01:10:26,705 [DeleteBlocksCommandHandlerThread-1] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 1 is either received out of order or retried, 2 <= 1001
dn5_1    | 2023-05-31 01:10:26,705 [DeleteBlocksCommandHandlerThread-2] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 2 is either received out of order or retried, 3 <= 3
dn5_1    | 2023-05-31 01:10:26,707 [DeleteBlocksCommandHandlerThread-0] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 1 is either received out of order or retried, 1001 <= 1001
dn5_1    | 2023-05-31 01:10:26,708 [DeleteBlocksCommandHandlerThread-4] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 1 is either received out of order or retried, 1 <= 1001
dn5_1    | 2023-05-31 01:10:35,012 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 2023-05-31 01:11:25,902 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 27.
dn5_1    | 2023-05-31 01:11:25,903 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 27.
dn5_1    | 2023-05-31 01:11:25,912 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is closed with bcsId 27.
dn5_1    | 2023-05-31 01:11:25,917 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 9.
dn5_1    | 2023-05-31 01:11:25,917 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 9.
dn5_1    | 2023-05-31 01:11:25,925 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is closed with bcsId 9.
dn5_1    | 2023-05-31 01:11:25,927 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 23.
dn5_1    | 2023-05-31 01:11:25,927 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 23.
dn5_1    | 2023-05-31 01:11:25,941 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1001 is closed with bcsId 23.
dn5_1    | 2023-05-31 01:11:25,956 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 40.
dn5_1    | 2023-05-31 01:11:25,956 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 40.
dn5_1    | 2023-05-31 01:11:25,965 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1002 is closed with bcsId 40.
dn5_1    | 2023-05-31 01:11:35,019 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 12/4988 blocks from 2 candidate containers.
dn5_1    | 2023-05-31 01:11:35,041 [BlockDeletingService#0] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/containerDir0/2/chunks/111677748019200002.block
dn5_1    | 2023-05-31 01:11:35,041 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/containerDir0/1/chunks/111677748019200001.block
dn5_1    | 2023-05-31 01:11:35,055 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/containerDir0/1/chunks/111677748019201001.block
dn5_1    | 2023-05-31 01:11:35,056 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/containerDir0/1/chunks/111677748019201002.block
dn5_1    | 2023-05-31 01:11:35,056 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/containerDir0/1/chunks/111677748019201003.block
dn5_1    | 2023-05-31 01:11:35,063 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-0a9c2b2d-6dd0-4dbb-84dd-6a492392cc3e/current/containerDir0/1/chunks/111677748019200003.block
dn5_1    | 2023-05-31 01:12:35,044 [BlockDeletingService#4] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 6/4994 blocks from 2 candidate containers.
dn5_1    | 2023-05-31 01:12:35,044 [BlockDeletingService#2] INFO background.BlockDeletingService: No transaction found in container 1 with pending delete block count 5
dn5_1    | 2023-05-31 01:12:35,063 [BlockDeletingService#4] INFO background.BlockDeletingService: No transaction found in container 2 with pending delete block count 1
