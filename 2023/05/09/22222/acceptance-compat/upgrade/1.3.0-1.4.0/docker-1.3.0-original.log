Attaching to ha_om2_1, ha_dn1_1, ha_scm1_1, ha_scm3_1, ha_dn3_1, ha_recon_1, ha_dn5_1, ha_om1_1, ha_om3_1, ha_dn4_1, ha_scm2_1, ha_dn2_1, ha_s3g_1
dn2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn2_1    | 2023-05-09 16:56:23,183 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn2_1    | /************************************************************
dn2_1    | STARTUP_MSG: Starting HddsDatanodeService
dn2_1    | STARTUP_MSG:   host = 7521f29bd509/10.9.0.18
dn2_1    | STARTUP_MSG:   args = []
dn2_1    | STARTUP_MSG:   version = 1.3.0
dn2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
dn2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
dn2_1    | STARTUP_MSG:   java = 11.0.14.1
dn2_1    | ************************************************************/
dn2_1    | 2023-05-09 16:56:23,216 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn2_1    | 2023-05-09 16:56:23,618 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn2_1    | 2023-05-09 16:56:24,450 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn2_1    | 2023-05-09 16:56:25,558 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn2_1    | 2023-05-09 16:56:25,570 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn2_1    | 2023-05-09 16:56:26,614 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:7521f29bd509 ip:10.9.0.18
dn2_1    | 2023-05-09 16:56:28,384 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn2_1    | 2023-05-09 16:56:29,546 [main] INFO reflections.Reflections: Reflections took 980 ms to scan 2 urls, producing 92 keys and 204 values 
dn2_1    | 2023-05-09 16:56:30,441 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn2_1    | 2023-05-09 16:56:31,465 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
dn2_1    | 2023-05-09 16:56:31,656 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn2_1    | 2023-05-09 16:56:31,659 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn2_1    | 2023-05-09 16:56:31,667 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn2_1    | 2023-05-09 16:56:31,749 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn2_1    | 2023-05-09 16:56:32,035 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-05-09 16:56:32,037 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
dn2_1    | 2023-05-09 16:56:32,066 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn2_1    | 2023-05-09 16:56:32,069 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn2_1    | 2023-05-09 16:56:32,088 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn2_1    | 2023-05-09 16:56:32,377 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn2_1    | 2023-05-09 16:56:32,386 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn2_1    | 2023-05-09 16:56:43,110 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn2_1    | 2023-05-09 16:56:43,783 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-05-09 16:56:44,412 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn2_1    | 2023-05-09 16:56:45,007 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-05-09 16:56:45,021 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn2_1    | 2023-05-09 16:56:45,034 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-05-09 16:56:45,041 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn2_1    | 2023-05-09 16:56:45,042 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn2_1    | 2023-05-09 16:56:45,044 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn2_1    | 2023-05-09 16:56:45,045 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn2_1    | 2023-05-09 16:56:45,047 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-05-09 16:56:45,047 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn2_1    | 2023-05-09 16:56:45,079 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-05-09 16:56:45,166 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-05-09 16:56:45,196 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn2_1    | 2023-05-09 16:56:45,231 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn2_1    | 2023-05-09 16:56:47,556 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn2_1    | 2023-05-09 16:56:47,571 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn2_1    | 2023-05-09 16:56:47,580 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn2_1    | 2023-05-09 16:56:47,581 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-05-09 16:56:47,581 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-05-09 16:56:47,610 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-05-09 16:56:47,737 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn2_1    | 2023-05-09 16:56:48,606 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn2_1    | 2023-05-09 16:56:48,698 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn2_1    | 2023-05-09 16:56:48,930 [main] INFO util.log: Logging initialized @36811ms to org.eclipse.jetty.util.log.Slf4jLog
dn2_1    | 2023-05-09 16:56:50,159 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn2_1    | 2023-05-09 16:56:50,292 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn2_1    | 2023-05-09 16:56:50,356 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn2_1    | 2023-05-09 16:56:50,415 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn2_1    | 2023-05-09 16:56:50,428 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn2_1    | 2023-05-09 16:56:50,429 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn2_1    | 2023-05-09 16:56:51,037 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn2_1    | 2023-05-09 16:56:51,063 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn2_1    | 2023-05-09 16:56:51,352 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn2_1    | 2023-05-09 16:56:51,353 [main] INFO server.session: No SessionScavenger set, using defaults
dn2_1    | 2023-05-09 16:56:51,372 [main] INFO server.session: node0 Scavenging every 600000ms
dn2_1    | 2023-05-09 16:56:51,527 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@79afa369{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn2_1    | 2023-05-09 16:56:51,528 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@79ba0a6f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
dn2_1    | 2023-05-09 16:56:53,598 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5a4dda2{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-16278056679480093715/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
dn2_1    | 2023-05-09 16:56:53,661 [main] INFO server.AbstractConnector: Started ServerConnector@4f363abd{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn2_1    | 2023-05-09 16:56:53,661 [main] INFO server.Server: Started @41542ms
dn2_1    | 2023-05-09 16:56:53,689 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn2_1    | 2023-05-09 16:56:53,689 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn2_1    | 2023-05-09 16:56:53,691 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn2_1    | 2023-05-09 16:56:53,712 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn2_1    | 2023-05-09 16:56:53,905 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@76966330] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn2_1    | 2023-05-09 16:56:54,443 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn2_1    | 2023-05-09 16:56:54,796 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn2_1    | 2023-05-09 16:56:57,344 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:56:57,344 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:56:57,344 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:56:57,347 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:56:58,349 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:56:58,349 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:56:58,350 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:56:58,351 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:56:59,351 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:56:59,351 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:56:59,352 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:56:59,352 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:00,352 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:00,353 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:00,353 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:00,353 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:01,353 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:01,354 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:01,354 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:01,354 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:02,359 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:02,359 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:02,359 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:03,360 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:03,361 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:03,362 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:04,361 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:04,363 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:04,363 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:05,362 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:05,364 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:05,364 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:06,363 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:06,365 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:06,380 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From 7521f29bd509/10.9.0.18 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:34086 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn2_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:34086 remote=recon/10.9.0.22:9891]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn3_1    | 2023-05-09 16:56:25,805 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn3_1    | /************************************************************
dn3_1    | STARTUP_MSG: Starting HddsDatanodeService
dn3_1    | STARTUP_MSG:   host = 84ef10f1d5b5/10.9.0.19
dn3_1    | STARTUP_MSG:   args = []
dn3_1    | STARTUP_MSG:   version = 1.3.0
dn3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
dn3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
dn3_1    | STARTUP_MSG:   java = 11.0.14.1
dn3_1    | ************************************************************/
dn3_1    | 2023-05-09 16:56:25,839 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn3_1    | 2023-05-09 16:56:26,400 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn3_1    | 2023-05-09 16:56:27,097 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn3_1    | 2023-05-09 16:56:28,449 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn3_1    | 2023-05-09 16:56:28,457 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn3_1    | 2023-05-09 16:56:29,477 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:84ef10f1d5b5 ip:10.9.0.19
dn3_1    | 2023-05-09 16:56:31,233 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn3_1    | 2023-05-09 16:56:32,488 [main] INFO reflections.Reflections: Reflections took 979 ms to scan 2 urls, producing 92 keys and 204 values 
dn3_1    | 2023-05-09 16:56:33,515 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn3_1    | 2023-05-09 16:56:34,803 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
dn3_1    | 2023-05-09 16:56:34,892 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn3_1    | 2023-05-09 16:56:34,904 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn3_1    | 2023-05-09 16:56:34,939 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn3_1    | 2023-05-09 16:56:35,193 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn3_1    | 2023-05-09 16:56:35,364 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-05-09 16:56:35,368 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
dn3_1    | 2023-05-09 16:56:35,384 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn3_1    | 2023-05-09 16:56:35,389 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn3_1    | 2023-05-09 16:56:35,430 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn3_1    | 2023-05-09 16:56:35,724 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn3_1    | 2023-05-09 16:56:35,741 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn3_1    | 2023-05-09 16:56:45,650 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn3_1    | 2023-05-09 16:56:46,152 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-05-09 16:56:46,650 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn3_1    | 2023-05-09 16:56:47,617 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-05-09 16:56:47,679 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn3_1    | 2023-05-09 16:56:47,682 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-05-09 16:56:47,691 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn3_1    | 2023-05-09 16:56:47,702 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn3_1    | 2023-05-09 16:56:47,706 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn3_1    | 2023-05-09 16:56:47,711 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn3_1    | 2023-05-09 16:56:47,730 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-05-09 16:56:47,748 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn3_1    | 2023-05-09 16:56:47,750 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-05-09 16:56:47,794 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-05-09 16:56:47,913 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn3_1    | 2023-05-09 16:56:47,939 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn3_1    | 2023-05-09 16:56:50,488 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn3_1    | 2023-05-09 16:56:50,534 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn3_1    | 2023-05-09 16:56:50,546 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn3_1    | 2023-05-09 16:56:50,546 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-05-09 16:56:50,576 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-05-09 16:56:50,608 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-05-09 16:56:50,742 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn3_1    | 2023-05-09 16:56:51,626 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn3_1    | 2023-05-09 16:56:51,750 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn3_1    | 2023-05-09 16:56:52,190 [main] INFO util.log: Logging initialized @37992ms to org.eclipse.jetty.util.log.Slf4jLog
dn3_1    | 2023-05-09 16:56:53,580 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn3_1    | 2023-05-09 16:56:53,644 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn3_1    | 2023-05-09 16:56:53,708 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn3_1    | 2023-05-09 16:56:53,716 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn3_1    | 2023-05-09 16:56:53,730 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn1_1    | 2023-05-09 16:56:26,655 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn1_1    | /************************************************************
dn1_1    | STARTUP_MSG: Starting HddsDatanodeService
dn1_1    | STARTUP_MSG:   host = 970067e7cec6/10.9.0.17
dn1_1    | STARTUP_MSG:   args = []
dn1_1    | STARTUP_MSG:   version = 1.3.0
dn3_1    | 2023-05-09 16:56:53,730 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn3_1    | 2023-05-09 16:56:54,411 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn3_1    | 2023-05-09 16:56:54,430 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn3_1    | 2023-05-09 16:56:54,861 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn3_1    | 2023-05-09 16:56:54,870 [main] INFO server.session: No SessionScavenger set, using defaults
dn3_1    | 2023-05-09 16:56:54,877 [main] INFO server.session: node0 Scavenging every 660000ms
dn3_1    | 2023-05-09 16:56:55,020 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@22d8f4ed{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn3_1    | 2023-05-09 16:56:55,048 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4b325930{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
dn3_1    | 2023-05-09 16:56:56,918 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@76c86567{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-1374847102745742297/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
dn3_1    | 2023-05-09 16:56:57,005 [main] INFO server.AbstractConnector: Started ServerConnector@5fed9976{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn3_1    | 2023-05-09 16:56:57,005 [main] INFO server.Server: Started @42863ms
dn3_1    | 2023-05-09 16:56:57,045 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn3_1    | 2023-05-09 16:56:57,046 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn3_1    | 2023-05-09 16:56:57,069 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn3_1    | 2023-05-09 16:56:57,122 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn3_1    | 2023-05-09 16:56:57,271 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@55bf4e50] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn3_1    | 2023-05-09 16:56:57,798 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn3_1    | 2023-05-09 16:56:58,205 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn3_1    | 2023-05-09 16:57:00,526 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:00,527 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:00,527 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:00,544 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:01,528 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:01,529 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:01,545 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:02,529 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:02,530 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:02,546 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:03,530 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:03,531 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:03,549 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:04,532 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:04,532 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:04,550 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:05,533 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:05,534 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:05,551 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
dn1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
dn1_1    | STARTUP_MSG:   java = 11.0.14.1
dn1_1    | ************************************************************/
dn1_1    | 2023-05-09 16:56:26,736 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn1_1    | 2023-05-09 16:56:27,152 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn1_1    | 2023-05-09 16:56:27,832 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn1_1    | 2023-05-09 16:56:28,848 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn1_1    | 2023-05-09 16:56:28,848 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn1_1    | 2023-05-09 16:56:29,868 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:970067e7cec6 ip:10.9.0.17
dn1_1    | 2023-05-09 16:56:31,769 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn1_1    | 2023-05-09 16:56:33,059 [main] INFO reflections.Reflections: Reflections took 1065 ms to scan 2 urls, producing 92 keys and 204 values 
dn1_1    | 2023-05-09 16:56:33,987 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn1_1    | 2023-05-09 16:56:35,147 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
dn1_1    | 2023-05-09 16:56:35,221 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn1_1    | 2023-05-09 16:56:35,227 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn1_1    | 2023-05-09 16:56:35,242 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn1_1    | 2023-05-09 16:56:35,392 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn1_1    | 2023-05-09 16:56:35,639 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-05-09 16:56:35,660 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
dn1_1    | 2023-05-09 16:56:35,687 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn1_1    | 2023-05-09 16:56:35,688 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn1_1    | 2023-05-09 16:56:35,688 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn1_1    | 2023-05-09 16:56:35,855 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn1_1    | 2023-05-09 16:56:35,856 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn1_1    | 2023-05-09 16:56:45,721 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn1_1    | 2023-05-09 16:56:46,291 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-05-09 16:56:46,804 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn1_1    | 2023-05-09 16:56:47,425 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-05-09 16:56:47,474 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn1_1    | 2023-05-09 16:56:47,476 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-05-09 16:56:47,482 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn1_1    | 2023-05-09 16:56:47,493 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn1_1    | 2023-05-09 16:56:47,493 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn1_1    | 2023-05-09 16:56:47,494 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn1_1    | 2023-05-09 16:56:47,501 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-05-09 16:56:47,526 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn1_1    | 2023-05-09 16:56:47,527 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-05-09 16:56:47,595 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 2023-05-09 16:56:47,659 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn1_1    | 2023-05-09 16:56:47,681 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn1_1    | 2023-05-09 16:56:50,131 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn1_1    | 2023-05-09 16:56:50,153 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn1_1    | 2023-05-09 16:56:50,185 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn1_1    | 2023-05-09 16:56:50,186 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-05-09 16:56:50,196 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-05-09 16:56:50,201 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-05-09 16:56:50,380 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn1_1    | 2023-05-09 16:56:51,481 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn1_1    | 2023-05-09 16:56:51,799 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn1_1    | 2023-05-09 16:56:52,084 [main] INFO util.log: Logging initialized @36614ms to org.eclipse.jetty.util.log.Slf4jLog
dn1_1    | 2023-05-09 16:56:53,450 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn1_1    | 2023-05-09 16:56:53,534 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn1_1    | 2023-05-09 16:56:53,668 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn1_1    | 2023-05-09 16:56:53,672 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn1_1    | 2023-05-09 16:56:53,698 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn2_1    | 2023-05-09 16:57:07,364 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:07,366 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:08,365 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:08,366 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:09,366 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:09,367 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:10,367 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:10,367 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:10,372 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From 7521f29bd509/10.9.0.18 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:47226 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn2_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:47226 remote=scm1/10.9.0.14:9861]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn2_1    | 2023-05-09 16:57:11,307 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-f33ab5ed-453e-4ae9-ac5c-d55092fa9737/DS-6f7b40e1-79af-426e-b663-f7a93cdfc3c8/container.db to cache
dn2_1    | 2023-05-09 16:57:11,307 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-f33ab5ed-453e-4ae9-ac5c-d55092fa9737/DS-6f7b40e1-79af-426e-b663-f7a93cdfc3c8/container.db for volume DS-6f7b40e1-79af-426e-b663-f7a93cdfc3c8
dn2_1    | 2023-05-09 16:57:11,326 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn2_1    | 2023-05-09 16:57:11,330 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
dn2_1    | 2023-05-09 16:57:11,369 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:11,369 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:11,370 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn1_1    | 2023-05-09 16:56:53,701 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn1_1    | 2023-05-09 16:56:54,492 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn1_1    | 2023-05-09 16:56:54,494 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn1_1    | 2023-05-09 16:56:54,757 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn1_1    | 2023-05-09 16:56:54,764 [main] INFO server.session: No SessionScavenger set, using defaults
dn1_1    | 2023-05-09 16:56:54,783 [main] INFO server.session: node0 Scavenging every 600000ms
dn1_1    | 2023-05-09 16:56:54,922 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@261de205{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn1_1    | 2023-05-09 16:56:54,923 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6c33da7a{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
dn1_1    | 2023-05-09 16:56:56,962 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@11a3a45f{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-2447147896891536289/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
dn1_1    | 2023-05-09 16:56:57,051 [main] INFO server.AbstractConnector: Started ServerConnector@6b37df8e{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn1_1    | 2023-05-09 16:56:57,064 [main] INFO server.Server: Started @41594ms
dn1_1    | 2023-05-09 16:56:57,080 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn1_1    | 2023-05-09 16:56:57,080 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn1_1    | 2023-05-09 16:56:57,092 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn1_1    | 2023-05-09 16:56:57,123 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn1_1    | 2023-05-09 16:56:57,320 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@938f5b6] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn1_1    | 2023-05-09 16:56:57,974 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn1_1    | 2023-05-09 16:56:58,344 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn1_1    | 2023-05-09 16:57:00,565 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:00,571 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:00,574 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:00,610 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:01,566 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:01,575 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:01,611 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:02,567 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:02,576 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:02,612 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:03,568 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:03,576 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:03,613 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:04,569 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:04,577 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:04,614 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:05,570 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:05,578 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:05,615 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | java.net.ConnectException: Call From 7521f29bd509/10.9.0.18 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn2_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.ConnectException: Connection refused
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn2_1    | 	... 12 more
dn2_1    | 2023-05-09 16:57:11,370 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn2_1    | java.net.ConnectException: Call From 7521f29bd509/10.9.0.18 to scm2:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn2_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.ConnectException: Connection refused
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn2_1    | 	... 12 more
dn2_1    | 2023-05-09 16:57:11,600 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 5d36dd6f-a30c-40ec-958f-b246a8287262
dn2_1    | 2023-05-09 16:57:11,689 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 5d36dd6f-a30c-40ec-958f-b246a8287262: start RPC server
dn2_1    | 2023-05-09 16:57:11,698 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 5d36dd6f-a30c-40ec-958f-b246a8287262: GrpcService started, listening on 9858
dn2_1    | 2023-05-09 16:57:11,699 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 5d36dd6f-a30c-40ec-958f-b246a8287262: GrpcService started, listening on 9856
dn2_1    | 2023-05-09 16:57:11,700 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 5d36dd6f-a30c-40ec-958f-b246a8287262: GrpcService started, listening on 9857
dn2_1    | 2023-05-09 16:57:11,713 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 5d36dd6f-a30c-40ec-958f-b246a8287262 is started using port 9858 for RATIS
dn2_1    | 2023-05-09 16:57:11,713 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 5d36dd6f-a30c-40ec-958f-b246a8287262 is started using port 9857 for RATIS_ADMIN
dn3_1    | 2023-05-09 16:57:05,585 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From 84ef10f1d5b5/10.9.0.19 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:57456 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn3_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:57456 remote=recon/10.9.0.22:9891]
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn3_1    | 2023-05-09 16:57:06,534 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:06,552 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:07,536 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:07,553 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:08,537 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:08,554 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:09,538 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:09,555 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:10,539 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:10,542 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From 84ef10f1d5b5/10.9.0.19 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:57106 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn3_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:57106 remote=scm1/10.9.0.14:9861]
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn3_1    | 2023-05-09 16:57:10,556 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:11,237 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn1_1    | 2023-05-09 16:57:05,652 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From 970067e7cec6/10.9.0.17 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:44246 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn1_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:44246 remote=recon/10.9.0.22:9891]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn1_1    | 2023-05-09 16:57:06,574 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:06,616 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:07,575 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:07,616 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:08,576 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:08,617 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:09,577 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:09,618 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:10,578 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:10,590 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From 970067e7cec6/10.9.0.17 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:47676 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn4_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn4_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn4_1    | 2023-05-09 16:56:25,001 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn4_1    | /************************************************************
dn4_1    | STARTUP_MSG: Starting HddsDatanodeService
dn4_1    | STARTUP_MSG:   host = 3ffda8dc18fe/10.9.0.20
dn4_1    | STARTUP_MSG:   args = []
dn4_1    | STARTUP_MSG:   version = 1.3.0
dn2_1    | 2023-05-09 16:57:11,713 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 5d36dd6f-a30c-40ec-958f-b246a8287262 is started using port 9856 for RATIS_SERVER
dn2_1    | 2023-05-09 16:57:11,731 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-5d36dd6f-a30c-40ec-958f-b246a8287262: Started
dn2_1    | 2023-05-09 16:57:11,914 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn2_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	... 1 more
dn2_1    | 2023-05-09 16:57:12,375 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:12,376 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:13,376 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:13,376 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:13,917 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn2_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	... 1 more
dn2_1    | 2023-05-09 16:57:14,377 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:14,377 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:15,377 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:15,378 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:16,379 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:16,379 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:17,380 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:17,380 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn1_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:47676 remote=scm1/10.9.0.14:9861]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn1_1    | 2023-05-09 16:57:10,619 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:11,309 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-f33ab5ed-453e-4ae9-ac5c-d55092fa9737/DS-63f3a629-08b0-4135-894a-48e74d05f5cf/container.db to cache
dn1_1    | 2023-05-09 16:57:11,309 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-f33ab5ed-453e-4ae9-ac5c-d55092fa9737/DS-63f3a629-08b0-4135-894a-48e74d05f5cf/container.db for volume DS-63f3a629-08b0-4135-894a-48e74d05f5cf
dn1_1    | 2023-05-09 16:57:11,323 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn1_1    | 2023-05-09 16:57:11,327 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
dn1_1    | 2023-05-09 16:57:11,582 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:11,621 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:11,626 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis f65ec1c2-2955-4db1-8a8e-2d0fe5785867
dn1_1    | 2023-05-09 16:57:11,733 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: f65ec1c2-2955-4db1-8a8e-2d0fe5785867: start RPC server
dn1_1    | 2023-05-09 16:57:11,743 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: f65ec1c2-2955-4db1-8a8e-2d0fe5785867: GrpcService started, listening on 9858
dn1_1    | 2023-05-09 16:57:11,747 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: f65ec1c2-2955-4db1-8a8e-2d0fe5785867: GrpcService started, listening on 9856
dn1_1    | 2023-05-09 16:57:11,748 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: f65ec1c2-2955-4db1-8a8e-2d0fe5785867: GrpcService started, listening on 9857
dn1_1    | 2023-05-09 16:57:11,767 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f65ec1c2-2955-4db1-8a8e-2d0fe5785867 is started using port 9858 for RATIS
dn1_1    | 2023-05-09 16:57:11,767 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f65ec1c2-2955-4db1-8a8e-2d0fe5785867 is started using port 9857 for RATIS_ADMIN
dn1_1    | 2023-05-09 16:57:11,767 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f65ec1c2-2955-4db1-8a8e-2d0fe5785867 is started using port 9856 for RATIS_SERVER
dn1_1    | 2023-05-09 16:57:11,768 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-f65ec1c2-2955-4db1-8a8e-2d0fe5785867: Started
dn1_1    | 2023-05-09 16:57:12,583 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:12,622 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:13,584 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:13,623 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:14,585 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:14,586 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn1_1    | java.net.ConnectException: Call From 970067e7cec6/10.9.0.17 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn4_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
dn4_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
dn4_1    | STARTUP_MSG:   java = 11.0.14.1
dn4_1    | ************************************************************/
dn4_1    | 2023-05-09 16:56:25,047 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn4_1    | 2023-05-09 16:56:25,442 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn4_1    | 2023-05-09 16:56:26,051 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn4_1    | 2023-05-09 16:56:26,960 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn4_1    | 2023-05-09 16:56:26,960 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn4_1    | 2023-05-09 16:56:28,098 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:3ffda8dc18fe ip:10.9.0.20
dn4_1    | 2023-05-09 16:56:30,128 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn4_1    | 2023-05-09 16:56:31,480 [main] INFO reflections.Reflections: Reflections took 1111 ms to scan 2 urls, producing 92 keys and 204 values 
dn4_1    | 2023-05-09 16:56:32,497 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn4_1    | 2023-05-09 16:56:33,525 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
dn4_1    | 2023-05-09 16:56:33,650 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn4_1    | 2023-05-09 16:56:33,652 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn4_1    | 2023-05-09 16:56:33,653 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn4_1    | 2023-05-09 16:56:33,908 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn4_1    | 2023-05-09 16:56:34,011 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn4_1    | 2023-05-09 16:56:34,022 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
dn4_1    | 2023-05-09 16:56:34,044 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn4_1    | 2023-05-09 16:56:34,045 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn4_1    | 2023-05-09 16:56:34,045 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn4_1    | 2023-05-09 16:56:34,358 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn4_1    | 2023-05-09 16:56:34,369 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn4_1    | 2023-05-09 16:56:43,616 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn4_1    | 2023-05-09 16:56:44,282 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn4_1    | 2023-05-09 16:56:44,825 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn4_1    | 2023-05-09 16:56:45,595 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn4_1    | 2023-05-09 16:56:45,603 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn4_1    | 2023-05-09 16:56:45,606 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn4_1    | 2023-05-09 16:56:45,606 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn4_1    | 2023-05-09 16:56:45,608 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn4_1    | 2023-05-09 16:56:45,616 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn4_1    | 2023-05-09 16:56:45,619 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn4_1    | 2023-05-09 16:56:45,620 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-09 16:56:45,637 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn4_1    | 2023-05-09 16:56:45,639 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-05-09 16:56:45,712 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-05-09 16:56:45,743 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn4_1    | 2023-05-09 16:56:45,765 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn4_1    | 2023-05-09 16:56:48,585 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn4_1    | 2023-05-09 16:56:48,588 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn4_1    | 2023-05-09 16:56:48,601 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn4_1    | 2023-05-09 16:56:48,612 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-05-09 16:56:48,613 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-05-09 16:56:48,619 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-05-09 16:56:48,719 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn4_1    | 2023-05-09 16:56:49,299 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn4_1    | 2023-05-09 16:56:49,435 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn4_1    | 2023-05-09 16:56:49,684 [main] INFO util.log: Logging initialized @35383ms to org.eclipse.jetty.util.log.Slf4jLog
dn4_1    | 2023-05-09 16:56:50,736 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn4_1    | 2023-05-09 16:56:50,757 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn4_1    | 2023-05-09 16:56:50,812 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn4_1    | 2023-05-09 16:56:50,837 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn4_1    | 2023-05-09 16:56:50,855 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn1_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.ConnectException: Connection refused
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn1_1    | 	... 12 more
dn1_1    | 2023-05-09 16:57:14,623 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:14,624 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn1_1    | java.net.ConnectException: Call From 970067e7cec6/10.9.0.17 to scm2:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn1_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.ConnectException: Connection refused
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn1_1    | 	... 12 more
dn1_1    | 2023-05-09 16:57:15,334 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn1_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn4_1    | 2023-05-09 16:56:50,865 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn4_1    | 2023-05-09 16:56:51,318 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn4_1    | 2023-05-09 16:56:51,345 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn4_1    | 2023-05-09 16:56:51,642 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn4_1    | 2023-05-09 16:56:51,642 [main] INFO server.session: No SessionScavenger set, using defaults
dn4_1    | 2023-05-09 16:56:51,654 [main] INFO server.session: node0 Scavenging every 600000ms
dn4_1    | 2023-05-09 16:56:51,737 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@69aabcb0{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn4_1    | 2023-05-09 16:56:51,751 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5d352de0{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
dn4_1    | 2023-05-09 16:56:54,001 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@10f405ff{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-7140774593435174716/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
dn4_1    | 2023-05-09 16:56:54,083 [main] INFO server.AbstractConnector: Started ServerConnector@377949f1{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn4_1    | 2023-05-09 16:56:54,084 [main] INFO server.Server: Started @39783ms
dn4_1    | 2023-05-09 16:56:54,097 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn4_1    | 2023-05-09 16:56:54,097 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn4_1    | 2023-05-09 16:56:54,115 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn4_1    | 2023-05-09 16:56:54,135 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn4_1    | 2023-05-09 16:56:54,320 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5b3cd2ee] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn4_1    | 2023-05-09 16:56:54,880 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn4_1    | 2023-05-09 16:56:55,232 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn4_1    | 2023-05-09 16:56:57,706 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:56:57,706 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:56:57,715 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:56:57,716 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:56:58,707 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:56:58,708 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:56:58,716 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:56:58,717 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:56:59,708 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:56:59,709 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:56:59,716 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:56:59,717 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:00,709 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:00,710 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:00,717 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:00,718 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:01,711 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:01,717 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:01,718 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:02,712 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:02,718 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:02,719 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:03,713 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:03,720 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:03,720 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:04,714 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:04,720 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:04,721 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:05,715 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:05,721 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:05,722 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:05,765 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn4_1    | java.net.SocketTimeoutException: Call From 3ffda8dc18fe/10.9.0.20 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:46194 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn4_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:46194 remote=recon/10.9.0.22:9891]
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn4_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn4_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn4_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn4_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn4_1    | 2023-05-09 16:57:06,718 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:06,723 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn5_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn5_1    | 2023-05-09 16:56:26,401 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn5_1    | /************************************************************
dn5_1    | STARTUP_MSG: Starting HddsDatanodeService
dn5_1    | STARTUP_MSG:   host = 2439abba3020/10.9.0.21
dn5_1    | STARTUP_MSG:   args = []
dn5_1    | STARTUP_MSG:   version = 1.3.0
dn5_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
dn5_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
dn5_1    | STARTUP_MSG:   java = 11.0.14.1
dn5_1    | ************************************************************/
dn5_1    | 2023-05-09 16:56:26,434 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn5_1    | 2023-05-09 16:56:26,859 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn5_1    | 2023-05-09 16:56:27,563 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn5_1    | 2023-05-09 16:56:28,921 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn5_1    | 2023-05-09 16:56:28,928 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn5_1    | 2023-05-09 16:56:30,102 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:2439abba3020 ip:10.9.0.21
dn5_1    | 2023-05-09 16:56:32,113 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn5_1    | 2023-05-09 16:56:33,282 [main] INFO reflections.Reflections: Reflections took 912 ms to scan 2 urls, producing 92 keys and 204 values 
dn5_1    | 2023-05-09 16:56:34,251 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn5_1    | 2023-05-09 16:56:35,355 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
dn5_1    | 2023-05-09 16:56:35,417 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn5_1    | 2023-05-09 16:56:35,439 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn5_1    | 2023-05-09 16:56:35,451 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn5_1    | 2023-05-09 16:56:35,593 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn5_1    | 2023-05-09 16:56:35,718 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 2023-05-09 16:56:35,725 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
dn5_1    | 2023-05-09 16:56:35,734 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn5_1    | 2023-05-09 16:56:35,735 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn5_1    | 2023-05-09 16:56:35,761 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn5_1    | 2023-05-09 16:56:35,973 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn5_1    | 2023-05-09 16:56:35,987 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn5_1    | 2023-05-09 16:56:45,998 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn5_1    | 2023-05-09 16:56:46,657 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 2023-05-09 16:56:47,088 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn5_1    | 2023-05-09 16:56:47,708 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-05-09 16:56:47,747 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn5_1    | 2023-05-09 16:56:47,751 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-05-09 16:56:47,751 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn5_1    | 2023-05-09 16:56:47,751 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn5_1    | 2023-05-09 16:56:47,757 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn5_1    | 2023-05-09 16:56:47,758 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn5_1    | 2023-05-09 16:56:47,795 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-05-09 16:56:47,798 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn5_1    | 2023-05-09 16:56:47,799 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-05-09 16:56:47,858 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-05-09 16:56:47,881 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn5_1    | 2023-05-09 16:56:47,892 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn5_1    | 2023-05-09 16:56:50,375 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn5_1    | 2023-05-09 16:56:50,427 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn5_1    | 2023-05-09 16:56:50,437 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn5_1    | 2023-05-09 16:56:50,442 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-05-09 16:56:50,446 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-05-09 16:56:50,467 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-05-09 16:56:50,707 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn5_1    | 2023-05-09 16:56:52,077 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn5_1    | 2023-05-09 16:56:52,323 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn5_1    | 2023-05-09 16:56:52,670 [main] INFO util.log: Logging initialized @37231ms to org.eclipse.jetty.util.log.Slf4jLog
dn5_1    | 2023-05-09 16:56:54,049 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn5_1    | 2023-05-09 16:56:54,121 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn5_1    | 2023-05-09 16:56:54,281 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn5_1    | 2023-05-09 16:56:54,292 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn5_1    | 2023-05-09 16:56:54,302 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn5_1    | 2023-05-09 16:56:54,312 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn5_1    | 2023-05-09 16:56:54,814 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn5_1    | 2023-05-09 16:56:54,839 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn5_1    | 2023-05-09 16:56:55,074 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn5_1    | 2023-05-09 16:56:55,074 [main] INFO server.session: No SessionScavenger set, using defaults
dn5_1    | 2023-05-09 16:56:55,086 [main] INFO server.session: node0 Scavenging every 600000ms
dn5_1    | 2023-05-09 16:56:55,153 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3434a4f0{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn5_1    | 2023-05-09 16:56:55,169 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@267f9765{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
dn5_1    | 2023-05-09 16:56:56,956 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7e5efcab{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-16569697622872153669/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	... 1 more
dn1_1    | 2023-05-09 16:57:15,589 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:15,626 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:16,590 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:16,627 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:17,590 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:17,627 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:18,591 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:18,628 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:19,592 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:19,629 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:20,595 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:20,630 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:21,596 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:21,631 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:22,596 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:22,632 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:23,597 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:23,633 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:24,598 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:24,633 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:25,599 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:25,634 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:26,600 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:26,635 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:27,601 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:27,636 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:18,381 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:18,381 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:19,382 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:19,382 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:20,382 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:20,383 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:21,383 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:21,384 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:22,384 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:22,385 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:23,385 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:23,386 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:24,387 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:24,387 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:25,387 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:25,388 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:26,388 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:26,390 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:27,390 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:27,391 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:28,391 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:28,391 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:29,392 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:29,392 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:30,392 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:30,393 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:31,393 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:31,395 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:32,395 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:32,396 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:33,396 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:33,396 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:34,396 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:34,397 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:35,402 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:36,403 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:37,404 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:38,405 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:39,409 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:40,410 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:41,411 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:42,412 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:43,413 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:43,713 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn2_1    | 2023-05-09 16:57:43,919 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn2_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	... 1 more
dn2_1    | 2023-05-09 16:57:44,415 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:45,219 [Command processor thread] INFO server.RaftServer: 5d36dd6f-a30c-40ec-958f-b246a8287262: addNew group-2490F334A92B:[5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] returns group-2490F334A92B:java.util.concurrent.CompletableFuture@5e1d6475[Not completed]
dn2_1    | 2023-05-09 16:57:45,448 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:45,454 [pool-22-thread-1] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262: new RaftServerImpl for group-2490F334A92B:[5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn2_1    | 2023-05-09 16:57:45,478 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-05-09 16:57:45,479 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-05-09 16:57:45,485 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-05-09 16:57:28,602 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:28,636 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:29,603 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:29,637 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:30,604 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:30,638 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:31,605 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:31,639 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:32,606 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:32,640 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:33,607 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:33,641 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:34,608 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:34,641 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:35,611 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:36,612 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:37,613 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:38,614 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:39,614 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:40,615 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:41,619 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:42,620 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:43,621 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:43,718 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn1_1    | 2023-05-09 16:57:44,621 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:45,336 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn1_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn4_1    | 2023-05-09 16:57:07,719 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:07,723 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:08,720 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:08,724 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:09,721 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:09,725 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:10,722 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:10,725 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:10,728 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn4_1    | java.net.SocketTimeoutException: Call From 3ffda8dc18fe/10.9.0.20 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:47838 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn4_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:47838 remote=scm1/10.9.0.14:9861]
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn4_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn4_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn4_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn4_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn4_1    | 2023-05-09 16:57:11,285 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-f33ab5ed-453e-4ae9-ac5c-d55092fa9737/DS-203b80a4-ed4a-4f2f-8343-ed5b464139ea/container.db to cache
dn4_1    | 2023-05-09 16:57:11,290 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-f33ab5ed-453e-4ae9-ac5c-d55092fa9737/DS-203b80a4-ed4a-4f2f-8343-ed5b464139ea/container.db for volume DS-203b80a4-ed4a-4f2f-8343-ed5b464139ea
dn4_1    | 2023-05-09 16:57:11,297 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn4_1    | 2023-05-09 16:57:11,304 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
dn4_1    | 2023-05-09 16:57:11,565 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 3aa72183-93e1-4d03-9ed4-71b6f7b45f69
dn4_1    | 2023-05-09 16:57:11,727 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:11,728 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn2_1    | 2023-05-09 16:57:45,486 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-05-09 16:57:45,490 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-05-09 16:57:45,490 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-05-09 16:57:45,552 [pool-22-thread-1] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B: ConfigurationManager, init=-1: peers:[5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-05-09 16:57:45,572 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-05-09 16:57:45,640 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-05-09 16:57:45,654 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-05-09 16:57:45,696 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-05-09 16:57:45,740 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-05-09 16:57:45,758 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-05-09 16:57:46,034 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-05-09 16:57:46,039 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-05-09 16:57:46,046 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-05-09 16:57:46,047 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-05-09 16:57:46,047 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-05-09 16:57:46,055 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/79915409-920e-4d1c-bcfe-2490f334a92b does not exist. Creating ...
dn2_1    | 2023-05-09 16:57:46,098 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/79915409-920e-4d1c-bcfe-2490f334a92b/in_use.lock acquired by nodename 6@7521f29bd509
dn2_1    | 2023-05-09 16:57:46,134 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/79915409-920e-4d1c-bcfe-2490f334a92b has been successfully formatted.
dn2_1    | 2023-05-09 16:57:46,232 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-2490F334A92B: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn2_1    | 2023-05-09 16:57:46,234 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-05-09 16:57:46,323 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-05-09 16:57:46,324 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-05-09 16:57:46,345 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-05-09 16:57:46,346 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-05-09 16:57:46,367 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-05-09 16:57:46,397 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-05-09 16:57:46,418 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-05-09 16:57:46,441 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/79915409-920e-4d1c-bcfe-2490f334a92b
dn2_1    | 2023-05-09 16:57:46,442 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-05-09 16:57:46,449 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:46,450 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-05-09 16:57:46,451 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-05-09 16:57:46,452 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-05-09 16:57:46,452 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-05-09 16:57:46,491 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-05-09 16:57:46,492 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-05-09 16:57:46,493 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-05-09 16:57:46,566 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-05-09 16:57:46,603 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-05-09 16:57:46,604 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-05-09 16:57:46,604 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-05-09 16:57:46,696 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-05-09 16:57:46,696 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-05-09 16:57:46,747 [pool-22-thread-1] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B: start as a follower, conf=-1: peers:[5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-09 16:57:46,748 [pool-22-thread-1] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn2_1    | 2023-05-09 16:57:46,749 [pool-22-thread-1] INFO impl.RoleInfo: 5d36dd6f-a30c-40ec-958f-b246a8287262: start 5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-FollowerState
dn2_1    | 2023-05-09 16:57:46,773 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-2490F334A92B,id=5d36dd6f-a30c-40ec-958f-b246a8287262
dn3_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	... 1 more
dn3_1    | 2023-05-09 16:57:11,312 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-f33ab5ed-453e-4ae9-ac5c-d55092fa9737/DS-19530a45-39e4-4144-bd47-21ac116d5f28/container.db to cache
dn3_1    | 2023-05-09 16:57:11,315 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-f33ab5ed-453e-4ae9-ac5c-d55092fa9737/DS-19530a45-39e4-4144-bd47-21ac116d5f28/container.db for volume DS-19530a45-39e4-4144-bd47-21ac116d5f28
dn3_1    | 2023-05-09 16:57:11,316 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn3_1    | 2023-05-09 16:57:11,320 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
dn3_1    | 2023-05-09 16:57:11,545 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:11,546 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 56216a20-9d0f-4921-897d-3244639db044
dn3_1    | 2023-05-09 16:57:11,557 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:11,668 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 56216a20-9d0f-4921-897d-3244639db044: start RPC server
dn3_1    | 2023-05-09 16:57:11,694 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 56216a20-9d0f-4921-897d-3244639db044: GrpcService started, listening on 9858
dn3_1    | 2023-05-09 16:57:11,699 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 56216a20-9d0f-4921-897d-3244639db044: GrpcService started, listening on 9856
dn3_1    | 2023-05-09 16:57:11,716 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 56216a20-9d0f-4921-897d-3244639db044: GrpcService started, listening on 9857
dn3_1    | 2023-05-09 16:57:11,730 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 56216a20-9d0f-4921-897d-3244639db044 is started using port 9858 for RATIS
dn3_1    | 2023-05-09 16:57:11,731 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 56216a20-9d0f-4921-897d-3244639db044 is started using port 9857 for RATIS_ADMIN
dn3_1    | 2023-05-09 16:57:11,731 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 56216a20-9d0f-4921-897d-3244639db044 is started using port 9856 for RATIS_SERVER
dn3_1    | 2023-05-09 16:57:11,750 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-56216a20-9d0f-4921-897d-3244639db044: Started
dn3_1    | 2023-05-09 16:57:12,546 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:12,558 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:13,547 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:13,559 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:14,548 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:14,549 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn3_1    | java.net.ConnectException: Call From 84ef10f1d5b5/10.9.0.19 to scm2:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn3_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.ConnectException: Connection refused
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn3_1    | 	... 12 more
dn3_1    | 2023-05-09 16:57:14,559 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:14,560 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn3_1    | java.net.ConnectException: Call From 84ef10f1d5b5/10.9.0.19 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn3_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn2_1    | 2023-05-09 16:57:46,798 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-05-09 16:57:46,799 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-05-09 16:57:46,799 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-05-09 16:57:46,800 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-05-09 16:57:46,800 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-05-09 16:57:46,812 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-05-09 16:57:46,960 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=79915409-920e-4d1c-bcfe-2490f334a92b
dn2_1    | 2023-05-09 16:57:46,967 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=79915409-920e-4d1c-bcfe-2490f334a92b.
dn2_1    | 2023-05-09 16:57:46,968 [Command processor thread] INFO server.RaftServer: 5d36dd6f-a30c-40ec-958f-b246a8287262: addNew group-D3D6EB9F6B5A:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER] returns group-D3D6EB9F6B5A:java.util.concurrent.CompletableFuture@45e642ca[Not completed]
dn2_1    | 2023-05-09 16:57:47,041 [pool-22-thread-1] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262: new RaftServerImpl for group-D3D6EB9F6B5A:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn2_1    | 2023-05-09 16:57:47,042 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-05-09 16:57:47,095 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-05-09 16:57:47,095 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-05-09 16:57:47,102 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-05-09 16:57:47,102 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-05-09 16:57:47,116 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-05-09 16:57:47,116 [pool-22-thread-1] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A: ConfigurationManager, init=-1: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-05-09 16:57:47,116 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-05-09 16:57:47,118 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-05-09 16:57:47,119 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-05-09 16:57:47,121 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-05-09 16:57:47,137 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-05-09 16:57:47,137 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-05-09 16:57:47,138 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-05-09 16:57:47,146 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-05-09 16:57:47,146 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-05-09 16:57:47,148 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-05-09 16:57:47,149 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-05-09 16:57:47,150 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/73e0c514-5c6d-4883-b850-d3d6eb9f6b5a does not exist. Creating ...
dn2_1    | 2023-05-09 16:57:47,173 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/73e0c514-5c6d-4883-b850-d3d6eb9f6b5a/in_use.lock acquired by nodename 6@7521f29bd509
dn2_1    | 2023-05-09 16:57:47,190 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/73e0c514-5c6d-4883-b850-d3d6eb9f6b5a has been successfully formatted.
dn2_1    | 2023-05-09 16:57:47,202 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-D3D6EB9F6B5A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn2_1    | 2023-05-09 16:57:47,202 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-05-09 16:57:47,203 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-05-09 16:57:47,203 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-05-09 16:57:47,203 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-05-09 16:57:47,203 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-05-09 16:57:47,203 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-05-09 16:57:47,204 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-05-09 16:57:47,209 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-05-09 16:57:47,209 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/73e0c514-5c6d-4883-b850-d3d6eb9f6b5a
dn2_1    | 2023-05-09 16:57:47,210 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-05-09 16:57:47,210 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | java.net.ConnectException: Call From 3ffda8dc18fe/10.9.0.20 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn4_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.net.ConnectException: Connection refused
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn4_1    | 	... 12 more
dn4_1    | 2023-05-09 16:57:11,746 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:11,747 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn4_1    | java.net.ConnectException: Call From 3ffda8dc18fe/10.9.0.20 to scm2:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn4_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.net.ConnectException: Connection refused
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn4_1    | 	... 12 more
dn4_1    | 2023-05-09 16:57:11,754 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: start RPC server
dn4_1    | 2023-05-09 16:57:11,781 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: GrpcService started, listening on 9858
dn4_1    | 2023-05-09 16:57:11,789 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: GrpcService started, listening on 9856
dn4_1    | 2023-05-09 16:57:11,798 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: GrpcService started, listening on 9857
dn4_1    | 2023-05-09 16:57:11,828 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3aa72183-93e1-4d03-9ed4-71b6f7b45f69 is started using port 9858 for RATIS
dn4_1    | 2023-05-09 16:57:11,828 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3aa72183-93e1-4d03-9ed4-71b6f7b45f69 is started using port 9857 for RATIS_ADMIN
dn5_1    | 2023-05-09 16:56:57,075 [main] INFO server.AbstractConnector: Started ServerConnector@3fdcde7a{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn5_1    | 2023-05-09 16:56:57,075 [main] INFO server.Server: Started @41636ms
dn5_1    | 2023-05-09 16:56:57,097 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn5_1    | 2023-05-09 16:56:57,097 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn5_1    | 2023-05-09 16:56:57,115 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn5_1    | 2023-05-09 16:56:57,139 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn5_1    | 2023-05-09 16:56:57,253 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@a93fd7f] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn5_1    | 2023-05-09 16:56:57,864 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn5_1    | 2023-05-09 16:56:58,264 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn5_1    | 2023-05-09 16:57:00,496 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:00,497 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:00,498 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:00,511 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:01,497 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:01,499 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:01,511 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:02,498 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:02,500 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:02,512 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:03,499 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:03,500 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:03,513 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:04,499 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:04,501 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:04,513 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:05,500 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:05,501 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:05,514 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.ConnectException: Connection refused
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
om1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1    | 2023-05-09 16:56:25,190 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1    | /************************************************************
om1_1    | STARTUP_MSG: Starting OzoneManager
om1_1    | STARTUP_MSG:   host = 367fa92bbc56/10.9.0.11
om1_1    | STARTUP_MSG:   args = [--init]
om1_1    | STARTUP_MSG:   version = 1.3.0
dn2_1    | 2023-05-09 16:57:47,210 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-05-09 16:57:47,210 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-05-09 16:57:47,210 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-05-09 16:57:47,210 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-05-09 16:57:47,210 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-05-09 16:57:47,210 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-05-09 16:57:47,211 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-05-09 16:57:47,227 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-05-09 16:57:47,236 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-05-09 16:57:47,236 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-05-09 16:57:47,238 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-05-09 16:57:47,239 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-05-09 16:57:47,278 [pool-22-thread-1] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A: start as a follower, conf=-1: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-09 16:57:47,279 [pool-22-thread-1] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn2_1    | 2023-05-09 16:57:47,279 [pool-22-thread-1] INFO impl.RoleInfo: 5d36dd6f-a30c-40ec-958f-b246a8287262: start 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-FollowerState
dn2_1    | 2023-05-09 16:57:47,282 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D3D6EB9F6B5A,id=5d36dd6f-a30c-40ec-958f-b246a8287262
dn2_1    | 2023-05-09 16:57:47,347 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-05-09 16:57:47,347 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-05-09 16:57:47,348 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-05-09 16:57:47,349 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-05-09 16:57:47,323 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-05-09 16:57:47,392 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=73e0c514-5c6d-4883-b850-d3d6eb9f6b5a
dn2_1    | 2023-05-09 16:57:47,456 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:47,456 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-05-09 16:57:48,479 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:49,480 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:50,483 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:51,153 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=73e0c514-5c6d-4883-b850-d3d6eb9f6b5a.
dn2_1    | 2023-05-09 16:57:51,154 [Command processor thread] INFO server.RaftServer: 5d36dd6f-a30c-40ec-958f-b246a8287262: addNew group-391CD66B588A:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] returns group-391CD66B588A:java.util.concurrent.CompletableFuture@51d4e2d0[Not completed]
dn2_1    | 2023-05-09 16:57:51,162 [pool-22-thread-1] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262: new RaftServerImpl for group-391CD66B588A:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn2_1    | 2023-05-09 16:57:51,170 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-05-09 16:57:51,170 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-05-09 16:57:51,170 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-05-09 16:57:51,171 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-05-09 16:57:51,171 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-05-09 16:57:51,171 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-05-09 16:57:51,171 [pool-22-thread-1] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A: ConfigurationManager, init=-1: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-05-09 16:57:51,171 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-05-09 16:57:51,172 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-05-09 16:57:51,172 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-05-09 16:57:51,172 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-05-09 16:57:51,177 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-05-09 16:57:51,177 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-05-09 16:57:51,178 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-05-09 16:57:51,180 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-05-09 16:57:51,180 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-05-09 16:57:51,180 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-05-09 16:57:51,181 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-05-09 16:57:51,182 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ffceeb78-bcf8-4a49-9756-391cd66b588a does not exist. Creating ...
dn2_1    | 2023-05-09 16:57:51,186 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ffceeb78-bcf8-4a49-9756-391cd66b588a/in_use.lock acquired by nodename 6@7521f29bd509
dn2_1    | 2023-05-09 16:57:51,207 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ffceeb78-bcf8-4a49-9756-391cd66b588a has been successfully formatted.
dn2_1    | 2023-05-09 16:57:51,213 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-391CD66B588A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn2_1    | 2023-05-09 16:57:51,214 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-05-09 16:57:51,220 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-05-09 16:57:51,220 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-05-09 16:57:51,220 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-05-09 16:57:51,220 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-05-09 16:57:51,220 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-05-09 16:57:51,221 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-05-09 16:57:51,221 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-05-09 16:57:51,221 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ffceeb78-bcf8-4a49-9756-391cd66b588a
dn2_1    | 2023-05-09 16:57:51,221 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-05-09 16:57:51,221 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-05-09 16:57:51,221 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-05-09 16:57:51,222 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-05-09 16:57:51,222 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-05-09 16:57:51,222 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-05-09 16:57:51,222 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-05-09 16:57:51,222 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-05-09 16:57:51,223 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-05-09 16:57:51,223 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-05-09 16:57:51,223 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-05-09 16:57:51,227 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-05-09 16:57:51,274 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-05-09 16:57:51,274 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-05-09 16:57:51,275 [pool-22-thread-1] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A: start as a follower, conf=-1: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-09 16:57:51,276 [pool-22-thread-1] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn2_1    | 2023-05-09 16:57:51,276 [pool-22-thread-1] INFO impl.RoleInfo: 5d36dd6f-a30c-40ec-958f-b246a8287262: start 5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-FollowerState
dn2_1    | 2023-05-09 16:57:51,283 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-391CD66B588A,id=5d36dd6f-a30c-40ec-958f-b246a8287262
dn2_1    | 2023-05-09 16:57:51,285 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-05-09 16:57:51,285 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-05-09 16:57:51,285 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-05-09 16:57:51,285 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
om1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om1_1    | STARTUP_MSG:   java = 11.0.14.1
om1_1    | ************************************************************/
om1_1    | 2023-05-09 16:56:25,263 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1    | 2023-05-09 16:56:34,611 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1    | 2023-05-09 16:56:37,714 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om1_1    | 2023-05-09 16:56:38,446 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1    | 2023-05-09 16:56:38,456 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
om1_1    | 2023-05-09 16:56:38,498 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-05-09 16:56:39,911 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om1_1    | 2023-05-09 16:56:43,958 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 367fa92bbc56/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-05-09 16:56:45,959 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 367fa92bbc56/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-05-09 16:56:47,961 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 367fa92bbc56/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-05-09 16:56:49,962 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 367fa92bbc56/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-05-09 16:56:51,964 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 367fa92bbc56/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-05-09 16:56:53,966 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 367fa92bbc56/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-05-09 16:56:55,971 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 367fa92bbc56/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
dn5_1    | 2023-05-09 16:57:05,562 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn5_1    | java.net.SocketTimeoutException: Call From 2439abba3020/10.9.0.21 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:55952 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn5_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:55952 remote=recon/10.9.0.22:9891]
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn5_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn5_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn5_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn5_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn5_1    | 2023-05-09 16:57:06,502 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:06,515 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:07,503 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:07,515 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:08,504 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:08,516 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:09,505 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:09,517 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:10,506 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:10,512 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn5_1    | java.net.SocketTimeoutException: Call From 2439abba3020/10.9.0.21 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:52870 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn2_1    | 2023-05-09 16:57:51,286 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-05-09 16:57:51,300 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=ffceeb78-bcf8-4a49-9756-391cd66b588a
dn2_1    | 2023-05-09 16:57:51,316 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-05-09 16:57:51,484 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:51,631 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=ffceeb78-bcf8-4a49-9756-391cd66b588a.
dn2_1    | 2023-05-09 16:57:51,984 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-FollowerState] INFO impl.FollowerState: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5234883633ns, electionTimeout:5184ms
dn2_1    | 2023-05-09 16:57:51,984 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-FollowerState] INFO impl.RoleInfo: 5d36dd6f-a30c-40ec-958f-b246a8287262: shutdown 5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-FollowerState
dn2_1    | 2023-05-09 16:57:51,984 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-FollowerState] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn2_1    | 2023-05-09 16:57:51,987 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn2_1    | 2023-05-09 16:57:51,987 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-FollowerState] INFO impl.RoleInfo: 5d36dd6f-a30c-40ec-958f-b246a8287262: start 5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-LeaderElection1
dn2_1    | 2023-05-09 16:57:51,999 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-LeaderElection1] INFO impl.LeaderElection: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-09 16:57:51,999 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-LeaderElection1] INFO impl.LeaderElection: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-LeaderElection1 ELECTION round 0: result PASSED (term=1)
dn2_1    | 2023-05-09 16:57:52,000 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-LeaderElection1] INFO impl.RoleInfo: 5d36dd6f-a30c-40ec-958f-b246a8287262: shutdown 5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-LeaderElection1
dn2_1    | 2023-05-09 16:57:52,001 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-LeaderElection1] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn2_1    | 2023-05-09 16:57:52,001 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-2490F334A92B with new leaderId: 5d36dd6f-a30c-40ec-958f-b246a8287262
dn2_1    | 2023-05-09 16:57:52,001 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-LeaderElection1] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B: change Leader from null to 5d36dd6f-a30c-40ec-958f-b246a8287262 at term 1 for becomeLeader, leader elected after 6304ms
dn2_1    | 2023-05-09 16:57:52,009 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn2_1    | 2023-05-09 16:57:52,016 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-05-09 16:57:52,022 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn2_1    | 2023-05-09 16:57:52,025 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn2_1    | 2023-05-09 16:57:52,028 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn2_1    | 2023-05-09 16:57:52,029 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn2_1    | 2023-05-09 16:57:52,041 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-05-09 16:57:52,046 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn2_1    | 2023-05-09 16:57:52,048 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-LeaderElection1] INFO impl.RoleInfo: 5d36dd6f-a30c-40ec-958f-b246a8287262: start 5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-LeaderStateImpl
dn2_1    | 2023-05-09 16:57:52,083 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-SegmentedRaftLogWorker: Starting segment from index:0
dn2_1    | 2023-05-09 16:57:52,138 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-LeaderElection1] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B: set configuration 0: peers:[5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-09 16:57:52,299 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-2490F334A92B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/79915409-920e-4d1c-bcfe-2490f334a92b/current/log_inprogress_0
dn2_1    | 2023-05-09 16:57:52,484 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:52,526 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-FollowerState] INFO impl.FollowerState: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5246494361ns, electionTimeout:5069ms
dn2_1    | 2023-05-09 16:57:52,526 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-FollowerState] INFO impl.RoleInfo: 5d36dd6f-a30c-40ec-958f-b246a8287262: shutdown 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-FollowerState
dn2_1    | 2023-05-09 16:57:52,526 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-FollowerState] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn2_1    | 2023-05-09 16:57:52,526 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	... 1 more
dn1_1    | 2023-05-09 16:57:45,623 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:46,630 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:46,660 [Command processor thread] INFO server.RaftServer: f65ec1c2-2955-4db1-8a8e-2d0fe5785867: addNew group-8B0224B40693:[f65ec1c2-2955-4db1-8a8e-2d0fe5785867|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] returns group-8B0224B40693:java.util.concurrent.CompletableFuture@71e0cbf6[Not completed]
dn1_1    | 2023-05-09 16:57:46,843 [pool-22-thread-1] INFO server.RaftServer$Division: f65ec1c2-2955-4db1-8a8e-2d0fe5785867: new RaftServerImpl for group-8B0224B40693:[f65ec1c2-2955-4db1-8a8e-2d0fe5785867|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn1_1    | 2023-05-09 16:57:46,851 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-05-09 16:57:46,859 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-05-09 16:57:46,860 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-05-09 16:57:46,860 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-05-09 16:57:46,874 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-05-09 16:57:46,874 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-05-09 16:57:46,897 [pool-22-thread-1] INFO server.RaftServer$Division: f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693: ConfigurationManager, init=-1: peers:[f65ec1c2-2955-4db1-8a8e-2d0fe5785867|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-05-09 16:57:46,906 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-05-09 16:57:46,988 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-05-09 16:57:47,006 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-05-09 16:57:47,056 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-05-09 16:57:47,078 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-05-09 16:57:47,079 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-05-09 16:57:47,342 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-05-09 16:57:47,360 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-05-09 16:57:47,370 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-05-09 16:57:47,370 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-05-09 16:57:47,371 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-05-09 16:57:47,372 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/59f9c2e5-7cfd-4b6f-bac2-8b0224b40693 does not exist. Creating ...
dn1_1    | 2023-05-09 16:57:47,400 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/59f9c2e5-7cfd-4b6f-bac2-8b0224b40693/in_use.lock acquired by nodename 7@970067e7cec6
dn1_1    | 2023-05-09 16:57:47,492 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/59f9c2e5-7cfd-4b6f-bac2-8b0224b40693 has been successfully formatted.
dn1_1    | 2023-05-09 16:57:47,525 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-8B0224B40693: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn1_1    | 2023-05-09 16:57:47,531 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-05-09 16:57:47,538 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-05-09 16:57:47,598 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-05-09 16:57:47,599 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-05-09 16:57:47,601 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-05-09 16:57:47,623 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-05-09 16:57:47,638 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:47,715 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-05-09 16:57:47,724 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-05-09 16:57:47,760 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/59f9c2e5-7cfd-4b6f-bac2-8b0224b40693
dn1_1    | 2023-05-09 16:57:47,762 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-05-09 16:57:47,762 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-05-09 16:57:47,775 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-05-09 16:57:47,776 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-05-09 16:57:47,777 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-05-09 16:57:47,778 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-05-09 16:57:47,785 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-05-09 16:57:47,786 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-05-09 16:57:47,835 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-05-09 16:57:47,841 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-05-09 16:57:47,842 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-05-09 16:57:47,863 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-05-09 16:57:47,919 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-05-09 16:57:47,937 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-05-09 16:57:47,967 [pool-22-thread-1] INFO server.RaftServer$Division: f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693: start as a follower, conf=-1: peers:[f65ec1c2-2955-4db1-8a8e-2d0fe5785867|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-09 16:57:47,968 [pool-22-thread-1] INFO server.RaftServer$Division: f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn1_1    | 2023-05-09 16:57:47,975 [pool-22-thread-1] INFO impl.RoleInfo: f65ec1c2-2955-4db1-8a8e-2d0fe5785867: start f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-FollowerState
dn1_1    | 2023-05-09 16:57:47,991 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-05-09 16:57:47,993 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8B0224B40693,id=f65ec1c2-2955-4db1-8a8e-2d0fe5785867
dn1_1    | 2023-05-09 16:57:48,003 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-05-09 16:57:48,009 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-05-09 16:57:48,010 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-05-09 16:57:48,012 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-05-09 16:57:48,016 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-05-09 16:57:48,139 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=59f9c2e5-7cfd-4b6f-bac2-8b0224b40693
dn1_1    | 2023-05-09 16:57:48,153 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=59f9c2e5-7cfd-4b6f-bac2-8b0224b40693.
dn1_1    | 2023-05-09 16:57:48,674 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:49,674 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:50,675 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:51,676 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:52,677 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:53,186 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-FollowerState] INFO impl.FollowerState: f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5210658127ns, electionTimeout:5161ms
dn1_1    | 2023-05-09 16:57:53,189 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-FollowerState] INFO impl.RoleInfo: f65ec1c2-2955-4db1-8a8e-2d0fe5785867: shutdown f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-FollowerState
dn1_1    | 2023-05-09 16:57:53,189 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-FollowerState] INFO server.RaftServer$Division: f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn1_1    | 2023-05-09 16:57:53,196 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn1_1    | 2023-05-09 16:57:53,196 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-FollowerState] INFO impl.RoleInfo: f65ec1c2-2955-4db1-8a8e-2d0fe5785867: start f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-LeaderElection1
dn1_1    | 2023-05-09 16:57:53,236 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-LeaderElection1] INFO impl.LeaderElection: f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[f65ec1c2-2955-4db1-8a8e-2d0fe5785867|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-09 16:57:53,236 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-LeaderElection1] INFO impl.LeaderElection: f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-LeaderElection1 ELECTION round 0: result PASSED (term=1)
dn1_1    | 2023-05-09 16:57:53,239 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-LeaderElection1] INFO impl.RoleInfo: f65ec1c2-2955-4db1-8a8e-2d0fe5785867: shutdown f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-LeaderElection1
dn1_1    | 2023-05-09 16:57:53,240 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-LeaderElection1] INFO server.RaftServer$Division: f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn1_1    | 2023-05-09 16:57:53,240 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8B0224B40693 with new leaderId: f65ec1c2-2955-4db1-8a8e-2d0fe5785867
dn1_1    | 2023-05-09 16:57:53,240 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-LeaderElection1] INFO server.RaftServer$Division: f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693: change Leader from null to f65ec1c2-2955-4db1-8a8e-2d0fe5785867 at term 1 for becomeLeader, leader elected after 6192ms
dn1_1    | 2023-05-09 16:57:53,293 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn1_1    | 2023-05-09 16:57:53,317 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-05-09 16:57:53,326 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn1_1    | 2023-05-09 16:57:53,342 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn1_1    | 2023-05-09 16:57:53,343 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn1_1    | 2023-05-09 16:57:53,344 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn1_1    | 2023-05-09 16:57:53,360 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-05-09 16:57:53,361 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn1_1    | 2023-05-09 16:57:53,366 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-LeaderElection1] INFO impl.RoleInfo: f65ec1c2-2955-4db1-8a8e-2d0fe5785867: start f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-LeaderStateImpl
dn1_1    | 2023-05-09 16:57:53,405 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-SegmentedRaftLogWorker: Starting segment from index:0
dn1_1    | 2023-05-09 16:57:53,454 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-LeaderElection1] INFO server.RaftServer$Division: f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693: set configuration 0: peers:[f65ec1c2-2955-4db1-8a8e-2d0fe5785867|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-05-09 16:57:53,678 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:53,696 [f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f65ec1c2-2955-4db1-8a8e-2d0fe5785867@group-8B0224B40693-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/59f9c2e5-7cfd-4b6f-bac2-8b0224b40693/current/log_inprogress_0
dn1_1    | 2023-05-09 16:57:54,679 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:55,680 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:56,692 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-05-09 16:57:59,434 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn2_1    | 2023-05-09 16:57:52,527 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-FollowerState] INFO impl.RoleInfo: 5d36dd6f-a30c-40ec-958f-b246a8287262: start 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-LeaderElection2
dn2_1    | 2023-05-09 16:57:52,530 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-LeaderElection2] INFO impl.LeaderElection: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-09 16:57:52,535 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-05-09 16:57:52,535 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-05-09 16:57:52,537 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 56216a20-9d0f-4921-897d-3244639db044
dn2_1    | 2023-05-09 16:57:52,551 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 3aa72183-93e1-4d03-9ed4-71b6f7b45f69
dn2_1    | 2023-05-09 16:57:52,777 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-LeaderElection2] INFO impl.LeaderElection: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
dn2_1    | 2023-05-09 16:57:52,779 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-LeaderElection2] INFO impl.LeaderElection:   Response 0: 5d36dd6f-a30c-40ec-958f-b246a8287262<-56216a20-9d0f-4921-897d-3244639db044#0:OK-t1
dn2_1    | 2023-05-09 16:57:52,779 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-LeaderElection2] INFO impl.LeaderElection:   Response 1: 5d36dd6f-a30c-40ec-958f-b246a8287262<-3aa72183-93e1-4d03-9ed4-71b6f7b45f69#0:FAIL-t1
dn2_1    | 2023-05-09 16:57:52,779 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-LeaderElection2] INFO impl.LeaderElection: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-LeaderElection2 ELECTION round 0: result REJECTED
dn2_1    | 2023-05-09 16:57:52,780 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
dn2_1    | 2023-05-09 16:57:52,780 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-LeaderElection2] INFO impl.RoleInfo: 5d36dd6f-a30c-40ec-958f-b246a8287262: shutdown 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-LeaderElection2
dn2_1    | 2023-05-09 16:57:52,780 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-LeaderElection2] INFO impl.RoleInfo: 5d36dd6f-a30c-40ec-958f-b246a8287262: start 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-FollowerState
dn2_1    | 2023-05-09 16:57:52,811 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-05-09 16:57:52,811 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-05-09 16:57:53,486 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:54,487 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:55,488 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:56,488 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-05-09 16:57:56,500 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-FollowerState] INFO impl.FollowerState: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5224184567ns, electionTimeout:5184ms
dn2_1    | 2023-05-09 16:57:56,500 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-FollowerState] INFO impl.RoleInfo: 5d36dd6f-a30c-40ec-958f-b246a8287262: shutdown 5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-FollowerState
dn2_1    | 2023-05-09 16:57:56,500 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-FollowerState] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn2_1    | 2023-05-09 16:57:56,500 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn2_1    | 2023-05-09 16:57:56,501 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-FollowerState] INFO impl.RoleInfo: 5d36dd6f-a30c-40ec-958f-b246a8287262: start 5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3
dn2_1    | 2023-05-09 16:57:56,506 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO impl.LeaderElection: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-09 16:57:56,523 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-05-09 16:57:56,523 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-05-09 16:57:56,537 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO impl.LeaderElection: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn2_1    | 2023-05-09 16:57:56,542 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO impl.LeaderElection:   Response 0: 5d36dd6f-a30c-40ec-958f-b246a8287262<-56216a20-9d0f-4921-897d-3244639db044#0:OK-t1
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn5_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:52870 remote=scm1/10.9.0.14:9861]
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn5_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn5_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn5_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn5_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn5_1    | 2023-05-09 16:57:10,517 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:11,281 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-f33ab5ed-453e-4ae9-ac5c-d55092fa9737/DS-7d82fa99-52eb-4c90-bf47-c0833b0b938b/container.db to cache
dn5_1    | 2023-05-09 16:57:11,281 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-f33ab5ed-453e-4ae9-ac5c-d55092fa9737/DS-7d82fa99-52eb-4c90-bf47-c0833b0b938b/container.db for volume DS-7d82fa99-52eb-4c90-bf47-c0833b0b938b
dn5_1    | 2023-05-09 16:57:11,287 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn5_1    | 2023-05-09 16:57:11,292 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
dn5_1    | 2023-05-09 16:57:11,500 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 99be2b94-18e1-4c1d-9084-53427369ee1a
dn5_1    | 2023-05-09 16:57:11,507 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:11,518 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:11,545 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 99be2b94-18e1-4c1d-9084-53427369ee1a: start RPC server
dn5_1    | 2023-05-09 16:57:11,546 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 99be2b94-18e1-4c1d-9084-53427369ee1a: GrpcService started, listening on 9858
dn5_1    | 2023-05-09 16:57:11,547 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 99be2b94-18e1-4c1d-9084-53427369ee1a: GrpcService started, listening on 9856
dn5_1    | 2023-05-09 16:57:11,548 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 99be2b94-18e1-4c1d-9084-53427369ee1a: GrpcService started, listening on 9857
dn5_1    | 2023-05-09 16:57:11,555 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 99be2b94-18e1-4c1d-9084-53427369ee1a is started using port 9858 for RATIS
dn5_1    | 2023-05-09 16:57:11,555 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 99be2b94-18e1-4c1d-9084-53427369ee1a is started using port 9857 for RATIS_ADMIN
dn5_1    | 2023-05-09 16:57:11,555 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 99be2b94-18e1-4c1d-9084-53427369ee1a is started using port 9856 for RATIS_SERVER
dn5_1    | 2023-05-09 16:57:11,562 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-99be2b94-18e1-4c1d-9084-53427369ee1a: Started
dn5_1    | 2023-05-09 16:57:12,508 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:12,519 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:13,509 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:13,520 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:14,509 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:14,510 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn5_1    | java.net.ConnectException: Call From 2439abba3020/10.9.0.21 to scm2:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn4_1    | 2023-05-09 16:57:11,828 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3aa72183-93e1-4d03-9ed4-71b6f7b45f69 is started using port 9856 for RATIS_SERVER
dn4_1    | 2023-05-09 16:57:11,835 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-3aa72183-93e1-4d03-9ed4-71b6f7b45f69: Started
dn4_1    | 2023-05-09 16:57:12,730 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:12,753 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:13,730 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:13,754 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:14,731 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:14,755 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:15,732 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:15,756 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:16,736 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:16,756 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:17,737 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:17,757 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:18,739 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:18,758 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:19,739 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:19,759 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:20,740 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:20,760 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:21,741 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:21,761 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:22,742 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:22,762 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:23,747 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:23,763 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:24,748 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:24,764 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:25,749 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn3_1    | 	... 12 more
dn3_1    | 2023-05-09 16:57:15,551 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:15,562 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:16,552 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:16,563 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:17,553 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:17,564 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:18,553 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:18,565 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:19,554 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:19,566 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:20,555 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:20,566 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:21,556 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:21,567 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:22,557 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:22,568 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:23,558 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:23,569 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:24,559 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:24,570 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:25,560 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:25,570 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:26,561 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:26,571 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:27,562 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:27,572 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-05-09 16:56:57,973 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 367fa92bbc56/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-05-09 16:56:59,975 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 367fa92bbc56/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-05-09 16:57:01,976 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 367fa92bbc56/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-05-09 16:57:03,986 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 367fa92bbc56/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-05-09 16:57:06,428 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:1f9ebad1-a542-47ab-b506-15f3d5e79738 is not the leader. Could not determine the leader node.
om1_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om1_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om1_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om1_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om1_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om1_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om1_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om1_1    | , while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-05-09 16:57:08,435 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 367fa92bbc56/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-05-09 16:57:10,436 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 367fa92bbc56/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-f33ab5ed-453e-4ae9-ac5c-d55092fa9737;layoutVersion=3
om1_1    | 2023-05-09 16:57:12,559 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om1_1    | /************************************************************
om1_1    | SHUTDOWN_MSG: Shutting down OzoneManager at 367fa92bbc56/10.9.0.11
om1_1    | ************************************************************/
om1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1    | 2023-05-09 16:57:14,681 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1    | /************************************************************
om1_1    | STARTUP_MSG: Starting OzoneManager
om1_1    | STARTUP_MSG:   host = 367fa92bbc56/10.9.0.11
om1_1    | STARTUP_MSG:   args = [--]
om1_1    | STARTUP_MSG:   version = 1.3.0
dn4_1    | 2023-05-09 16:57:25,765 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:26,750 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:26,768 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:27,752 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:27,769 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:28,753 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:28,770 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:29,753 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:29,771 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:30,754 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:30,771 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:31,755 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:31,772 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:32,756 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:32,773 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:33,757 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:33,774 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:34,758 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:34,775 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:35,759 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:36,760 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:37,760 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:38,761 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:39,762 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:40,763 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:41,764 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:42,765 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:43,714 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn4_1    | 2023-05-09 16:57:43,766 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:28,563 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:28,572 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:29,564 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:29,573 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:30,566 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:30,575 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:31,571 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:31,575 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:32,572 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:32,576 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:33,573 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:33,577 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:34,574 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:34,581 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:35,590 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:36,591 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:37,592 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:38,592 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:39,593 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:40,594 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:41,595 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:42,595 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:43,596 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:43,740 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn3_1    | 2023-05-09 16:57:44,597 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:45,241 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn3_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	... 1 more
dn3_1    | 2023-05-09 16:57:45,600 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:46,601 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:46,688 [Command processor thread] INFO server.RaftServer: 56216a20-9d0f-4921-897d-3244639db044: addNew group-4FE9FF9ECA5C:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] returns group-4FE9FF9ECA5C:java.util.concurrent.CompletableFuture@1bcb5af4[Not completed]
dn3_1    | 2023-05-09 16:57:46,862 [pool-22-thread-1] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044: new RaftServerImpl for group-4FE9FF9ECA5C:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn3_1    | 2023-05-09 16:57:46,882 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-05-09 16:57:46,886 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-05-09 16:57:46,887 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-05-09 16:57:46,891 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-05-09 16:57:46,897 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-05-09 16:57:46,900 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-05-09 16:57:46,947 [pool-22-thread-1] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C: ConfigurationManager, init=-1: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-05-09 16:57:46,949 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-05-09 16:57:47,031 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-05-09 16:57:47,032 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-05-09 16:57:47,081 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-05-09 16:57:47,091 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-05-09 16:57:47,099 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-05-09 16:57:47,307 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-05-09 16:57:47,335 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-05-09 16:57:47,338 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-05-09 16:57:47,344 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-05-09 16:57:47,346 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-05-09 16:57:47,349 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/8c529fe3-ebe2-46fe-b4b1-4fe9ff9eca5c does not exist. Creating ...
dn3_1    | 2023-05-09 16:57:47,373 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/8c529fe3-ebe2-46fe-b4b1-4fe9ff9eca5c/in_use.lock acquired by nodename 6@84ef10f1d5b5
dn3_1    | 2023-05-09 16:57:47,411 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/8c529fe3-ebe2-46fe-b4b1-4fe9ff9eca5c has been successfully formatted.
dn3_1    | 2023-05-09 16:57:47,439 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-4FE9FF9ECA5C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-05-09 16:57:47,495 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-05-09 16:57:47,626 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-05-09 16:57:47,626 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-05-09 16:57:47,643 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-05-09 16:57:47,626 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:47,653 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-05-09 16:57:47,682 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-05-09 16:57:47,718 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-05-09 16:57:47,719 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-05-09 16:57:47,787 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/8c529fe3-ebe2-46fe-b4b1-4fe9ff9eca5c
dn3_1    | 2023-05-09 16:57:47,806 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-05-09 16:57:47,815 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-05-09 16:57:47,816 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-05-09 16:57:44,365 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn4_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.util.concurrent.TimeoutException
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	... 1 more
dn4_1    | 2023-05-09 16:57:44,767 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:45,687 [Command processor thread] INFO server.RaftServer: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: addNew group-1A6E129F2A79:[3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] returns group-1A6E129F2A79:java.util.concurrent.CompletableFuture@4edfebd7[Not completed]
dn4_1    | 2023-05-09 16:57:45,770 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:45,792 [pool-22-thread-1] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: new RaftServerImpl for group-1A6E129F2A79:[3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn4_1    | 2023-05-09 16:57:45,812 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-05-09 16:57:45,812 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-05-09 16:57:45,812 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-05-09 16:57:45,838 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-05-09 16:57:45,838 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-05-09 16:57:45,841 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-05-09 16:57:45,853 [pool-22-thread-1] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79: ConfigurationManager, init=-1: peers:[3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-05-09 16:57:45,870 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-05-09 16:57:45,937 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-05-09 16:57:45,938 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-05-09 16:57:45,986 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-05-09 16:57:46,012 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-05-09 16:57:46,012 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-05-09 16:57:46,362 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-05-09 16:57:46,382 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-05-09 16:57:46,383 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-05-09 16:57:46,384 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-05-09 16:57:46,394 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-05-09 16:57:46,422 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/c3e11b37-a77f-4003-8d5f-1a6e129f2a79 does not exist. Creating ...
dn4_1    | 2023-05-09 16:57:46,478 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/c3e11b37-a77f-4003-8d5f-1a6e129f2a79/in_use.lock acquired by nodename 7@3ffda8dc18fe
dn4_1    | 2023-05-09 16:57:46,569 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/c3e11b37-a77f-4003-8d5f-1a6e129f2a79 has been successfully formatted.
dn4_1    | 2023-05-09 16:57:46,630 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-1A6E129F2A79: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn4_1    | 2023-05-09 16:57:46,719 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-05-09 16:57:46,771 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:46,788 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-05-09 16:57:46,816 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-09 16:57:46,817 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-05-09 16:57:46,829 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-05-09 16:57:46,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-05-09 16:57:46,880 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-05-09 16:57:46,900 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-05-09 16:57:46,915 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/c3e11b37-a77f-4003-8d5f-1a6e129f2a79
dn4_1    | 2023-05-09 16:57:46,943 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-05-09 16:57:46,943 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-05-09 16:57:46,948 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-05-09 16:57:46,954 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-05-09 16:57:46,958 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-05-09 16:57:46,959 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-05-09 16:57:46,999 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-05-09 16:57:46,999 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-05-09 16:57:47,056 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-05-09 16:57:47,075 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-05-09 16:57:47,078 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-05-09 16:57:47,080 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-05-09 16:57:47,115 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-05-09 16:57:47,134 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-05-09 16:57:47,136 [pool-22-thread-1] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79: start as a follower, conf=-1: peers:[3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-09 16:57:47,136 [pool-22-thread-1] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn4_1    | 2023-05-09 16:57:47,176 [pool-22-thread-1] INFO impl.RoleInfo: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: start 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-FollowerState
dn4_1    | 2023-05-09 16:57:47,234 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-05-09 16:57:47,235 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-09 16:57:47,254 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1A6E129F2A79,id=3aa72183-93e1-4d03-9ed4-71b6f7b45f69
dn4_1    | 2023-05-09 16:57:47,263 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-05-09 16:57:47,264 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-05-09 16:57:47,276 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-05-09 16:57:47,277 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-05-09 16:57:47,410 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=c3e11b37-a77f-4003-8d5f-1a6e129f2a79
dn4_1    | 2023-05-09 16:57:47,412 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=c3e11b37-a77f-4003-8d5f-1a6e129f2a79.
dn4_1    | 2023-05-09 16:57:47,420 [Command processor thread] INFO server.RaftServer: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: addNew group-D3D6EB9F6B5A:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER] returns group-D3D6EB9F6B5A:java.util.concurrent.CompletableFuture@60f2e3ea[Not completed]
dn4_1    | 2023-05-09 16:57:47,463 [pool-22-thread-1] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: new RaftServerImpl for group-D3D6EB9F6B5A:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn4_1    | 2023-05-09 16:57:47,470 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-05-09 16:57:47,470 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-05-09 16:57:47,473 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-05-09 16:57:47,483 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-05-09 16:57:47,503 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-05-09 16:57:47,507 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-05-09 16:57:47,521 [pool-22-thread-1] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A: ConfigurationManager, init=-1: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-05-09 16:57:47,521 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-05-09 16:57:47,522 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1    | 2023-05-09 16:56:24,146 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1    | /************************************************************
om3_1    | STARTUP_MSG: Starting OzoneManager
om3_1    | STARTUP_MSG:   host = 6eb4a2a6e0d5/10.9.0.13
om3_1    | STARTUP_MSG:   args = [--init]
om3_1    | STARTUP_MSG:   version = 1.3.0
om1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om3_1    | STARTUP_MSG:   java = 11.0.14.1
om3_1    | ************************************************************/
om3_1    | 2023-05-09 16:56:24,204 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1    | 2023-05-09 16:56:32,332 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1    | 2023-05-09 16:56:35,744 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om3_1    | 2023-05-09 16:56:36,557 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1    | 2023-05-09 16:56:36,559 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om3_1    | 2023-05-09 16:56:36,570 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-05-09 16:56:37,740 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om3_1    | 2023-05-09 16:56:41,021 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6eb4a2a6e0d5/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-05-09 16:56:43,023 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6eb4a2a6e0d5/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-05-09 16:56:45,025 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6eb4a2a6e0d5/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-05-09 16:56:47,027 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6eb4a2a6e0d5/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-05-09 16:56:49,028 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6eb4a2a6e0d5/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-05-09 16:56:51,030 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6eb4a2a6e0d5/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-05-09 16:56:53,035 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6eb4a2a6e0d5/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1    | 2023-05-09 16:56:25,337 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1    | /************************************************************
om2_1    | STARTUP_MSG: Starting OzoneManager
om2_1    | STARTUP_MSG:   host = ee9fbcddb50a/10.9.0.12
om2_1    | STARTUP_MSG:   args = [--init]
om2_1    | STARTUP_MSG:   version = 1.3.0
om1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om1_1    | STARTUP_MSG:   java = 11.0.14.1
om1_1    | ************************************************************/
om1_1    | 2023-05-09 16:57:14,692 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1    | 2023-05-09 16:57:18,381 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1    | 2023-05-09 16:57:19,961 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om1_1    | 2023-05-09 16:57:20,187 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1    | 2023-05-09 16:57:20,188 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
om1_1    | 2023-05-09 16:57:20,204 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-05-09 16:57:20,367 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om1_1    | 2023-05-09 16:57:21,541 [main] INFO reflections.Reflections: Reflections took 1018 ms to scan 1 urls, producing 114 keys and 335 values [using 2 cores]
om1_1    | 2023-05-09 16:57:21,617 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-05-09 16:57:22,929 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om1_1    | 2023-05-09 16:57:23,123 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om1_1    | 2023-05-09 16:57:25,819 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-05-09 16:57:26,503 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om1_1    | 2023-05-09 16:57:26,504 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om1_1    | 2023-05-09 16:57:27,351 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om1_1    | 2023-05-09 16:57:27,541 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om1_1    | 2023-05-09 16:57:27,640 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1    | 2023-05-09 16:57:27,643 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om1_1    | 2023-05-09 16:57:27,675 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om1_1    | 2023-05-09 16:57:28,247 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om1_1    | 2023-05-09 16:57:28,312 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1    | 2023-05-09 16:57:28,598 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om1:9872, om3:9872, om2:9872
om1_1    | 2023-05-09 16:57:28,653 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om1_1    | 2023-05-09 16:57:28,792 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om1_1    | 2023-05-09 16:57:29,083 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om1_1    | 2023-05-09 16:57:29,084 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om1_1    | 2023-05-09 16:57:29,085 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om1_1    | 2023-05-09 16:57:29,087 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om1_1    | 2023-05-09 16:57:29,087 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om1_1    | 2023-05-09 16:57:29,087 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om1_1    | 2023-05-09 16:57:29,088 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om1_1    | 2023-05-09 16:57:29,100 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-05-09 16:57:29,100 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om1_1    | 2023-05-09 16:57:29,101 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1    | 2023-05-09 16:57:29,134 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1    | 2023-05-09 16:57:29,151 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om1_1    | 2023-05-09 16:57:29,152 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om1_1    | 2023-05-09 16:57:29,880 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om1_1    | 2023-05-09 16:57:29,895 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om1_1    | 2023-05-09 16:57:29,895 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om1_1    | 2023-05-09 16:57:29,895 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1    | 2023-05-09 16:57:29,896 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | 2023-05-09 16:57:29,902 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1    | 2023-05-09 16:57:29,939 [main] INFO server.RaftServer: om1: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@c3719e5[Not completed]
om1_1    | 2023-05-09 16:57:29,939 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om1_1    | 2023-05-09 16:57:30,029 [main] INFO om.OzoneManager: Creating RPC Server
om1_1    | 2023-05-09 16:57:30,030 [pool-26-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om1_1    | 2023-05-09 16:57:30,203 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om1_1    | 2023-05-09 16:57:30,226 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om1_1    | 2023-05-09 16:57:30,226 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om1_1    | 2023-05-09 16:57:30,226 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
dn2_1    | 2023-05-09 16:57:56,542 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO impl.LeaderElection: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3 ELECTION round 0: result PASSED
dn2_1    | 2023-05-09 16:57:56,543 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO impl.RoleInfo: 5d36dd6f-a30c-40ec-958f-b246a8287262: shutdown 5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3
dn2_1    | 2023-05-09 16:57:56,543 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn2_1    | 2023-05-09 16:57:56,543 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-391CD66B588A with new leaderId: 5d36dd6f-a30c-40ec-958f-b246a8287262
dn2_1    | 2023-05-09 16:57:56,544 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A: change Leader from null to 5d36dd6f-a30c-40ec-958f-b246a8287262 at term 1 for becomeLeader, leader elected after 5371ms
dn2_1    | 2023-05-09 16:57:56,544 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn2_1    | 2023-05-09 16:57:56,544 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-05-09 16:57:56,544 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn2_1    | 2023-05-09 16:57:56,545 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn2_1    | 2023-05-09 16:57:56,546 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn2_1    | 2023-05-09 16:57:56,546 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn2_1    | 2023-05-09 16:57:56,546 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-05-09 16:57:56,546 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn2_1    | 2023-05-09 16:57:56,592 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn2_1    | 2023-05-09 16:57:56,592 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-05-09 16:57:56,592 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn2_1    | 2023-05-09 16:57:56,601 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn2_1    | 2023-05-09 16:57:56,601 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-05-09 16:57:56,601 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-05-09 16:57:56,602 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-05-09 16:57:56,602 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn2_1    | 2023-05-09 16:57:56,607 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn2_1    | 2023-05-09 16:57:56,608 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-05-09 16:57:56,609 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn2_1    | 2023-05-09 16:57:56,610 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn2_1    | 2023-05-09 16:57:56,610 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-05-09 16:57:56,610 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-05-09 16:57:56,610 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-05-09 16:57:56,612 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn2_1    | 2023-05-09 16:57:56,614 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO impl.RoleInfo: 5d36dd6f-a30c-40ec-958f-b246a8287262: start 5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderStateImpl
dn2_1    | 2023-05-09 16:57:56,615 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-SegmentedRaftLogWorker: Starting segment from index:0
dn2_1    | 2023-05-09 16:57:56,618 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ffceeb78-bcf8-4a49-9756-391cd66b588a/current/log_inprogress_0
dn2_1    | 2023-05-09 16:57:56,646 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A-LeaderElection3] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-391CD66B588A: set configuration 0: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-09 16:57:57,897 [grpc-default-executor-0] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A: receive requestVote(ELECTION, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69, group-D3D6EB9F6B5A, 2, (t:0, i:0))
dn2_1    | 2023-05-09 16:57:57,899 [grpc-default-executor-0] INFO impl.VoteContext: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-FOLLOWER: accept ELECTION from 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: our priority 0 <= candidate's priority 1
dn2_1    | 2023-05-09 16:57:57,899 [grpc-default-executor-0] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:3aa72183-93e1-4d03-9ed4-71b6f7b45f69
dn2_1    | 2023-05-09 16:57:57,902 [grpc-default-executor-0] INFO impl.RoleInfo: 5d36dd6f-a30c-40ec-958f-b246a8287262: shutdown 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-FollowerState
dn2_1    | 2023-05-09 16:57:57,902 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-FollowerState] INFO impl.FollowerState: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-FollowerState was interrupted
dn2_1    | 2023-05-09 16:57:57,902 [grpc-default-executor-0] INFO impl.RoleInfo: 5d36dd6f-a30c-40ec-958f-b246a8287262: start 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-FollowerState
dn2_1    | 2023-05-09 16:57:57,903 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-05-09 16:57:57,903 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-05-09 16:57:57,910 [grpc-default-executor-0] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A replies to ELECTION vote request: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69<-5d36dd6f-a30c-40ec-958f-b246a8287262#0:OK-t2. Peer's state: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A:t2, leader=null, voted=3aa72183-93e1-4d03-9ed4-71b6f7b45f69, raftlog=Memoized:5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-09 16:57:58,086 [5d36dd6f-a30c-40ec-958f-b246a8287262-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D3D6EB9F6B5A with new leaderId: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69
dn2_1    | 2023-05-09 16:57:58,086 [5d36dd6f-a30c-40ec-958f-b246a8287262-server-thread1] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A: change Leader from null to 3aa72183-93e1-4d03-9ed4-71b6f7b45f69 at term 2 for appendEntries, leader elected after 10964ms
dn2_1    | 2023-05-09 16:57:58,088 [5d36dd6f-a30c-40ec-958f-b246a8287262-server-thread1] INFO server.RaftServer$Division: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A: set configuration 0: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-05-09 16:57:58,091 [5d36dd6f-a30c-40ec-958f-b246a8287262-server-thread1] INFO segmented.SegmentedRaftLogWorker: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-SegmentedRaftLogWorker: Starting segment from index:0
dn2_1    | 2023-05-09 16:57:58,095 [5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5d36dd6f-a30c-40ec-958f-b246a8287262@group-D3D6EB9F6B5A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/73e0c514-5c6d-4883-b850-d3d6eb9f6b5a/current/log_inprogress_0
dn2_1    | 2023-05-09 16:57:59,433 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
om1_1    | 2023-05-09 16:57:30,227 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | 2023-05-09 16:57:30,227 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om1_1    | 2023-05-09 16:57:30,367 [pool-26-thread-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om1_1    | 2023-05-09 16:57:30,382 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1    | 2023-05-09 16:57:30,434 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om1_1    | 2023-05-09 16:57:30,434 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om1_1    | 2023-05-09 16:57:30,569 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om1_1    | 2023-05-09 16:57:30,629 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om1_1    | 2023-05-09 16:57:30,630 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om1_1    | 2023-05-09 16:57:31,469 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1    | 2023-05-09 16:57:31,480 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om1_1    | 2023-05-09 16:57:31,544 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om1_1    | 2023-05-09 16:57:31,551 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om1_1    | 2023-05-09 16:57:31,554 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om1_1    | 2023-05-09 16:57:32,594 [main] INFO reflections.Reflections: Reflections took 2384 ms to scan 8 urls, producing 23 keys and 521 values [using 2 cores]
om1_1    | 2023-05-09 16:57:33,073 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om1_1    | 2023-05-09 16:57:33,144 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om1_1    | 2023-05-09 16:57:33,793 [Listener at om1/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om1_1    | 2023-05-09 16:57:33,864 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om1_1    | 2023-05-09 16:57:33,864 [Listener at om1/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om1_1    | 2023-05-09 16:57:34,121 [Listener at om1/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om1/10.9.0.11:9862
om1_1    | 2023-05-09 16:57:34,121 [Listener at om1/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om1_1    | 2023-05-09 16:57:34,143 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
om1_1    | 2023-05-09 16:57:34,154 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@367fa92bbc56
om1_1    | 2023-05-09 16:57:34,259 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
om1_1    | 2023-05-09 16:57:34,283 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om1_1    | 2023-05-09 16:57:34,327 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1    | 2023-05-09 16:57:34,328 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-05-09 16:57:34,329 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om1_1    | 2023-05-09 16:57:34,332 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om1_1    | 2023-05-09 16:57:34,343 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1    | 2023-05-09 16:57:34,349 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om1_1    | 2023-05-09 16:57:34,354 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1    | 2023-05-09 16:57:34,371 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om1_1    | 2023-05-09 16:57:34,395 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om1_1    | 2023-05-09 16:57:34,399 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om1_1    | 2023-05-09 16:57:34,412 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1    | 2023-05-09 16:57:34,415 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om1_1    | 2023-05-09 16:57:34,415 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1    | 2023-05-09 16:57:34,443 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om1_1    | 2023-05-09 16:57:34,449 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om1_1    | 2023-05-09 16:57:34,450 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om1_1    | 2023-05-09 16:57:34,483 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om1_1    | 2023-05-09 16:57:34,486 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om1_1    | 2023-05-09 16:57:34,487 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om1_1    | 2023-05-09 16:57:34,487 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om1_1    | 2023-05-09 16:57:34,512 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om1_1    | 2023-05-09 16:57:34,512 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om1_1    | 2023-05-09 16:57:34,514 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-05-09 16:57:34,518 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn4_1    | 2023-05-09 16:57:47,522 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-05-09 16:57:47,522 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-05-09 16:57:47,522 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-05-09 16:57:47,523 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-05-09 16:57:47,525 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-05-09 16:57:47,550 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-05-09 16:57:47,551 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-05-09 16:57:47,552 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-05-09 16:57:47,552 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-05-09 16:57:47,553 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/73e0c514-5c6d-4883-b850-d3d6eb9f6b5a does not exist. Creating ...
dn4_1    | 2023-05-09 16:57:47,567 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/73e0c514-5c6d-4883-b850-d3d6eb9f6b5a/in_use.lock acquired by nodename 7@3ffda8dc18fe
dn4_1    | 2023-05-09 16:57:47,572 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/73e0c514-5c6d-4883-b850-d3d6eb9f6b5a has been successfully formatted.
dn4_1    | 2023-05-09 16:57:47,587 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-D3D6EB9F6B5A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn4_1    | 2023-05-09 16:57:47,587 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-05-09 16:57:47,597 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-05-09 16:57:47,597 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-09 16:57:47,599 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-05-09 16:57:47,599 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-05-09 16:57:47,600 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-05-09 16:57:47,600 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-05-09 16:57:47,601 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-05-09 16:57:47,602 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/73e0c514-5c6d-4883-b850-d3d6eb9f6b5a
dn4_1    | 2023-05-09 16:57:47,602 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-05-09 16:57:47,602 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-05-09 16:57:47,620 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-05-09 16:57:47,621 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-05-09 16:57:47,623 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-05-09 16:57:47,624 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-05-09 16:57:47,624 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-05-09 16:57:47,624 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-05-09 16:57:47,624 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-05-09 16:57:47,630 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-05-09 16:57:47,630 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-05-09 16:57:47,634 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-05-09 16:57:47,634 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-05-09 16:57:47,642 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-05-09 16:57:47,695 [pool-22-thread-1] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A: start as a follower, conf=-1: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-09 16:57:47,697 [pool-22-thread-1] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn4_1    | 2023-05-09 16:57:47,699 [pool-22-thread-1] INFO impl.RoleInfo: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: start 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-FollowerState
dn4_1    | 2023-05-09 16:57:47,702 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D3D6EB9F6B5A,id=3aa72183-93e1-4d03-9ed4-71b6f7b45f69
dn4_1    | 2023-05-09 16:57:47,705 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-05-09 16:57:47,706 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-05-09 16:57:47,706 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-05-09 16:57:47,706 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-05-09 16:57:47,707 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-05-09 16:57:47,720 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-09 16:57:47,721 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=73e0c514-5c6d-4883-b850-d3d6eb9f6b5a
om1_1    | 2023-05-09 16:57:34,521 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
om1_1    | 2023-05-09 16:57:34,544 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1    | 2023-05-09 16:57:34,548 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 2023-05-09 16:57:34,553 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om1
om1_1    | 2023-05-09 16:57:34,556 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om1_1    | 2023-05-09 16:57:34,570 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om1_1    | 2023-05-09 16:57:34,571 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om1_1    | 2023-05-09 16:57:34,584 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om1_1    | 2023-05-09 16:57:34,611 [Listener at om1/9862] INFO server.RaftServer: om1: start RPC server
om1_1    | 2023-05-09 16:57:34,765 [Listener at om1/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om1_1    | 2023-05-09 16:57:34,774 [Listener at om1/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om1_1    | 2023-05-09 16:57:34,801 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om1_1    | 2023-05-09 16:57:34,929 [Listener at om1/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om1_1    | 2023-05-09 16:57:34,934 [Listener at om1/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om1_1    | 2023-05-09 16:57:34,995 [Listener at om1/9862] INFO util.log: Logging initialized @22220ms to org.eclipse.jetty.util.log.Slf4jLog
om1_1    | 2023-05-09 16:57:35,613 [Listener at om1/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om1_1    | 2023-05-09 16:57:35,667 [Listener at om1/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om1_1    | 2023-05-09 16:57:35,733 [Listener at om1/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om1_1    | 2023-05-09 16:57:35,740 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om1_1    | 2023-05-09 16:57:35,740 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om1_1    | 2023-05-09 16:57:35,740 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om1_1    | 2023-05-09 16:57:36,061 [Listener at om1/9862] INFO http.HttpServer2: Jetty bound to port 9874
om1_1    | 2023-05-09 16:57:36,063 [Listener at om1/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om1_1    | 2023-05-09 16:57:36,252 [Listener at om1/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om1_1    | 2023-05-09 16:57:36,260 [Listener at om1/9862] INFO server.session: No SessionScavenger set, using defaults
om1_1    | 2023-05-09 16:57:36,283 [Listener at om1/9862] INFO server.session: node0 Scavenging every 660000ms
om1_1    | 2023-05-09 16:57:36,361 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@104a287c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om1_1    | 2023-05-09 16:57:36,367 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2fbd390{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/static,AVAILABLE}
om1_1    | 2023-05-09 16:57:37,962 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@734fbae3{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_3_0_jar-_-any-3412290461910101239/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/ozoneManager}
om1_1    | 2023-05-09 16:57:38,019 [Listener at om1/9862] INFO server.AbstractConnector: Started ServerConnector@4e4894d{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om1_1    | 2023-05-09 16:57:38,030 [Listener at om1/9862] INFO server.Server: Started @25255ms
om1_1    | 2023-05-09 16:57:38,042 [Listener at om1/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om1_1    | 2023-05-09 16:57:38,042 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om1_1    | 2023-05-09 16:57:38,044 [Listener at om1/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om1_1    | 2023-05-09 16:57:38,044 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1    | 2023-05-09 16:57:38,066 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om1_1    | 2023-05-09 16:57:38,179 [Listener at om1/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om1_1    | 2023-05-09 16:57:38,372 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2084e65a] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om1_1    | 2023-05-09 16:57:39,730 [om1@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om1@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5208851274ns, electionTimeout:5180ms
om1_1    | 2023-05-09 16:57:39,731 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-FollowerState
om1_1    | 2023-05-09 16:57:39,731 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om1_1    | 2023-05-09 16:57:39,734 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om1_1    | 2023-05-09 16:57:39,734 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-LeaderElection1
om1_1    | 2023-05-09 16:57:39,762 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-05-09 16:57:39,879 [om1@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om3
om1_1    | 2023-05-09 16:57:39,879 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1    | 2023-05-09 16:57:39,879 [om1@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
om1_1    | 2023-05-09 16:57:39,892 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om2_1    | STARTUP_MSG:   java = 11.0.14.1
om2_1    | ************************************************************/
om2_1    | 2023-05-09 16:56:25,439 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1    | 2023-05-09 16:56:35,177 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1    | 2023-05-09 16:56:38,333 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om2_1    | 2023-05-09 16:56:38,790 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1    | 2023-05-09 16:56:38,796 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
om2_1    | 2023-05-09 16:56:38,825 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-05-09 16:56:40,176 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om2_1    | 2023-05-09 16:56:43,552 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ee9fbcddb50a/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-05-09 16:56:45,553 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ee9fbcddb50a/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-05-09 16:56:47,555 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ee9fbcddb50a/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-05-09 16:56:49,556 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ee9fbcddb50a/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-05-09 16:56:51,558 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ee9fbcddb50a/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-05-09 16:56:53,559 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ee9fbcddb50a/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-05-09 16:56:55,562 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ee9fbcddb50a/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-05-09 16:56:57,568 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ee9fbcddb50a/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-05-09 16:56:59,571 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ee9fbcddb50a/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-05-09 16:57:01,572 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ee9fbcddb50a/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-05-09 16:57:03,574 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ee9fbcddb50a/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-05-09 16:57:06,438 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:1f9ebad1-a542-47ab-b506-15f3d5e79738 is not the leader. Could not determine the leader node.
om2_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om2_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om2_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om2_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om2_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om2_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om2_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om2_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om2_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om2_1    | , while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-05-09 16:57:08,440 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ee9fbcddb50a/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-05-09 16:57:10,442 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ee9fbcddb50a/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-f33ab5ed-453e-4ae9-ac5c-d55092fa9737;layoutVersion=3
om2_1    | 2023-05-09 16:57:12,550 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om2_1    | /************************************************************
om2_1    | SHUTDOWN_MSG: Shutting down OzoneManager at ee9fbcddb50a/10.9.0.12
om2_1    | ************************************************************/
om2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1    | 2023-05-09 16:57:14,740 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1    | /************************************************************
om2_1    | STARTUP_MSG: Starting OzoneManager
om2_1    | STARTUP_MSG:   host = ee9fbcddb50a/10.9.0.12
om2_1    | STARTUP_MSG:   args = [--]
om2_1    | STARTUP_MSG:   version = 1.3.0
om3_1    | 2023-05-09 16:56:55,037 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6eb4a2a6e0d5/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-05-09 16:56:57,039 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6eb4a2a6e0d5/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-05-09 16:56:59,041 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6eb4a2a6e0d5/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-05-09 16:57:01,042 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6eb4a2a6e0d5/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-05-09 16:57:03,044 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6eb4a2a6e0d5/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-05-09 16:57:05,046 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6eb4a2a6e0d5/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-05-09 16:57:07,048 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6eb4a2a6e0d5/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-05-09 16:57:09,082 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:1f9ebad1-a542-47ab-b506-15f3d5e79738 is not the leader. Could not determine the leader node.
om3_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om3_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om3_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om3_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om3_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om3_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om3_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om3_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om3_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om3_1    | , while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-05-09 16:57:11,089 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6eb4a2a6e0d5/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 16 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-05-09 16:57:13,091 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6eb4a2a6e0d5/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 17 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-f33ab5ed-453e-4ae9-ac5c-d55092fa9737;layoutVersion=3
om3_1    | 2023-05-09 16:57:15,174 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om3_1    | /************************************************************
om3_1    | SHUTDOWN_MSG: Shutting down OzoneManager at 6eb4a2a6e0d5/10.9.0.13
om3_1    | ************************************************************/
om3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1    | 2023-05-09 16:57:20,000 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1    | /************************************************************
om3_1    | STARTUP_MSG: Starting OzoneManager
om3_1    | STARTUP_MSG:   host = 6eb4a2a6e0d5/10.9.0.13
om3_1    | STARTUP_MSG:   args = [--]
om3_1    | STARTUP_MSG:   version = 1.3.0
om3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn5_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.ConnectException: Connection refused
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn5_1    | 	... 12 more
dn5_1    | 2023-05-09 16:57:14,520 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:14,521 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn5_1    | java.net.ConnectException: Call From 2439abba3020/10.9.0.21 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn5_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.ConnectException: Connection refused
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn5_1    | 	... 12 more
dn5_1    | 2023-05-09 16:57:15,280 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn5_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
om3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om3_1    | STARTUP_MSG:   java = 11.0.14.1
om3_1    | ************************************************************/
om3_1    | 2023-05-09 16:57:20,008 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1    | 2023-05-09 16:57:24,011 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1    | 2023-05-09 16:57:25,383 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om3_1    | 2023-05-09 16:57:25,711 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1    | 2023-05-09 16:57:25,711 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om3_1    | 2023-05-09 16:57:25,728 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-05-09 16:57:25,886 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om3_1    | 2023-05-09 16:57:27,184 [main] INFO reflections.Reflections: Reflections took 1069 ms to scan 1 urls, producing 114 keys and 335 values [using 2 cores]
om3_1    | 2023-05-09 16:57:27,264 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-05-09 16:57:28,244 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om3_1    | 2023-05-09 16:57:28,448 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om3_1    | 2023-05-09 16:57:31,203 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-05-09 16:57:31,812 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om3_1    | 2023-05-09 16:57:31,816 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om3_1    | 2023-05-09 16:57:32,409 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om3_1    | 2023-05-09 16:57:32,569 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om3_1    | 2023-05-09 16:57:32,648 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1    | 2023-05-09 16:57:32,658 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om3_1    | 2023-05-09 16:57:32,697 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om3_1    | 2023-05-09 16:57:33,395 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om3_1    | 2023-05-09 16:57:33,446 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1    | 2023-05-09 16:57:33,778 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om3:9872, om1:9872, om2:9872
om3_1    | 2023-05-09 16:57:33,870 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om3_1    | 2023-05-09 16:57:34,093 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om3_1    | 2023-05-09 16:57:34,403 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om3_1    | 2023-05-09 16:57:34,422 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om3_1    | 2023-05-09 16:57:34,433 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om3_1    | 2023-05-09 16:57:34,437 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om3_1    | 2023-05-09 16:57:34,437 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om3_1    | 2023-05-09 16:57:34,437 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om3_1    | 2023-05-09 16:57:34,438 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om3_1    | 2023-05-09 16:57:34,445 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-05-09 16:57:34,461 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om3_1    | 2023-05-09 16:57:34,461 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1    | 2023-05-09 16:57:34,486 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om3_1    | 2023-05-09 16:57:34,527 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om3_1    | 2023-05-09 16:57:34,527 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om3_1    | 2023-05-09 16:57:35,966 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om3_1    | 2023-05-09 16:57:35,983 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om3_1    | 2023-05-09 16:57:35,983 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om3_1    | 2023-05-09 16:57:35,984 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1    | 2023-05-09 16:57:35,984 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1    | 2023-05-09 16:57:36,001 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1    | 2023-05-09 16:57:36,093 [main] INFO server.RaftServer: om3: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@34d480b9[Not completed]
om3_1    | 2023-05-09 16:57:36,094 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om3_1    | 2023-05-09 16:57:36,164 [main] INFO om.OzoneManager: Creating RPC Server
om3_1    | 2023-05-09 16:57:36,261 [pool-26-thread-1] INFO server.RaftServer$Division: om3: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om3_1    | 2023-05-09 16:57:36,374 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om3_1    | 2023-05-09 16:57:36,382 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om3_1    | 2023-05-09 16:57:36,382 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-05-09 16:57:47,838 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-05-09 16:57:47,842 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-05-09 16:57:47,853 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-05-09 16:57:47,853 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-05-09 16:57:47,865 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-05-09 16:57:47,910 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-05-09 16:57:47,929 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-05-09 16:57:47,942 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-05-09 16:57:47,945 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-05-09 16:57:47,989 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-05-09 16:57:47,989 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-05-09 16:57:48,011 [pool-22-thread-1] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C: start as a follower, conf=-1: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-09 16:57:48,018 [pool-22-thread-1] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn3_1    | 2023-05-09 16:57:48,031 [pool-22-thread-1] INFO impl.RoleInfo: 56216a20-9d0f-4921-897d-3244639db044: start 56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-FollowerState
dn3_1    | 2023-05-09 16:57:48,046 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-05-09 16:57:48,046 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-05-09 16:57:48,051 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4FE9FF9ECA5C,id=56216a20-9d0f-4921-897d-3244639db044
dn3_1    | 2023-05-09 16:57:48,056 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-05-09 16:57:48,062 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-05-09 16:57:48,067 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-05-09 16:57:48,069 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-05-09 16:57:48,211 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=8c529fe3-ebe2-46fe-b4b1-4fe9ff9eca5c
dn3_1    | 2023-05-09 16:57:48,215 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=8c529fe3-ebe2-46fe-b4b1-4fe9ff9eca5c.
dn3_1    | 2023-05-09 16:57:48,240 [Command processor thread] INFO server.RaftServer: 56216a20-9d0f-4921-897d-3244639db044: addNew group-D3D6EB9F6B5A:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER] returns group-D3D6EB9F6B5A:java.util.concurrent.CompletableFuture@366e054e[Not completed]
dn3_1    | 2023-05-09 16:57:48,245 [pool-22-thread-1] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044: new RaftServerImpl for group-D3D6EB9F6B5A:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn3_1    | 2023-05-09 16:57:48,287 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-05-09 16:57:48,287 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-05-09 16:57:48,288 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-05-09 16:57:48,289 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-05-09 16:57:48,291 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-05-09 16:57:48,291 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-05-09 16:57:48,294 [pool-22-thread-1] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A: ConfigurationManager, init=-1: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-05-09 16:57:48,298 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-05-09 16:57:48,302 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-05-09 16:57:48,311 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-05-09 16:57:48,313 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-05-09 16:57:48,317 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-05-09 16:57:48,318 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-05-09 16:57:48,324 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-05-09 16:57:48,330 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-05-09 16:57:48,331 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-05-09 16:57:47,784 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:48,788 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:49,796 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:50,797 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:51,426 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=73e0c514-5c6d-4883-b850-d3d6eb9f6b5a.
dn4_1    | 2023-05-09 16:57:51,426 [Command processor thread] INFO server.RaftServer: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: addNew group-391CD66B588A:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] returns group-391CD66B588A:java.util.concurrent.CompletableFuture@67d217fb[Not completed]
dn4_1    | 2023-05-09 16:57:51,437 [pool-22-thread-1] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: new RaftServerImpl for group-391CD66B588A:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn4_1    | 2023-05-09 16:57:51,438 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-05-09 16:57:51,438 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-05-09 16:57:51,439 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-05-09 16:57:51,439 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-05-09 16:57:51,439 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-05-09 16:57:51,439 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-05-09 16:57:51,439 [pool-22-thread-1] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A: ConfigurationManager, init=-1: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-05-09 16:57:51,440 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-05-09 16:57:51,440 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-05-09 16:57:51,441 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-05-09 16:57:51,441 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-05-09 16:57:51,441 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-05-09 16:57:51,442 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-05-09 16:57:51,443 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-05-09 16:57:51,444 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-05-09 16:57:51,450 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-05-09 16:57:51,450 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-05-09 16:57:51,450 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-05-09 16:57:51,451 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ffceeb78-bcf8-4a49-9756-391cd66b588a does not exist. Creating ...
dn4_1    | 2023-05-09 16:57:51,453 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ffceeb78-bcf8-4a49-9756-391cd66b588a/in_use.lock acquired by nodename 7@3ffda8dc18fe
dn4_1    | 2023-05-09 16:57:51,458 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ffceeb78-bcf8-4a49-9756-391cd66b588a has been successfully formatted.
dn4_1    | 2023-05-09 16:57:51,459 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-391CD66B588A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn4_1    | 2023-05-09 16:57:51,459 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-05-09 16:57:51,459 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-05-09 16:57:51,459 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-09 16:57:51,460 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-05-09 16:57:51,460 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-05-09 16:57:51,460 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-05-09 16:57:51,460 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-05-09 16:57:51,461 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-05-09 16:57:51,461 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ffceeb78-bcf8-4a49-9756-391cd66b588a
dn4_1    | 2023-05-09 16:57:51,462 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-05-09 16:57:51,471 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
om1_1    | 2023-05-09 16:57:40,755 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-05-09 16:57:41,919 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1: ELECTION REJECTED received 1 response(s) and 1 exception(s):
om1_1    | 2023-05-09 16:57:41,920 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om1<-om2#0:FAIL-t1
om1_1    | 2023-05-09 16:57:41,920 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-05-09 16:57:41,921 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 ELECTION round 0: result REJECTED
om1_1    | 2023-05-09 16:57:41,922 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
om1_1    | 2023-05-09 16:57:41,922 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-LeaderElection1
om1_1    | 2023-05-09 16:57:41,922 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
om1_1    | 2023-05-09 16:57:42,069 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1    | 2023-05-09 16:57:42,069 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 2023-05-09 16:57:42,577 [grpc-default-executor-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(ELECTION, om2, group-D66704EFC61C, 1, (t:0, i:~))
om1_1    | 2023-05-09 16:57:42,578 [grpc-default-executor-1] INFO impl.VoteContext: om1@group-D66704EFC61C-FOLLOWER: reject ELECTION from om2: already has voted for om1 at current term 1
om1_1    | 2023-05-09 16:57:42,594 [grpc-default-executor-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to ELECTION vote request: om2<-om1#0:FAIL-t1. Peer's state: om1@group-D66704EFC61C:t1, leader=null, voted=om1, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-05-09 16:57:44,939 [om1-server-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: change Leader from null to om2 at term 1 for appendEntries, leader elected after 14370ms
om1_1    | 2023-05-09 16:57:44,945 [om1-server-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-05-09 16:57:44,963 [om1-server-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:0
om1_1    | 2023-05-09 16:57:45,607 [om1@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0
om1_1    | 2023-05-09 16:57:48,666 [om1@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om1_1    | [id: "om1"
om1_1    | address: "om1:9872"
om1_1    | startupRole: FOLLOWER
om1_1    | , id: "om3"
om1_1    | address: "om3:9872"
om1_1    | startupRole: FOLLOWER
om1_1    | , id: "om2"
om1_1    | address: "om2:9872"
om1_1    | startupRole: FOLLOWER
om1_1    | ]
om1_1    | 2023-05-09 16:58:54,107 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:old1-volume for user:hadoop
om1_1    | 2023-05-09 16:58:56,711 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: old1-bucket of layout LEGACY in volume: old1-volume
om1_1    | 2023-05-09 16:59:05,521 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: old1-bucket of layout LEGACY in volume: s3v
om1_1    | 2023-05-09 16:59:14,829 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:old1-bucket in volume:s3v
om1_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om1_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:206)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:311)
om1_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om1_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om1_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | 2023-05-09 16:59:34,865 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om1 Received prepare request with log index 23
om1_1    | 2023-05-09 16:59:34,868 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: om1 waiting for index 23 to flush to OM DB and index 24 to flush to Ratis state machine.
om1_1    | 2023-05-09 16:59:39,869 [OM StateMachine ApplyTransaction Thread - 0] INFO ratis.OzoneManagerStateMachine: Current Snapshot Index (t:1, i:24)
om1_1    | 2023-05-09 16:59:39,871 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 24 -> 24
om1_1    | 2023-05-09 16:59:39,871 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally -1 -> 24
om1_1    | 2023-05-09 16:59:39,872 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: Closing segment log_inprogress_0 to index: 24
om1_1    | 2023-05-09 16:59:39,874 [om1@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0 to /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_0-24
om1_1    | 2023-05-09 16:59:39,886 [OM StateMachine ApplyTransaction Thread - 0] INFO raftlog.RaftLog: om1@group-D66704EFC61C-SegmentedRaftLog: snapshotIndex: updateIncreasingly -1 -> 24
om1_1    | 2023-05-09 16:59:39,905 [OM StateMachine ApplyTransaction Thread - 0] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 23 to file /data/metadata/current/prepareMarker
om1_1    | 2023-05-09 16:59:39,907 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om1 prepared at log index 23. Returning response txnID: 23
om1_1    |  with log index 23
dn4_1    | 2023-05-09 16:57:51,471 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-05-09 16:57:51,471 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-05-09 16:57:51,472 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-05-09 16:57:51,472 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-05-09 16:57:51,472 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-05-09 16:57:51,473 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-05-09 16:57:51,474 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-05-09 16:57:51,474 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-05-09 16:57:51,477 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-05-09 16:57:51,477 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-05-09 16:57:51,477 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-05-09 16:57:51,478 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-05-09 16:57:51,478 [pool-22-thread-1] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A: start as a follower, conf=-1: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-09 16:57:51,478 [pool-22-thread-1] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn4_1    | 2023-05-09 16:57:51,479 [pool-22-thread-1] INFO impl.RoleInfo: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: start 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A-FollowerState
dn4_1    | 2023-05-09 16:57:51,495 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-391CD66B588A,id=3aa72183-93e1-4d03-9ed4-71b6f7b45f69
dn4_1    | 2023-05-09 16:57:51,495 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-05-09 16:57:51,495 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-05-09 16:57:51,495 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-05-09 16:57:51,495 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-05-09 16:57:51,497 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-05-09 16:57:51,513 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=ffceeb78-bcf8-4a49-9756-391cd66b588a
dn4_1    | 2023-05-09 16:57:51,547 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-09 16:57:51,769 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=ffceeb78-bcf8-4a49-9756-391cd66b588a.
dn4_1    | 2023-05-09 16:57:51,797 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:52,371 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-FollowerState] INFO impl.FollowerState: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5194958309ns, electionTimeout:5135ms
dn4_1    | 2023-05-09 16:57:52,372 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-FollowerState] INFO impl.RoleInfo: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: shutdown 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-FollowerState
dn4_1    | 2023-05-09 16:57:52,372 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-FollowerState] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn4_1    | 2023-05-09 16:57:52,375 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn4_1    | 2023-05-09 16:57:52,376 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-FollowerState] INFO impl.RoleInfo: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: start 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-LeaderElection1
dn4_1    | 2023-05-09 16:57:52,383 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-LeaderElection1] INFO impl.LeaderElection: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-09 16:57:52,384 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-LeaderElection1] INFO impl.LeaderElection: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-LeaderElection1 ELECTION round 0: result PASSED (term=1)
dn4_1    | 2023-05-09 16:57:52,395 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-LeaderElection1] INFO impl.RoleInfo: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: shutdown 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-LeaderElection1
dn4_1    | 2023-05-09 16:57:52,396 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-LeaderElection1] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn4_1    | 2023-05-09 16:57:52,396 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-1A6E129F2A79 with new leaderId: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69
dn4_1    | 2023-05-09 16:57:52,396 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-LeaderElection1] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79: change Leader from null to 3aa72183-93e1-4d03-9ed4-71b6f7b45f69 at term 1 for becomeLeader, leader elected after 6412ms
dn4_1    | 2023-05-09 16:57:52,404 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn3_1    | 2023-05-09 16:57:48,333 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-05-09 16:57:48,333 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-05-09 16:57:48,336 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/73e0c514-5c6d-4883-b850-d3d6eb9f6b5a does not exist. Creating ...
dn3_1    | 2023-05-09 16:57:48,343 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/73e0c514-5c6d-4883-b850-d3d6eb9f6b5a/in_use.lock acquired by nodename 6@84ef10f1d5b5
dn3_1    | 2023-05-09 16:57:48,357 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/73e0c514-5c6d-4883-b850-d3d6eb9f6b5a has been successfully formatted.
dn3_1    | 2023-05-09 16:57:48,424 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-D3D6EB9F6B5A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-05-09 16:57:48,424 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-05-09 16:57:48,424 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-05-09 16:57:48,459 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-05-09 16:57:48,482 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-05-09 16:57:48,483 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-05-09 16:57:48,483 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-05-09 16:57:48,486 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-05-09 16:57:48,491 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-05-09 16:57:48,492 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/73e0c514-5c6d-4883-b850-d3d6eb9f6b5a
dn3_1    | 2023-05-09 16:57:48,492 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-05-09 16:57:48,492 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-05-09 16:57:48,493 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-05-09 16:57:48,493 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-05-09 16:57:48,494 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-05-09 16:57:48,494 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-05-09 16:57:48,494 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-05-09 16:57:48,495 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-05-09 16:57:48,496 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-05-09 16:57:48,497 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-05-09 16:57:48,518 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-05-09 16:57:48,519 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-05-09 16:57:48,530 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-05-09 16:57:48,531 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-05-09 16:57:48,531 [pool-22-thread-1] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A: start as a follower, conf=-1: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-09 16:57:48,531 [pool-22-thread-1] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn3_1    | 2023-05-09 16:57:48,542 [pool-22-thread-1] INFO impl.RoleInfo: 56216a20-9d0f-4921-897d-3244639db044: start 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-FollowerState
dn3_1    | 2023-05-09 16:57:48,543 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D3D6EB9F6B5A,id=56216a20-9d0f-4921-897d-3244639db044
dn3_1    | 2023-05-09 16:57:48,543 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-05-09 16:57:48,543 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-05-09 16:57:48,543 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-05-09 16:57:48,543 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-05-09 16:57:48,545 [56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-05-09 16:57:48,546 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=73e0c514-5c6d-4883-b850-d3d6eb9f6b5a
dn3_1    | 2023-05-09 16:57:48,637 [56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-05-09 16:57:48,673 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:49,677 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:50,685 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:51,402 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=73e0c514-5c6d-4883-b850-d3d6eb9f6b5a.
dn3_1    | 2023-05-09 16:57:51,407 [pool-22-thread-1] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044: new RaftServerImpl for group-391CD66B588A:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn3_1    | 2023-05-09 16:57:51,408 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-05-09 16:57:51,408 [Command processor thread] INFO server.RaftServer: 56216a20-9d0f-4921-897d-3244639db044: addNew group-391CD66B588A:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] returns group-391CD66B588A:java.util.concurrent.CompletableFuture@85a11a1[Not completed]
dn3_1    | 2023-05-09 16:57:51,411 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-05-09 16:57:51,412 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-05-09 16:57:51,412 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-05-09 16:57:51,412 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-05-09 16:57:51,412 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-05-09 16:57:51,412 [pool-22-thread-1] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A: ConfigurationManager, init=-1: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-05-09 16:57:51,412 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-05-09 16:57:51,413 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-05-09 16:57:51,413 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-05-09 16:57:51,413 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-05-09 16:57:51,413 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-05-09 16:57:51,413 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-05-09 16:57:51,416 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-05-09 16:57:51,427 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-05-09 16:57:51,427 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-05-09 16:57:51,427 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-05-09 16:57:51,427 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-05-09 16:57:51,427 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ffceeb78-bcf8-4a49-9756-391cd66b588a does not exist. Creating ...
dn3_1    | 2023-05-09 16:57:51,431 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ffceeb78-bcf8-4a49-9756-391cd66b588a/in_use.lock acquired by nodename 6@84ef10f1d5b5
dn3_1    | 2023-05-09 16:57:51,467 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ffceeb78-bcf8-4a49-9756-391cd66b588a has been successfully formatted.
dn3_1    | 2023-05-09 16:57:51,468 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-391CD66B588A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-05-09 16:57:51,488 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-05-09 16:57:51,488 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-05-09 16:57:51,488 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-05-09 16:57:51,488 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-05-09 16:57:51,488 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-05-09 16:57:51,488 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-05-09 16:57:51,489 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-05-09 16:57:51,489 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-05-09 16:57:51,489 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ffceeb78-bcf8-4a49-9756-391cd66b588a
dn3_1    | 2023-05-09 16:57:51,489 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-05-09 16:57:51,489 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-05-09 16:57:51,489 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-05-09 16:57:51,489 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-05-09 16:57:51,489 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-05-09 16:57:51,489 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-05-09 16:57:51,490 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-05-09 16:57:51,490 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-05-09 16:57:51,490 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-05-09 16:57:51,491 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-05-09 16:57:51,494 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	... 1 more
dn5_1    | 2023-05-09 16:57:15,512 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:15,522 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:16,513 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:16,523 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:17,514 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:17,524 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:18,515 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:18,525 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:19,516 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:19,525 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:20,517 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:20,526 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:21,518 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:21,527 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:22,519 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:22,528 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:23,520 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:23,529 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:24,521 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:24,530 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:25,522 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:25,531 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:26,522 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:26,531 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:27,523 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:27,533 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:52,413 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-05-09 16:57:52,416 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn4_1    | 2023-05-09 16:57:52,421 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn4_1    | 2023-05-09 16:57:52,422 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn4_1    | 2023-05-09 16:57:52,422 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn4_1    | 2023-05-09 16:57:52,426 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-05-09 16:57:52,428 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn4_1    | 2023-05-09 16:57:52,432 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-LeaderElection1] INFO impl.RoleInfo: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: start 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-LeaderStateImpl
dn4_1    | 2023-05-09 16:57:52,506 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-05-09 16:57:52,524 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-LeaderElection1] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79: set configuration 0: peers:[3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-09 16:57:52,656 [grpc-default-executor-0] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A: receive requestVote(ELECTION, 5d36dd6f-a30c-40ec-958f-b246a8287262, group-D3D6EB9F6B5A, 1, (t:0, i:0))
dn4_1    | 2023-05-09 16:57:52,658 [grpc-default-executor-0] INFO impl.VoteContext: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-FOLLOWER: reject ELECTION from 5d36dd6f-a30c-40ec-958f-b246a8287262: our priority 1 > candidate's priority 0
dn4_1    | 2023-05-09 16:57:52,658 [grpc-default-executor-0] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:5d36dd6f-a30c-40ec-958f-b246a8287262
dn4_1    | 2023-05-09 16:57:52,658 [grpc-default-executor-0] INFO impl.RoleInfo: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: shutdown 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-FollowerState
dn4_1    | 2023-05-09 16:57:52,658 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-FollowerState] INFO impl.FollowerState: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-FollowerState was interrupted
dn4_1    | 2023-05-09 16:57:52,677 [grpc-default-executor-0] INFO impl.RoleInfo: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: start 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-FollowerState
dn4_1    | 2023-05-09 16:57:52,681 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-05-09 16:57:52,681 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-09 16:57:52,737 [grpc-default-executor-0] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A replies to ELECTION vote request: 5d36dd6f-a30c-40ec-958f-b246a8287262<-3aa72183-93e1-4d03-9ed4-71b6f7b45f69#0:FAIL-t1. Peer's state: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A:t1, leader=null, voted=null, raftlog=Memoized:3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-09 16:57:52,802 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:52,866 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-1A6E129F2A79-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/c3e11b37-a77f-4003-8d5f-1a6e129f2a79/current/log_inprogress_0
dn4_1    | 2023-05-09 16:57:53,803 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:54,804 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:55,805 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-05-09 16:57:56,528 [grpc-default-executor-0] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A: receive requestVote(ELECTION, 5d36dd6f-a30c-40ec-958f-b246a8287262, group-391CD66B588A, 1, (t:0, i:0))
dn4_1    | 2023-05-09 16:57:56,528 [grpc-default-executor-0] INFO impl.VoteContext: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A-FOLLOWER: accept ELECTION from 5d36dd6f-a30c-40ec-958f-b246a8287262: our priority 0 <= candidate's priority 1
dn4_1    | 2023-05-09 16:57:56,528 [grpc-default-executor-0] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:5d36dd6f-a30c-40ec-958f-b246a8287262
dn4_1    | 2023-05-09 16:57:56,528 [grpc-default-executor-0] INFO impl.RoleInfo: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: shutdown 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A-FollowerState
dn4_1    | 2023-05-09 16:57:56,528 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A-FollowerState] INFO impl.FollowerState: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A-FollowerState was interrupted
dn4_1    | 2023-05-09 16:57:56,530 [grpc-default-executor-0] INFO impl.RoleInfo: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: start 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A-FollowerState
dn4_1    | 2023-05-09 16:57:56,538 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om2_1    | STARTUP_MSG:   java = 11.0.14.1
om2_1    | ************************************************************/
om2_1    | 2023-05-09 16:57:14,753 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1    | 2023-05-09 16:57:18,724 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1    | 2023-05-09 16:57:20,973 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om2_1    | 2023-05-09 16:57:21,325 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1    | 2023-05-09 16:57:21,336 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
om2_1    | 2023-05-09 16:57:21,352 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-05-09 16:57:21,489 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om2_1    | 2023-05-09 16:57:22,734 [main] INFO reflections.Reflections: Reflections took 1023 ms to scan 1 urls, producing 114 keys and 335 values [using 2 cores]
om2_1    | 2023-05-09 16:57:22,785 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-05-09 16:57:23,833 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om2_1    | 2023-05-09 16:57:24,010 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om2_1    | 2023-05-09 16:57:26,934 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-05-09 16:57:27,337 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om2_1    | 2023-05-09 16:57:27,338 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om2_1    | 2023-05-09 16:57:27,969 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om2_1    | 2023-05-09 16:57:28,129 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om2_1    | 2023-05-09 16:57:28,271 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1    | 2023-05-09 16:57:28,275 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om2_1    | 2023-05-09 16:57:28,321 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om2_1    | 2023-05-09 16:57:29,133 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om2_1    | 2023-05-09 16:57:29,193 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1    | 2023-05-09 16:57:29,370 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om2:9872, om1:9872, om3:9872
om2_1    | 2023-05-09 16:57:29,433 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om2_1    | 2023-05-09 16:57:29,596 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om2_1    | 2023-05-09 16:57:29,830 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om2_1    | 2023-05-09 16:57:29,839 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om2_1    | 2023-05-09 16:57:29,843 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om2_1    | 2023-05-09 16:57:29,845 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om2_1    | 2023-05-09 16:57:29,845 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om2_1    | 2023-05-09 16:57:29,845 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om2_1    | 2023-05-09 16:57:29,846 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om2_1    | 2023-05-09 16:57:29,860 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-05-09 16:57:29,861 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om2_1    | 2023-05-09 16:57:29,862 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1    | 2023-05-09 16:57:29,893 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1    | 2023-05-09 16:57:29,922 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om2_1    | 2023-05-09 16:57:29,923 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om2_1    | 2023-05-09 16:57:30,883 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om2_1    | 2023-05-09 16:57:30,908 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om2_1    | 2023-05-09 16:57:30,924 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om2_1    | 2023-05-09 16:57:30,932 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1    | 2023-05-09 16:57:30,948 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | 2023-05-09 16:57:30,956 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1    | 2023-05-09 16:57:31,012 [main] INFO server.RaftServer: om2: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@7015ebef[Not completed]
om2_1    | 2023-05-09 16:57:31,012 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om2_1    | 2023-05-09 16:57:31,204 [main] INFO om.OzoneManager: Creating RPC Server
om2_1    | 2023-05-09 16:57:31,276 [pool-26-thread-1] INFO server.RaftServer$Division: om2: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om2_1    | 2023-05-09 16:57:31,293 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om2_1    | 2023-05-09 16:57:31,294 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om2_1    | 2023-05-09 16:57:31,298 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om2_1    | 2023-05-09 16:57:31,306 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1    | 2023-05-09 16:57:36,382 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1    | 2023-05-09 16:57:36,383 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1    | 2023-05-09 16:57:36,384 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om3_1    | 2023-05-09 16:57:36,657 [pool-26-thread-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om3_1    | 2023-05-09 16:57:36,733 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1    | 2023-05-09 16:57:36,853 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om3_1    | 2023-05-09 16:57:36,923 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om3_1    | 2023-05-09 16:57:37,011 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om3_1    | 2023-05-09 16:57:37,052 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om3_1    | 2023-05-09 16:57:37,317 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om3_1    | 2023-05-09 16:57:38,097 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1    | 2023-05-09 16:57:38,102 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om3_1    | 2023-05-09 16:57:38,164 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om3_1    | 2023-05-09 16:57:38,166 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om3_1    | 2023-05-09 16:57:38,167 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om3_1    | 2023-05-09 16:57:39,209 [main] INFO reflections.Reflections: Reflections took 2693 ms to scan 8 urls, producing 23 keys and 521 values [using 2 cores]
om3_1    | 2023-05-09 16:57:39,676 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1    | 2023-05-09 16:57:39,707 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om3_1    | 2023-05-09 16:57:40,418 [Listener at om3/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om3_1    | 2023-05-09 16:57:40,497 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om3_1    | 2023-05-09 16:57:40,497 [Listener at om3/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om3_1    | 2023-05-09 16:57:40,712 [Listener at om3/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om3/10.9.0.13:9862
om3_1    | 2023-05-09 16:57:40,725 [Listener at om3/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om3 at port 9872
om3_1    | 2023-05-09 16:57:40,736 [om3-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
om3_1    | 2023-05-09 16:57:40,761 [om3-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 6@6eb4a2a6e0d5
om3_1    | 2023-05-09 16:57:40,878 [om3-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
om3_1    | 2023-05-09 16:57:40,881 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om3_1    | 2023-05-09 16:57:40,928 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om3_1    | 2023-05-09 16:57:40,928 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-05-09 16:57:40,937 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om3_1    | 2023-05-09 16:57:40,946 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om3_1    | 2023-05-09 16:57:40,949 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1    | 2023-05-09 16:57:40,961 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om3_1    | 2023-05-09 16:57:40,975 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om3_1    | 2023-05-09 16:57:41,015 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om3@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om3_1    | 2023-05-09 16:57:41,026 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om3_1    | 2023-05-09 16:57:41,044 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om3_1    | 2023-05-09 16:57:41,045 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1    | 2023-05-09 16:57:41,051 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om3_1    | 2023-05-09 16:57:41,056 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om3_1    | 2023-05-09 16:57:41,063 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om3_1    | 2023-05-09 16:57:41,066 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om3_1    | 2023-05-09 16:57:41,075 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om3_1    | 2023-05-09 16:57:41,163 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om3_1    | 2023-05-09 16:57:41,164 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om3_1    | 2023-05-09 16:57:41,164 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om3_1    | 2023-05-09 16:57:41,165 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om3_1    | 2023-05-09 16:57:41,207 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om3_1    | 2023-05-09 16:57:41,207 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om3_1    | 2023-05-09 16:57:41,224 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-09 16:57:56,538 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-09 16:57:56,540 [grpc-default-executor-0] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A replies to ELECTION vote request: 5d36dd6f-a30c-40ec-958f-b246a8287262<-3aa72183-93e1-4d03-9ed4-71b6f7b45f69#0:OK-t1. Peer's state: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A:t1, leader=null, voted=5d36dd6f-a30c-40ec-958f-b246a8287262, raftlog=Memoized:3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-09 16:57:56,742 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-391CD66B588A with new leaderId: 5d36dd6f-a30c-40ec-958f-b246a8287262
dn4_1    | 2023-05-09 16:57:56,742 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69-server-thread1] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A: change Leader from null to 5d36dd6f-a30c-40ec-958f-b246a8287262 at term 1 for appendEntries, leader elected after 5300ms
dn4_1    | 2023-05-09 16:57:56,768 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69-server-thread2] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A: set configuration 0: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-09 16:57:56,768 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69-server-thread2] INFO segmented.SegmentedRaftLogWorker: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-05-09 16:57:56,777 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-391CD66B588A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ffceeb78-bcf8-4a49-9756-391cd66b588a/current/log_inprogress_0
dn4_1    | 2023-05-09 16:57:57,820 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-FollowerState] INFO impl.FollowerState: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5142614945ns, electionTimeout:5138ms
dn4_1    | 2023-05-09 16:57:57,820 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-FollowerState] INFO impl.RoleInfo: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: shutdown 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-FollowerState
dn4_1    | 2023-05-09 16:57:57,821 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-FollowerState] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
dn4_1    | 2023-05-09 16:57:57,821 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn4_1    | 2023-05-09 16:57:57,821 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-FollowerState] INFO impl.RoleInfo: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: start 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2
dn4_1    | 2023-05-09 16:57:57,824 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO impl.LeaderElection: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for -1: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-09 16:57:57,829 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 56216a20-9d0f-4921-897d-3244639db044
dn4_1    | 2023-05-09 16:57:57,829 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-05-09 16:57:57,838 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-05-09 16:57:57,843 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 5d36dd6f-a30c-40ec-958f-b246a8287262
dn4_1    | 2023-05-09 16:57:57,933 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO impl.LeaderElection: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn4_1    | 2023-05-09 16:57:57,933 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO impl.LeaderElection:   Response 0: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69<-5d36dd6f-a30c-40ec-958f-b246a8287262#0:OK-t2
dn4_1    | 2023-05-09 16:57:57,933 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO impl.LeaderElection: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2 ELECTION round 0: result PASSED
dn4_1    | 2023-05-09 16:57:57,938 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO impl.RoleInfo: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: shutdown 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2
dn4_1    | 2023-05-09 16:57:57,938 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
dn4_1    | 2023-05-09 16:57:57,938 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D3D6EB9F6B5A with new leaderId: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69
dn4_1    | 2023-05-09 16:57:57,939 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A: change Leader from null to 3aa72183-93e1-4d03-9ed4-71b6f7b45f69 at term 2 for becomeLeader, leader elected after 10416ms
dn4_1    | 2023-05-09 16:57:57,939 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn4_1    | 2023-05-09 16:57:57,940 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-05-09 16:57:57,940 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
om3_1    | 2023-05-09 16:57:41,225 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
om3_1    | 2023-05-09 16:57:41,234 [om3-impl-thread1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
om3_1    | 2023-05-09 16:57:41,243 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-05-09 16:57:41,245 [om3-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om3
om3_1    | 2023-05-09 16:57:41,258 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-05-09 16:57:41,259 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om3_1    | 2023-05-09 16:57:41,259 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om3_1    | 2023-05-09 16:57:41,260 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om3_1    | 2023-05-09 16:57:41,260 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om3_1    | 2023-05-09 16:57:41,282 [Listener at om3/9862] INFO server.RaftServer: om3: start RPC server
om3_1    | 2023-05-09 16:57:41,545 [Listener at om3/9862] INFO server.GrpcService: om3: GrpcService started, listening on 9872
om3_1    | 2023-05-09 16:57:41,563 [Listener at om3/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om3_1    | 2023-05-09 16:57:41,563 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om3: Started
om3_1    | 2023-05-09 16:57:41,692 [Listener at om3/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om3_1    | 2023-05-09 16:57:41,692 [Listener at om3/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om3_1    | 2023-05-09 16:57:41,768 [Listener at om3/9862] INFO util.log: Logging initialized @25961ms to org.eclipse.jetty.util.log.Slf4jLog
om3_1    | 2023-05-09 16:57:42,312 [Listener at om3/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om3_1    | 2023-05-09 16:57:42,371 [Listener at om3/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om3_1    | 2023-05-09 16:57:42,386 [Listener at om3/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om3_1    | 2023-05-09 16:57:42,418 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om3_1    | 2023-05-09 16:57:42,418 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om3_1    | 2023-05-09 16:57:42,419 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om3_1    | 2023-05-09 16:57:42,830 [Listener at om3/9862] INFO http.HttpServer2: Jetty bound to port 9874
om3_1    | 2023-05-09 16:57:42,831 [Listener at om3/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om3_1    | 2023-05-09 16:57:43,072 [Listener at om3/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om3_1    | 2023-05-09 16:57:43,129 [Listener at om3/9862] INFO server.session: No SessionScavenger set, using defaults
om3_1    | 2023-05-09 16:57:43,133 [Listener at om3/9862] INFO server.session: node0 Scavenging every 660000ms
om3_1    | 2023-05-09 16:57:43,303 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2fbd390{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om3_1    | 2023-05-09 16:57:43,337 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@aca3c85{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/static,AVAILABLE}
om3_1    | 2023-05-09 16:57:43,934 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(ELECTION, om2, group-D66704EFC61C, 1, (t:0, i:~))
om3_1    | 2023-05-09 16:57:43,981 [grpc-default-executor-0] INFO impl.VoteContext: om3@group-D66704EFC61C-FOLLOWER: accept ELECTION from om2: our priority 0 <= candidate's priority 0
om3_1    | 2023-05-09 16:57:43,986 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:om2
om3_1    | 2023-05-09 16:57:43,986 [grpc-default-executor-0] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-FollowerState
om3_1    | 2023-05-09 16:57:43,991 [om3@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om3@group-D66704EFC61C-FollowerState was interrupted
om3_1    | 2023-05-09 16:57:43,991 [grpc-default-executor-0] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
om3_1    | 2023-05-09 16:57:43,992 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-05-09 16:57:43,997 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-05-09 16:57:44,068 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to ELECTION vote request: om2<-om3#0:OK-t1. Peer's state: om3@group-D66704EFC61C:t1, leader=null, voted=om2, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-05-09 16:57:45,025 [om3-server-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: change Leader from null to om2 at term 1 for appendEntries, leader elected after 8014ms
om3_1    | 2023-05-09 16:57:45,066 [om3-server-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-05-09 16:57:45,121 [om3-server-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:0
om3_1    | 2023-05-09 16:57:45,995 [om3@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0
om3_1    | 2023-05-09 16:57:46,077 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2326180c{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_3_0_jar-_-any-17688584220212105107/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/ozoneManager}
om3_1    | 2023-05-09 16:57:46,150 [Listener at om3/9862] INFO server.AbstractConnector: Started ServerConnector@baa9ce4{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
recon_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1  | 2023-05-09 16:56:24,615 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1  | /************************************************************
recon_1  | STARTUP_MSG: Starting ReconServer
recon_1  | STARTUP_MSG:   host = 1328eb730c59/10.9.0.22
recon_1  | STARTUP_MSG:   args = []
recon_1  | STARTUP_MSG:   version = 1.3.0
recon_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar
recon_1  | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
recon_1  | STARTUP_MSG:   java = 11.0.14.1
recon_1  | ************************************************************/
recon_1  | 2023-05-09 16:56:24,687 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1  | WARNING: An illegal reflective access operation has occurred
recon_1  | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1  | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1  | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1  | WARNING: All illegal access operations will be denied in a future release
recon_1  | 2023-05-09 16:56:29,396 [main] INFO reflections.Reflections: Reflections took 626 ms to scan 1 urls, producing 16 keys and 49 values 
recon_1  | 2023-05-09 16:56:34,501 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1  | 2023-05-09 16:56:36,588 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1  | 2023-05-09 16:56:44,802 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1  | 2023-05-09 16:56:46,835 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1  | 2023-05-09 16:56:46,883 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.047 seconds to initialized 0 records to KEY_CONTAINER table
recon_1  | 2023-05-09 16:56:46,891 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1  | 2023-05-09 16:56:47,143 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1  | 2023-05-09 16:56:47,149 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1  | 2023-05-09 16:56:51,028 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
recon_1  | 2023-05-09 16:56:54,972 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1  | 2023-05-09 16:56:55,107 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1  | 2023-05-09 16:56:55,186 [main] INFO util.log: Logging initialized @40004ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1  | 2023-05-09 16:56:55,739 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1  | 2023-05-09 16:56:55,766 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1  | 2023-05-09 16:56:55,788 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1  | 2023-05-09 16:56:55,797 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1  | 2023-05-09 16:56:55,797 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1  | 2023-05-09 16:56:55,797 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1  | 2023-05-09 16:56:56,886 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1  | 2023-05-09 16:56:58,277 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1  | 2023-05-09 16:56:58,311 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1  | 2023-05-09 16:56:58,333 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1  | 2023-05-09 16:56:58,389 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1  | 2023-05-09 16:56:58,402 [main] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
recon_1  | 2023-05-09 16:56:59,130 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-05-09 16:56:59,528 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-05-09 16:56:59,660 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
recon_1  | 2023-05-09 16:56:59,664 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1  | 2023-05-09 16:56:59,755 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-05-09 16:56:59,961 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
recon_1  | 2023-05-09 16:57:00,083 [main] INFO reflections.Reflections: Reflections took 106 ms to scan 3 urls, producing 112 keys and 252 values 
recon_1  | 2023-05-09 16:57:00,186 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1  | 2023-05-09 16:57:00,228 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1  | 2023-05-09 16:57:00,234 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1  | 2023-05-09 16:57:00,241 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1  | 2023-05-09 16:57:00,298 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1  | 2023-05-09 16:57:00,331 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1  | 2023-05-09 16:57:00,405 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1  | 2023-05-09 16:57:00,446 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1  | 2023-05-09 16:57:00,668 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
recon_1  | 2023-05-09 16:57:00,668 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1  | 2023-05-09 16:57:00,840 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1  | 2023-05-09 16:57:00,908 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1  | 2023-05-09 16:57:00,908 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1  | 2023-05-09 16:57:01,219 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1  | 2023-05-09 16:57:01,222 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
recon_1  | 2023-05-09 16:57:01,327 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1  | 2023-05-09 16:57:01,330 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1  | 2023-05-09 16:57:01,333 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 600000ms
recon_1  | 2023-05-09 16:57:01,389 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4a216eb4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1  | 2023-05-09 16:57:01,395 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@689faf79{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar!/webapps/static,AVAILABLE}
recon_1  | 2023-05-09 16:57:04,356 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@dd3e1e3{recon,/,file:///tmp/jetty-0_0_0_0-9888-ozone-recon-1_3_0_jar-_-any-2453022322444570998/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar!/webapps/recon}
recon_1  | 2023-05-09 16:57:04,382 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@6aa7e176{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1  | 2023-05-09 16:57:04,383 [Listener at 0.0.0.0/9891] INFO server.Server: Started @49200ms
recon_1  | 2023-05-09 16:57:04,389 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1  | 2023-05-09 16:57:04,389 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1  | 2023-05-09 16:57:04,395 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1  | 2023-05-09 16:57:04,395 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1  | 2023-05-09 16:57:04,405 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1  | 2023-05-09 16:57:04,433 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1  | 2023-05-09 16:57:04,433 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1  | 2023-05-09 16:57:04,433 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-05-09 16:57:31,306 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | 2023-05-09 16:57:31,307 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1    | 2023-05-09 16:57:31,416 [pool-26-thread-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om2_1    | 2023-05-09 16:57:31,418 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1    | 2023-05-09 16:57:31,520 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1    | 2023-05-09 16:57:31,522 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om2_1    | 2023-05-09 16:57:31,617 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om2_1    | 2023-05-09 16:57:31,716 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om2_1    | 2023-05-09 16:57:31,725 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om2_1    | 2023-05-09 16:57:32,489 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1    | 2023-05-09 16:57:32,490 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om2_1    | 2023-05-09 16:57:32,490 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om2_1    | 2023-05-09 16:57:32,493 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om2_1    | 2023-05-09 16:57:32,499 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om2_1    | 2023-05-09 16:57:34,180 [main] INFO reflections.Reflections: Reflections took 2725 ms to scan 8 urls, producing 23 keys and 521 values [using 2 cores]
om2_1    | 2023-05-09 16:57:34,681 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om2_1    | 2023-05-09 16:57:34,764 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om2_1    | 2023-05-09 16:57:35,229 [Listener at om2/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om2_1    | 2023-05-09 16:57:35,383 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om2_1    | 2023-05-09 16:57:35,383 [Listener at om2/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om2_1    | 2023-05-09 16:57:35,744 [Listener at om2/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om2/10.9.0.12:9862
om2_1    | 2023-05-09 16:57:35,746 [Listener at om2/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om2 at port 9872
om2_1    | 2023-05-09 16:57:35,749 [om2-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
om2_1    | 2023-05-09 16:57:35,776 [om2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@ee9fbcddb50a
om2_1    | 2023-05-09 16:57:35,914 [om2-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
om2_1    | 2023-05-09 16:57:35,928 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om2_1    | 2023-05-09 16:57:36,003 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om2_1    | 2023-05-09 16:57:36,003 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-05-09 16:57:36,006 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om2_1    | 2023-05-09 16:57:36,015 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om2_1    | 2023-05-09 16:57:36,026 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1    | 2023-05-09 16:57:36,046 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om2_1    | 2023-05-09 16:57:36,069 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om2_1    | 2023-05-09 16:57:36,135 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om2@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om2_1    | 2023-05-09 16:57:36,163 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om2_1    | 2023-05-09 16:57:36,164 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om2_1    | 2023-05-09 16:57:36,169 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1    | 2023-05-09 16:57:36,173 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om2_1    | 2023-05-09 16:57:36,177 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om2_1    | 2023-05-09 16:57:36,195 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om2_1    | 2023-05-09 16:57:36,197 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om2_1    | 2023-05-09 16:57:36,197 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om2_1    | 2023-05-09 16:57:36,240 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om2_1    | 2023-05-09 16:57:36,241 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om2_1    | 2023-05-09 16:57:36,247 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om2_1    | 2023-05-09 16:57:36,249 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om2_1    | 2023-05-09 16:57:36,259 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om2_1    | 2023-05-09 16:57:36,262 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om2_1    | 2023-05-09 16:57:36,275 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-05-09 16:57:36,276 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
om2_1    | 2023-05-09 16:57:36,278 [om2-impl-thread1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
om2_1    | 2023-05-09 16:57:36,289 [om2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om2
om2_1    | 2023-05-09 16:57:36,299 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om2_1    | 2023-05-09 16:57:36,308 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-05-09 16:57:36,309 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-05-09 16:57:36,313 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om2_1    | 2023-05-09 16:57:36,314 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om2_1    | 2023-05-09 16:57:36,315 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om2_1    | 2023-05-09 16:57:36,354 [Listener at om2/9862] INFO server.RaftServer: om2: start RPC server
om2_1    | 2023-05-09 16:57:36,601 [Listener at om2/9862] INFO server.GrpcService: om2: GrpcService started, listening on 9872
om2_1    | 2023-05-09 16:57:36,609 [Listener at om2/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om2_1    | 2023-05-09 16:57:36,610 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om2: Started
om2_1    | 2023-05-09 16:57:36,827 [Listener at om2/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om2_1    | 2023-05-09 16:57:36,831 [Listener at om2/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om2_1    | 2023-05-09 16:57:36,916 [Listener at om2/9862] INFO util.log: Logging initialized @24127ms to org.eclipse.jetty.util.log.Slf4jLog
om2_1    | 2023-05-09 16:57:37,509 [Listener at om2/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om2_1    | 2023-05-09 16:57:37,566 [Listener at om2/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om2_1    | 2023-05-09 16:57:37,604 [Listener at om2/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om2_1    | 2023-05-09 16:57:37,621 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om2_1    | 2023-05-09 16:57:37,626 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om2_1    | 2023-05-09 16:57:37,627 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om2_1    | 2023-05-09 16:57:37,826 [Listener at om2/9862] INFO http.HttpServer2: Jetty bound to port 9874
om2_1    | 2023-05-09 16:57:37,830 [Listener at om2/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om2_1    | 2023-05-09 16:57:37,947 [Listener at om2/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om2_1    | 2023-05-09 16:57:37,949 [Listener at om2/9862] INFO server.session: No SessionScavenger set, using defaults
om2_1    | 2023-05-09 16:57:37,956 [Listener at om2/9862] INFO server.session: node0 Scavenging every 660000ms
om2_1    | 2023-05-09 16:57:38,062 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5f3b84bd{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om2_1    | 2023-05-09 16:57:38,067 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5323999f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/static,AVAILABLE}
om2_1    | 2023-05-09 16:57:39,715 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5d7f1e59{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_3_0_jar-_-any-4255109896893967933/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/ozoneManager}
om2_1    | 2023-05-09 16:57:39,757 [Listener at om2/9862] INFO server.AbstractConnector: Started ServerConnector@2d4fb0d8{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om2_1    | 2023-05-09 16:57:39,760 [Listener at om2/9862] INFO server.Server: Started @26971ms
om2_1    | 2023-05-09 16:57:39,779 [Listener at om2/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om2_1    | 2023-05-09 16:57:39,779 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om2_1    | 2023-05-09 16:57:39,780 [Listener at om2/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om2_1    | 2023-05-09 16:57:39,787 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn4_1    | 2023-05-09 16:57:57,941 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn4_1    | 2023-05-09 16:57:57,941 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn4_1    | 2023-05-09 16:57:57,941 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn4_1    | 2023-05-09 16:57:57,942 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-05-09 16:57:57,944 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn4_1    | 2023-05-09 16:57:57,989 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn4_1    | 2023-05-09 16:57:57,990 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-09 16:57:57,990 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn4_1    | 2023-05-09 16:57:57,998 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn4_1    | 2023-05-09 16:57:58,000 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-05-09 16:57:58,000 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-05-09 16:57:58,000 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-05-09 16:57:58,001 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn4_1    | 2023-05-09 16:57:58,003 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn4_1    | 2023-05-09 16:57:58,003 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-05-09 16:57:58,004 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn4_1    | 2023-05-09 16:57:58,004 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn4_1    | 2023-05-09 16:57:58,005 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-05-09 16:57:58,005 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-05-09 16:57:58,005 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-05-09 16:57:58,005 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn4_1    | 2023-05-09 16:57:58,006 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO impl.RoleInfo: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: start 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderStateImpl
dn4_1    | 2023-05-09 16:57:58,009 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-05-09 16:57:58,012 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/73e0c514-5c6d-4883-b850-d3d6eb9f6b5a/current/log_inprogress_0
dn4_1    | 2023-05-09 16:57:58,034 [3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A-LeaderElection2] INFO server.RaftServer$Division: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69@group-D3D6EB9F6B5A: set configuration 0: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-05-09 16:57:59,456 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
om2_1    | 2023-05-09 16:57:39,871 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om2_1    | 2023-05-09 16:57:39,976 [Listener at om2/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om2_1    | 2023-05-09 16:57:40,069 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7cc1f72c] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om2_1    | 2023-05-09 16:57:41,463 [om2@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om2@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5185631674ns, electionTimeout:5148ms
om2_1    | 2023-05-09 16:57:41,467 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-FollowerState
om2_1    | 2023-05-09 16:57:41,504 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om2_1    | 2023-05-09 16:57:41,507 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om2_1    | 2023-05-09 16:57:41,510 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-LeaderElection1
om2_1    | 2023-05-09 16:57:41,572 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-05-09 16:57:41,620 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(ELECTION, om1, group-D66704EFC61C, 1, (t:0, i:~))
om2_1    | 2023-05-09 16:57:41,622 [grpc-default-executor-0] INFO impl.VoteContext: om2@group-D66704EFC61C-CANDIDATE: reject ELECTION from om1: already has voted for om2 at current term 1
om2_1    | 2023-05-09 16:57:41,702 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to ELECTION vote request: om1<-om2#0:FAIL-t1. Peer's state: om2@group-D66704EFC61C:t1, leader=null, voted=om2, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-05-09 16:57:41,747 [om2@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
om2_1    | 2023-05-09 16:57:41,748 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-05-09 16:57:41,748 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-05-09 16:57:41,750 [om2@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om3
om2_1    | 2023-05-09 16:57:44,170 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1: ELECTION PASSED received 2 response(s) and 0 exception(s):
om2_1    | 2023-05-09 16:57:44,171 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om2<-om1#0:FAIL-t1
om2_1    | 2023-05-09 16:57:44,171 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 1: om2<-om3#0:OK-t1
om2_1    | 2023-05-09 16:57:44,171 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 ELECTION round 0: result PASSED
om2_1    | 2023-05-09 16:57:44,171 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-LeaderElection1
om2_1    | 2023-05-09 16:57:44,172 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om2_1    | 2023-05-09 16:57:44,172 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: change Leader from null to om2 at term 1 for becomeLeader, leader elected after 12554ms
om2_1    | 2023-05-09 16:57:44,187 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om2_1    | 2023-05-09 16:57:44,192 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om2_1    | 2023-05-09 16:57:44,198 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om2_1    | 2023-05-09 16:57:44,211 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om2_1    | 2023-05-09 16:57:44,211 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om2_1    | 2023-05-09 16:57:44,215 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om2_1    | 2023-05-09 16:57:44,221 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om2_1    | 2023-05-09 16:57:44,238 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om2_1    | 2023-05-09 16:57:44,270 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om2_1    | 2023-05-09 16:57:44,290 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-05-09 16:57:44,291 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om2_1    | 2023-05-09 16:57:44,298 [om2@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om2_1    | 2023-05-09 16:57:44,299 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1    | 2023-05-09 16:57:44,301 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1    | 2023-05-09 16:57:44,301 [om2@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1    | 2023-05-09 16:57:44,301 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om2_1    | 2023-05-09 16:57:44,326 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om2_1    | 2023-05-09 16:57:44,326 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-05-09 16:57:44,326 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om2_1    | 2023-05-09 16:57:44,332 [om2@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om2_1    | 2023-05-09 16:57:44,332 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1    | 2023-05-09 16:57:44,332 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1    | 2023-05-09 16:57:44,338 [om2@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1    | 2023-05-09 16:57:44,343 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om2_1    | 2023-05-09 16:57:44,345 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-LeaderStateImpl
om2_1    | 2023-05-09 16:57:44,432 [om2@group-D66704EFC61C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:0
om2_1    | 2023-05-09 16:57:44,655 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-05-09 16:57:45,182 [om2@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0
om2_1    | 2023-05-09 16:57:46,191 [om2@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om2_1    | [id: "om1"
om2_1    | address: "om1:9872"
om2_1    | startupRole: FOLLOWER
om2_1    | , id: "om3"
om2_1    | address: "om3:9872"
om2_1    | startupRole: FOLLOWER
om2_1    | , id: "om2"
om2_1    | address: "om2:9872"
om2_1    | startupRole: FOLLOWER
om2_1    | ]
om2_1    | 2023-05-09 16:58:05,900 [qtp322612414-48] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
om2_1    | 2023-05-09 16:58:05,919 [qtp322612414-48] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1683651485902 in 17 milliseconds
om2_1    | 2023-05-09 16:58:05,975 [qtp322612414-48] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 53 milliseconds
om2_1    | 2023-05-09 16:58:05,975 [qtp322612414-48] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1683651485902
om2_1    | 2023-05-09 16:58:54,100 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:old1-volume for user:hadoop
om2_1    | 2023-05-09 16:58:56,719 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: old1-bucket of layout LEGACY in volume: old1-volume
om2_1    | 2023-05-09 16:59:05,515 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: old1-bucket of layout LEGACY in volume: s3v
om2_1    | 2023-05-09 16:59:14,817 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:old1-bucket in volume:s3v
om2_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om2_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:206)
om2_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:311)
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om2_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1    | 2023-05-09 16:59:34,860 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om2 Received prepare request with log index 23
om2_1    | 2023-05-09 16:59:34,861 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: om2 waiting for index 23 to flush to OM DB and index 24 to flush to Ratis state machine.
om2_1    | 2023-05-09 16:59:39,865 [OM StateMachine ApplyTransaction Thread - 0] INFO ratis.OzoneManagerStateMachine: Current Snapshot Index (t:1, i:24)
om2_1    | 2023-05-09 16:59:39,866 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 24 -> 24
om2_1    | 2023-05-09 16:59:39,866 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally -1 -> 24
om2_1    | 2023-05-09 16:59:39,866 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: Closing segment log_inprogress_0 to index: 24
om2_1    | 2023-05-09 16:59:39,868 [om2@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0 to /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_0-24
om2_1    | 2023-05-09 16:59:39,868 [OM StateMachine ApplyTransaction Thread - 0] INFO raftlog.RaftLog: om2@group-D66704EFC61C-SegmentedRaftLog: snapshotIndex: updateIncreasingly -1 -> 24
om2_1    | 2023-05-09 16:59:39,880 [OM StateMachine ApplyTransaction Thread - 0] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 23 to file /data/metadata/current/prepareMarker
om2_1    | 2023-05-09 16:59:39,882 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om2 prepared at log index 23. Returning response txnID: 23
om2_1    |  with log index 23
om3_1    | 2023-05-09 16:57:46,151 [Listener at om3/9862] INFO server.Server: Started @30343ms
om3_1    | 2023-05-09 16:57:46,166 [Listener at om3/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om3_1    | 2023-05-09 16:57:46,166 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om3_1    | 2023-05-09 16:57:46,176 [Listener at om3/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om3_1    | 2023-05-09 16:57:46,208 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om3_1    | 2023-05-09 16:57:46,276 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om3_1    | 2023-05-09 16:57:46,410 [Listener at om3/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om3_1    | 2023-05-09 16:57:46,663 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1d8e9d3e] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om3_1    | 2023-05-09 16:57:48,966 [om3@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om3_1    | [id: "om1"
om3_1    | address: "om1:9872"
om3_1    | startupRole: FOLLOWER
om3_1    | , id: "om3"
om3_1    | address: "om3:9872"
om3_1    | startupRole: FOLLOWER
om3_1    | , id: "om2"
om3_1    | address: "om2:9872"
om3_1    | startupRole: FOLLOWER
om3_1    | ]
om3_1    | 2023-05-09 16:58:54,065 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:old1-volume for user:hadoop
om3_1    | 2023-05-09 16:58:56,719 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: old1-bucket of layout LEGACY in volume: old1-volume
om3_1    | 2023-05-09 16:59:05,544 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: old1-bucket of layout LEGACY in volume: s3v
om3_1    | 2023-05-09 16:59:14,820 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:old1-bucket in volume:s3v
om3_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om3_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:206)
om3_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:311)
om3_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om3_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om3_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om3_1    | 2023-05-09 16:59:34,870 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om3 Received prepare request with log index 23
om3_1    | 2023-05-09 16:59:34,873 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: om3 waiting for index 23 to flush to OM DB and index 24 to flush to Ratis state machine.
om3_1    | 2023-05-09 16:59:39,874 [OM StateMachine ApplyTransaction Thread - 0] INFO ratis.OzoneManagerStateMachine: Current Snapshot Index (t:1, i:24)
om3_1    | 2023-05-09 16:59:39,876 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 24 -> 24
om3_1    | 2023-05-09 16:59:39,876 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally -1 -> 24
om3_1    | 2023-05-09 16:59:39,876 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: Closing segment log_inprogress_0 to index: 24
om3_1    | 2023-05-09 16:59:39,879 [om3@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0 to /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_0-24
om3_1    | 2023-05-09 16:59:39,895 [OM StateMachine ApplyTransaction Thread - 0] INFO raftlog.RaftLog: om3@group-D66704EFC61C-SegmentedRaftLog: snapshotIndex: updateIncreasingly -1 -> 24
om3_1    | 2023-05-09 16:59:39,910 [OM StateMachine ApplyTransaction Thread - 0] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 23 to file /data/metadata/current/prepareMarker
om3_1    | 2023-05-09 16:59:40,095 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om3 prepared at log index 23. Returning response txnID: 23
om3_1    |  with log index 23
recon_1  | 2023-05-09 16:57:04,433 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1  | 2023-05-09 16:57:04,435 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1  | 2023-05-09 16:57:06,538 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 1328eb730c59/10.9.0.22 to scm2:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1  | 2023-05-09 16:57:08,540 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 1328eb730c59/10.9.0.22 to scm3:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1  | 2023-05-09 16:57:10,588 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:1f9ebad1-a542-47ab-b506-15f3d5e79738 is not the leader. Could not determine the leader node.
recon_1  | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1  | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
recon_1  | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:193)
recon_1  | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:62732)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1  | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | , while invoking $Proxy43.submitRequest over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1  | 2023-05-09 16:57:12,589 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 1328eb730c59/10.9.0.22 to scm2:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1  | 2023-05-09 16:57:14,591 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 1328eb730c59/10.9.0.22 to scm3:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1  | 2023-05-09 16:57:16,970 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 7 pipelines from SCM.
recon_1  | 2023-05-09 16:57:16,971 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1  | 2023-05-09 16:57:16,972 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=8c529fe3-ebe2-46fe-b4b1-4fe9ff9eca5c from SCM.
recon_1  | 2023-05-09 16:57:17,196 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 8c529fe3-ebe2-46fe-b4b1-4fe9ff9eca5c, Nodes: 56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:13.275Z[UTC]].
recon_1  | 2023-05-09 16:57:17,236 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=73e0c514-5c6d-4883-b850-d3d6eb9f6b5a from SCM.
recon_1  | 2023-05-09 16:57:17,246 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 73e0c514-5c6d-4883-b850-d3d6eb9f6b5a, Nodes: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:13.298Z[UTC]].
recon_1  | 2023-05-09 16:57:17,247 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=6a90de28-7562-4068-9adb-d7e83643323b from SCM.
recon_1  | 2023-05-09 16:57:17,262 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 6a90de28-7562-4068-9adb-d7e83643323b, Nodes: 99be2b94-18e1-4c1d-9084-53427369ee1a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:13.363Z[UTC]].
recon_1  | 2023-05-09 16:57:17,266 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=59f9c2e5-7cfd-4b6f-bac2-8b0224b40693 from SCM.
dn5_1    | 2023-05-09 16:57:28,524 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:28,533 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:29,525 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:29,535 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:30,527 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:30,537 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:31,528 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:31,538 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:32,529 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:32,538 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:33,530 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:33,539 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:34,531 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:34,540 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:35,543 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:36,544 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:37,544 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:38,545 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:39,546 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:40,547 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:41,548 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:42,549 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:43,550 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:43,722 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn5_1    | 2023-05-09 16:57:44,551 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:45,282 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn5_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	... 1 more
dn5_1    | 2023-05-09 16:57:45,283 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn5_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	... 1 more
dn5_1    | 2023-05-09 16:57:45,553 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:46,553 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:46,743 [Command processor thread] INFO server.RaftServer: 99be2b94-18e1-4c1d-9084-53427369ee1a: addNew group-D7E83643323B:[99be2b94-18e1-4c1d-9084-53427369ee1a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:1|startupRole:FOLLOWER] returns group-D7E83643323B:java.util.concurrent.CompletableFuture@1758cef2[Not completed]
dn5_1    | 2023-05-09 16:57:46,869 [pool-22-thread-1] INFO server.RaftServer$Division: 99be2b94-18e1-4c1d-9084-53427369ee1a: new RaftServerImpl for group-D7E83643323B:[99be2b94-18e1-4c1d-9084-53427369ee1a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn5_1    | 2023-05-09 16:57:46,898 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-05-09 16:57:46,901 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-05-09 16:57:46,902 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-05-09 16:57:46,905 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-05-09 16:57:46,906 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-05-09 16:57:46,906 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-05-09 16:57:46,941 [pool-22-thread-1] INFO server.RaftServer$Division: 99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B: ConfigurationManager, init=-1: peers:[99be2b94-18e1-4c1d-9084-53427369ee1a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-05-09 16:57:46,945 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-05-09 16:57:51,494 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
s3g_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1    | 2023-05-09 16:56:17,510 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
dn3_1    | 2023-05-09 16:57:51,499 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
s3g_1    | 2023-05-09 16:56:17,549 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1    | 2023-05-09 16:56:17,745 [main] INFO util.log: Logging initialized @6986ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1    | 2023-05-09 16:56:19,081 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn3_1    | 2023-05-09 16:57:51,501 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-05-09 16:57:51,520 [pool-22-thread-1] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A: start as a follower, conf=-1: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-09 16:57:51,523 [pool-22-thread-1] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn3_1    | 2023-05-09 16:57:51,523 [pool-22-thread-1] INFO impl.RoleInfo: 56216a20-9d0f-4921-897d-3244639db044: start 56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A-FollowerState
dn3_1    | 2023-05-09 16:57:51,524 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-391CD66B588A,id=56216a20-9d0f-4921-897d-3244639db044
dn3_1    | 2023-05-09 16:57:51,525 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-05-09 16:57:51,525 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-05-09 16:57:51,528 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-05-09 16:57:51,529 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-05-09 16:57:51,556 [56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-05-09 16:57:51,557 [56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-05-09 16:57:51,565 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=ffceeb78-bcf8-4a49-9756-391cd66b588a
dn3_1    | 2023-05-09 16:57:51,686 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-05-09 16:57:17,267 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 59f9c2e5-7cfd-4b6f-bac2-8b0224b40693, Nodes: f65ec1c2-2955-4db1-8a8e-2d0fe5785867{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:13.381Z[UTC]].
dn3_1    | 2023-05-09 16:57:51,760 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=ffceeb78-bcf8-4a49-9756-391cd66b588a.
dn3_1    | 2023-05-09 16:57:52,607 [grpc-default-executor-1] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A: receive requestVote(ELECTION, 5d36dd6f-a30c-40ec-958f-b246a8287262, group-D3D6EB9F6B5A, 1, (t:0, i:0))
dn5_1    | 2023-05-09 16:57:47,030 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm2_1   | Waiting for the service scm1:9894
scm3_1   | Waiting for the service scm2:9894
scm3_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn3_1    | 2023-05-09 16:57:52,608 [grpc-default-executor-1] INFO impl.VoteContext: 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-FOLLOWER: accept ELECTION from 5d36dd6f-a30c-40ec-958f-b246a8287262: our priority 0 <= candidate's priority 0
dn3_1    | 2023-05-09 16:57:52,609 [grpc-default-executor-1] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:5d36dd6f-a30c-40ec-958f-b246a8287262
dn5_1    | 2023-05-09 16:57:47,048 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm3_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn3_1    | 2023-05-09 16:57:52,614 [grpc-default-executor-1] INFO impl.RoleInfo: 56216a20-9d0f-4921-897d-3244639db044: shutdown 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-FollowerState
dn3_1    | 2023-05-09 16:57:52,615 [56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-FollowerState] INFO impl.FollowerState: 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-FollowerState was interrupted
dn5_1    | 2023-05-09 16:57:47,100 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
scm3_1   | 2023-05-09 16:57:41,031 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3_1   | /************************************************************
dn3_1    | 2023-05-09 16:57:52,618 [grpc-default-executor-1] INFO impl.RoleInfo: 56216a20-9d0f-4921-897d-3244639db044: start 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-FollowerState
dn3_1    | 2023-05-09 16:57:52,619 [56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-05-09 16:57:52,625 [56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-05-09 16:57:47,107 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
scm3_1   | STARTUP_MSG: Starting StorageContainerManager
scm3_1   | STARTUP_MSG:   host = f4564b650973/10.9.0.16
dn3_1    | 2023-05-09 16:57:52,646 [grpc-default-executor-1] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A replies to ELECTION vote request: 5d36dd6f-a30c-40ec-958f-b246a8287262<-56216a20-9d0f-4921-897d-3244639db044#0:OK-t1. Peer's state: 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A:t1, leader=null, voted=5d36dd6f-a30c-40ec-958f-b246a8287262, raftlog=Memoized:56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-09 16:57:52,693 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:53,059 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-FollowerState] INFO impl.FollowerState: 56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5027489953ns, electionTimeout:5011ms
dn5_1    | 2023-05-09 16:57:47,125 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm3_1   | STARTUP_MSG:   args = [--bootstrap]
scm2_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn3_1    | 2023-05-09 16:57:53,059 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-FollowerState] INFO impl.RoleInfo: 56216a20-9d0f-4921-897d-3244639db044: shutdown 56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-FollowerState
dn3_1    | 2023-05-09 16:57:53,060 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-FollowerState] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn3_1    | 2023-05-09 16:57:53,062 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn5_1    | 2023-05-09 16:57:47,487 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-05-09 16:57:47,523 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-05-09 16:57:47,547 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-05-09 16:57:47,556 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-05-09 16:57:47,556 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-05-09 16:57:47,566 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 2023-05-09 16:56:19,309 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
dn5_1    | 2023-05-09 16:57:47,568 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/6a90de28-7562-4068-9adb-d7e83643323b does not exist. Creating ...
scm1_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1_1   | 2023-05-09 16:56:32,421 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
recon_1  | 2023-05-09 16:57:17,273 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=79915409-920e-4d1c-bcfe-2490f334a92b from SCM.
recon_1  | 2023-05-09 16:57:17,286 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 79915409-920e-4d1c-bcfe-2490f334a92b, Nodes: 5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:12.040Z[UTC]].
s3g_1    | 2023-05-09 16:56:19,374 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1    | 2023-05-09 16:56:19,408 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
scm1_1   | /************************************************************
scm1_1   | STARTUP_MSG: Starting StorageContainerManager
recon_1  | 2023-05-09 16:57:17,286 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=ffceeb78-bcf8-4a49-9756-391cd66b588a from SCM.
recon_1  | 2023-05-09 16:57:17,291 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: ffceeb78-bcf8-4a49-9756-391cd66b588a, Nodes: 56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:13.315Z[UTC]].
s3g_1    | 2023-05-09 16:56:19,408 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1    | 2023-05-09 16:56:19,408 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm1_1   | STARTUP_MSG:   host = 36f642d4db64/10.9.0.14
scm1_1   | STARTUP_MSG:   args = [--init]
recon_1  | 2023-05-09 16:57:17,292 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=c3e11b37-a77f-4003-8d5f-1a6e129f2a79 from SCM.
recon_1  | 2023-05-09 16:57:17,300 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: c3e11b37-a77f-4003-8d5f-1a6e129f2a79, Nodes: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:12.389Z[UTC]].
s3g_1    | 2023-05-09 16:56:19,902 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1    | /************************************************************
scm1_1   | STARTUP_MSG:   version = 1.3.0
scm1_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
recon_1  | 2023-05-09 16:57:17,306 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1  | 2023-05-09 16:57:17,306 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
s3g_1    | STARTUP_MSG: Starting Gateway
s3g_1    | STARTUP_MSG:   host = d61f7142161f/10.9.0.23
dn5_1    | 2023-05-09 16:57:47,620 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/6a90de28-7562-4068-9adb-d7e83643323b/in_use.lock acquired by nodename 7@2439abba3020
dn5_1    | 2023-05-09 16:57:47,674 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/6a90de28-7562-4068-9adb-d7e83643323b has been successfully formatted.
scm3_1   | STARTUP_MSG:   version = 1.3.0
scm3_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
s3g_1    | STARTUP_MSG:   args = []
s3g_1    | STARTUP_MSG:   version = 1.3.0
dn5_1    | 2023-05-09 16:57:47,714 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-D7E83643323B: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
scm1_1   | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm1_1   | STARTUP_MSG:   java = 11.0.14.1
dn3_1    | 2023-05-09 16:57:53,062 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-FollowerState] INFO impl.RoleInfo: 56216a20-9d0f-4921-897d-3244639db044: start 56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-LeaderElection1
dn5_1    | 2023-05-09 16:57:47,807 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
s3g_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar
s3g_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
s3g_1    | STARTUP_MSG:   java = 11.0.14.1
s3g_1    | ************************************************************/
s3g_1    | 2023-05-09 16:56:19,977 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1    | 2023-05-09 16:56:20,214 [main] INFO s3.Gateway: Starting Ozone S3 gateway
dn5_1    | 2023-05-09 16:57:47,913 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-05-09 16:57:53,072 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-LeaderElection1] INFO impl.LeaderElection: 56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 2023-05-09 16:57:17,353 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1  | 2023-05-09 16:57:17,367 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
s3g_1    | 2023-05-09 16:56:20,845 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1    | 2023-05-09 16:56:21,894 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1    | 2023-05-09 16:56:21,911 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1    | 2023-05-09 16:56:22,233 [main] INFO http.HttpServer2: Jetty bound to port 9878
dn5_1    | 2023-05-09 16:57:47,913 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-05-09 16:57:53,073 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-LeaderElection1] INFO impl.LeaderElection: 56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-LeaderElection1 ELECTION round 0: result PASSED (term=1)
recon_1  | 2023-05-09 16:57:17,463 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1  | 2023-05-09 16:57:17,463 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
s3g_1    | 2023-05-09 16:56:22,271 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn5_1    | 2023-05-09 16:57:47,961 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-05-09 16:57:53,074 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-LeaderElection1] INFO impl.RoleInfo: 56216a20-9d0f-4921-897d-3244639db044: shutdown 56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-LeaderElection1
recon_1  | 2023-05-09 16:57:17,477 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1  | 2023-05-09 16:57:17,523 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
s3g_1    | 2023-05-09 16:56:22,646 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1    | 2023-05-09 16:56:22,647 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1    | 2023-05-09 16:56:22,648 [main] INFO server.session: node0 Scavenging every 600000ms
s3g_1    | 2023-05-09 16:56:23,018 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5a9f4771{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1    | 2023-05-09 16:56:23,019 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6c0d7c83{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar!/webapps/static,AVAILABLE}
s3g_1    | WARNING: An illegal reflective access operation has occurred
s3g_1    | WARNING: Illegal reflective access by org.jboss.weld.util.reflection.Formats (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to constructor com.sun.org.apache.bcel.internal.classfile.ClassParser(java.io.InputStream,java.lang.String)
dn5_1    | 2023-05-09 16:57:47,968 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-05-09 16:57:47,983 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-05-09 16:57:48,002 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
s3g_1    | WARNING: Please consider reporting this to the maintainers of org.jboss.weld.util.reflection.Formats
s3g_1    | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1    | WARNING: All illegal access operations will be denied in a future release
s3g_1    | May 09, 2023 4:56:52 PM org.glassfish.jersey.internal.Errors logErrors
dn5_1    | 2023-05-09 16:57:48,013 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-05-09 16:57:48,034 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/6a90de28-7562-4068-9adb-d7e83643323b
dn5_1    | 2023-05-09 16:57:48,046 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-05-09 16:57:53,074 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-LeaderElection1] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn3_1    | 2023-05-09 16:57:53,075 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-4FE9FF9ECA5C with new leaderId: 56216a20-9d0f-4921-897d-3244639db044
dn3_1    | 2023-05-09 16:57:53,075 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-LeaderElection1] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C: change Leader from null to 56216a20-9d0f-4921-897d-3244639db044 at term 1 for becomeLeader, leader elected after 6010ms
dn3_1    | 2023-05-09 16:57:53,084 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn5_1    | 2023-05-09 16:57:48,054 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-05-09 16:57:48,056 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-05-09 16:57:48,059 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-05-09 16:57:48,061 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
s3g_1    | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
recon_1  | 2023-05-09 16:57:17,516 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 7 pipelines in house.
dn3_1    | 2023-05-09 16:57:53,100 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-05-09 16:57:53,102 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-05-09 16:57:53,106 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn5_1    | 2023-05-09 16:57:48,063 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-05-09 16:57:48,066 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
recon_1  | 2023-05-09 16:57:17,638 [IPC Server handler 14 on default port 9891] INFO ipc.Server: IPC Server handler 14 on default port 9891: skipped Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:46194
dn3_1    | 2023-05-09 16:57:53,106 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-05-09 16:57:53,108 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn3_1    | 2023-05-09 16:57:53,115 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-05-09 16:57:48,069 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-05-09 16:57:48,164 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
recon_1  | 2023-05-09 16:57:17,639 [IPC Server handler 16 on default port 9891] INFO ipc.Server: IPC Server handler 16 on default port 9891: skipped Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:55952
dn3_1    | 2023-05-09 16:57:53,118 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn3_1    | 2023-05-09 16:57:53,136 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-LeaderElection1] INFO impl.RoleInfo: 56216a20-9d0f-4921-897d-3244639db044: start 56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-LeaderStateImpl
dn3_1    | 2023-05-09 16:57:53,162 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 2023-05-09 16:57:48,202 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-05-09 16:57:48,206 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-05-09 16:57:53,237 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-LeaderElection1] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C: set configuration 0: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-09 16:57:53,362 [56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 56216a20-9d0f-4921-897d-3244639db044@group-4FE9FF9ECA5C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/8c529fe3-ebe2-46fe-b4b1-4fe9ff9eca5c/current/log_inprogress_0
dn3_1    | 2023-05-09 16:57:53,694 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm3_1   | STARTUP_MSG:   java = 11.0.14.1
scm3_1   | ************************************************************/
scm3_1   | 2023-05-09 16:57:41,078 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1_1   | ************************************************************/
recon_1  | 2023-05-09 16:57:17,639 [IPC Server handler 19 on default port 9891] INFO ipc.Server: IPC Server handler 19 on default port 9891: skipped Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:44246
dn3_1    | 2023-05-09 16:57:54,695 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:55,696 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:56,513 [grpc-default-executor-1] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A: receive requestVote(ELECTION, 5d36dd6f-a30c-40ec-958f-b246a8287262, group-391CD66B588A, 1, (t:0, i:0))
scm3_1   | 2023-05-09 16:57:41,288 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3_1   | 2023-05-09 16:57:41,368 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
recon_1  | 2023-05-09 16:57:17,639 [IPC Server handler 18 on default port 9891] INFO ipc.Server: IPC Server handler 18 on default port 9891: skipped Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:60854
s3g_1    | 
s3g_1    | 2023-05-09 16:56:52,233 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4baf997{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-ozone-s3gateway-1_3_0_jar-_-any-15294392621205522438/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar!/webapps/s3gateway}
dn3_1    | 2023-05-09 16:57:56,513 [grpc-default-executor-1] INFO impl.VoteContext: 56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A-FOLLOWER: accept ELECTION from 5d36dd6f-a30c-40ec-958f-b246a8287262: our priority 0 <= candidate's priority 1
scm3_1   | 2023-05-09 16:57:41,378 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3_1   | 2023-05-09 16:57:41,591 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3:9894 and Ratis port: 9894
recon_1  | 2023-05-09 16:57:17,639 [IPC Server handler 20 on default port 9891] INFO ipc.Server: IPC Server handler 20 on default port 9891: skipped Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:57464
s3g_1    | 2023-05-09 16:56:52,312 [main] INFO server.AbstractConnector: Started ServerConnector@54504ecd{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1    | 2023-05-09 16:56:52,314 [main] INFO server.Server: Started @41556ms
s3g_1    | 2023-05-09 16:56:52,352 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm3_1   | 2023-05-09 16:57:41,593 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3
scm3_1   | 2023-05-09 16:57:41,952 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863]
recon_1  | 2023-05-09 16:57:17,639 [IPC Server handler 15 on default port 9891] INFO ipc.Server: IPC Server handler 15 on default port 9891: skipped Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:36762
s3g_1    | 2023-05-09 16:56:52,352 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1    | 2023-05-09 16:56:52,359 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
scm2_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm3_1   | 2023-05-09 16:57:42,925 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-f33ab5ed-453e-4ae9-ac5c-d55092fa9737, SCMID 887fcf03-8b52-47cd-92ee-2ac01e19b0e0
scm3_1   | 2023-05-09 16:57:42,941 [main] INFO server.StorageContainerManager: Primary SCM Node ID 1f9ebad1-a542-47ab-b506-15f3d5e79738
recon_1  | 2023-05-09 16:57:17,652 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 176 milliseconds.
s3g_1    | 2023-05-09 16:59:13,540 [qtp384515747-20] INFO audit.AuditLogger: Refresh DebugCmdSet for S3GAudit to [].
dn3_1    | 2023-05-09 16:57:56,513 [grpc-default-executor-1] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:5d36dd6f-a30c-40ec-958f-b246a8287262
scm2_1   | 2023-05-09 16:56:56,876 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3_1   | 2023-05-09 16:57:42,974 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm3_1   | /************************************************************
recon_1  | 2023-05-09 16:57:18,280 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:44254: output error
s3g_1    | 2023-05-09 16:59:13,561 [qtp384515747-20] INFO audit.AuditLogger: Refresh DebugCmdSet for S3GAudit to [].
dn3_1    | 2023-05-09 16:57:56,513 [grpc-default-executor-1] INFO impl.RoleInfo: 56216a20-9d0f-4921-897d-3244639db044: shutdown 56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A-FollowerState
scm2_1   | /************************************************************
scm3_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at f4564b650973/10.9.0.16
scm3_1   | ************************************************************/
scm1_1   | 2023-05-09 16:56:32,548 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1  | 2023-05-09 16:57:18,283 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:46204: output error
s3g_1    | 2023-05-09 16:59:13,565 [qtp384515747-20] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
s3g_1    | 2023-05-09 16:59:13,566 [qtp384515747-20] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
s3g_1    | 2023-05-09 16:59:14,790 [qtp384515747-20] INFO rpc.RpcClient: Creating Bucket: s3v/old1-bucket, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
dn3_1    | 2023-05-09 16:57:56,513 [56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A-FollowerState] INFO impl.FollowerState: 56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A-FollowerState was interrupted
dn3_1    | 2023-05-09 16:57:56,513 [grpc-default-executor-1] INFO impl.RoleInfo: 56216a20-9d0f-4921-897d-3244639db044: start 56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A-FollowerState
scm1_1   | 2023-05-09 16:56:33,454 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
s3g_1    | 2023-05-09 16:59:15,689 [qtp384515747-16] WARN impl.MetricsSystemImpl: S3Gateway metrics system already initialized!
s3g_1    | 2023-05-09 16:59:15,927 [qtp384515747-16] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn5_1    | 2023-05-09 16:57:48,210 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-05-09 16:57:56,534 [56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-05-09 16:57:56,534 [grpc-default-executor-1] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A replies to ELECTION vote request: 5d36dd6f-a30c-40ec-958f-b246a8287262<-56216a20-9d0f-4921-897d-3244639db044#0:OK-t1. Peer's state: 56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A:t1, leader=null, voted=5d36dd6f-a30c-40ec-958f-b246a8287262, raftlog=Memoized:56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-09 16:56:33,996 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
recon_1  | 2023-05-09 16:57:18,286 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
scm2_1   | STARTUP_MSG: Starting StorageContainerManager
dn5_1    | 2023-05-09 16:57:48,283 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-05-09 16:57:48,283 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm3_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1_1   | 2023-05-09 16:56:34,130 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
recon_1  | java.nio.channels.ClosedChannelException
scm2_1   | STARTUP_MSG:   host = 9b61320f6a6f/10.9.0.15
dn5_1    | 2023-05-09 16:57:48,296 [pool-22-thread-1] INFO server.RaftServer$Division: 99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B: start as a follower, conf=-1: peers:[99be2b94-18e1-4c1d-9084-53427369ee1a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-05-09 16:57:56,536 [56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-09 16:56:34,749 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1:9894 and Ratis port: 9894
dn5_1    | 2023-05-09 16:57:48,321 [pool-22-thread-1] INFO server.RaftServer$Division: 99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn3_1    | 2023-05-09 16:57:56,697 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-05-09 16:57:56,767 [56216a20-9d0f-4921-897d-3244639db044-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-391CD66B588A with new leaderId: 5d36dd6f-a30c-40ec-958f-b246a8287262
scm1_1   | 2023-05-09 16:56:34,766 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1
dn5_1    | 2023-05-09 16:57:48,322 [pool-22-thread-1] INFO impl.RoleInfo: 99be2b94-18e1-4c1d-9084-53427369ee1a: start 99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-FollowerState
dn3_1    | 2023-05-09 16:57:56,767 [56216a20-9d0f-4921-897d-3244639db044-server-thread1] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A: change Leader from null to 5d36dd6f-a30c-40ec-958f-b246a8287262 at term 1 for appendEntries, leader elected after 5353ms
dn3_1    | 2023-05-09 16:57:56,788 [56216a20-9d0f-4921-897d-3244639db044-server-thread2] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A: set configuration 0: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-09 16:56:36,688 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn5_1    | 2023-05-09 16:57:48,350 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D7E83643323B,id=99be2b94-18e1-4c1d-9084-53427369ee1a
dn3_1    | 2023-05-09 16:57:56,789 [56216a20-9d0f-4921-897d-3244639db044-server-thread2] INFO segmented.SegmentedRaftLogWorker: 56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A-SegmentedRaftLogWorker: Starting segment from index:0
dn3_1    | 2023-05-09 16:57:56,794 [56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 56216a20-9d0f-4921-897d-3244639db044@group-391CD66B588A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ffceeb78-bcf8-4a49-9756-391cd66b588a/current/log_inprogress_0
scm1_1   | 2023-05-09 16:56:38,214 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-05-09 16:57:48,356 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-05-09 16:57:57,626 [56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-05-09 16:57:57,626 [56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-09 16:56:38,216 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
dn5_1    | 2023-05-09 16:57:48,356 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-05-09 16:57:57,914 [grpc-default-executor-0] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A: receive requestVote(ELECTION, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69, group-D3D6EB9F6B5A, 2, (t:0, i:0))
dn3_1    | 2023-05-09 16:57:57,914 [grpc-default-executor-0] INFO impl.VoteContext: 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-FOLLOWER: accept ELECTION from 3aa72183-93e1-4d03-9ed4-71b6f7b45f69: our priority 0 <= candidate's priority 1
scm1_1   | 2023-05-09 16:56:38,275 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-05-09 16:57:48,373 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-05-09 16:57:57,915 [grpc-default-executor-0] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:3aa72183-93e1-4d03-9ed4-71b6f7b45f69
dn3_1    | 2023-05-09 16:57:57,915 [grpc-default-executor-0] INFO impl.RoleInfo: 56216a20-9d0f-4921-897d-3244639db044: shutdown 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-FollowerState
scm1_1   | 2023-05-09 16:56:38,275 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
dn5_1    | 2023-05-09 16:57:48,378 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-05-09 16:57:57,915 [56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-FollowerState] INFO impl.FollowerState: 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-FollowerState was interrupted
dn3_1    | 2023-05-09 16:57:57,915 [grpc-default-executor-0] INFO impl.RoleInfo: 56216a20-9d0f-4921-897d-3244639db044: start 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-FollowerState
scm2_1   | STARTUP_MSG:   args = [--bootstrap]
scm2_1   | STARTUP_MSG:   version = 1.3.0
scm3_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn3_1    | 2023-05-09 16:57:57,940 [grpc-default-executor-0] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A replies to ELECTION vote request: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69<-56216a20-9d0f-4921-897d-3244639db044#0:OK-t2. Peer's state: 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A:t2, leader=null, voted=3aa72183-93e1-4d03-9ed4-71b6f7b45f69, raftlog=Memoized:56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-05-09 16:57:48,390 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm1_1   | 2023-05-09 16:56:38,275 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm3_1   | 2023-05-09 16:57:49,765 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3_1   | /************************************************************
dn5_1    | 2023-05-09 16:57:48,395 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1   | 2023-05-09 16:56:38,276 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm3_1   | STARTUP_MSG: Starting StorageContainerManager
scm3_1   | STARTUP_MSG:   host = f4564b650973/10.9.0.16
dn5_1    | 2023-05-09 16:57:48,567 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-05-09 16:56:38,277 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm3_1   | STARTUP_MSG:   args = []
scm3_1   | STARTUP_MSG:   version = 1.3.0
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn5_1    | 2023-05-09 16:57:48,625 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=6a90de28-7562-4068-9adb-d7e83643323b
dn3_1    | 2023-05-09 16:57:57,950 [56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm3_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
scm2_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
scm2_1   | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm2_1   | STARTUP_MSG:   java = 11.0.14.1
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm2_1   | ************************************************************/
scm2_1   | 2023-05-09 16:56:56,926 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2_1   | 2023-05-09 16:56:57,236 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm3_1   | STARTUP_MSG:   java = 11.0.14.1
scm2_1   | 2023-05-09 16:56:57,372 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm2_1   | 2023-05-09 16:56:57,372 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm2_1   | 2023-05-09 16:56:57,513 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2:9894 and Ratis port: 9894
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm3_1   | ************************************************************/
scm2_1   | 2023-05-09 16:56:57,519 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2
scm2_1   | 2023-05-09 16:56:57,984 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
scm2_1   | 2023-05-09 16:57:00,694 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9b61320f6a6f/10.9.0.15 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm3_1   | 2023-05-09 16:57:49,799 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2_1   | 2023-05-09 16:57:02,696 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9b61320f6a6f/10.9.0.15 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
scm2_1   | 2023-05-09 16:57:04,697 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9b61320f6a6f/10.9.0.15 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
scm2_1   | 2023-05-09 16:57:06,765 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:1f9ebad1-a542-47ab-b506-15f3d5e79738 is not the leader. Could not determine the leader node.
scm2_1   | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
scm3_1   | 2023-05-09 16:57:50,035 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-05-09 16:56:38,319 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-05-09 16:56:38,342 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
dn3_1    | 2023-05-09 16:57:57,951 [56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-05-09 16:57:58,095 [56216a20-9d0f-4921-897d-3244639db044-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D3D6EB9F6B5A with new leaderId: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69
scm3_1   | 2023-05-09 16:57:50,192 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1_1   | 2023-05-09 16:56:38,342 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1_1   | 2023-05-09 16:56:38,481 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
dn3_1    | 2023-05-09 16:57:58,095 [56216a20-9d0f-4921-897d-3244639db044-server-thread1] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A: change Leader from null to 3aa72183-93e1-4d03-9ed4-71b6f7b45f69 at term 2 for appendEntries, leader elected after 9782ms
dn3_1    | 2023-05-09 16:57:58,096 [56216a20-9d0f-4921-897d-3244639db044-server-thread1] INFO server.RaftServer$Division: 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A: set configuration 0: peers:[56216a20-9d0f-4921-897d-3244639db044|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, 5d36dd6f-a30c-40ec-958f-b246a8287262|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-09 16:56:38,619 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm1_1   | 2023-05-09 16:56:38,653 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
dn3_1    | 2023-05-09 16:57:58,098 [56216a20-9d0f-4921-897d-3244639db044-server-thread1] INFO segmented.SegmentedRaftLogWorker: 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-SegmentedRaftLogWorker: Starting segment from index:0
dn3_1    | 2023-05-09 16:57:58,106 [56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 56216a20-9d0f-4921-897d-3244639db044@group-D3D6EB9F6B5A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/73e0c514-5c6d-4883-b850-d3d6eb9f6b5a/current/log_inprogress_0
scm1_1   | 2023-05-09 16:56:42,236 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm1_1   | 2023-05-09 16:56:42,306 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
dn3_1    | 2023-05-09 16:57:59,429 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn5_1    | 2023-05-09 16:57:48,628 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=6a90de28-7562-4068-9adb-d7e83643323b.
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
dn5_1    | 2023-05-09 16:57:49,568 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:50,569 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:51,570 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:52,571 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:53,439 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-FollowerState] INFO impl.FollowerState: 99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5117416530ns, electionTimeout:5065ms
dn5_1    | 2023-05-09 16:57:53,441 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-FollowerState] INFO impl.RoleInfo: 99be2b94-18e1-4c1d-9084-53427369ee1a: shutdown 99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-FollowerState
scm3_1   | 2023-05-09 16:57:50,223 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3_1   | 2023-05-09 16:57:50,376 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3:9894 and Ratis port: 9894
scm3_1   | 2023-05-09 16:57:50,377 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3
scm1_1   | 2023-05-09 16:56:42,311 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm2_1   | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
scm3_1   | 2023-05-09 16:57:52,248 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3_1   | 2023-05-09 16:57:52,797 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn5_1    | 2023-05-09 16:57:53,441 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-FollowerState] INFO server.RaftServer$Division: 99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn5_1    | 2023-05-09 16:57:53,444 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn5_1    | 2023-05-09 16:57:53,444 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-FollowerState] INFO impl.RoleInfo: 99be2b94-18e1-4c1d-9084-53427369ee1a: start 99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-LeaderElection1
dn5_1    | 2023-05-09 16:57:53,474 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-LeaderElection1] INFO impl.LeaderElection: 99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[99be2b94-18e1-4c1d-9084-53427369ee1a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-09 16:56:42,322 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1_1   | 2023-05-09 16:56:42,341 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-05-09 16:57:53,475 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-LeaderElection1] INFO impl.LeaderElection: 99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm3_1   | 2023-05-09 16:57:53,307 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
scm2_1   | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
dn5_1    | 2023-05-09 16:57:53,475 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-LeaderElection1] INFO impl.RoleInfo: 99be2b94-18e1-4c1d-9084-53427369ee1a: shutdown 99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-LeaderElection1
dn5_1    | 2023-05-09 16:57:53,476 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-LeaderElection1] INFO server.RaftServer$Division: 99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm2_1   | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
scm3_1   | 2023-05-09 16:57:53,315 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
dn5_1    | 2023-05-09 16:57:53,477 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D7E83643323B with new leaderId: 99be2b94-18e1-4c1d-9084-53427369ee1a
dn5_1    | 2023-05-09 16:57:53,494 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-LeaderElection1] INFO server.RaftServer$Division: 99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B: change Leader from null to 99be2b94-18e1-4c1d-9084-53427369ee1a at term 1 for becomeLeader, leader elected after 6377ms
dn5_1    | 2023-05-09 16:57:53,518 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
dn5_1    | 2023-05-09 16:57:53,533 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-05-09 16:57:53,534 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn5_1    | 2023-05-09 16:57:53,538 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 2023-05-09 16:57:53,473 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn5_1    | 2023-05-09 16:57:53,538 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 2023-05-09 16:57:53,539 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn5_1    | 2023-05-09 16:57:53,548 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm2_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
scm1_1   | 2023-05-09 16:56:42,397 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3_1   | 2023-05-09 16:57:53,525 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:887fcf03-8b52-47cd-92ee-2ac01e19b0e0
dn5_1    | 2023-05-09 16:57:53,550 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn5_1    | 2023-05-09 16:57:53,559 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-LeaderElection1] INFO impl.RoleInfo: 99be2b94-18e1-4c1d-9084-53427369ee1a: start 99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-LeaderStateImpl
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 2023-05-09 16:56:42,500 [main] INFO server.RaftServer: 1f9ebad1-a542-47ab-b506-15f3d5e79738: addNew group-D55092FA9737:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|priority:0|startupRole:FOLLOWER] returns group-D55092FA9737:java.util.concurrent.CompletableFuture@305f031[Not completed]
scm3_1   | 2023-05-09 16:57:53,725 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1_1   | 2023-05-09 16:56:42,805 [pool-2-thread-1] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738: new RaftServerImpl for group-D55092FA9737:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
scm1_1   | 2023-05-09 16:56:42,926 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm3_1   | 2023-05-09 16:57:53,806 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-05-09 16:57:53,575 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-05-09 16:57:53,603 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 2023-05-09 16:57:53,666 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-LeaderElection1] INFO server.RaftServer$Division: 99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B: set configuration 0: peers:[99be2b94-18e1-4c1d-9084-53427369ee1a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1   | 2023-05-09 16:56:42,933 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
recon_1  | 2023-05-09 16:57:18,310 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
scm3_1   | 2023-05-09 16:57:53,808 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm3_1   | 2023-05-09 16:57:53,808 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm2_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
scm2_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1  | java.nio.channels.ClosedChannelException
scm3_1   | 2023-05-09 16:57:53,808 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm1_1   | 2023-05-09 16:56:42,933 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1_1   | 2023-05-09 16:56:42,935 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1_1   | 2023-05-09 16:56:42,938 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 2023-05-09 16:57:53,809 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm1_1   | 2023-05-09 16:56:42,939 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1_1   | 2023-05-09 16:56:42,983 [pool-2-thread-1] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737: ConfigurationManager, init=-1: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm1_1   | 2023-05-09 16:56:43,016 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 2023-05-09 16:56:43,096 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1_1   | 2023-05-09 16:56:43,107 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1_1   | 2023-05-09 16:56:43,403 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm1_1   | 2023-05-09 16:56:43,442 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1_1   | 2023-05-09 16:56:43,455 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1_1   | 2023-05-09 16:56:43,963 [pool-2-thread-1] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm3_1   | 2023-05-09 16:57:53,809 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm2_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
scm1_1   | 2023-05-09 16:56:46,035 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-05-09 16:56:46,050 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm3_1   | 2023-05-09 16:57:53,810 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
dn5_1    | 2023-05-09 16:57:53,738 [99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 99be2b94-18e1-4c1d-9084-53427369ee1a@group-D7E83643323B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/6a90de28-7562-4068-9adb-d7e83643323b/current/log_inprogress_0
dn5_1    | 2023-05-09 16:57:54,576 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-05-09 16:56:46,054 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm3_1   | 2023-05-09 16:57:53,811 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm1_1   | 2023-05-09 16:56:46,079 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1_1   | 2023-05-09 16:56:46,080 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm3_1   | 2023-05-09 16:57:53,812 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm3_1   | 2023-05-09 16:57:53,813 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm3_1   | 2023-05-09 16:57:53,821 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-05-09 16:57:18,314 [IPC Server handler 3 on default port 9891] WARN ipc.Server: IPC Server handler 3 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:34086: output error
recon_1  | 2023-05-09 16:57:18,314 [IPC Server handler 3 on default port 9891] INFO ipc.Server: IPC Server handler 3 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2_1   | , while invoking $Proxy14.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
scm2_1   | 2023-05-09 16:57:08,768 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9b61320f6a6f/10.9.0.15 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
scm2_1   | 2023-05-09 16:57:10,788 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:1f9ebad1-a542-47ab-b506-15f3d5e79738 is not the leader. Could not determine the leader node.
dn5_1    | 2023-05-09 16:57:55,577 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm1_1   | 2023-05-09 16:56:46,099 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737 does not exist. Creating ...
scm2_1   | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
dn5_1    | 2023-05-09 16:57:56,578 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm3_1   | 2023-05-09 16:57:53,824 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm1_1   | 2023-05-09 16:56:46,133 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737/in_use.lock acquired by nodename 13@36f642d4db64
dn5_1    | 2023-05-09 16:57:59,432 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm3_1   | 2023-05-09 16:57:53,824 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm1_1   | 2023-05-09 16:56:46,231 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737 has been successfully formatted.
scm1_1   | 2023-05-09 16:56:46,290 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm3_1   | 2023-05-09 16:57:54,159 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm1_1   | 2023-05-09 16:56:46,461 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1_1   | 2023-05-09 16:56:46,487 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm3_1   | 2023-05-09 16:57:54,164 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm2_1   | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
scm1_1   | 2023-05-09 16:56:46,500 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm3_1   | 2023-05-09 16:57:54,166 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm2_1   | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2_1   | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
scm2_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-05-09 16:57:18,322 [IPC Server handler 10 on default port 9891] WARN ipc.Server: IPC Server handler 10 on default port 9891, call Call#8 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:47590: output error
scm1_1   | 2023-05-09 16:56:46,503 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
recon_1  | 2023-05-09 16:57:18,326 [IPC Server handler 10 on default port 9891] INFO ipc.Server: IPC Server handler 10 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
scm2_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
scm2_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 2023-05-09 16:56:46,505 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3_1   | 2023-05-09 16:57:54,167 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3_1   | 2023-05-09 16:57:54,167 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1_1   | 2023-05-09 16:56:46,621 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | 2023-05-09 16:56:46,626 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1_1   | 2023-05-09 16:56:46,689 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737
scm3_1   | 2023-05-09 16:57:54,176 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
scm1_1   | 2023-05-09 16:56:46,766 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1_1   | 2023-05-09 16:56:46,766 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1_1   | 2023-05-09 16:56:46,835 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3_1   | 2023-05-09 16:57:54,188 [main] INFO server.RaftServer: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0: addNew group-D55092FA9737:[] returns group-D55092FA9737:java.util.concurrent.CompletableFuture@2b8bd14b[Not completed]
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
scm1_1   | 2023-05-09 16:56:46,842 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1_1   | 2023-05-09 16:56:46,851 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | 2023-05-09 16:56:46,852 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm3_1   | 2023-05-09 16:57:54,208 [pool-16-thread-1] INFO server.RaftServer$Division: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0: new RaftServerImpl for group-D55092FA9737:[] with SCMStateMachine:uninitialized
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1_1   | 2023-05-09 16:56:46,894 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1_1   | 2023-05-09 16:56:46,912 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1_1   | 2023-05-09 16:56:47,048 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm3_1   | 2023-05-09 16:57:54,210 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 2023-05-09 16:56:47,054 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1_1   | 2023-05-09 16:56:47,063 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1_1   | 2023-05-09 16:56:47,073 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm3_1   | 2023-05-09 16:57:54,210 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 2023-05-09 16:56:47,208 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-05-09 16:56:47,219 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-05-09 16:56:47,303 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737: start as a follower, conf=-1: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-09 16:56:47,326 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1   | 2023-05-09 16:56:47,329 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO impl.RoleInfo: 1f9ebad1-a542-47ab-b506-15f3d5e79738: start 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-FollowerState
scm3_1   | 2023-05-09 16:57:54,211 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1_1   | 2023-05-09 16:56:47,498 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-09 16:56:47,540 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2_1   | , while invoking $Proxy14.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
scm2_1   | 2023-05-09 16:57:12,790 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9b61320f6a6f/10.9.0.15 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
scm1_1   | 2023-05-09 16:56:47,557 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D55092FA9737,id=1f9ebad1-a542-47ab-b506-15f3d5e79738
scm1_1   | 2023-05-09 16:56:47,569 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1   | 2023-05-09 16:56:47,570 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm3_1   | 2023-05-09 16:57:54,211 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3_1   | 2023-05-09 16:57:54,211 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1_1   | 2023-05-09 16:56:47,570 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1_1   | 2023-05-09 16:56:47,571 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1   | 2023-05-09 16:56:47,610 [main] INFO server.RaftServer: 1f9ebad1-a542-47ab-b506-15f3d5e79738: start RPC server
scm3_1   | 2023-05-09 16:57:54,211 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm3_1   | 2023-05-09 16:57:54,218 [pool-16-thread-1] INFO server.RaftServer$Division: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm1_1   | 2023-05-09 16:56:48,327 [main] INFO server.GrpcService: 1f9ebad1-a542-47ab-b506-15f3d5e79738: GrpcService started, listening on 9894
scm1_1   | 2023-05-09 16:56:48,374 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-1f9ebad1-a542-47ab-b506-15f3d5e79738: Started
scm1_1   | 2023-05-09 16:56:52,619 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-FollowerState] INFO impl.FollowerState: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5290690338ns, electionTimeout:5050ms
scm3_1   | 2023-05-09 16:57:54,218 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2_1   | 2023-05-09 16:57:14,845 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-f33ab5ed-453e-4ae9-ac5c-d55092fa9737, SCMID 66fc2679-3361-4751-923f-5ded42c3e66c
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm2_1   | 2023-05-09 16:57:14,845 [main] INFO server.StorageContainerManager: Primary SCM Node ID 1f9ebad1-a542-47ab-b506-15f3d5e79738
scm2_1   | 2023-05-09 16:57:14,864 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm1_1   | 2023-05-09 16:56:52,628 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-FollowerState] INFO impl.RoleInfo: 1f9ebad1-a542-47ab-b506-15f3d5e79738: shutdown 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-FollowerState
scm1_1   | 2023-05-09 16:56:52,679 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-FollowerState] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm1_1   | 2023-05-09 16:56:52,712 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm3_1   | 2023-05-09 16:57:54,223 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm3_1   | 2023-05-09 16:57:54,224 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1_1   | 2023-05-09 16:56:52,715 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-FollowerState] INFO impl.RoleInfo: 1f9ebad1-a542-47ab-b506-15f3d5e79738: start 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1
scm1_1   | 2023-05-09 16:56:52,813 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO impl.LeaderElection: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-09 16:56:52,828 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO impl.LeaderElection: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm1_1   | 2023-05-09 16:56:52,829 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO impl.RoleInfo: 1f9ebad1-a542-47ab-b506-15f3d5e79738: shutdown 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1
scm2_1   | /************************************************************
scm2_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at 9b61320f6a6f/10.9.0.15
scm1_1   | 2023-05-09 16:56:52,839 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm1_1   | 2023-05-09 16:56:52,842 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737: change Leader from null to 1f9ebad1-a542-47ab-b506-15f3d5e79738 at term 1 for becomeLeader, leader elected after 9439ms
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm2_1   | ************************************************************/
scm2_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1_1   | 2023-05-09 16:56:53,005 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1_1   | 2023-05-09 16:56:53,116 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1_1   | 2023-05-09 16:56:53,135 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm2_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2_1   | 2023-05-09 16:57:20,044 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm1_1   | 2023-05-09 16:56:53,218 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm2_1   | /************************************************************
scm2_1   | STARTUP_MSG: Starting StorageContainerManager
scm1_1   | 2023-05-09 16:56:53,246 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm2_1   | STARTUP_MSG:   host = 9b61320f6a6f/10.9.0.15
scm2_1   | STARTUP_MSG:   args = []
scm1_1   | 2023-05-09 16:56:53,259 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1   | STARTUP_MSG:   version = 1.3.0
scm2_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
scm1_1   | 2023-05-09 16:56:53,509 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm2_1   | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm2_1   | STARTUP_MSG:   java = 11.0.14.1
scm1_1   | 2023-05-09 16:56:53,528 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm1_1   | 2023-05-09 16:56:53,549 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO impl.RoleInfo: 1f9ebad1-a542-47ab-b506-15f3d5e79738: start 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderStateImpl
scm1_1   | 2023-05-09 16:56:53,902 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-SegmentedRaftLogWorker: Starting segment from index:0
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm2_1   | ************************************************************/
scm2_1   | 2023-05-09 16:57:20,086 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1_1   | 2023-05-09 16:56:54,520 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737: set configuration 0: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1   | 2023-05-09 16:57:20,218 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2_1   | 2023-05-09 16:57:20,285 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1_1   | 2023-05-09 16:56:55,117 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737/current/log_inprogress_0
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm2_1   | 2023-05-09 16:57:20,336 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm2_1   | 2023-05-09 16:57:20,507 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2:9894 and Ratis port: 9894
scm1_1   | 2023-05-09 16:56:56,422 [main] INFO server.RaftServer: 1f9ebad1-a542-47ab-b506-15f3d5e79738: close
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1   | 2023-05-09 16:57:20,509 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2
scm2_1   | 2023-05-09 16:57:22,876 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-05-09 16:56:56,431 [main] INFO server.GrpcService: 1f9ebad1-a542-47ab-b506-15f3d5e79738: shutdown server GrpcServerProtocolService now
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 2023-05-09 16:57:23,517 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2_1   | 2023-05-09 16:57:24,180 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
scm1_1   | 2023-05-09 16:56:56,431 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737: shutdown
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 2023-05-09 16:57:24,187 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm2_1   | 2023-05-09 16:57:24,499 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1_1   | 2023-05-09 16:56:56,494 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-D55092FA9737,id=1f9ebad1-a542-47ab-b506-15f3d5e79738
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2_1   | 2023-05-09 16:57:24,566 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:66fc2679-3361-4751-923f-5ded42c3e66c
scm2_1   | 2023-05-09 16:57:24,833 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1_1   | 2023-05-09 16:56:56,495 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO impl.RoleInfo: 1f9ebad1-a542-47ab-b506-15f3d5e79738: shutdown 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderStateImpl
scm1_1   | 2023-05-09 16:56:56,571 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO impl.PendingRequests: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-PendingRequests: sendNotLeaderResponses
scm1_1   | 2023-05-09 16:56:56,614 [main] INFO server.GrpcService: 1f9ebad1-a542-47ab-b506-15f3d5e79738: shutdown server GrpcServerProtocolService successfully
scm1_1   | 2023-05-09 16:56:56,632 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO impl.StateMachineUpdater: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater: set stopIndex = 0
scm1_1   | 2023-05-09 16:56:56,636 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO impl.StateMachineUpdater: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater: Took a snapshot at index 0
scm2_1   | 2023-05-09 16:57:25,077 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm3_1   | 2023-05-09 16:57:54,233 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1_1   | 2023-05-09 16:56:56,641 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO impl.StateMachineUpdater: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm1_1   | 2023-05-09 16:56:56,680 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737: closes. applyIndex: 0
scm1_1   | 2023-05-09 16:56:56,744 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
scm2_1   | 2023-05-09 16:57:25,103 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm2_1   | 2023-05-09 16:57:25,104 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-05-09 16:57:18,321 [IPC Server handler 6 on default port 9891] WARN ipc.Server: IPC Server handler 6 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:57456: output error
recon_1  | 2023-05-09 16:57:18,326 [IPC Server handler 6 on default port 9891] INFO ipc.Server: IPC Server handler 6 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
scm3_1   | 2023-05-09 16:57:54,236 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm3_1   | 2023-05-09 16:57:54,236 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm2_1   | 2023-05-09 16:57:25,105 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm2_1   | 2023-05-09 16:57:25,105 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm3_1   | 2023-05-09 16:57:54,392 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3_1   | 2023-05-09 16:57:54,393 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm2_1   | 2023-05-09 16:57:25,105 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm2_1   | 2023-05-09 16:57:25,106 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1   | 2023-05-09 16:57:25,111 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-05-09 16:57:25,134 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2_1   | 2023-05-09 16:57:25,136 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2_1   | 2023-05-09 16:57:25,172 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-05-09 16:57:18,321 [IPC Server handler 7 on default port 9891] WARN ipc.Server: IPC Server handler 7 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:55968: output error
recon_1  | 2023-05-09 16:57:18,314 [IPC Server handler 13 on default port 9891] WARN ipc.Server: IPC Server handler 13 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:41288: output error
scm3_1   | 2023-05-09 16:57:54,393 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm3_1   | 2023-05-09 16:57:54,394 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1  | 2023-05-09 16:57:18,327 [IPC Server handler 13 on default port 9891] INFO ipc.Server: IPC Server handler 13 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 2023-05-09 16:57:54,398 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm3_1   | 2023-05-09 16:57:54,400 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm3_1   | 2023-05-09 16:57:54,401 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm3_1   | 2023-05-09 16:57:54,401 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm3_1   | 2023-05-09 16:57:54,414 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
scm3_1   | 2023-05-09 16:57:54,549 [main] INFO reflections.Reflections: Reflections took 111 ms to scan 3 urls, producing 112 keys and 252 values 
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm1_1   | 2023-05-09 16:56:56,962 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-SegmentedRaftLogWorker close()
scm3_1   | 2023-05-09 16:57:54,599 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm3_1   | 2023-05-09 16:57:54,600 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm1_1   | 2023-05-09 16:56:56,990 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-1f9ebad1-a542-47ab-b506-15f3d5e79738: Stopped
scm3_1   | 2023-05-09 16:57:54,602 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm3_1   | 2023-05-09 16:57:54,603 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1   | 2023-05-09 16:57:25,190 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm3_1   | 2023-05-09 16:57:54,632 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm1_1   | 2023-05-09 16:56:57,003 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-05-09 16:56:57,019 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-f33ab5ed-453e-4ae9-ac5c-d55092fa9737; layoutVersion=4; scmId=1f9ebad1-a542-47ab-b506-15f3d5e79738
scm2_1   | 2023-05-09 16:57:25,191 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm3_1   | 2023-05-09 16:57:54,642 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm1_1   | 2023-05-09 16:56:57,047 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 2023-05-09 16:57:26,542 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm3_1   | 2023-05-09 16:57:54,643 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1_1   | /************************************************************
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 2023-05-09 16:57:26,579 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm3_1   | 2023-05-09 16:57:54,647 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm1_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at 36f642d4db64/10.9.0.14
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2_1   | 2023-05-09 16:57:26,579 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm3_1   | 2023-05-09 16:57:54,667 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm1_1   | ************************************************************/
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2_1   | 2023-05-09 16:57:26,580 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3_1   | 2023-05-09 16:57:54,667 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1  | 2023-05-09 16:57:18,314 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:34102: output error
scm2_1   | 2023-05-09 16:57:26,580 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3_1   | 2023-05-09 16:57:54,671 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm1_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1  | 2023-05-09 16:57:18,330 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891 caught an exception
scm2_1   | 2023-05-09 16:57:26,597 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2_1   | 2023-05-09 16:57:26,651 [main] INFO server.RaftServer: 66fc2679-3361-4751-923f-5ded42c3e66c: addNew group-D55092FA9737:[] returns group-D55092FA9737:java.util.concurrent.CompletableFuture@2b8bd14b[Not completed]
scm1_1   | 2023-05-09 16:57:00,493 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
recon_1  | java.nio.channels.ClosedChannelException
scm2_1   | 2023-05-09 16:57:26,769 [pool-16-thread-1] INFO server.RaftServer$Division: 66fc2679-3361-4751-923f-5ded42c3e66c: new RaftServerImpl for group-D55092FA9737:[] with SCMStateMachine:uninitialized
scm3_1   | 2023-05-09 16:57:54,671 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm1_1   | /************************************************************
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 2023-05-09 16:57:26,781 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm3_1   | 2023-05-09 16:57:54,680 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm1_1   | STARTUP_MSG: Starting StorageContainerManager
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 2023-05-09 16:57:26,791 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm3_1   | 2023-05-09 16:57:54,680 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm1_1   | STARTUP_MSG:   host = 36f642d4db64/10.9.0.14
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm2_1   | 2023-05-09 16:57:26,792 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm3_1   | 2023-05-09 16:57:54,685 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm1_1   | STARTUP_MSG:   args = []
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm2_1   | 2023-05-09 16:57:26,792 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1_1   | STARTUP_MSG:   version = 1.3.0
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm3_1   | 2023-05-09 16:57:54,687 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm2_1   | 2023-05-09 16:57:26,794 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm1_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
scm2_1   | 2023-05-09 16:57:26,796 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm1_1   | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm3_1   | 2023-05-09 16:57:54,724 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm3_1   | 2023-05-09 16:57:54,741 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm2_1   | 2023-05-09 16:57:26,868 [pool-16-thread-1] INFO server.RaftServer$Division: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm1_1   | STARTUP_MSG:   java = 11.0.14.1
scm3_1   | 2023-05-09 16:57:54,769 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1   | 2023-05-09 16:57:26,873 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1_1   | ************************************************************/
scm3_1   | 2023-05-09 16:57:54,779 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 2023-05-09 16:57:00,505 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm3_1   | 2023-05-09 16:57:54,780 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 2023-05-09 16:57:26,901 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1_1   | 2023-05-09 16:57:00,613 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3_1   | 2023-05-09 16:57:54,786 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2_1   | 2023-05-09 16:57:26,911 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1_1   | 2023-05-09 16:57:00,675 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm3_1   | 2023-05-09 16:57:54,789 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2_1   | 2023-05-09 16:57:26,998 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm2_1   | 2023-05-09 16:57:27,021 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm3_1   | 2023-05-09 16:57:54,790 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
recon_1  | 2023-05-09 16:57:18,330 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
scm2_1   | 2023-05-09 16:57:27,022 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1_1   | 2023-05-09 16:57:00,699 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3_1   | 2023-05-09 16:57:55,550 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1  | java.nio.channels.ClosedChannelException
scm2_1   | 2023-05-09 16:57:27,974 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-05-09 16:57:00,784 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1:9894 and Ratis port: 9894
scm3_1   | 2023-05-09 16:57:55,655 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3_1   | 2023-05-09 16:57:55,788 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm3_1   | 2023-05-09 16:57:55,950 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3_1   | 2023-05-09 16:57:55,965 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3_1   | 2023-05-09 16:57:55,967 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm2_1   | 2023-05-09 16:57:27,983 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 2023-05-09 16:57:00,789 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1
scm3_1   | 2023-05-09 16:57:56,172 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2_1   | 2023-05-09 16:57:27,984 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 2023-05-09 16:57:01,682 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3_1   | 2023-05-09 16:57:56,183 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2_1   | 2023-05-09 16:57:27,985 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm1_1   | 2023-05-09 16:57:01,912 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3_1   | 2023-05-09 16:57:56,184 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm2_1   | 2023-05-09 16:57:27,993 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm1_1   | 2023-05-09 16:57:02,312 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
scm3_1   | 2023-05-09 16:57:56,284 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm2_1   | 2023-05-09 16:57:27,996 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm1_1   | 2023-05-09 16:57:02,315 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm3_1   | 2023-05-09 16:57:56,292 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm2_1   | 2023-05-09 16:57:27,996 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm1_1   | 2023-05-09 16:57:02,450 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm3_1   | Container Balancer status:
scm2_1   | 2023-05-09 16:57:27,996 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm2_1   | 2023-05-09 16:57:28,216 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
scm1_1   | 2023-05-09 16:57:02,473 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:1f9ebad1-a542-47ab-b506-15f3d5e79738
scm3_1   | Key                            Value
scm2_1   | 2023-05-09 16:57:29,010 [main] INFO reflections.Reflections: Reflections took 636 ms to scan 3 urls, producing 112 keys and 252 values 
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm1_1   | 2023-05-09 16:57:02,578 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm3_1   | Running                        true
scm2_1   | 2023-05-09 16:57:29,275 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm1_1   | 2023-05-09 16:57:02,701 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm3_1   | Container Balancer Configuration values:
scm2_1   | 2023-05-09 16:57:29,275 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm1_1   | 2023-05-09 16:57:02,708 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm3_1   | Key                                                Value
scm2_1   | 2023-05-09 16:57:29,278 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm1_1   | 2023-05-09 16:57:02,709 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm1_1   | 2023-05-09 16:57:02,711 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm2_1   | 2023-05-09 16:57:29,289 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm3_1   | Threshold                                          10
scm1_1   | 2023-05-09 16:57:02,712 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm1_1   | 2023-05-09 16:57:02,712 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm3_1   | Max Datanodes to Involve per Iteration(percent)    20
scm2_1   | 2023-05-09 16:57:29,443 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm3_1   | Max Size to Move per Iteration                     500GB
scm2_1   | 2023-05-09 16:57:29,490 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm2_1   | 2023-05-09 16:57:29,503 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1_1   | 2023-05-09 16:57:02,714 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm3_1   | Max Size Entering Target per Iteration             26GB
scm2_1   | 2023-05-09 16:57:29,520 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 2023-05-09 16:57:02,717 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-05-09 16:57:29,636 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1   | 2023-05-09 16:57:02,721 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1_1   | 2023-05-09 16:57:02,723 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1_1   | 2023-05-09 16:57:02,732 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1_1   | 2023-05-09 16:57:02,736 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm1_1   | 2023-05-09 16:57:02,737 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm1_1   | 2023-05-09 16:57:03,099 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm1_1   | 2023-05-09 16:57:03,105 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm1_1   | 2023-05-09 16:57:03,106 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm1_1   | 2023-05-09 16:57:03,106 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1_1   | 2023-05-09 16:57:03,106 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1_1   | 2023-05-09 16:57:03,114 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1_1   | 2023-05-09 16:57:03,123 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServer: 1f9ebad1-a542-47ab-b506-15f3d5e79738: found a subdirectory /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737
scm1_1   | 2023-05-09 16:57:03,131 [main] INFO server.RaftServer: 1f9ebad1-a542-47ab-b506-15f3d5e79738: addNew group-D55092FA9737:[] returns group-D55092FA9737:java.util.concurrent.CompletableFuture@2b8bd14b[Not completed]
scm1_1   | 2023-05-09 16:57:03,163 [pool-16-thread-1] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738: new RaftServerImpl for group-D55092FA9737:[] with SCMStateMachine:uninitialized
scm1_1   | 2023-05-09 16:57:03,168 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm1_1   | 2023-05-09 16:57:03,170 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1_1   | 2023-05-09 16:57:03,171 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1_1   | 2023-05-09 16:57:03,171 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1_1   | 2023-05-09 16:57:03,171 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1_1   | 2023-05-09 16:57:03,172 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1_1   | 2023-05-09 16:57:03,194 [pool-16-thread-1] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm1_1   | 2023-05-09 16:57:03,194 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1_1   | 2023-05-09 16:57:03,201 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1_1   | 2023-05-09 16:57:03,201 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1_1   | 2023-05-09 16:57:03,225 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1_1   | 2023-05-09 16:57:03,235 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1_1   | 2023-05-09 16:57:03,235 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1_1   | 2023-05-09 16:57:03,519 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3_1   | Max Size Leaving Source per Iteration              26GB
scm1_1   | 2023-05-09 16:57:03,519 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm2_1   | 2023-05-09 16:57:29,636 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
recon_1  | 2023-05-09 16:57:18,344 [IPC Server handler 5 on default port 9891] WARN ipc.Server: IPC Server handler 5 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:53676: output error
scm3_1   | 
scm1_1   | 2023-05-09 16:57:03,520 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm2_1   | 2023-05-09 16:57:29,662 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
recon_1  | 2023-05-09 16:57:18,345 [IPC Server handler 5 on default port 9891] INFO ipc.Server: IPC Server handler 5 on default port 9891 caught an exception
scm3_1   | 2023-05-09 16:57:56,292 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm1_1   | 2023-05-09 16:57:03,520 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm2_1   | 2023-05-09 16:57:29,662 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
recon_1  | java.nio.channels.ClosedChannelException
scm3_1   | 2023-05-09 16:57:56,293 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm1_1   | 2023-05-09 16:57:03,525 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 2023-05-09 16:57:56,301 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm1_1   | 2023-05-09 16:57:03,531 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm2_1   | 2023-05-09 16:57:29,683 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm3_1   | 2023-05-09 16:57:56,305 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm1_1   | 2023-05-09 16:57:03,531 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm2_1   | 2023-05-09 16:57:29,696 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm3_1   | 2023-05-09 16:57:56,308 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737 does not exist. Creating ...
scm1_1   | 2023-05-09 16:57:03,531 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm2_1   | 2023-05-09 16:57:29,723 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm3_1   | 2023-05-09 16:57:56,324 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737/in_use.lock acquired by nodename 6@f4564b650973
scm1_1   | 2023-05-09 16:57:03,598 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm2_1   | 2023-05-09 16:57:29,724 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm3_1   | 2023-05-09 16:57:56,359 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737 has been successfully formatted.
scm1_1   | 2023-05-09 16:57:03,863 [main] INFO reflections.Reflections: Reflections took 234 ms to scan 3 urls, producing 112 keys and 252 values 
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm2_1   | 2023-05-09 16:57:29,936 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm3_1   | 2023-05-09 16:57:56,370 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1_1   | 2023-05-09 16:57:03,947 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1   | 2023-05-09 16:57:30,053 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm3_1   | 2023-05-09 16:57:56,386 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1_1   | 2023-05-09 16:57:03,947 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm2_1   | 2023-05-09 16:57:30,184 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm3_1   | 2023-05-09 16:57:56,386 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-05-09 16:57:03,958 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm2_1   | 2023-05-09 16:57:30,250 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm3_1   | 2023-05-09 16:57:56,387 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1_1   | 2023-05-09 16:57:03,959 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1   | 2023-05-09 16:57:30,258 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
scm3_1   | 2023-05-09 16:57:56,390 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm3_1   | 2023-05-09 16:57:56,406 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm2_1   | 2023-05-09 16:57:30,279 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm3_1   | 2023-05-09 16:57:56,413 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm3_1   | 2023-05-09 16:57:56,414 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1   | 2023-05-09 16:57:30,292 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 2023-05-09 16:57:30,306 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3_1   | 2023-05-09 16:57:56,425 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 2023-05-09 16:57:04,037 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm2_1   | 2023-05-09 16:57:33,541 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3_1   | 2023-05-09 16:57:56,426 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1   | 2023-05-09 16:57:04,051 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm2_1   | 2023-05-09 16:57:33,649 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1   | 2023-05-09 16:57:04,052 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1_1   | 2023-05-09 16:57:04,062 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1  | 2023-05-09 16:57:44,050 [IPC Server handler 0 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/5d36dd6f-a30c-40ec-958f-b246a8287262
scm1_1   | 2023-05-09 16:57:04,119 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm2_1   | 2023-05-09 16:57:33,842 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm3_1   | 2023-05-09 16:57:56,426 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
recon_1  | 2023-05-09 16:57:44,054 [IPC Server handler 0 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-05-09 16:57:04,119 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1_1   | 2023-05-09 16:57:04,123 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm2_1   | 2023-05-09 16:57:34,183 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1  | 2023-05-09 16:57:44,093 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 5d36dd6f-a30c-40ec-958f-b246a8287262 to Node DB.
scm2_1   | 2023-05-09 16:57:34,222 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1  | 2023-05-09 16:57:44,379 [IPC Server handler 34 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3aa72183-93e1-4d03-9ed4-71b6f7b45f69
recon_1  | 2023-05-09 16:57:44,380 [IPC Server handler 34 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-05-09 16:57:56,433 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3_1   | 2023-05-09 16:57:56,435 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
recon_1  | 2023-05-09 16:57:44,382 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 3aa72183-93e1-4d03-9ed4-71b6f7b45f69 to Node DB.
scm2_1   | 2023-05-09 16:57:34,225 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
recon_1  | 2023-05-09 16:57:45,292 [IPC Server handler 9 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/56216a20-9d0f-4921-897d-3244639db044
scm1_1   | 2023-05-09 16:57:04,123 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm1_1   | 2023-05-09 16:57:04,133 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm2_1   | 2023-05-09 16:57:34,454 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1  | 2023-05-09 16:57:45,292 [IPC Server handler 9 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-05-09 16:57:04,136 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm1_1   | 2023-05-09 16:57:04,141 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm2_1   | 2023-05-09 16:57:34,501 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1  | 2023-05-09 16:57:45,294 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 56216a20-9d0f-4921-897d-3244639db044 to Node DB.
scm1_1   | 2023-05-09 16:57:04,145 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm1_1   | 2023-05-09 16:57:04,200 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
recon_1  | 2023-05-09 16:57:45,327 [IPC Server handler 8 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/99be2b94-18e1-4c1d-9084-53427369ee1a
scm3_1   | 2023-05-09 16:57:56,440 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm2_1   | 2023-05-09 16:57:34,530 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm1_1   | 2023-05-09 16:57:04,247 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
recon_1  | 2023-05-09 16:57:45,327 [IPC Server handler 8 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 99be2b94-18e1-4c1d-9084-53427369ee1a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-05-09 16:57:56,443 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1_1   | 2023-05-09 16:57:04,312 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
recon_1  | 2023-05-09 16:57:45,328 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 99be2b94-18e1-4c1d-9084-53427369ee1a to Node DB.
scm3_1   | 2023-05-09 16:57:56,444 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3_1   | 2023-05-09 16:57:56,444 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1_1   | 2023-05-09 16:57:04,352 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
recon_1  | 2023-05-09 16:57:45,352 [IPC Server handler 5 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f65ec1c2-2955-4db1-8a8e-2d0fe5785867
scm3_1   | 2023-05-09 16:57:56,465 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm3_1   | 2023-05-09 16:57:56,467 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1_1   | 2023-05-09 16:57:04,356 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
recon_1  | 2023-05-09 16:57:45,352 [IPC Server handler 5 on default port 9891] INFO node.SCMNodeManager: Registered Data node : f65ec1c2-2955-4db1-8a8e-2d0fe5785867{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-05-09 16:57:56,472 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm3_1   | 2023-05-09 16:57:56,472 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1_1   | 2023-05-09 16:57:04,364 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
recon_1  | 2023-05-09 16:57:45,353 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node f65ec1c2-2955-4db1-8a8e-2d0fe5785867 to Node DB.
scm3_1   | 2023-05-09 16:57:56,483 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm3_1   | 2023-05-09 16:57:56,485 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-05-09 16:57:04,367 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
recon_1  | 2023-05-09 16:57:46,213 [IPC Server handler 9 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ha_dn2_1.ha_net
scm3_1   | 2023-05-09 16:57:56,499 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServer$Division: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737: start with initializing state, conf=-1: peers:[]|listeners:[], old=null
scm2_1   | 2023-05-09 16:57:34,746 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm1_1   | 2023-05-09 16:57:04,376 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
recon_1  | 2023-05-09 16:57:46,214 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=79915409-920e-4d1c-bcfe-2490f334a92b reported by 5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-05-09 16:57:34,768 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm3_1   | 2023-05-09 16:57:56,505 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServer$Division: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737: changes role from      null to FOLLOWER at term 0 for startInitializing
scm3_1   | 2023-05-09 16:57:56,521 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D55092FA9737,id=887fcf03-8b52-47cd-92ee-2ac01e19b0e0
recon_1  | 2023-05-09 16:57:46,214 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 79915409-920e-4d1c-bcfe-2490f334a92b, Nodes: 5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:5d36dd6f-a30c-40ec-958f-b246a8287262, CreationTimestamp2023-05-09T16:57:12.040Z[UTC]] moved to OPEN state
scm3_1   | 2023-05-09 16:57:56,542 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3_1   | 2023-05-09 16:57:56,549 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm2_1   | Container Balancer status:
recon_1  | 2023-05-09 16:57:46,703 [IPC Server handler 0 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ha_dn4_1.ha_net
recon_1  | 2023-05-09 16:57:46,705 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=c3e11b37-a77f-4003-8d5f-1a6e129f2a79 reported by 3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-05-09 16:57:46,705 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: c3e11b37-a77f-4003-8d5f-1a6e129f2a79, Nodes: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3aa72183-93e1-4d03-9ed4-71b6f7b45f69, CreationTimestamp2023-05-09T16:57:12.389Z[UTC]] moved to OPEN state
recon_1  | 2023-05-09 16:57:47,237 [IPC Server handler 4 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn2_1.ha_net
recon_1  | 2023-05-09 16:57:47,237 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=73e0c514-5c6d-4883-b850-d3d6eb9f6b5a reported by 5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-05-09 16:57:47,540 [IPC Server handler 14 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ha_dn3_1.ha_net
scm2_1   | Key                            Value
recon_1  | 2023-05-09 16:57:47,541 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=8c529fe3-ebe2-46fe-b4b1-4fe9ff9eca5c reported by 56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-05-09 16:57:47,541 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 8c529fe3-ebe2-46fe-b4b1-4fe9ff9eca5c, Nodes: 56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:56216a20-9d0f-4921-897d-3244639db044, CreationTimestamp2023-05-09T16:57:13.275Z[UTC]] moved to OPEN state
recon_1  | 2023-05-09 16:57:47,580 [IPC Server handler 20 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn4_1.ha_net
recon_1  | 2023-05-09 16:57:47,581 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=73e0c514-5c6d-4883-b850-d3d6eb9f6b5a reported by 3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-05-09 16:57:47,613 [IPC Server handler 15 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ha_dn1_1.ha_net
recon_1  | 2023-05-09 16:57:47,613 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=59f9c2e5-7cfd-4b6f-bac2-8b0224b40693 reported by f65ec1c2-2955-4db1-8a8e-2d0fe5785867{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | Running                        true
recon_1  | 2023-05-09 16:57:47,614 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 59f9c2e5-7cfd-4b6f-bac2-8b0224b40693, Nodes: f65ec1c2-2955-4db1-8a8e-2d0fe5785867{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:f65ec1c2-2955-4db1-8a8e-2d0fe5785867, CreationTimestamp2023-05-09T16:57:13.381Z[UTC]] moved to OPEN state
recon_1  | 2023-05-09 16:57:47,844 [IPC Server handler 4 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ha_dn5_1.ha_net
recon_1  | 2023-05-09 16:57:47,845 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=6a90de28-7562-4068-9adb-d7e83643323b reported by 99be2b94-18e1-4c1d-9084-53427369ee1a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-05-09 16:57:47,846 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6a90de28-7562-4068-9adb-d7e83643323b, Nodes: 99be2b94-18e1-4c1d-9084-53427369ee1a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:99be2b94-18e1-4c1d-9084-53427369ee1a, CreationTimestamp2023-05-09T16:57:13.363Z[UTC]] moved to OPEN state
scm1_1   | 2023-05-09 16:57:05,115 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1_1   | 2023-05-09 16:57:05,134 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2_1   | Container Balancer Configuration values:
recon_1  | 2023-05-09 16:57:48,419 [IPC Server handler 85 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn3_1.ha_net
scm1_1   | 2023-05-09 16:57:05,184 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm1_1   | 2023-05-09 16:57:05,236 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2_1   | Key                                                Value
recon_1  | 2023-05-09 16:57:48,421 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=73e0c514-5c6d-4883-b850-d3d6eb9f6b5a reported by 56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-05-09 16:57:05,240 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1_1   | 2023-05-09 16:57:05,252 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm2_1   | Threshold                                          10
recon_1  | 2023-05-09 16:57:51,254 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=73e0c514-5c6d-4883-b850-d3d6eb9f6b5a reported by 5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-05-09 16:57:05,328 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1_1   | 2023-05-09 16:57:05,335 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2_1   | Max Datanodes to Involve per Iteration(percent)    20
recon_1  | 2023-05-09 16:57:51,255 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ffceeb78-bcf8-4a49-9756-391cd66b588a reported by 5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-05-09 16:57:05,336 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm1_1   | 2023-05-09 16:57:05,384 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm2_1   | Max Size to Move per Iteration                     500GB
recon_1  | 2023-05-09 16:57:51,470 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=73e0c514-5c6d-4883-b850-d3d6eb9f6b5a reported by 3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-05-09 16:57:05,385 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm1_1   | Container Balancer status:
recon_1  | 2023-05-09 16:57:51,470 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ffceeb78-bcf8-4a49-9756-391cd66b588a reported by 3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | Key                            Value
scm1_1   | Running                        true
scm2_1   | Max Size Entering Target per Iteration             26GB
recon_1  | 2023-05-09 16:57:51,489 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=73e0c514-5c6d-4883-b850-d3d6eb9f6b5a reported by 56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-05-09 16:57:51,490 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ffceeb78-bcf8-4a49-9756-391cd66b588a reported by 56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-05-09 16:57:53,264 [IPC Server handler 17 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn1_1.ha_net
recon_1  | 2023-05-09 16:57:53,484 [IPC Server handler 14 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn5_1.ha_net
recon_1  | 2023-05-09 16:58:04,434 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1  | 2023-05-09 16:58:04,435 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
scm2_1   | Max Size Leaving Source per Iteration              26GB
recon_1  | 2023-05-09 16:58:06,050 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1683651484435
recon_1  | 2023-05-09 16:58:06,061 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-05-09 16:58:06,062 [pool-27-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-05-09 16:58:06,167 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1683651484435.
scm3_1   | 2023-05-09 16:57:56,555 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm3_1   | 2023-05-09 16:57:56,560 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm2_1   | 
scm3_1   | 2023-05-09 16:57:56,593 [Listener at 0.0.0.0/9860] INFO server.RaftServer: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0: start RPC server
scm3_1   | 2023-05-09 16:57:56,858 [Listener at 0.0.0.0/9860] INFO server.GrpcService: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0: GrpcService started, listening on 9894
recon_1  | 2023-05-09 16:58:06,196 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
scm3_1   | 2023-05-09 16:57:56,870 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-887fcf03-8b52-47cd-92ee-2ac01e19b0e0: Started
scm3_1   | 2023-05-09 16:57:56,894 [Listener at 0.0.0.0/9860] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863]
scm2_1   | 2023-05-09 16:57:34,768 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm3_1   | 2023-05-09 16:57:57,679 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737: receive installSnapshot: 1f9ebad1-a542-47ab-b506-15f3d5e79738->887fcf03-8b52-47cd-92ee-2ac01e19b0e0#0-t2,notify:(t:1, i:0)
scm3_1   | 2023-05-09 16:57:57,719 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
recon_1  | 2023-05-09 16:58:06,201 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a reprocess run of NSSummaryTaskWithFSO
scm2_1   | 2023-05-09 16:57:34,768 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm3_1   | 2023-05-09 16:57:57,719 [grpc-default-executor-0] INFO server.RaftServer$Division: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737: change Leader from null to 1f9ebad1-a542-47ab-b506-15f3d5e79738 at term 2 for installSnapshot, leader elected after 3486ms
scm3_1   | 2023-05-09 16:57:57,726 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737: Received notification to install snapshot at index 0
recon_1  | 2023-05-09 16:58:06,201 [pool-49-thread-2] INFO tasks.NSSummaryTaskWithLegacy: Completed a reprocess run of NSSummaryTaskWithLegacy
scm2_1   | 2023-05-09 16:57:34,796 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm3_1   | 2023-05-09 16:57:57,727 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737: InstallSnapshot notification result: ALREADY_INSTALLED, current snapshot index: -1
scm3_1   | 2023-05-09 16:57:58,053 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737: set new configuration index: 19
recon_1  | 2023-05-09 16:58:06,509 [pool-28-thread-1] INFO tasks.TableCountTask: Completed a 'reprocess' run of TableCountTask.
scm2_1   | 2023-05-09 16:57:34,806 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm3_1   | configurationEntry {
scm3_1   |   peers {
recon_1  | 2023-05-09 16:58:06,510 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
scm2_1   | 2023-05-09 16:57:34,807 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737 does not exist. Creating ...
scm3_1   |     id: "1f9ebad1-a542-47ab-b506-15f3d5e79738"
scm3_1   |     address: "scm1:9894"
recon_1  | 2023-05-09 16:58:06,510 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
scm2_1   | 2023-05-09 16:57:34,855 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737/in_use.lock acquired by nodename 7@9b61320f6a6f
scm3_1   |     startupRole: FOLLOWER
scm1_1   | Container Balancer Configuration values:
recon_1  | 2023-05-09 16:58:06,510 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
scm3_1   |   }
recon_1  | 2023-05-09 16:58:06,520 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
scm2_1   | 2023-05-09 16:57:34,939 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737 has been successfully formatted.
scm3_1   |   peers {
scm3_1   |     id: "66fc2679-3361-4751-923f-5ded42c3e66c"
recon_1  | 2023-05-09 16:58:06,520 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.01 seconds to process 0 keys.
scm2_1   | 2023-05-09 16:57:34,964 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1_1   | Key                                                Value
scm1_1   | Threshold                                          10
recon_1  | 2023-05-09 16:58:06,552 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
scm1_1   | Max Datanodes to Involve per Iteration(percent)    20
scm1_1   | Max Size to Move per Iteration                     500GB
scm2_1   | 2023-05-09 16:57:35,008 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
recon_1  | 2023-05-09 16:58:06,554 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
recon_1  | 2023-05-09 16:58:23,101 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=73e0c514-5c6d-4883-b850-d3d6eb9f6b5a reported by 56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3_1   |     address: "scm2:9894"
recon_1  | 2023-05-09 16:58:23,105 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ffceeb78-bcf8-4a49-9756-391cd66b588a reported by 56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | Max Size Entering Target per Iteration             26GB
scm1_1   | Max Size Leaving Source per Iteration              26GB
scm2_1   | 2023-05-09 16:57:35,008 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1  | 2023-05-09 16:58:26,554 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=73e0c514-5c6d-4883-b850-d3d6eb9f6b5a reported by 5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 
recon_1  | 2023-05-09 16:58:26,555 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ffceeb78-bcf8-4a49-9756-391cd66b588a reported by 5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-05-09 16:58:26,555 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: ffceeb78-bcf8-4a49-9756-391cd66b588a, Nodes: 56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:5d36dd6f-a30c-40ec-958f-b246a8287262, CreationTimestamp2023-05-09T16:57:13.315Z[UTC]] moved to OPEN state
recon_1  | 2023-05-09 16:58:28,000 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=73e0c514-5c6d-4883-b850-d3d6eb9f6b5a reported by 3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-05-09 16:57:35,016 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
recon_1  | 2023-05-09 16:58:28,001 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 73e0c514-5c6d-4883-b850-d3d6eb9f6b5a, Nodes: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:3aa72183-93e1-4d03-9ed4-71b6f7b45f69, CreationTimestamp2023-05-09T16:57:13.298Z[UTC]] moved to OPEN state
recon_1  | 2023-05-09 16:59:01,779 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #1 got from ha_dn4_1.ha_net.
recon_1  | 2023-05-09 16:59:01,828 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1  | 2023-05-09 16:59:06,582 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1  | 2023-05-09 16:59:06,585 [pool-27-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-05-09 16:59:06,585 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm2_1   | 2023-05-09 16:57:35,017 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
recon_1  | 2023-05-09 16:59:06,585 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm1_1   | 2023-05-09 16:57:05,385 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm1_1   | 2023-05-09 16:57:05,385 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
recon_1  | 2023-05-09 16:59:06,585 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-05-09 16:59:06,585 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm3_1   |     startupRole: FOLLOWER
scm2_1   | 2023-05-09 16:57:35,047 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1_1   | 2023-05-09 16:57:05,388 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
recon_1  | 2023-05-09 16:59:06,585 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm3_1   |   }
scm2_1   | 2023-05-09 16:57:35,091 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | 2023-05-09 16:57:05,391 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
recon_1  | 2023-05-09 16:59:06,585 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
scm3_1   | }
scm1_1   | 2023-05-09 16:57:05,396 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737/in_use.lock acquired by nodename 7@36f642d4db64
recon_1  | 2023-05-09 16:59:06,585 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 2 
recon_1  | 2023-05-09 16:59:06,667 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
scm2_1   | 2023-05-09 16:57:35,096 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1_1   | 2023-05-09 16:57:05,401 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=1f9ebad1-a542-47ab-b506-15f3d5e79738} from /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737/current/raft-meta
recon_1  | 2023-05-09 16:59:06,682 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 6, SequenceNumber diff: 16, SequenceNumber Lag from OM 0.
scm3_1   |  from snapshot
scm2_1   | 2023-05-09 16:57:35,138 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737
scm1_1   | 2023-05-09 16:57:05,425 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737: set configuration 0: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-09 16:57:58,057 [grpc-default-executor-0] INFO server.RaftServer$Division: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737: set configuration 19: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 2023-05-09 16:59:06,682 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 16 records
scm2_1   | 2023-05-09 16:57:35,152 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1_1   | 2023-05-09 16:57:05,427 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm3_1   | 2023-05-09 16:57:58,079 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737: reply installSnapshot: 1f9ebad1-a542-47ab-b506-15f3d5e79738<-887fcf03-8b52-47cd-92ee-2ac01e19b0e0#0:OK-t0,ALREADY_INSTALLED
scm2_1   | 2023-05-09 16:57:35,153 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
recon_1  | 2023-05-09 16:59:06,689 [pool-28-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
recon_1  | 2023-05-09 16:59:06,690 [pool-28-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
recon_1  | 2023-05-09 16:59:06,890 [pool-28-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
recon_1  | 2023-05-09 16:59:06,907 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 1 OM DB update event(s).
scm3_1   | 2023-05-09 16:57:58,150 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0: Completed INSTALL_SNAPSHOT, lastRequest: 1f9ebad1-a542-47ab-b506-15f3d5e79738->887fcf03-8b52-47cd-92ee-2ac01e19b0e0#0-t2,notify:(t:1, i:0)
scm3_1   | 2023-05-09 16:57:58,203 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-server-thread1] INFO impl.RoleInfo: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0: start 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-FollowerState
scm3_1   | 2023-05-09 16:57:58,227 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-server-thread1] INFO server.RaftServer$Division: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm3_1   | 2023-05-09 16:57:58,230 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-server-thread1] INFO server.RaftServer$Division: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737: inconsistency entries. Reply:1f9ebad1-a542-47ab-b506-15f3d5e79738<-887fcf03-8b52-47cd-92ee-2ac01e19b0e0#1:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
recon_1  | 2023-05-09 16:59:06,948 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
scm2_1   | 2023-05-09 16:57:35,154 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3_1   | 2023-05-09 16:57:58,243 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-server-thread1] INFO server.RaftServer$Division: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm1_1   | 2023-05-09 16:57:05,433 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm2_1   | 2023-05-09 16:57:35,183 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm2_1   | 2023-05-09 16:57:35,184 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | 2023-05-09 16:57:05,433 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 2023-05-09 16:57:58,243 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-server-thread1] INFO server.RaftServer$Division: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737: inconsistency entries. Reply:1f9ebad1-a542-47ab-b506-15f3d5e79738<-887fcf03-8b52-47cd-92ee-2ac01e19b0e0#0:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm3_1   | 2023-05-09 16:57:58,272 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-server-thread1] INFO server.RaftServer$Division: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737: set configuration 0: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-05-09 16:57:35,185 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm3_1   | 2023-05-09 16:57:58,272 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-server-thread1] INFO server.RaftServer$Division: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737: set configuration 1: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-09 16:57:58,273 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-server-thread1] INFO server.RaftServer$Division: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737: set configuration 17: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1_1   | 2023-05-09 16:57:05,434 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm3_1   | 2023-05-09 16:57:58,273 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-server-thread1] INFO server.RaftServer$Division: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737: set configuration 19: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-05-09 16:57:35,185 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1_1   | 2023-05-09 16:57:05,434 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm3_1   | 2023-05-09 16:57:58,278 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-server-thread1] INFO segmented.SegmentedRaftLogWorker: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-SegmentedRaftLogWorker: Starting segment from index:0
scm1_1   | 2023-05-09 16:57:05,440 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3_1   | 2023-05-09 16:57:58,288 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-server-thread1] INFO segmented.SegmentedRaftLogWorker: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm3_1   | 2023-05-09 16:57:58,308 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-server-thread2] INFO server.RaftServer$Division: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737: set configuration 0: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-05-09 16:57:35,185 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3_1   | 2023-05-09 16:57:58,318 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-server-thread2] INFO server.RaftServer$Division: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737: set configuration 1: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-09 16:57:58,318 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-server-thread2] INFO server.RaftServer$Division: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737: set configuration 17: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1_1   | 2023-05-09 16:57:05,444 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | 2023-05-09 16:57:05,444 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm2_1   | 2023-05-09 16:57:35,281 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1_1   | 2023-05-09 16:57:05,449 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737
scm1_1   | 2023-05-09 16:57:05,449 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1_1   | 2023-05-09 16:57:05,449 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1_1   | 2023-05-09 16:57:05,450 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2_1   | 2023-05-09 16:57:35,281 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1_1   | 2023-05-09 16:57:05,450 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1_1   | 2023-05-09 16:57:05,451 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | 2023-05-09 16:57:05,451 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1_1   | 2023-05-09 16:57:05,451 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm2_1   | 2023-05-09 16:57:35,310 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1_1   | 2023-05-09 16:57:05,452 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3_1   | 2023-05-09 16:57:58,318 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-server-thread2] INFO server.RaftServer$Division: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737: set configuration 19: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-09 16:57:05,459 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1_1   | 2023-05-09 16:57:05,459 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm2_1   | 2023-05-09 16:57:35,318 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1_1   | 2023-05-09 16:57:05,459 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1_1   | 2023-05-09 16:57:05,460 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1_1   | 2023-05-09 16:57:05,479 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737: set configuration 0: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-09 16:57:05,480 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737/current/log_inprogress_0
scm2_1   | 2023-05-09 16:57:35,410 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-05-09 16:57:05,483 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 0
scm1_1   | 2023-05-09 16:57:05,483 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-05-09 16:57:05,575 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737: start as a follower, conf=0: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-09 16:57:05,575 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm2_1   | 2023-05-09 16:57:35,411 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-05-09 16:57:05,577 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO impl.RoleInfo: 1f9ebad1-a542-47ab-b506-15f3d5e79738: start 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-FollowerState
scm1_1   | 2023-05-09 16:57:05,605 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-05-09 16:57:05,605 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-05-09 16:57:05,611 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D55092FA9737,id=1f9ebad1-a542-47ab-b506-15f3d5e79738
scm2_1   | 2023-05-09 16:57:35,430 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServer$Division: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737: start with initializing state, conf=-1: peers:[]|listeners:[], old=null
scm1_1   | 2023-05-09 16:57:05,615 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1   | 2023-05-09 16:57:05,616 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1_1   | 2023-05-09 16:57:05,616 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1_1   | 2023-05-09 16:57:05,617 [1f9ebad1-a542-47ab-b506-15f3d5e79738-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm2_1   | 2023-05-09 16:57:35,451 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServer$Division: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737: changes role from      null to FOLLOWER at term 0 for startInitializing
scm1_1   | 2023-05-09 16:57:05,624 [Listener at 0.0.0.0/9860] INFO server.RaftServer: 1f9ebad1-a542-47ab-b506-15f3d5e79738: start RPC server
scm1_1   | 2023-05-09 16:57:05,707 [Listener at 0.0.0.0/9860] INFO server.GrpcService: 1f9ebad1-a542-47ab-b506-15f3d5e79738: GrpcService started, listening on 9894
scm3_1   | 2023-05-09 16:57:58,502 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737/current/log_inprogress_0
scm2_1   | 2023-05-09 16:57:35,460 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D55092FA9737,id=66fc2679-3361-4751-923f-5ded42c3e66c
scm3_1   | 2023-05-09 16:57:58,515 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737/current/log_inprogress_0 to /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737/current/log_0-0
scm1_1   | 2023-05-09 16:57:05,716 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm2_1   | 2023-05-09 16:57:35,462 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3_1   | 2023-05-09 16:57:58,550 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737/current/log_inprogress_1
scm1_1   | 2023-05-09 16:57:05,716 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm2_1   | 2023-05-09 16:57:35,462 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm3_1   | 2023-05-09 16:57:58,572 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:05,720 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-1f9ebad1-a542-47ab-b506-15f3d5e79738: Started
scm2_1   | 2023-05-09 16:57:35,491 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm3_1   | 2023-05-09 16:57:58,575 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm1_1   | 2023-05-09 16:57:05,791 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm2_1   | 2023-05-09 16:57:35,492 [66fc2679-3361-4751-923f-5ded42c3e66c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm3_1   | 2023-05-09 16:57:58,576 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm1_1   | 2023-05-09 16:57:05,809 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm2_1   | 2023-05-09 16:57:35,519 [Listener at 0.0.0.0/9860] INFO server.RaftServer: 66fc2679-3361-4751-923f-5ded42c3e66c: start RPC server
scm3_1   | 2023-05-09 16:57:58,580 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm1_1   | 2023-05-09 16:57:05,809 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm2_1   | 2023-05-09 16:57:36,194 [Listener at 0.0.0.0/9860] INFO server.GrpcService: 66fc2679-3361-4751-923f-5ded42c3e66c: GrpcService started, listening on 9894
scm3_1   | 2023-05-09 16:57:58,583 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-server-thread1] INFO server.RaftServer$Division: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737: set configuration 31: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 887fcf03-8b52-47cd-92ee-2ac01e19b0e0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1_1   | 2023-05-09 16:57:06,068 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm2_1   | 2023-05-09 16:57:36,252 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-66fc2679-3361-4751-923f-5ded42c3e66c: Started
scm3_1   | 2023-05-09 16:57:58,589 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0-server-thread2] INFO server.RaftServer$Division: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737: set configuration 33: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 887fcf03-8b52-47cd-92ee-2ac01e19b0e0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-09 16:57:06,070 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2_1   | 2023-05-09 16:57:36,333 [Listener at 0.0.0.0/9860] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
scm3_1   | 2023-05-09 16:57:58,627 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm1_1   | 2023-05-09 16:57:06,072 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm2_1   | 2023-05-09 16:57:40,415 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737: receive installSnapshot: 1f9ebad1-a542-47ab-b506-15f3d5e79738->66fc2679-3361-4751-923f-5ded42c3e66c#0-t2,notify:(t:1, i:0)
scm3_1   | 2023-05-09 16:57:58,660 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-05-09 16:57:06,096 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm2_1   | 2023-05-09 16:57:40,451 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm3_1   | 2023-05-09 16:57:58,803 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl: Successfully added SCM scm3 to group group-D55092FA9737:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 887fcf03-8b52-47cd-92ee-2ac01e19b0e0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm1_1   | 2023-05-09 16:57:06,097 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm2_1   | 2023-05-09 16:57:40,451 [grpc-default-executor-0] INFO server.RaftServer$Division: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737: change Leader from null to 1f9ebad1-a542-47ab-b506-15f3d5e79738 at term 2 for installSnapshot, leader elected after 13462ms
scm3_1   | 2023-05-09 16:57:58,804 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm1_1   | 2023-05-09 16:57:06,098 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2_1   | 2023-05-09 16:57:40,468 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737: Received notification to install snapshot at index 0
scm3_1   | 2023-05-09 16:57:59,150 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 79915409-920e-4d1c-bcfe-2490f334a92b, Nodes: 5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:12.040Z[UTC]].
scm1_1   | 2023-05-09 16:57:06,117 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm2_1   | 2023-05-09 16:57:40,477 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737: InstallSnapshot notification result: ALREADY_INSTALLED, current snapshot index: -1
scm3_1   | 2023-05-09 16:57:59,179 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:06,160 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@559991f5] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm2_1   | 2023-05-09 16:57:41,129 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737: set new configuration index: 1
scm3_1   | 2023-05-09 16:57:59,180 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm1_1   | 2023-05-09 16:57:06,177 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm2_1   | configurationEntry {
scm3_1   | 2023-05-09 16:57:59,180 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm1_1   | 2023-05-09 16:57:06,178 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm2_1   |   peers {
scm3_1   | 2023-05-09 16:57:59,199 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm1_1   | 2023-05-09 16:57:06,236 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @7902ms to org.eclipse.jetty.util.log.Slf4jLog
scm2_1   |     id: "1f9ebad1-a542-47ab-b506-15f3d5e79738"
scm3_1   | 2023-05-09 16:57:59,208 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: c3e11b37-a77f-4003-8d5f-1a6e129f2a79, Nodes: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:12.389Z[UTC]].
scm1_1   | 2023-05-09 16:57:06,475 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm2_1   |     address: "scm1:9894"
scm1_1   | 2023-05-09 16:57:06,479 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm3_1   | 2023-05-09 16:57:59,230 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-05-09 16:57:59,231 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 8c529fe3-ebe2-46fe-b4b1-4fe9ff9eca5c, Nodes: 56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:13.275Z[UTC]].
scm2_1   |     startupRole: FOLLOWER
scm3_1   | 2023-05-09 16:57:59,232 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:06,484 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm2_1   |   }
scm3_1   | 2023-05-09 16:57:59,237 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 73e0c514-5c6d-4883-b850-d3d6eb9f6b5a, Nodes: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:13.298Z[UTC]].
scm1_1   | 2023-05-09 16:57:06,485 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm2_1   | }
scm3_1   | 2023-05-09 16:57:59,238 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:06,485 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm2_1   |  from snapshot
scm3_1   | 2023-05-09 16:57:59,240 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: ffceeb78-bcf8-4a49-9756-391cd66b588a, Nodes: 56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:13.315Z[UTC]].
scm1_1   | 2023-05-09 16:57:06,486 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm2_1   | 2023-05-09 16:57:41,163 [grpc-default-executor-0] INFO server.RaftServer$Division: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737: set configuration 1: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-09 16:57:59,255 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:06,513 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm2_1   | 2023-05-09 16:57:41,193 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737: reply installSnapshot: 1f9ebad1-a542-47ab-b506-15f3d5e79738<-66fc2679-3361-4751-923f-5ded42c3e66c#0:OK-t0,ALREADY_INSTALLED
scm3_1   | 2023-05-09 16:57:59,259 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 6a90de28-7562-4068-9adb-d7e83643323b, Nodes: 99be2b94-18e1-4c1d-9084-53427369ee1a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:13.363Z[UTC]].
scm1_1   | 2023-05-09 16:57:06,514 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm2_1   | 2023-05-09 16:57:41,311 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 66fc2679-3361-4751-923f-5ded42c3e66c: Completed INSTALL_SNAPSHOT, lastRequest: 1f9ebad1-a542-47ab-b506-15f3d5e79738->66fc2679-3361-4751-923f-5ded42c3e66c#0-t2,notify:(t:1, i:0)
scm3_1   | 2023-05-09 16:57:59,263 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:06,539 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm2_1   | 2023-05-09 16:57:41,549 [66fc2679-3361-4751-923f-5ded42c3e66c-server-thread1] INFO impl.RoleInfo: 66fc2679-3361-4751-923f-5ded42c3e66c: start 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-FollowerState
scm3_1   | 2023-05-09 16:57:59,266 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 59f9c2e5-7cfd-4b6f-bac2-8b0224b40693, Nodes: f65ec1c2-2955-4db1-8a8e-2d0fe5785867{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:13.381Z[UTC]].
scm1_1   | 2023-05-09 16:57:06,539 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm2_1   | 2023-05-09 16:57:41,571 [66fc2679-3361-4751-923f-5ded42c3e66c-server-thread1] INFO server.RaftServer$Division: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm3_1   | 2023-05-09 16:57:59,267 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:06,541 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm2_1   | 2023-05-09 16:57:41,574 [66fc2679-3361-4751-923f-5ded42c3e66c-server-thread1] INFO server.RaftServer$Division: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737: inconsistency entries. Reply:1f9ebad1-a542-47ab-b506-15f3d5e79738<-66fc2679-3361-4751-923f-5ded42c3e66c#0:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm3_1   | 2023-05-09 16:57:59,278 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:06,555 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@611c3eae{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm2_1   | 2023-05-09 16:57:41,631 [66fc2679-3361-4751-923f-5ded42c3e66c-server-thread2] INFO server.RaftServer$Division: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm3_1   | 2023-05-09 16:57:59,287 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:06,556 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1ab53860{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/static,AVAILABLE}
scm2_1   | 2023-05-09 16:57:41,631 [66fc2679-3361-4751-923f-5ded42c3e66c-server-thread2] INFO server.RaftServer$Division: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737: inconsistency entries. Reply:1f9ebad1-a542-47ab-b506-15f3d5e79738<-66fc2679-3361-4751-923f-5ded42c3e66c#1:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm3_1   | 2023-05-09 16:57:59,288 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:06,873 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@35e2b89f{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_3_0_jar-_-any-11943668708409479083/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/scm}
scm2_1   | 2023-05-09 16:57:41,647 [66fc2679-3361-4751-923f-5ded42c3e66c-server-thread2] INFO server.RaftServer$Division: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737: set configuration 0: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-09 16:57:59,291 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:06,881 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@539fc5d1{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm2_1   | 2023-05-09 16:57:41,657 [66fc2679-3361-4751-923f-5ded42c3e66c-server-thread2] INFO server.RaftServer$Division: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737: set configuration 1: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-09 16:57:59,294 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm1_1   | 2023-05-09 16:57:06,882 [Listener at 0.0.0.0/9860] INFO server.Server: Started @8548ms
scm2_1   | 2023-05-09 16:57:41,672 [66fc2679-3361-4751-923f-5ded42c3e66c-server-thread2] INFO segmented.SegmentedRaftLogWorker: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-SegmentedRaftLogWorker: Starting segment from index:0
scm3_1   | 2023-05-09 16:57:59,301 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm1_1   | 2023-05-09 16:57:06,884 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm2_1   | 2023-05-09 16:57:41,794 [66fc2679-3361-4751-923f-5ded42c3e66c-server-thread2] INFO segmented.SegmentedRaftLogWorker: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm3_1   | 2023-05-09 16:57:59,302 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:06,884 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm2_1   | 2023-05-09 16:57:41,853 [66fc2679-3361-4751-923f-5ded42c3e66c-server-thread2] INFO server.RaftServer$Division: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737: set configuration 0: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-09 16:57:59,983 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm1_1   | 2023-05-09 16:57:06,886 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm2_1   | 2023-05-09 16:57:41,855 [66fc2679-3361-4751-923f-5ded42c3e66c-server-thread2] INFO server.RaftServer$Division: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737: set configuration 1: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-09 16:57:59,986 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-05-09 16:57:10,770 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-FollowerState] INFO impl.FollowerState: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5193536105ns, electionTimeout:5161ms
scm2_1   | 2023-05-09 16:57:42,367 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737/current/log_inprogress_0
scm3_1   | 2023-05-09 16:57:59,990 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm1_1   | 2023-05-09 16:57:10,771 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-FollowerState] INFO impl.RoleInfo: 1f9ebad1-a542-47ab-b506-15f3d5e79738: shutdown 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-FollowerState
scm2_1   | 2023-05-09 16:57:42,395 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737/current/log_inprogress_0 to /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737/current/log_0-0
scm3_1   | 2023-05-09 16:58:00,227 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm1_1   | 2023-05-09 16:57:10,772 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-FollowerState] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm2_1   | 2023-05-09 16:57:42,506 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737/current/log_inprogress_1
scm3_1   | 2023-05-09 16:58:00,234 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm1_1   | 2023-05-09 16:57:10,778 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm2_1   | 2023-05-09 16:57:42,534 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-05-09 16:58:00,246 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-05-09 16:57:10,778 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-FollowerState] INFO impl.RoleInfo: 1f9ebad1-a542-47ab-b506-15f3d5e79738: start 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1
scm1_1   | 2023-05-09 16:57:10,806 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO impl.LeaderElection: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-09 16:57:10,807 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO impl.LeaderElection: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm1_1   | 2023-05-09 16:57:10,808 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO impl.RoleInfo: 1f9ebad1-a542-47ab-b506-15f3d5e79738: shutdown 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1
scm1_1   | 2023-05-09 16:57:10,808 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm1_1   | 2023-05-09 16:57:10,808 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm2_1   | 2023-05-09 16:57:42,549 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm3_1   | 2023-05-09 16:58:00,247 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm1_1   | 2023-05-09 16:57:10,808 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm2_1   | 2023-05-09 16:57:42,550 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3_1   | 2023-05-09 16:58:00,333 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1144a02b] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm1_1   | 2023-05-09 16:57:10,810 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737: change Leader from null to 1f9ebad1-a542-47ab-b506-15f3d5e79738 at term 2 for becomeLeader, leader elected after 7583ms
scm1_1   | 2023-05-09 16:57:10,816 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1_1   | 2023-05-09 16:57:10,827 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm2_1   | 2023-05-09 16:57:42,551 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm3_1   | 2023-05-09 16:58:00,346 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm1_1   | 2023-05-09 16:57:10,833 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm2_1   | 2023-05-09 16:57:42,564 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm3_1   | 2023-05-09 16:58:00,348 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm1_1   | 2023-05-09 16:57:10,841 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm2_1   | 2023-05-09 16:57:42,614 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3_1   | 2023-05-09 16:58:00,380 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @16859ms to org.eclipse.jetty.util.log.Slf4jLog
scm1_1   | 2023-05-09 16:57:10,841 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm3_1   | 2023-05-09 16:58:00,605 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm2_1   | 2023-05-09 16:57:42,721 [66fc2679-3361-4751-923f-5ded42c3e66c-server-thread2] INFO server.RaftServer$Division: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737: set configuration 17: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1_1   | 2023-05-09 16:57:10,842 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm3_1   | 2023-05-09 16:58:00,620 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm2_1   | 2023-05-09 16:57:42,783 [66fc2679-3361-4751-923f-5ded42c3e66c-server-thread2] INFO server.RaftServer$Division: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737: set configuration 19: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-09 16:57:10,846 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm3_1   | 2023-05-09 16:58:00,633 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm2_1   | 2023-05-09 16:57:43,123 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl: Successfully added SCM scm2 to group group-D55092FA9737:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm1_1   | 2023-05-09 16:57:10,848 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm3_1   | 2023-05-09 16:58:00,635 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm2_1   | 2023-05-09 16:57:43,143 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm1_1   | 2023-05-09 16:57:10,850 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO impl.RoleInfo: 1f9ebad1-a542-47ab-b506-15f3d5e79738: start 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderStateImpl
scm3_1   | 2023-05-09 16:58:00,635 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm3_1   | 2023-05-09 16:58:00,637 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm1_1   | 2023-05-09 16:57:10,856 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm3_1   | 2023-05-09 16:58:00,708 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm2_1   | 2023-05-09 16:57:43,312 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 79915409-920e-4d1c-bcfe-2490f334a92b, Nodes: 5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:12.040Z[UTC]].
scm1_1   | 2023-05-09 16:57:10,859 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737/current/log_inprogress_0 to /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737/current/log_0-0
scm3_1   | 2023-05-09 16:58:00,711 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm2_1   | 2023-05-09 16:57:43,317 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:10,867 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderElection1] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737: set configuration 1: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-05-09 16:58:00,892 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm2_1   | 2023-05-09 16:57:43,317 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm1_1   | 2023-05-09 16:57:10,879 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/f33ab5ed-453e-4ae9-ac5c-d55092fa9737/current/log_inprogress_1
scm3_1   | 2023-05-09 16:58:00,892 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm2_1   | 2023-05-09 16:57:43,317 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm1_1   | 2023-05-09 16:57:10,885 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm1_1   | 2023-05-09 16:57:10,885 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm2_1   | 2023-05-09 16:57:43,411 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: c3e11b37-a77f-4003-8d5f-1a6e129f2a79, Nodes: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:12.389Z[UTC]].
scm1_1   | 2023-05-09 16:57:10,892 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-05-09 16:58:00,895 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
scm2_1   | 2023-05-09 16:57:43,440 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:10,892 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm3_1   | 2023-05-09 16:58:00,947 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5a85b4e6{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm2_1   | 2023-05-09 16:57:43,442 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 8c529fe3-ebe2-46fe-b4b1-4fe9ff9eca5c, Nodes: 56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:13.275Z[UTC]].
scm1_1   | 2023-05-09 16:57:10,893 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3_1   | 2023-05-09 16:58:00,952 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@23ea8830{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/static,AVAILABLE}
scm2_1   | 2023-05-09 16:57:43,442 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:10,893 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm3_1   | 2023-05-09 16:58:01,496 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6ea66c33{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_3_0_jar-_-any-15764544536789439464/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/scm}
scm2_1   | 2023-05-09 16:57:43,451 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 73e0c514-5c6d-4883-b850-d3d6eb9f6b5a, Nodes: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:13.298Z[UTC]].
scm1_1   | 2023-05-09 16:57:10,907 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm3_1   | 2023-05-09 16:58:01,515 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@4adf3582{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm2_1   | 2023-05-09 16:57:43,469 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:10,912 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3_1   | 2023-05-09 16:58:01,515 [Listener at 0.0.0.0/9860] INFO server.Server: Started @17994ms
scm2_1   | 2023-05-09 16:57:43,483 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: ffceeb78-bcf8-4a49-9756-391cd66b588a, Nodes: 56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:13.315Z[UTC]].
scm1_1   | 2023-05-09 16:57:11,055 [IPC Server handler 6 on default port 9861] WARN ipc.Server: IPC Server handler 6 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:47226: output error
scm3_1   | 2023-05-09 16:58:01,525 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm2_1   | 2023-05-09 16:57:43,490 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:11,095 [IPC Server handler 6 on default port 9861] INFO ipc.Server: IPC Server handler 6 on default port 9861 caught an exception
scm3_1   | 2023-05-09 16:58:01,525 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm2_1   | 2023-05-09 16:57:43,499 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 6a90de28-7562-4068-9adb-d7e83643323b, Nodes: 99be2b94-18e1-4c1d-9084-53427369ee1a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:13.363Z[UTC]].
scm1_1   | java.nio.channels.ClosedChannelException
scm3_1   | 2023-05-09 16:58:01,527 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm2_1   | 2023-05-09 16:57:43,499 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 2023-05-09 16:58:23,111 [IPC Server handler 4 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/56216a20-9d0f-4921-897d-3244639db044
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 2023-05-09 16:58:23,119 [IPC Server handler 4 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-05-09 16:57:43,507 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 59f9c2e5-7cfd-4b6f-bac2-8b0224b40693, Nodes: f65ec1c2-2955-4db1-8a8e-2d0fe5785867{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:13.381Z[UTC]].
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm3_1   | 2023-05-09 16:58:23,132 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-05-09 16:57:43,508 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm3_1   | 2023-05-09 16:58:23,135 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm2_1   | 2023-05-09 16:57:43,713 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#13 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:53428: output error
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm3_1   | 2023-05-09 16:58:23,138 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-05-09 16:57:43,719 [IPC Server handler 7 on default port 9861] WARN ipc.Server: IPC Server handler 7 on default port 9861, call Call#12 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:43164: output error
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm3_1   | 2023-05-09 16:58:23,248 [IPC Server handler 3 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f65ec1c2-2955-4db1-8a8e-2d0fe5785867
scm2_1   | 2023-05-09 16:57:43,713 [IPC Server handler 2 on default port 9861] WARN ipc.Server: IPC Server handler 2 on default port 9861, call Call#12 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:56130: output error
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm3_1   | 2023-05-09 16:58:23,250 [IPC Server handler 3 on default port 9861] INFO node.SCMNodeManager: Registered Data node : f65ec1c2-2955-4db1-8a8e-2d0fe5785867{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-05-09 16:57:43,727 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861 caught an exception
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm3_1   | 2023-05-09 16:58:23,250 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm3_1   | 2023-05-09 16:58:23,253 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm3_1   | 2023-05-09 16:58:23,254 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 2023-05-09 16:57:11,095 [IPC Server handler 3 on default port 9861] WARN ipc.Server: IPC Server handler 3 on default port 9861, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:47838: output error
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm1_1   | 2023-05-09 16:57:11,110 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861 caught an exception
scm3_1   | 2023-05-09 16:58:23,493 [IPC Server handler 7 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/99be2b94-18e1-4c1d-9084-53427369ee1a
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm1_1   | java.nio.channels.ClosedChannelException
scm3_1   | 2023-05-09 16:58:23,493 [IPC Server handler 7 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 99be2b94-18e1-4c1d-9084-53427369ee1a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-05-09 16:58:23,494 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-05-09 16:58:23,494 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm3_1   | 2023-05-09 16:58:23,494 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm3_1   | 2023-05-09 16:58:23,494 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm3_1   | 2023-05-09 16:58:23,495 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm3_1   | 2023-05-09 16:58:23,495 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm3_1   | 2023-05-09 16:58:23,496 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm3_1   | 2023-05-09 16:58:23,501 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm3_1   | 2023-05-09 16:58:26,556 [IPC Server handler 13 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/5d36dd6f-a30c-40ec-958f-b246a8287262
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm3_1   | 2023-05-09 16:58:26,557 [IPC Server handler 13 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm3_1   | 2023-05-09 16:58:26,557 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 2023-05-09 16:58:26,557 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 2023-05-09 16:58:26,590 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm3_1   | 2023-05-09 16:58:28,006 [IPC Server handler 4 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3aa72183-93e1-4d03-9ed4-71b6f7b45f69
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm3_1   | 2023-05-09 16:58:28,007 [IPC Server handler 4 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-05-09 16:58:28,009 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 73e0c514-5c6d-4883-b850-d3d6eb9f6b5a, Nodes: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:3aa72183-93e1-4d03-9ed4-71b6f7b45f69, CreationTimestamp2023-05-09T16:57:13.298Z[UTC]] moved to OPEN state
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm3_1   | 2023-05-09 16:58:28,010 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 2023-05-09 16:57:11,073 [IPC Server handler 5 on default port 9861] WARN ipc.Server: IPC Server handler 5 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:52870: output error
scm2_1   | 2023-05-09 16:57:43,728 [IPC Server handler 7 on default port 9861] INFO ipc.Server: IPC Server handler 7 on default port 9861 caught an exception
scm3_1   | 2023-05-09 16:58:28,039 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 2, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:11,116 [IPC Server handler 5 on default port 9861] INFO ipc.Server: IPC Server handler 5 on default port 9861 caught an exception
scm3_1   | 2023-05-09 16:58:28,102 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm2_1   | java.nio.channels.ClosedChannelException
scm1_1   | java.nio.channels.ClosedChannelException
scm3_1   | 2023-05-09 16:58:28,102 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 2023-05-09 16:58:28,102 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 2023-05-09 16:58:28,102 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm3_1   | 2023-05-09 16:58:28,102 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm3_1   | 2023-05-09 16:58:28,102 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm3_1   | 2023-05-09 16:58:59,484 [887fcf03-8b52-47cd-92ee-2ac01e19b0e0@group-D55092FA9737-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1   | 2023-05-09 16:57:11,063 [IPC Server handler 9 on default port 9861] WARN ipc.Server: IPC Server handler 9 on default port 9861, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:47676: output error
scm1_1   | 2023-05-09 16:57:11,117 [IPC Server handler 9 on default port 9861] INFO ipc.Server: IPC Server handler 9 on default port 9861 caught an exception
scm1_1   | java.nio.channels.ClosedChannelException
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm2_1   | 2023-05-09 16:57:43,728 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm2_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm1_1   | 2023-05-09 16:57:11,101 [IPC Server handler 7 on default port 9861] WARN ipc.Server: IPC Server handler 7 on default port 9861, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:57106: output error
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm1_1   | 2023-05-09 16:57:11,117 [IPC Server handler 7 on default port 9861] INFO ipc.Server: IPC Server handler 7 on default port 9861 caught an exception
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm1_1   | java.nio.channels.ClosedChannelException
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm2_1   | 2023-05-09 16:57:43,734 [IPC Server handler 0 on default port 9861] WARN ipc.Server: IPC Server handler 0 on default port 9861, call Call#12 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:39552: output error
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm2_1   | 2023-05-09 16:57:43,734 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861 caught an exception
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm1_1   | 2023-05-09 16:57:11,988 [IPC Server handler 54 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/5d36dd6f-a30c-40ec-958f-b246a8287262
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm1_1   | 2023-05-09 16:57:11,990 [IPC Server handler 54 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-05-09 16:57:12,005 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1_1   | 2023-05-09 16:57:12,041 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=79915409-920e-4d1c-bcfe-2490f334a92b to datanode:5d36dd6f-a30c-40ec-958f-b246a8287262
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 2023-05-09 16:57:12,041 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 2023-05-09 16:57:12,047 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1   | 2023-05-09 16:57:12,050 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1   | 2023-05-09 16:57:12,196 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 79915409-920e-4d1c-bcfe-2490f334a92b, Nodes: 5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:12.040Z[UTC]].
scm2_1   | 2023-05-09 16:57:43,739 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#13 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:41210: output error
scm1_1   | 2023-05-09 16:57:12,199 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-05-09 16:57:43,743 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
scm1_1   | 2023-05-09 16:57:12,384 [IPC Server handler 10 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3aa72183-93e1-4d03-9ed4-71b6f7b45f69
scm2_1   | java.nio.channels.ClosedChannelException
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 2023-05-09 16:57:12,385 [IPC Server handler 10 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-05-09 16:57:12,386 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1_1   | 2023-05-09 16:57:12,387 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 2023-05-09 16:57:12,389 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=c3e11b37-a77f-4003-8d5f-1a6e129f2a79 to datanode:3aa72183-93e1-4d03-9ed4-71b6f7b45f69
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm1_1   | 2023-05-09 16:57:12,407 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: c3e11b37-a77f-4003-8d5f-1a6e129f2a79, Nodes: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:12.389Z[UTC]].
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm1_1   | 2023-05-09 16:57:12,407 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm1_1   | 2023-05-09 16:57:13,274 [IPC Server handler 10 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/56216a20-9d0f-4921-897d-3244639db044
scm1_1   | 2023-05-09 16:57:13,275 [IPC Server handler 10 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm1_1   | 2023-05-09 16:57:13,275 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm1_1   | 2023-05-09 16:57:13,275 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=8c529fe3-ebe2-46fe-b4b1-4fe9ff9eca5c to datanode:56216a20-9d0f-4921-897d-3244639db044
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm1_1   | 2023-05-09 16:57:13,276 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm1_1   | 2023-05-09 16:57:13,276 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm1_1   | 2023-05-09 16:57:13,276 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm1_1   | 2023-05-09 16:57:13,276 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm1_1   | 2023-05-09 16:57:13,284 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1_1   | 2023-05-09 16:57:13,284 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 2023-05-09 16:57:13,289 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 8c529fe3-ebe2-46fe-b4b1-4fe9ff9eca5c, Nodes: 56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:13.275Z[UTC]].
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 2023-05-09 16:57:13,289 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1   | 2023-05-09 16:57:13,298 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=73e0c514-5c6d-4883-b850-d3d6eb9f6b5a to datanode:3aa72183-93e1-4d03-9ed4-71b6f7b45f69
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1   | 2023-05-09 16:57:13,303 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=73e0c514-5c6d-4883-b850-d3d6eb9f6b5a to datanode:56216a20-9d0f-4921-897d-3244639db044
scm2_1   | 2023-05-09 16:57:43,787 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm1_1   | 2023-05-09 16:57:13,303 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=73e0c514-5c6d-4883-b850-d3d6eb9f6b5a to datanode:5d36dd6f-a30c-40ec-958f-b246a8287262
scm2_1   | 2023-05-09 16:57:43,859 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm1_1   | 2023-05-09 16:57:13,313 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 73e0c514-5c6d-4883-b850-d3d6eb9f6b5a, Nodes: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:13.298Z[UTC]].
scm1_1   | 2023-05-09 16:57:13,313 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:13,315 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ffceeb78-bcf8-4a49-9756-391cd66b588a to datanode:56216a20-9d0f-4921-897d-3244639db044
scm2_1   | 2023-05-09 16:57:43,859 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm1_1   | 2023-05-09 16:57:13,321 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ffceeb78-bcf8-4a49-9756-391cd66b588a to datanode:5d36dd6f-a30c-40ec-958f-b246a8287262
scm2_1   | 2023-05-09 16:57:44,025 [IPC Server handler 10 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/5d36dd6f-a30c-40ec-958f-b246a8287262
scm1_1   | 2023-05-09 16:57:13,322 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ffceeb78-bcf8-4a49-9756-391cd66b588a to datanode:3aa72183-93e1-4d03-9ed4-71b6f7b45f69
scm2_1   | 2023-05-09 16:57:44,034 [IPC Server handler 10 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-05-09 16:57:13,327 [IPC Server handler 11 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/99be2b94-18e1-4c1d-9084-53427369ee1a
scm2_1   | 2023-05-09 16:57:44,167 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 2023-05-09 16:57:13,327 [IPC Server handler 11 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 99be2b94-18e1-4c1d-9084-53427369ee1a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-05-09 16:57:44,144 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm1_1   | 2023-05-09 16:57:13,330 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm2_1   | 2023-05-09 16:57:44,441 [IPC Server handler 11 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3aa72183-93e1-4d03-9ed4-71b6f7b45f69
scm1_1   | 2023-05-09 16:57:13,333 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: ffceeb78-bcf8-4a49-9756-391cd66b588a, Nodes: 56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:13.315Z[UTC]].
scm2_1   | 2023-05-09 16:57:44,475 [IPC Server handler 11 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-05-09 16:57:13,343 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-05-09 16:57:44,476 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-05-09 16:57:44,482 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm1_1   | 2023-05-09 16:57:13,347 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=ffceeb78-bcf8-4a49-9756-391cd66b588a contains same datanodes as previous pipelines: PipelineID=73e0c514-5c6d-4883-b850-d3d6eb9f6b5a nodeIds: 56216a20-9d0f-4921-897d-3244639db044, 5d36dd6f-a30c-40ec-958f-b246a8287262, 3aa72183-93e1-4d03-9ed4-71b6f7b45f69
scm1_1   | 2023-05-09 16:57:13,363 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=6a90de28-7562-4068-9adb-d7e83643323b to datanode:99be2b94-18e1-4c1d-9084-53427369ee1a
scm2_1   | 2023-05-09 16:57:45,291 [IPC Server handler 11 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/56216a20-9d0f-4921-897d-3244639db044
scm1_1   | 2023-05-09 16:57:13,370 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 6a90de28-7562-4068-9adb-d7e83643323b, Nodes: 99be2b94-18e1-4c1d-9084-53427369ee1a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:13.363Z[UTC]].
scm2_1   | 2023-05-09 16:57:45,304 [IPC Server handler 16 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/99be2b94-18e1-4c1d-9084-53427369ee1a
scm1_1   | 2023-05-09 16:57:13,370 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-05-09 16:57:45,322 [IPC Server handler 16 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 99be2b94-18e1-4c1d-9084-53427369ee1a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-05-09 16:57:13,374 [IPC Server handler 12 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f65ec1c2-2955-4db1-8a8e-2d0fe5785867
scm2_1   | 2023-05-09 16:57:45,323 [IPC Server handler 11 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-05-09 16:57:13,375 [IPC Server handler 12 on default port 9861] INFO node.SCMNodeManager: Registered Data node : f65ec1c2-2955-4db1-8a8e-2d0fe5785867{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-05-09 16:57:45,323 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 2023-05-09 16:57:13,375 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm2_1   | 2023-05-09 16:57:45,323 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-05-09 16:57:45,324 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm1_1   | 2023-05-09 16:57:13,381 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=59f9c2e5-7cfd-4b6f-bac2-8b0224b40693 to datanode:f65ec1c2-2955-4db1-8a8e-2d0fe5785867
scm2_1   | 2023-05-09 16:57:45,340 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm1_1   | 2023-05-09 16:57:13,386 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 59f9c2e5-7cfd-4b6f-bac2-8b0224b40693, Nodes: f65ec1c2-2955-4db1-8a8e-2d0fe5785867{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-05-09T16:57:13.381Z[UTC]].
scm2_1   | 2023-05-09 16:57:45,340 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm1_1   | 2023-05-09 16:57:13,388 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-05-09 16:57:45,341 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm1_1   | 2023-05-09 16:57:37,540 [IPC Server handler 2 on default port 9863] INFO ha.SCMRatisServerImpl: 1f9ebad1-a542-47ab-b506-15f3d5e79738: Submitting SetConfiguration request to Ratis server with new SCM peers list: [1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|priority:0|startupRole:FOLLOWER]
scm2_1   | 2023-05-09 16:57:45,411 [IPC Server handler 17 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f65ec1c2-2955-4db1-8a8e-2d0fe5785867
scm2_1   | 2023-05-09 16:57:45,361 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm2_1   | 2023-05-09 16:57:45,416 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 2023-05-09 16:57:37,552 [IPC Server handler 2 on default port 9863] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737: receive setConfiguration SetConfigurationRequest:client-5BC94DDB2632->1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737, cid=7, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm2_1   | 2023-05-09 16:57:45,418 [IPC Server handler 17 on default port 9861] INFO node.SCMNodeManager: Registered Data node : f65ec1c2-2955-4db1-8a8e-2d0fe5785867{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-05-09 16:57:37,552 [IPC Server handler 2 on default port 9863] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-5BC94DDB2632->1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737, cid=7, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm2_1   | 2023-05-09 16:57:45,495 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 2023-05-09 16:57:37,631 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm2_1   | 2023-05-09 16:57:45,633 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm1_1   | 2023-05-09 16:57:37,631 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-05-09 16:57:45,644 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-05-09 16:57:37,631 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm2_1   | 2023-05-09 16:57:45,686 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm2_1   | 2023-05-09 16:57:45,751 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm2_1   | 2023-05-09 16:57:45,752 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm1_1   | 2023-05-09 16:57:37,647 [IPC Server handler 2 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm2_1   | 2023-05-09 16:57:45,759 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-05-09 16:57:37,654 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2_1   | 2023-05-09 16:57:45,790 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm1_1   | 2023-05-09 16:57:37,655 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2_1   | 2023-05-09 16:57:45,962 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-66fc2679-3361-4751-923f-5ded42c3e66c: Detected pause in JVM or host machine (eg GC): pause of approximately 102142787ns. No GCs detected.
scm1_1   | 2023-05-09 16:57:37,655 [IPC Server handler 2 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm2_1   | 2023-05-09 16:57:46,002 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@59b492ec] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm2_1   | 2023-05-09 16:57:46,119 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm1_1   | 2023-05-09 16:57:37,655 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm2_1   | 2023-05-09 16:57:46,119 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm1_1   | 2023-05-09 16:57:37,684 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->66fc2679-3361-4751-923f-5ded42c3e66c-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->66fc2679-3361-4751-923f-5ded42c3e66c-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:1, i:0)
scm2_1   | 2023-05-09 16:57:46,281 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @31087ms to org.eclipse.jetty.util.log.Slf4jLog
scm1_1   | 2023-05-09 16:57:37,837 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->66fc2679-3361-4751-923f-5ded42c3e66c-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->66fc2679-3361-4751-923f-5ded42c3e66c-GrpcLogAppender: send 1f9ebad1-a542-47ab-b506-15f3d5e79738->66fc2679-3361-4751-923f-5ded42c3e66c#0-t2,notify:(t:1, i:0)
scm1_1   | 2023-05-09 16:57:37,858 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->66fc2679-3361-4751-923f-5ded42c3e66c-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcServerProtocolClient: Build channel for 66fc2679-3361-4751-923f-5ded42c3e66c
scm2_1   | 2023-05-09 16:57:46,527 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 79915409-920e-4d1c-bcfe-2490f334a92b, Nodes: 5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:5d36dd6f-a30c-40ec-958f-b246a8287262, CreationTimestamp2023-05-09T16:57:12.040Z[UTC]] moved to OPEN state
scm1_1   | 2023-05-09 16:57:41,335 [grpc-default-executor-0] INFO server.GrpcLogAppender: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->66fc2679-3361-4751-923f-5ded42c3e66c-InstallSnapshotResponseHandler: received the first reply 1f9ebad1-a542-47ab-b506-15f3d5e79738<-66fc2679-3361-4751-923f-5ded42c3e66c#0:OK-t0,ALREADY_INSTALLED
scm2_1   | 2023-05-09 16:57:46,648 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:41,363 [grpc-default-executor-0] INFO server.GrpcLogAppender: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->66fc2679-3361-4751-923f-5ded42c3e66c-InstallSnapshotResponseHandler: Follower snapshot is already at index 0.
scm2_1   | 2023-05-09 16:57:46,824 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:41,364 [grpc-default-executor-0] INFO leader.FollowerInfo: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->66fc2679-3361-4751-923f-5ded42c3e66c: snapshotIndex: setUnconditionally 0 -> 0
scm2_1   | 2023-05-09 16:57:47,539 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-05-09 16:57:41,364 [grpc-default-executor-0] INFO leader.FollowerInfo: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->66fc2679-3361-4751-923f-5ded42c3e66c: matchIndex: setUnconditionally 0 -> 0
scm2_1   | 2023-05-09 16:57:47,614 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-05-09 16:57:41,365 [grpc-default-executor-0] INFO leader.FollowerInfo: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->66fc2679-3361-4751-923f-5ded42c3e66c: nextIndex: setUnconditionally 0 -> 1
scm2_1   | 2023-05-09 16:57:47,619 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 8c529fe3-ebe2-46fe-b4b1-4fe9ff9eca5c, Nodes: 56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:56216a20-9d0f-4921-897d-3244639db044, CreationTimestamp2023-05-09T16:57:13.275Z[UTC]] moved to OPEN state
scm1_1   | 2023-05-09 16:57:41,365 [grpc-default-executor-0] INFO leader.FollowerInfo: Follower 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->66fc2679-3361-4751-923f-5ded42c3e66c acknowledged installing snapshot
scm2_1   | 2023-05-09 16:57:47,663 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 59f9c2e5-7cfd-4b6f-bac2-8b0224b40693, Nodes: f65ec1c2-2955-4db1-8a8e-2d0fe5785867{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:f65ec1c2-2955-4db1-8a8e-2d0fe5785867, CreationTimestamp2023-05-09T16:57:13.381Z[UTC]] moved to OPEN state
scm1_1   | 2023-05-09 16:57:41,365 [grpc-default-executor-0] INFO leader.FollowerInfo: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->66fc2679-3361-4751-923f-5ded42c3e66c: nextIndex: updateToMax old=1, new=1, updated? false
scm2_1   | 2023-05-09 16:57:47,664 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-05-09 16:57:41,583 [grpc-default-executor-0] INFO leader.FollowerInfo: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->66fc2679-3361-4751-923f-5ded42c3e66c: nextIndex: updateUnconditionally 17 -> 0
scm2_1   | 2023-05-09 16:57:47,688 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:41,641 [grpc-default-executor-0] INFO leader.FollowerInfo: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->66fc2679-3361-4751-923f-5ded42c3e66c: nextIndex: updateUnconditionally 17 -> 0
scm2_1   | 2023-05-09 16:57:47,782 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:42,537 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderStateImpl] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737: set configuration 17: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm2_1   | 2023-05-09 16:57:47,926 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6a90de28-7562-4068-9adb-d7e83643323b, Nodes: 99be2b94-18e1-4c1d-9084-53427369ee1a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:99be2b94-18e1-4c1d-9084-53427369ee1a, CreationTimestamp2023-05-09T16:57:13.363Z[UTC]] moved to OPEN state
scm1_1   | 2023-05-09 16:57:42,760 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderStateImpl] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737: set configuration 19: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-05-09 16:57:48,103 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:42,792 [IPC Server handler 2 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: 66fc2679-3361-4751-923f-5ded42c3e66c.
scm2_1   | 2023-05-09 16:57:48,450 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-05-09 16:57:46,231 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 79915409-920e-4d1c-bcfe-2490f334a92b, Nodes: 5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:5d36dd6f-a30c-40ec-958f-b246a8287262, CreationTimestamp2023-05-09T16:57:12.040Z[UTC]] moved to OPEN state
scm2_1   | 2023-05-09 16:57:48,636 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm1_1   | 2023-05-09 16:57:46,415 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-05-09 16:57:48,650 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm1_1   | 2023-05-09 16:57:46,540 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-05-09 16:57:46,697 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: c3e11b37-a77f-4003-8d5f-1a6e129f2a79, Nodes: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3aa72183-93e1-4d03-9ed4-71b6f7b45f69, CreationTimestamp2023-05-09T16:57:12.389Z[UTC]] moved to OPEN state
scm1_1   | 2023-05-09 16:57:46,780 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:46,785 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-05-09 16:57:48,734 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm1_1   | 2023-05-09 16:57:47,271 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-05-09 16:57:47,518 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 8c529fe3-ebe2-46fe-b4b1-4fe9ff9eca5c, Nodes: 56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:56216a20-9d0f-4921-897d-3244639db044, CreationTimestamp2023-05-09T16:57:13.275Z[UTC]] moved to OPEN state
scm1_1   | 2023-05-09 16:57:47,656 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:47,700 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 59f9c2e5-7cfd-4b6f-bac2-8b0224b40693, Nodes: f65ec1c2-2955-4db1-8a8e-2d0fe5785867{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:f65ec1c2-2955-4db1-8a8e-2d0fe5785867, CreationTimestamp2023-05-09T16:57:13.381Z[UTC]] moved to OPEN state
scm1_1   | 2023-05-09 16:57:47,700 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-05-09 16:57:47,717 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-05-09 16:57:47,950 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:47,995 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6a90de28-7562-4068-9adb-d7e83643323b, Nodes: 99be2b94-18e1-4c1d-9084-53427369ee1a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:99be2b94-18e1-4c1d-9084-53427369ee1a, CreationTimestamp2023-05-09T16:57:13.363Z[UTC]] moved to OPEN state
scm1_1   | 2023-05-09 16:57:47,995 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-05-09 16:57:48,066 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:57:48,167 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-05-09 16:57:48,380 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-05-09 16:57:51,273 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-05-09 16:57:51,493 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-05-09 16:57:51,501 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-05-09 16:57:53,255 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-05-09 16:57:53,496 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-05-09 16:57:57,092 [IPC Server handler 62 on default port 9863] INFO ha.SCMRatisServerImpl: 1f9ebad1-a542-47ab-b506-15f3d5e79738: Submitting SetConfiguration request to Ratis server with new SCM peers list: [1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 887fcf03-8b52-47cd-92ee-2ac01e19b0e0|rpc:scm3:9894|priority:0|startupRole:FOLLOWER]
scm1_1   | 2023-05-09 16:57:57,093 [IPC Server handler 62 on default port 9863] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737: receive setConfiguration SetConfigurationRequest:client-5BC94DDB2632->1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737, cid=13, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 887fcf03-8b52-47cd-92ee-2ac01e19b0e0|rpc:scm3:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1_1   | 2023-05-09 16:57:57,093 [IPC Server handler 62 on default port 9863] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-5BC94DDB2632->1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737, cid=13, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 887fcf03-8b52-47cd-92ee-2ac01e19b0e0|rpc:scm3:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm2_1   | 2023-05-09 16:57:48,736 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm1_1   | 2023-05-09 16:57:57,094 [IPC Server handler 62 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm2_1   | 2023-05-09 16:57:48,745 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm1_1   | 2023-05-09 16:57:57,095 [IPC Server handler 62 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-05-09 16:57:48,746 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm1_1   | 2023-05-09 16:57:57,095 [IPC Server handler 62 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm2_1   | 2023-05-09 16:57:49,030 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm1_1   | 2023-05-09 16:57:57,096 [IPC Server handler 62 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm2_1   | 2023-05-09 16:57:49,032 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm1_1   | 2023-05-09 16:57:57,096 [IPC Server handler 62 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2_1   | 2023-05-09 16:57:49,237 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm1_1   | 2023-05-09 16:57:57,102 [IPC Server handler 62 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2_1   | 2023-05-09 16:57:49,238 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm1_1   | 2023-05-09 16:57:57,103 [IPC Server handler 62 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1_1   | 2023-05-09 16:57:57,103 [IPC Server handler 62 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm2_1   | 2023-05-09 16:57:49,257 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm1_1   | 2023-05-09 16:57:57,109 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->887fcf03-8b52-47cd-92ee-2ac01e19b0e0-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->887fcf03-8b52-47cd-92ee-2ac01e19b0e0-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:1, i:0)
scm2_1   | 2023-05-09 16:57:49,310 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3babcaed{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm1_1   | 2023-05-09 16:57:57,110 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->887fcf03-8b52-47cd-92ee-2ac01e19b0e0-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->887fcf03-8b52-47cd-92ee-2ac01e19b0e0-GrpcLogAppender: send 1f9ebad1-a542-47ab-b506-15f3d5e79738->887fcf03-8b52-47cd-92ee-2ac01e19b0e0#0-t2,notify:(t:1, i:0)
scm2_1   | 2023-05-09 16:57:49,330 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@75ed125a{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/static,AVAILABLE}
scm1_1   | 2023-05-09 16:57:57,111 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->887fcf03-8b52-47cd-92ee-2ac01e19b0e0-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcServerProtocolClient: Build channel for 887fcf03-8b52-47cd-92ee-2ac01e19b0e0
scm2_1   | 2023-05-09 16:57:50,726 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@701d2b59{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_3_0_jar-_-any-10342313784205807359/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/scm}
scm1_1   | 2023-05-09 16:57:58,148 [grpc-default-executor-0] INFO server.GrpcLogAppender: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->887fcf03-8b52-47cd-92ee-2ac01e19b0e0-InstallSnapshotResponseHandler: received the first reply 1f9ebad1-a542-47ab-b506-15f3d5e79738<-887fcf03-8b52-47cd-92ee-2ac01e19b0e0#0:OK-t0,ALREADY_INSTALLED
scm2_1   | 2023-05-09 16:57:50,806 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@25a290ee{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm1_1   | 2023-05-09 16:57:58,148 [grpc-default-executor-0] INFO server.GrpcLogAppender: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->887fcf03-8b52-47cd-92ee-2ac01e19b0e0-InstallSnapshotResponseHandler: Follower snapshot is already at index 0.
scm2_1   | 2023-05-09 16:57:50,807 [Listener at 0.0.0.0/9860] INFO server.Server: Started @35613ms
scm1_1   | 2023-05-09 16:57:58,150 [grpc-default-executor-0] INFO leader.FollowerInfo: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->887fcf03-8b52-47cd-92ee-2ac01e19b0e0: snapshotIndex: setUnconditionally 0 -> 0
scm2_1   | 2023-05-09 16:57:50,832 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm1_1   | 2023-05-09 16:57:58,150 [grpc-default-executor-0] INFO leader.FollowerInfo: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->887fcf03-8b52-47cd-92ee-2ac01e19b0e0: matchIndex: setUnconditionally 0 -> 0
scm2_1   | 2023-05-09 16:57:50,835 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm1_1   | 2023-05-09 16:57:58,152 [grpc-default-executor-0] INFO leader.FollowerInfo: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->887fcf03-8b52-47cd-92ee-2ac01e19b0e0: nextIndex: setUnconditionally 0 -> 1
scm2_1   | 2023-05-09 16:57:50,841 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm1_1   | 2023-05-09 16:57:58,152 [grpc-default-executor-0] INFO leader.FollowerInfo: Follower 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->887fcf03-8b52-47cd-92ee-2ac01e19b0e0 acknowledged installing snapshot
scm2_1   | 2023-05-09 16:57:53,272 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-05-09 16:57:58,153 [grpc-default-executor-0] INFO leader.FollowerInfo: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->887fcf03-8b52-47cd-92ee-2ac01e19b0e0: nextIndex: updateToMax old=1, new=1, updated? false
scm2_1   | 2023-05-09 16:57:53,500 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-05-09 16:57:58,243 [grpc-default-executor-1] INFO leader.FollowerInfo: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->887fcf03-8b52-47cd-92ee-2ac01e19b0e0: nextIndex: updateUnconditionally 31 -> 0
scm1_1   | 2023-05-09 16:57:58,264 [grpc-default-executor-1] INFO leader.FollowerInfo: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737->887fcf03-8b52-47cd-92ee-2ac01e19b0e0: nextIndex: updateUnconditionally 31 -> 0
scm2_1   | 2023-05-09 16:57:58,577 [66fc2679-3361-4751-923f-5ded42c3e66c-server-thread2] INFO server.RaftServer$Division: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737: set configuration 31: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 887fcf03-8b52-47cd-92ee-2ac01e19b0e0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1_1   | 2023-05-09 16:57:58,575 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderStateImpl] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737: set configuration 31: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 887fcf03-8b52-47cd-92ee-2ac01e19b0e0|rpc:scm3:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1_1   | 2023-05-09 16:57:58,586 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-LeaderStateImpl] INFO server.RaftServer$Division: 1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737: set configuration 33: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 887fcf03-8b52-47cd-92ee-2ac01e19b0e0|rpc:scm3:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-05-09 16:57:58,595 [66fc2679-3361-4751-923f-5ded42c3e66c-server-thread2] INFO server.RaftServer$Division: 66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737: set configuration 33: peers:[1f9ebad1-a542-47ab-b506-15f3d5e79738|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 66fc2679-3361-4751-923f-5ded42c3e66c|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 887fcf03-8b52-47cd-92ee-2ac01e19b0e0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-05-09 16:57:58,605 [IPC Server handler 62 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: 887fcf03-8b52-47cd-92ee-2ac01e19b0e0.
scm2_1   | 2023-05-09 16:57:59,438 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-05-09 16:58:23,119 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-05-09 16:57:59,452 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-05-09 16:57:59,480 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 73e0c514-5c6d-4883-b850-d3d6eb9f6b5a, Nodes: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:3aa72183-93e1-4d03-9ed4-71b6f7b45f69, CreationTimestamp2023-05-09T16:57:13.298Z[UTC]] moved to OPEN state
scm1_1   | 2023-05-09 16:58:23,291 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-05-09 16:57:59,483 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: ffceeb78-bcf8-4a49-9756-391cd66b588a, Nodes: 56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:5d36dd6f-a30c-40ec-958f-b246a8287262, CreationTimestamp2023-05-09T16:57:13.315Z[UTC]] moved to OPEN state
scm2_1   | 2023-05-09 16:57:59,484 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-05-09 16:58:23,506 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-05-09 16:58:23,287 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-05-09 16:58:23,497 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-05-09 16:58:26,598 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm1_1   | 2023-05-09 16:58:26,564 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: ffceeb78-bcf8-4a49-9756-391cd66b588a, Nodes: 56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:5d36dd6f-a30c-40ec-958f-b246a8287262, CreationTimestamp2023-05-09T16:57:13.315Z[UTC]] moved to OPEN state
scm1_1   | 2023-05-09 16:58:26,564 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-05-09 16:58:26,579 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm2_1   | 2023-05-09 16:58:28,004 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 73e0c514-5c6d-4883-b850-d3d6eb9f6b5a, Nodes: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:3aa72183-93e1-4d03-9ed4-71b6f7b45f69, CreationTimestamp2023-05-09T16:57:13.298Z[UTC]] moved to OPEN state
scm2_1   | 2023-05-09 16:58:28,008 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm1_1   | 2023-05-09 16:58:26,585 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm2_1   | 2023-05-09 16:58:28,008 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm2_1   | 2023-05-09 16:58:28,008 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm1_1   | 2023-05-09 16:58:26,586 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm2_1   | 2023-05-09 16:58:28,008 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm2_1   | 2023-05-09 16:58:28,008 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm1_1   | 2023-05-09 16:58:26,586 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm2_1   | 2023-05-09 16:58:28,009 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm2_1   | 2023-05-09 16:58:59,488 [66fc2679-3361-4751-923f-5ded42c3e66c@group-D55092FA9737-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm1_1   | 2023-05-09 16:58:26,586 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm1_1   | 2023-05-09 16:58:26,587 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm1_1   | 2023-05-09 16:58:26,592 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm1_1   | 2023-05-09 16:58:26,592 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm1_1   | 2023-05-09 16:58:26,592 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm1_1   | 2023-05-09 16:58:26,592 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm1_1   | 2023-05-09 16:58:26,603 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm1_1   | 2023-05-09 16:58:28,004 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 73e0c514-5c6d-4883-b850-d3d6eb9f6b5a, Nodes: 3aa72183-93e1-4d03-9ed4-71b6f7b45f69{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}56216a20-9d0f-4921-897d-3244639db044{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5d36dd6f-a30c-40ec-958f-b246a8287262{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:3aa72183-93e1-4d03-9ed4-71b6f7b45f69, CreationTimestamp2023-05-09T16:57:13.298Z[UTC]] moved to OPEN state
scm1_1   | 2023-05-09 16:58:34,332 [Under Replicated Processor] INFO replication.UnderReplicatedProcessor: Processed 0 under replicated containers, failed processing 0
scm1_1   | 2023-05-09 16:58:34,357 [Over Replicated Processor] INFO replication.OverReplicatedProcessor: Processed 0 over replicated containers, failed processing 0
scm1_1   | 2023-05-09 16:58:59,386 [IPC Server handler 62 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm1_1   | 2023-05-09 16:58:59,484 [1f9ebad1-a542-47ab-b506-15f3d5e79738@group-D55092FA9737-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm1_1   | 2023-05-09 16:58:59,508 [IPC Server handler 62 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm1_1   | 2023-05-09 16:59:04,332 [Under Replicated Processor] INFO replication.UnderReplicatedProcessor: Processed 0 under replicated containers, failed processing 0
scm1_1   | 2023-05-09 16:59:04,358 [Over Replicated Processor] INFO replication.OverReplicatedProcessor: Processed 0 over replicated containers, failed processing 0
scm1_1   | 2023-05-09 16:59:34,333 [Under Replicated Processor] INFO replication.UnderReplicatedProcessor: Processed 0 under replicated containers, failed processing 0
scm1_1   | 2023-05-09 16:59:34,359 [Over Replicated Processor] INFO replication.OverReplicatedProcessor: Processed 0 over replicated containers, failed processing 0
