rm: cannot remove '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/*': No such file or directory
Executing test in compatibility
Removing network compatibility_default
Network compatibility_default not found.
Creating network "compatibility_default" with the default driver
Pulling datanode (apache/ozone-runner:20230104-1)...
20230104-1: Pulling from apache/ozone-runner
Digest: sha256:bc4c3ecdf9e653e2287e8b99449222807ee6011b520beae9a8874d9b8fc037f3
Status: Downloaded newer image for apache/ozone-runner:20230104-1
Creating compatibility_scm_1 ... 
Creating compatibility_om_1  ... 
Creating compatibility_datanode_1 ... 
Creating compatibility_recon_1    ... 
Creating compatibility_s3g_1      ... 
Creating compatibility_om_1       ... done
Creating compatibility_s3g_1      ... done
Creating compatibility_recon_1    ... done
Creating compatibility_scm_1      ... done
Creating compatibility_datanode_1 ... done
WARNING: HADOOP_OPTS has been deprecated by OZONE_OPTS.
SECONDS: 35
com.google.protobuf.ServiceException: java.net.ConnectException: Call From 5062e9f8f3f4/172.18.0.3 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.3:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 5062e9f8f3f4/172.18.0.3 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.3:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 5062e9f8f3f4/172.18.0.3 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.3:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:f3a51ef3-d5c3-45ba-8964-d4b3b719507c is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026) , while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.3:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:f3a51ef3-d5c3-45ba-8964-d4b3b719507c is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026) , while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.3:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:f3a51ef3-d5c3-45ba-8964-d4b3b719507c is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026) , while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.3:9860 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6. SCM is out of safe mode.
WARNING: HADOOP_OPTS has been deprecated by OZONE_OPTS.
SECONDS: 45
SCM is out of safe mode.
Safe mode is off
No OM HA service, no need to wait
==============================================================================
Dn :: Test datanode compatibility                                             
==============================================================================
Picks up command line options                                         | PASS |
------------------------------------------------------------------------------
Dn :: Test datanode compatibility                                     | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/compatibility/result/robot-compatibility-compatibility-dn-datanode.xml
==============================================================================
Om :: Test om compatibility                                                   
==============================================================================
Picks up command line options                                         | PASS |
------------------------------------------------------------------------------
Om :: Test om compatibility                                           | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/compatibility/result/robot-compatibility-compatibility-dn-datanode-1.xml
==============================================================================
Recon :: Test recon compatibility                                             
==============================================================================
Picks up command line options                                         | PASS |
------------------------------------------------------------------------------
Recon :: Test recon compatibility                                     | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/compatibility/result/robot-compatibility-compatibility-dn-datanode-2.xml
==============================================================================
Scm :: Test scm compatibility                                                 
==============================================================================
Picks up command line options                                         | PASS |
------------------------------------------------------------------------------
Scm :: Test scm compatibility                                         | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/compatibility/result/robot-compatibility-compatibility-dn-datanode-3.xml
==============================================================================
Dn-One-Rocksdb :: Test merge rocksdb in datanode compatibility                
==============================================================================
Create a container and check container schema version                 | PASS |
------------------------------------------------------------------------------
Dn-One-Rocksdb :: Test merge rocksdb in datanode compatibility        | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/compatibility/result/robot-compatibility-compatibility-dn-datanode-4.xml
Stopping compatibility_datanode_1 ... 
Stopping compatibility_s3g_1      ... 
Stopping compatibility_recon_1    ... 
Stopping compatibility_scm_1      ... 
Stopping compatibility_om_1       ... 
Stopping compatibility_s3g_1      ... done
Stopping compatibility_recon_1    ... done
Stopping compatibility_om_1       ... done
Stopping compatibility_datanode_1 ... done
Stopping compatibility_scm_1      ... done
Removing compatibility_datanode_1 ... 
Removing compatibility_s3g_1      ... 
Removing compatibility_recon_1    ... 
Removing compatibility_scm_1      ... 
Removing compatibility_om_1       ... 
Removing compatibility_datanode_1 ... done
Removing compatibility_scm_1      ... done
Removing compatibility_om_1       ... done
Removing compatibility_s3g_1      ... done
Removing compatibility_recon_1    ... done
Removing network compatibility_default
Log:     /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/compatibility/result/log.html
Report:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/compatibility/result/report.html
Output:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/compatibility.xml
removed 'compatibility/result/robot-compatibility-compatibility-dn-datanode-1.xml'
removed 'compatibility/result/robot-compatibility-compatibility-dn-datanode-2.xml'
removed 'compatibility/result/robot-compatibility-compatibility-dn-datanode-3.xml'
removed 'compatibility/result/robot-compatibility-compatibility-dn-datanode-4.xml'
removed 'compatibility/result/robot-compatibility-compatibility-dn-datanode.xml'
removed 'compatibility/result/log.html'
removed 'compatibility/result/report.html'
renamed 'compatibility/result/dn-audit-c75284606f47.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/compatibility/dn-audit-c75284606f47.log'
renamed 'compatibility/result/docker-compatibility-compatibility-dn-datanode.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/compatibility/docker-compatibility-compatibility-dn-datanode.log'
renamed 'compatibility/result/om-audit-114fdf8d88f2.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/compatibility/om-audit-114fdf8d88f2.log'
renamed 'compatibility/result/s3g-audit-8a90589756ed.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/compatibility/s3g-audit-8a90589756ed.log'
renamed 'compatibility/result/scm-audit-5062e9f8f3f4.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/compatibility/scm-audit-5062e9f8f3f4.log'
Executing test in upgrade
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/data/dn2': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/data/om2': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/data/dn5': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/data/scm1': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/data/dn3': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/data/dn1': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/data/scm2': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/data/om1': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/data/dn4': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/data/s3g': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/data/recon': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/data/scm3': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/data/om3': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/data': Operation not permitted
Using docker cluster defined in /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/compose/ha/docker-compose.yaml
Executing test in /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade
--- RUNNING NON-ROLLING UPGRADE TEST FROM 1.3.0 TO 1.4.0 ---
--- SETTING UP OLD VERSION 1.3.0 ---
--- RUNNING WITH OLD VERSION 1.3.0 ---
Removing network ha_net
Network ha_net not found.
Creating network "ha_net" with driver "bridge"
Pulling om1 (apache/ozone:1.3.0)...
1.3.0: Pulling from apache/ozone
Digest: sha256:e3ca5257269d23a09d878fbabc947a0ea65d9040903341478d38d5ed8cc702fd
Status: Downloaded newer image for apache/ozone:1.3.0
Creating ha_dn2_1 ... 
Creating ha_s3g_1 ... 
Creating ha_scm2_1 ... 
Creating ha_dn4_1  ... 
Creating ha_om3_1  ... 
Creating ha_om1_1  ... 
Creating ha_dn5_1  ... 
Creating ha_recon_1 ... 
Creating ha_dn3_1   ... 
Creating ha_scm1_1  ... 
Creating ha_scm3_1  ... 
Creating ha_dn1_1   ... 
Creating ha_om2_1   ... 
Creating ha_s3g_1   ... done
Creating ha_scm2_1  ... done
Creating ha_dn2_1   ... done
Creating ha_om3_1   ... done
Creating ha_dn4_1   ... done
Creating ha_scm1_1  ... done
Creating ha_dn3_1   ... done
Creating ha_recon_1 ... done
Creating ha_scm3_1  ... done
Creating ha_om1_1   ... done
Creating ha_dn5_1   ... done
Creating ha_om2_1   ... done
Creating ha_dn1_1   ... done
SECONDS: 62
com.google.protobuf.ServiceException: java.net.ConnectException: Call From 36f642d4db64/10.9.0.14 to scm2:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 36f642d4db64/10.9.0.14 to scm3:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 36f642d4db64/10.9.0.14 to scm1:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 36f642d4db64/10.9.0.14 to scm2:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 36f642d4db64/10.9.0.14 to scm3:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:1f9ebad1-a542-47ab-b506-15f3d5e79738 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:193) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:62732) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976) , while invoking $Proxy19.submitRequest over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9860 after 6 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 36f642d4db64/10.9.0.14 to scm2:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9860 after 7 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 36f642d4db64/10.9.0.14 to scm3:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9860 after 8 failover attempts. Trying to failover after sleeping for 2000ms. SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 79
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 99
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 107
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 114
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 120
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 125
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 131
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 136
SCM is out of safe mode.
Safe mode is off
Found OM leader for service omservice: om2 : LEADER (om2)
Replaced OM order with om2,om3,om1 in ha_recon_1
Replaced OM order with om2,om3,om1 in ha_s3g_1
Replaced OM order with om2,om3,om1 in ha_scm1_1
Replaced OM order with om2,om3,om1 in ha_scm2_1
Replaced OM order with om2,om3,om1 in ha_scm3_1
==============================================================================
Check-Finalization :: Finalize Upgrade of the Ozone cluster                   
==============================================================================
Check OM Finalized                                                    | PASS |
------------------------------------------------------------------------------
Check SCM Finalized                                                   | PASS |
------------------------------------------------------------------------------
Check-Finalization :: Finalize Upgrade of the Ozone cluster           | PASS |
2 tests, 2 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.3.0-original.xml
==============================================================================
Generate :: Generate data                                                     
==============================================================================
Create a volume and bucket                                            | PASS |
------------------------------------------------------------------------------
Create key                                                            | PASS |
------------------------------------------------------------------------------
Create a bucket in s3v volume                                         | PASS |
------------------------------------------------------------------------------
Create key in the bucket in s3v volume                                | PASS |
------------------------------------------------------------------------------
Setup credentials for S3                                              | PASS |
------------------------------------------------------------------------------
Try to create a bucket using S3 API                                   | PASS |
------------------------------------------------------------------------------
Create key using S3 API                                               | PASS |
------------------------------------------------------------------------------
Generate :: Generate data                                             | PASS |
7 tests, 7 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.3.0-original-1.xml
==============================================================================
Validate :: Smoketest ozone cluster startup                                   
==============================================================================
Read data from previously created key                                 | PASS |
------------------------------------------------------------------------------
Setup credentials for S3                                              | PASS |
------------------------------------------------------------------------------
Read key created with Ozone Shell using S3 API                        | PASS |
------------------------------------------------------------------------------
Read key created with S3 API using S3 API                             | PASS |
------------------------------------------------------------------------------
Validate :: Smoketest ozone cluster startup                           | PASS |
4 tests, 4 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.3.0-original-2.xml
==============================================================================
Prepare :: Prepares OMs                                                       
==============================================================================
Prepare Ozone Manager                                                 | PASS |
------------------------------------------------------------------------------
Prepare :: Prepares OMs                                               | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.3.0-original-3.xml
Stopping ha_om2_1   ... 
Stopping ha_dn1_1   ... 
Stopping ha_scm1_1  ... 
Stopping ha_scm3_1  ... 
Stopping ha_dn3_1   ... 
Stopping ha_recon_1 ... 
Stopping ha_dn5_1   ... 
Stopping ha_om1_1   ... 
Stopping ha_om3_1   ... 
Stopping ha_dn4_1   ... 
Stopping ha_scm2_1  ... 
Stopping ha_dn2_1   ... 
Stopping ha_s3g_1   ... 
Stopping ha_s3g_1   ... done
Stopping ha_om3_1   ... done
Stopping ha_om1_1   ... done
Stopping ha_om2_1   ... done
Stopping ha_dn4_1   ... done
Stopping ha_dn1_1   ... done
Stopping ha_dn5_1   ... done
Stopping ha_dn3_1   ... done
Stopping ha_dn2_1   ... done
Stopping ha_recon_1 ... done
Stopping ha_scm2_1  ... done
Stopping ha_scm1_1  ... done
Stopping ha_scm3_1  ... done
Removing ha_om2_1   ... 
Removing ha_dn1_1   ... 
Removing ha_scm1_1  ... 
Removing ha_scm3_1  ... 
Removing ha_dn3_1   ... 
Removing ha_recon_1 ... 
Removing ha_dn5_1   ... 
Removing ha_om1_1   ... 
Removing ha_om3_1   ... 
Removing ha_dn4_1   ... 
Removing ha_scm2_1  ... 
Removing ha_dn2_1   ... 
Removing ha_s3g_1   ... 
Removing ha_om2_1   ... done
Removing ha_dn4_1   ... done
Removing ha_om1_1   ... done
Removing ha_dn2_1   ... done
Removing ha_om3_1   ... done
Removing ha_recon_1 ... done
Removing ha_scm1_1  ... done
Removing ha_scm2_1  ... done
Removing ha_s3g_1   ... done
Removing ha_dn1_1   ... done
Removing ha_dn3_1   ... done
Removing ha_dn5_1   ... done
Removing ha_scm3_1  ... done
Removing network ha_net
--- RUNNING WITH NEW VERSION 1.4.0 PRE-FINALIZED ---
Removing network ha_net
Network ha_net not found.
Creating network "ha_net" with driver "bridge"
Creating ha_scm3_1 ... 
Creating ha_dn4_1  ... 
Creating ha_om3_1  ... 
Creating ha_dn3_1  ... 
Creating ha_dn2_1  ... 
Creating ha_dn5_1  ... 
Creating ha_scm1_1 ... 
Creating ha_om1_1  ... 
Creating ha_recon_1 ... 
Creating ha_om2_1   ... 
Creating ha_s3g_1   ... 
Creating ha_dn1_1   ... 
Creating ha_scm2_1  ... 
Creating ha_scm3_1  ... done
Creating ha_om1_1   ... done
Creating ha_om3_1   ... done
Creating ha_recon_1 ... done
Creating ha_dn4_1   ... done
Creating ha_scm2_1  ... done
Creating ha_dn2_1   ... done
Creating ha_scm1_1  ... done
Creating ha_s3g_1   ... done
Creating ha_dn5_1   ... done
Creating ha_dn3_1   ... done
Creating ha_om2_1   ... done
Creating ha_dn1_1   ... done
SECONDS: 73
com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:1f9ebad1-a542-47ab-b506-15f3d5e79738 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026) , while invoking $Proxy20.submitRequest over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 317869e6ebfd/10.9.0.14 to scm2:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 317869e6ebfd/10.9.0.14 to scm3:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:1f9ebad1-a542-47ab-b506-15f3d5e79738 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026) , while invoking $Proxy20.submitRequest over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9860 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:66fc2679-3361-4751-923f-5ded42c3e66c is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026) , while invoking $Proxy20.submitRequest over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9860 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 317869e6ebfd/10.9.0.14 to scm3:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9860 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:1f9ebad1-a542-47ab-b506-15f3d5e79738 is not the leader. Suggested leader is Server:scm2:9860. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:106) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026) , while invoking $Proxy20.submitRequest over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9860 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9. SCM is in safe mode. validated:false, DataNodeSafeModeRule, registered datanodes (=0) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:false, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=2)
SECONDS: 104
SCM is out of safe mode.
Safe mode is off
Found OM leader for service omservice: om1 : LEADER (om1)
Replaced OM order with om1,om3,om2 in ha_recon_1
Replaced OM order with om1,om3,om2 in ha_s3g_1
Replaced OM order with om1,om3,om2 in ha_scm1_1
Replaced OM order with om1,om3,om2 in ha_scm2_1
Replaced OM order with om1,om3,om2 in ha_scm3_1
==============================================================================
Validate :: Smoketest ozone cluster startup                                   
==============================================================================
Read data from previously created key                                 | PASS |
------------------------------------------------------------------------------
Setup credentials for S3                                              | PASS |
------------------------------------------------------------------------------
Read key created with Ozone Shell using S3 API                        | PASS |
------------------------------------------------------------------------------
Read key created with S3 API using S3 API                             | PASS |
------------------------------------------------------------------------------
Validate :: Smoketest ozone cluster startup                           | PASS |
4 tests, 4 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.4.0-pre-finalized.xml
==============================================================================
Generate :: Generate data                                                     
==============================================================================
Create key                                                            | PASS |
------------------------------------------------------------------------------
Create key in the bucket in s3v volume                                | PASS |
------------------------------------------------------------------------------
Setup credentials for S3                                              | PASS |
------------------------------------------------------------------------------
Try to create a bucket using S3 API                                   | PASS |
------------------------------------------------------------------------------
Create key using S3 API                                               | PASS |
------------------------------------------------------------------------------
Generate :: Generate data                                             | PASS |
5 tests, 5 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.4.0-pre-finalized-1.xml
==============================================================================
Generate :: Generate data                                                     
==============================================================================
Create a volume and bucket                                            | PASS |
------------------------------------------------------------------------------
Create key                                                            | PASS |
------------------------------------------------------------------------------
Create a bucket in s3v volume                                         | PASS |
------------------------------------------------------------------------------
Create key in the bucket in s3v volume                                | PASS |
------------------------------------------------------------------------------
Setup credentials for S3                                              | PASS |
------------------------------------------------------------------------------
Try to create a bucket using S3 API                                   | PASS |
------------------------------------------------------------------------------
Create key using S3 API                                               | PASS |
------------------------------------------------------------------------------
Generate :: Generate data                                             | PASS |
7 tests, 7 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.4.0-pre-finalized-2.xml
==============================================================================
Validate :: Smoketest ozone cluster startup                                   
==============================================================================
Read data from previously created key                                 | PASS |
------------------------------------------------------------------------------
Setup credentials for S3                                              | PASS |
------------------------------------------------------------------------------
Read key created with Ozone Shell using S3 API                        | PASS |
------------------------------------------------------------------------------
Read key created with S3 API using S3 API                             | PASS |
------------------------------------------------------------------------------
Validate :: Smoketest ozone cluster startup                           | PASS |
4 tests, 4 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.4.0-pre-finalized-3.xml
==============================================================================
Prepare :: Prepares OMs                                                       
==============================================================================
Prepare Ozone Manager                                                 | PASS |
------------------------------------------------------------------------------
Prepare :: Prepares OMs                                               | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.4.0-pre-finalized-4.xml
Stopping ha_scm2_1  ... 
Stopping ha_s3g_1   ... 
Stopping ha_dn1_1   ... 
Stopping ha_om2_1   ... 
Stopping ha_recon_1 ... 
Stopping ha_scm1_1  ... 
Stopping ha_dn3_1   ... 
Stopping ha_om1_1   ... 
Stopping ha_dn5_1   ... 
Stopping ha_dn2_1   ... 
Stopping ha_om3_1   ... 
Stopping ha_dn4_1   ... 
Stopping ha_scm3_1  ... 
Stopping ha_om1_1   ... done
Stopping ha_s3g_1   ... done
Stopping ha_om3_1   ... done
Stopping ha_om2_1   ... done
Stopping ha_dn1_1   ... done
Stopping ha_dn2_1   ... done
Stopping ha_dn3_1   ... done
Stopping ha_dn5_1   ... done
Stopping ha_recon_1 ... done
Stopping ha_dn4_1   ... done
Stopping ha_scm2_1  ... done
Stopping ha_scm1_1  ... done
Stopping ha_scm3_1  ... done
Removing ha_scm2_1  ... 
Removing ha_s3g_1   ... 
Removing ha_dn1_1   ... 
Removing ha_om2_1   ... 
Removing ha_recon_1 ... 
Removing ha_scm1_1  ... 
Removing ha_dn3_1   ... 
Removing ha_om1_1   ... 
Removing ha_dn5_1   ... 
Removing ha_dn2_1   ... 
Removing ha_om3_1   ... 
Removing ha_dn4_1   ... 
Removing ha_scm3_1  ... 
Removing ha_dn2_1   ... done
Removing ha_scm2_1  ... done
Removing ha_dn1_1   ... done
Removing ha_dn4_1   ... done
Removing ha_scm1_1  ... done
Removing ha_om1_1   ... done
Removing ha_s3g_1   ... done
Removing ha_om3_1   ... done
Removing ha_recon_1 ... done
Removing ha_dn5_1   ... done
Removing ha_om2_1   ... done
Removing ha_dn3_1   ... done
Removing ha_scm3_1  ... done
Removing network ha_net
--- RUNNING WITH OLD VERSION 1.3.0 AFTER DOWNGRADE ---
Removing network ha_net
Network ha_net not found.
Creating network "ha_net" with driver "bridge"
Creating ha_om2_1 ... 
Creating ha_dn4_1 ... 
Creating ha_om1_1 ... 
Creating ha_dn5_1 ... 
Creating ha_scm1_1 ... 
Creating ha_om3_1  ... 
Creating ha_recon_1 ... 
Creating ha_dn3_1   ... 
Creating ha_dn1_1   ... 
Creating ha_s3g_1   ... 
Creating ha_scm3_1  ... 
Creating ha_scm2_1  ... 
Creating ha_dn2_1   ... 
Creating ha_dn4_1   ... done
Creating ha_scm1_1  ... done
Creating ha_recon_1 ... done
Creating ha_om2_1   ... done
Creating ha_scm2_1  ... done
Creating ha_dn5_1   ... done
Creating ha_scm3_1  ... done
Creating ha_om1_1   ... done
Creating ha_om3_1   ... done
Creating ha_s3g_1   ... done
Creating ha_dn1_1   ... done
Creating ha_dn3_1   ... done
Creating ha_dn2_1   ... done
SECONDS: 70
com.google.protobuf.ServiceException: java.net.ConnectException: Call From bc5ae2ea854d/10.9.0.14 to scm2:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From bc5ae2ea854d/10.9.0.14 to scm3:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:1f9ebad1-a542-47ab-b506-15f3d5e79738 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:193) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:62732) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976) , while invoking $Proxy19.submitRequest over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From bc5ae2ea854d/10.9.0.14 to scm2:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From bc5ae2ea854d/10.9.0.14 to scm3:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:1f9ebad1-a542-47ab-b506-15f3d5e79738 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:193) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:62732) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976) , while invoking $Proxy19.submitRequest over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9860 after 6 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:66fc2679-3361-4751-923f-5ded42c3e66c is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:193) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:62732) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976) , while invoking $Proxy19.submitRequest over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9860 after 7 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From bc5ae2ea854d/10.9.0.14 to scm3:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9860 after 8 failover attempts. Trying to failover after sleeping for 2000ms. SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:true, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=1) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=2) >= threshold (=2)
SECONDS: 87
SCM is out of safe mode.
Safe mode is off
Found OM leader for service omservice: om1 : LEADER (om1)
Replaced OM order with om1,om3,om2 in ha_recon_1
Replaced OM order with om1,om3,om2 in ha_s3g_1
Replaced OM order with om1,om3,om2 in ha_scm1_1
Replaced OM order with om1,om3,om2 in ha_scm2_1
Replaced OM order with om1,om3,om2 in ha_scm3_1
==============================================================================
Check-Finalization :: Finalize Upgrade of the Ozone cluster                   
==============================================================================
Check OM Finalized                                                    | PASS |
------------------------------------------------------------------------------
Check SCM Finalized                                                   | PASS |
------------------------------------------------------------------------------
Check-Finalization :: Finalize Upgrade of the Ozone cluster           | PASS |
2 tests, 2 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.3.0-downgraded.xml
==============================================================================
Validate :: Smoketest ozone cluster startup                                   
==============================================================================
Read data from previously created key                                 | PASS |
------------------------------------------------------------------------------
Setup credentials for S3                                              | PASS |
------------------------------------------------------------------------------
Read key created with Ozone Shell using S3 API                        | PASS |
------------------------------------------------------------------------------
Read key created with S3 API using S3 API                             | PASS |
------------------------------------------------------------------------------
Validate :: Smoketest ozone cluster startup                           | PASS |
4 tests, 4 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.3.0-downgraded-1.xml
==============================================================================
Validate :: Smoketest ozone cluster startup                                   
==============================================================================
Read data from previously created key                                 | PASS |
------------------------------------------------------------------------------
Setup credentials for S3                                              | PASS |
------------------------------------------------------------------------------
Read key created with Ozone Shell using S3 API                        | PASS |
------------------------------------------------------------------------------
Read key created with S3 API using S3 API                             | PASS |
------------------------------------------------------------------------------
Validate :: Smoketest ozone cluster startup                           | PASS |
4 tests, 4 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.3.0-downgraded-2.xml
==============================================================================
Generate :: Generate data                                                     
==============================================================================
Create a volume and bucket                                            | PASS |
------------------------------------------------------------------------------
Create key                                                            | PASS |
------------------------------------------------------------------------------
Create a bucket in s3v volume                                         | PASS |
------------------------------------------------------------------------------
Create key in the bucket in s3v volume                                | PASS |
------------------------------------------------------------------------------
Setup credentials for S3                                              | PASS |
------------------------------------------------------------------------------
Try to create a bucket using S3 API                                   | PASS |
------------------------------------------------------------------------------
Create key using S3 API                                               | PASS |
------------------------------------------------------------------------------
Generate :: Generate data                                             | PASS |
7 tests, 7 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.3.0-downgraded-3.xml
==============================================================================
Validate :: Smoketest ozone cluster startup                                   
==============================================================================
Read data from previously created key                                 | PASS |
------------------------------------------------------------------------------
Setup credentials for S3                                              | PASS |
------------------------------------------------------------------------------
Read key created with Ozone Shell using S3 API                        | PASS |
------------------------------------------------------------------------------
Read key created with S3 API using S3 API                             | PASS |
------------------------------------------------------------------------------
Validate :: Smoketest ozone cluster startup                           | PASS |
4 tests, 4 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.3.0-downgraded-4.xml
==============================================================================
Generate :: Generate data                                                     
==============================================================================
Create key                                                            | PASS |
------------------------------------------------------------------------------
Create key in the bucket in s3v volume                                | PASS |
------------------------------------------------------------------------------
Setup credentials for S3                                              | PASS |
------------------------------------------------------------------------------
Try to create a bucket using S3 API                                   | PASS |
------------------------------------------------------------------------------
Create key using S3 API                                               | PASS |
------------------------------------------------------------------------------
Generate :: Generate data                                             | PASS |
5 tests, 5 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.3.0-downgraded-5.xml
==============================================================================
Prepare :: Prepares OMs                                                       
==============================================================================
Prepare Ozone Manager                                                 | FAIL |
Test timeout 5 minutes exceeded.
------------------------------------------------------------------------------
Prepare :: Prepares OMs                                               | FAIL |
1 test, 0 passed, 1 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.3.0-downgraded-6.xml
jstack 7 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/ha_om1_1_OzoneManagerStarter.stack
jstack 7 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/ha_om2_1_OzoneManagerStarter.stack
Error response from daemon: Container 9df1c9177a3ad65d33abd6328f82ca861a1e29d63a17ca42620e236b20216931 is not running
jstack 6 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/ha_recon_1_ReconServer.stack
jstack 7 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/ha_s3g_1_Gateway.stack
jstack 2275 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/ha_scm1_1_OzoneAdmin.stack
jstack 7 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/ha_scm1_1_StorageContainerManagerStarter.stack
jstack 7 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/ha_scm2_1_StorageContainerManagerStarter.stack
jstack 7 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/ha_scm3_1_StorageContainerManagerStarter.stack
Stopping ha_dn2_1   ... 
Stopping ha_scm2_1  ... 
Stopping ha_scm3_1  ... 
Stopping ha_s3g_1   ... 
Stopping ha_dn1_1   ... 
Stopping ha_dn3_1   ... 
Stopping ha_scm1_1  ... 
Stopping ha_recon_1 ... 
Stopping ha_dn5_1   ... 
Stopping ha_om1_1   ... 
Stopping ha_om2_1   ... 
Stopping ha_dn4_1   ... 
Stopping ha_om2_1   ... done
Stopping ha_om1_1   ... done
Stopping ha_s3g_1   ... done
Stopping ha_recon_1 ... done
Stopping ha_dn4_1   ... done
Stopping ha_dn5_1   ... done
Stopping ha_dn3_1   ... done
Stopping ha_dn1_1   ... done
Stopping ha_dn2_1   ... done
Stopping ha_scm2_1  ... done
Stopping ha_scm1_1  ... done
Stopping ha_scm3_1  ... done
Removing ha_dn2_1   ... 
Removing ha_scm2_1  ... 
Removing ha_scm3_1  ... 
Removing ha_s3g_1   ... 
Removing ha_dn1_1   ... 
Removing ha_dn3_1   ... 
Removing ha_om3_1   ... 
Removing ha_scm1_1  ... 
Removing ha_recon_1 ... 
Removing ha_dn5_1   ... 
Removing ha_om1_1   ... 
Removing ha_om2_1   ... 
Removing ha_dn4_1   ... 
Removing ha_dn4_1   ... done
Removing ha_dn3_1   ... done
Removing ha_dn2_1   ... done
Removing ha_om3_1   ... done
Removing ha_s3g_1   ... done
Removing ha_dn1_1   ... done
Removing ha_scm2_1  ... done
Removing ha_om2_1   ... done
Removing ha_dn5_1   ... done
Removing ha_scm1_1  ... done
Removing ha_recon_1 ... done
Removing ha_scm3_1  ... done
Removing ha_om1_1   ... done
Removing network ha_net
ERROR: Test execution of /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade is FAILED!!!!
Output:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/result/1.3.0-1.4.0.xml
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/robot-1.3.0-downgraded-1.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/robot-1.3.0-downgraded-2.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/robot-1.3.0-downgraded-3.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/robot-1.3.0-downgraded-4.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/robot-1.3.0-downgraded-5.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/robot-1.3.0-downgraded-6.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/robot-1.3.0-downgraded.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/robot-1.3.0-original-1.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/robot-1.3.0-original-2.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/robot-1.3.0-original-3.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/robot-1.3.0-original.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/robot-1.4.0-pre-finalized-1.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/robot-1.4.0-pre-finalized-2.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/robot-1.4.0-pre-finalized-3.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/robot-1.4.0-pre-finalized-4.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/robot-1.4.0-pre-finalized.xml'
renamed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/docker-1.3.0-downgraded.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/result/1.3.0-1.4.0/docker-1.3.0-downgraded.log'
renamed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/docker-1.3.0-original.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/result/1.3.0-1.4.0/docker-1.3.0-original.log'
renamed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/docker-1.4.0-pre-finalized.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/result/1.3.0-1.4.0/docker-1.4.0-pre-finalized.log'
renamed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/ha_om1_1_OzoneManagerStarter.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/result/1.3.0-1.4.0/ha_om1_1_OzoneManagerStarter.stack'
renamed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/ha_om2_1_OzoneManagerStarter.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/result/1.3.0-1.4.0/ha_om2_1_OzoneManagerStarter.stack'
renamed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/ha_recon_1_ReconServer.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/result/1.3.0-1.4.0/ha_recon_1_ReconServer.stack'
renamed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/ha_s3g_1_Gateway.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/result/1.3.0-1.4.0/ha_s3g_1_Gateway.stack'
renamed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/ha_scm1_1_OzoneAdmin.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/result/1.3.0-1.4.0/ha_scm1_1_OzoneAdmin.stack'
renamed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/ha_scm1_1_StorageContainerManagerStarter.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/result/1.3.0-1.4.0/ha_scm1_1_StorageContainerManagerStarter.stack'
renamed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/ha_scm2_1_StorageContainerManagerStarter.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/result/1.3.0-1.4.0/ha_scm2_1_StorageContainerManagerStarter.stack'
renamed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/execution/1.3.0-1.4.0/result/ha_scm3_1_StorageContainerManagerStarter.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/result/1.3.0-1.4.0/ha_scm3_1_StorageContainerManagerStarter.stack'
Log:     /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/result/log.html
Report:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/upgrade/result/report.html
ERROR: Test execution of upgrade is FAILED!!!!
Output:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/upgrade.xml
removed 'upgrade/result/1.3.0-1.4.0.xml'
removed 'upgrade/result/log.html'
removed 'upgrade/result/report.html'
renamed 'upgrade/result/1.3.0-1.4.0' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/upgrade/1.3.0-1.4.0'
Executing test in xcompat
Starting cluster with COMPOSE_FILE=new-cluster.yaml:clients.yaml
Removing network xcompat_default
Network xcompat_default not found.
Creating network "xcompat_default" with the default driver
Pulling old_client_1_2_1 (apache/ozone:1.2.1)...
1.2.1: Pulling from apache/ozone
Digest: sha256:1a5f4e28568aaa4aaea6ed560d45b167e0dd8274660e81d746379c2aa6594b7b
Status: Downloaded newer image for apache/ozone:1.2.1
Pulling old_client_1_1_0 (apache/ozone:1.1.0)...
1.1.0: Pulling from apache/ozone
Digest: sha256:6dd13dca7a08ad072b027d30187363bc3552c63947325e871cb51199fbaef172
Status: Downloaded newer image for apache/ozone:1.1.0
Pulling old_client_1_0_0 (apache/ozone:1.0.0)...
1.0.0: Pulling from apache/ozone
Digest: sha256:8d797cb22fc643a302e9bf34e81131ad5cab7256ae99813d3d8fff01263f26a9
Status: Downloaded newer image for apache/ozone:1.0.0
Creating xcompat_old_client_1_1_0_1 ... 
Creating xcompat_scm_1              ... 
Creating xcompat_recon_1            ... 
Creating xcompat_datanode_1         ... 
Creating xcompat_datanode_2         ... 
Creating xcompat_datanode_3         ... 
Creating xcompat_s3g_1              ... 
Creating xcompat_old_client_1_0_0_1 ... 
Creating xcompat_old_client_1_2_1_1 ... 
Creating xcompat_new_client_1       ... 
Creating xcompat_om_1               ... 
Creating xcompat_scm_1              ... done
Creating xcompat_recon_1            ... done
Creating xcompat_datanode_3         ... done
Creating xcompat_old_client_1_2_1_1 ... done
Creating xcompat_old_client_1_1_0_1 ... done
Creating xcompat_new_client_1       ... done
Creating xcompat_datanode_2         ... done
Creating xcompat_old_client_1_0_0_1 ... done
Creating xcompat_s3g_1              ... done
Creating xcompat_om_1               ... done
Creating xcompat_datanode_1         ... done
SECONDS: 53
com.google.protobuf.ServiceException: java.net.ConnectException: Call From 1f80a59fb5eb/172.19.0.2 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy21.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.2:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 1f80a59fb5eb/172.19.0.2 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy21.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.2:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 1f80a59fb5eb/172.19.0.2 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy21.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.2:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:fba5044f-c294-4135-a734-e2abf895cac4 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026) , while invoking $Proxy21.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.2:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:fba5044f-c294-4135-a734-e2abf895cac4 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026) , while invoking $Proxy21.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.2:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5. SCM is in safe mode. validated:false, DataNodeSafeModeRule, registered datanodes (=1) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 70
SCM is out of safe mode.
Safe mode is off
No OM HA service, no need to wait
2023-05-09 17:18:08,554 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2023-05-09 17:18:08,922 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2023-05-09 17:18:08,922 [main] INFO impl.MetricsSystemImpl: ozone-freon metrics system started
2023-05-09 17:18:09,134 [main] INFO freon.BaseFreonGenerator: Executing test with prefix warmup and number-of-tests 1
2023-05-09 17:18:09,228 [Thread-5] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2023-05-09 17:18:10,236 [Thread-5] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2023-05-09 17:18:11,241 [Thread-5] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2023-05-09 17:18:12,241 [Thread-5] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2023-05-09 17:18:13,242 [Thread-5] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2023-05-09 17:18:13,592 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om1 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:244)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:232)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:225)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:177)
	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
, while invoking $Proxy25.submitRequest over nodeId=null,nodeAddress=om:9862 after 1 failover attempts. Trying to failover after sleeping for 4000ms. Current retry count: 1.
2023-05-09 17:18:14,243 [Thread-5] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2023-05-09 17:18:15,244 [Thread-5] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2023-05-09 17:18:16,245 [Thread-5] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2023-05-09 17:18:17,245 [Thread-5] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2023-05-09 17:18:18,246 [Thread-5] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2023-05-09 17:18:18,258 [main] INFO rpc.RpcClient: Creating Volume: vol1, with hadoop as owner and space quota set to -1 bytes, counts quota set to -1
2023-05-09 17:18:18,490 [main] INFO rpc.RpcClient: Creating Bucket: vol1/bucket1, with server-side default bucket layout, hadoop as owner, Versioning false, Storage Type set to DISK and Encryption set to false, Replication Type set to server-side default replication type, Namespace Quota set to -1, Space Quota set to -1 
2023-05-09 17:18:19,246 [Thread-5] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2023-05-09 17:18:19,833 [pool-2-thread-1] WARN impl.MetricsSystemImpl: ozone-freon metrics system already initialized!
2023-05-09 17:18:20,251 [Thread-5] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2023-05-09 17:18:20,267 [pool-2-thread-1] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
2023-05-09 17:18:21,253 [Thread-5] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2023-05-09 17:18:22,254 [Thread-5] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2023-05-09 17:18:23,261 [Thread-5] INFO freon.ProgressBar: Progress: 100.00 % (1 out of 1)
2023-05-09 17:18:23,335 [shutdown-hook-0] INFO metrics: type=TIMER, name=key-create, count=1, min=3928.605206, max=3928.605206, mean=3928.605206, stddev=0.0, median=3928.605206, p75=3928.605206, p95=3928.605206, p98=3928.605206, p99=3928.605206, p999=3928.605206, mean_rate=0.21039976542445993, m1=0.0, m5=0.0, m15=0.0, rate_unit=events/second, duration_unit=milliseconds
2023-05-09 17:18:23,340 [shutdown-hook-0] INFO freon.BaseFreonGenerator: Total execution time (sec): 14
2023-05-09 17:18:23,340 [shutdown-hook-0] INFO freon.BaseFreonGenerator: Failures: 0
2023-05-09 17:18:23,340 [shutdown-hook-0] INFO freon.BaseFreonGenerator: Successful executions: 1
==============================================================================
xcompat-cluster-1.3.0-client-1.3.0-write :: Write Compatibility               
==============================================================================
Key Can Be Written                                                    | PASS |
------------------------------------------------------------------------------
Dir Can Be Created                                                    | PASS |
------------------------------------------------------------------------------
File Can Be Put                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.3.0-write :: Write Compatibility       | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.3.0-read-1.3.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.3.0-read-1.3.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-1.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.0.0-write :: Write Compatibility               
==============================================================================
Key Can Be Written                                                    | PASS |
------------------------------------------------------------------------------
Dir Can Be Created                                                    | PASS |
------------------------------------------------------------------------------
File Can Be Put                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.0.0-write :: Write Compatibility       | PASS |
3 critical tests, 3 passed, 0 failed
3 tests total, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-2.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.0.0-read-1.0.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.0.0-read-1.0.0 :: Read Compatibility   | PASS |
3 critical tests, 3 passed, 0 failed
3 tests total, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-3.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.0.0-read-1.3.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.0.0-read-1.3.0 :: Read Compatibility   | PASS |
3 critical tests, 3 passed, 0 failed
3 tests total, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-4.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.3.0-read-1.0.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.3.0-read-1.0.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-5.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.1.0-write :: Write Compatibility               
==============================================================================
Key Can Be Written                                                    | PASS |
------------------------------------------------------------------------------
Dir Can Be Created                                                    | PASS |
------------------------------------------------------------------------------
File Can Be Put                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.1.0-write :: Write Compatibility       | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-6.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.1.0-read-1.1.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.1.0-read-1.1.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-7.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.1.0-read-1.3.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.1.0-read-1.3.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-8.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.3.0-read-1.1.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.3.0-read-1.1.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-9.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.2.1-write :: Write Compatibility               
==============================================================================
Key Can Be Written                                                    | PASS |
------------------------------------------------------------------------------
Dir Can Be Created                                                    | PASS |
------------------------------------------------------------------------------
File Can Be Put                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.2.1-write :: Write Compatibility       | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-10.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.2.1-read-1.2.1 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.2.1-read-1.2.1 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-11.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.2.1-read-1.3.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.2.1-read-1.3.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-12.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.3.0-read-1.2.1 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.3.0-read-1.2.1 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-13.xml
Stopping xcompat_om_1               ... 
Stopping xcompat_new_client_1       ... 
Stopping xcompat_old_client_1_0_0_1 ... 
Stopping xcompat_datanode_1         ... 
Stopping xcompat_old_client_1_2_1_1 ... 
Stopping xcompat_datanode_2         ... 
Stopping xcompat_datanode_3         ... 
Stopping xcompat_s3g_1              ... 
Stopping xcompat_scm_1              ... 
Stopping xcompat_old_client_1_1_0_1 ... 
Stopping xcompat_recon_1            ... 
Stopping xcompat_old_client_1_1_0_1 ... done
Stopping xcompat_old_client_1_2_1_1 ... done
Stopping xcompat_old_client_1_0_0_1 ... done
Stopping xcompat_new_client_1       ... done
Stopping xcompat_s3g_1              ... done
Stopping xcompat_om_1               ... done
Stopping xcompat_datanode_1         ... done
Stopping xcompat_recon_1            ... done
Stopping xcompat_datanode_3         ... done
Stopping xcompat_datanode_2         ... done
Stopping xcompat_scm_1              ... done
Removing xcompat_om_1               ... 
Removing xcompat_new_client_1       ... 
Removing xcompat_old_client_1_0_0_1 ... 
Removing xcompat_datanode_1         ... 
Removing xcompat_old_client_1_2_1_1 ... 
Removing xcompat_datanode_2         ... 
Removing xcompat_datanode_3         ... 
Removing xcompat_s3g_1              ... 
Removing xcompat_scm_1              ... 
Removing xcompat_old_client_1_1_0_1 ... 
Removing xcompat_recon_1            ... 
Removing xcompat_datanode_1         ... done
Removing xcompat_om_1               ... done
Removing xcompat_old_client_1_0_0_1 ... done
Removing xcompat_datanode_2         ... done
Removing xcompat_old_client_1_2_1_1 ... done
Removing xcompat_old_client_1_1_0_1 ... done
Removing xcompat_datanode_3         ... done
Removing xcompat_new_client_1       ... done
Removing xcompat_scm_1              ... done
Removing xcompat_s3g_1              ... done
Removing xcompat_recon_1            ... done
Removing network xcompat_default
Starting cluster with COMPOSE_FILE=old-cluster.yaml:clients.yaml
Removing network xcompat_default
Network xcompat_default not found.
Creating network "xcompat_default" with the default driver
Creating xcompat_recon_1 ... 
Creating xcompat_old_client_1_2_1_1 ... 
Creating xcompat_s3g_1              ... 
Creating xcompat_new_client_1       ... 
Creating xcompat_datanode_1         ... 
Creating xcompat_datanode_2         ... 
Creating xcompat_datanode_3         ... 
Creating xcompat_old_client_1_0_0_1 ... 
Creating xcompat_om_1               ... 
Creating xcompat_old_client_1_1_0_1 ... 
Creating xcompat_scm_1              ... 
Creating xcompat_new_client_1       ... done
Creating xcompat_old_client_1_2_1_1 ... done
Creating xcompat_old_client_1_0_0_1 ... done
Creating xcompat_s3g_1              ... done
Creating xcompat_recon_1            ... done
Creating xcompat_datanode_1         ... done
Creating xcompat_datanode_2         ... done
Creating xcompat_old_client_1_1_0_1 ... done
Creating xcompat_scm_1              ... done
Creating xcompat_datanode_3         ... done
Creating xcompat_om_1               ... done
SECONDS: 25
Retrying connect to server: scm/172.20.0.12:9860. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) Retrying connect to server: scm/172.20.0.12:9860. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) Retrying connect to server: scm/172.20.0.12:9860. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) Retrying connect to server: scm/172.20.0.12:9860. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) Retrying connect to server: scm/172.20.0.12:9860. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) Retrying connect to server: scm/172.20.0.12:9860. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 31
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 35
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 39
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 43
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 47
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 51
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 55
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 59
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 63
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 68
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 72
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 76
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 80
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 84
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registeredDns 3 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 90
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registeredDns 3 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 94
SCM is out of safe mode.
Safe mode is off
No OM HA service, no need to wait
2023-05-09 17:24:25 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
2023-05-09 17:24:25 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
2023-05-09 17:24:25 INFO  MetricsSystemImpl:191 - ozone-freon metrics system started
2023-05-09 17:24:25 INFO  BaseFreonGenerator:247 - Executing test with prefix warmup
2023-05-09 17:24:25 INFO  ProgressBar:163 - Progress: 0.00 % (0 out of 1)
2023-05-09 17:24:25 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-05-09 17:24:26 INFO  RpcClient:314 - Creating Volume: vol1, with hadoop as owner.
2023-05-09 17:24:26 INFO  RpcClient:459 - Creating Bucket: vol1/bucket1, with Versioning false and Storage Type set to DISK and Encryption set to false 
2023-05-09 17:24:26 INFO  ProgressBar:163 - Progress: 0.00 % (0 out of 1)
2023-05-09 17:24:26 WARN  MetricsSystemImpl:151 - ozone-freon metrics system already initialized!
2023-05-09 17:24:27 INFO  ProgressBar:163 - Progress: 0.00 % (0 out of 1)
2023-05-09 17:24:28 INFO  ProgressBar:163 - Progress: 0.00 % (0 out of 1)
2023-05-09 17:24:29 INFO  ProgressBar:163 - Progress: 100.00 % (1 out of 1)
2023-05-09 17:24:29 INFO  metrics:107 - type=TIMER, name=key-create, count=1, min=2344.455895, max=2344.455895, mean=2344.455895, stddev=0.0, median=2344.455895, p75=2344.455895, p95=2344.455895, p98=2344.455895, p99=2344.455895, p999=2344.455895, mean_rate=0.3069001273572, m1=0.0, m5=0.0, m15=0.0, rate_unit=events/second, duration_unit=milliseconds
2023-05-09 17:24:29 INFO  BaseFreonGenerator:75 - Total execution time (sec): 4
2023-05-09 17:24:29 INFO  BaseFreonGenerator:75 - Failures: 0
2023-05-09 17:24:29 INFO  BaseFreonGenerator:75 - Successful executions: 1
==============================================================================
xcompat-cluster-1.0.0-client-1.3.0-write :: Write Compatibility               
==============================================================================
Key Can Be Written                                                    | PASS |
------------------------------------------------------------------------------
Dir Can Be Created                                                    | PASS |
------------------------------------------------------------------------------
File Can Be Put                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.0.0-client-1.3.0-write :: Write Compatibility       | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-14.xml
==============================================================================
xcompat-cluster-1.0.0-client-1.3.0-read-1.3.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.0.0-client-1.3.0-read-1.3.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-15.xml
==============================================================================
xcompat-cluster-1.0.0-client-1.0.0-write :: Write Compatibility               
==============================================================================
Key Can Be Written                                                    | PASS |
------------------------------------------------------------------------------
Dir Can Be Created                                                    | PASS |
------------------------------------------------------------------------------
File Can Be Put                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.0.0-client-1.0.0-write :: Write Compatibility       | PASS |
3 critical tests, 3 passed, 0 failed
3 tests total, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-16.xml
==============================================================================
xcompat-cluster-1.0.0-client-1.0.0-read-1.0.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.0.0-client-1.0.0-read-1.0.0 :: Read Compatibility   | PASS |
3 critical tests, 3 passed, 0 failed
3 tests total, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-17.xml
==============================================================================
xcompat-cluster-1.0.0-client-1.0.0-read-1.3.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.0.0-client-1.0.0-read-1.3.0 :: Read Compatibility   | PASS |
3 critical tests, 3 passed, 0 failed
3 tests total, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-18.xml
==============================================================================
xcompat-cluster-1.0.0-client-1.3.0-read-1.0.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.0.0-client-1.3.0-read-1.0.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-19.xml
Stopping xcompat_scm_1              ... 
Stopping xcompat_old_client_1_1_0_1 ... 
Stopping xcompat_om_1               ... 
Stopping xcompat_datanode_3         ... 
Stopping xcompat_datanode_1         ... 
Stopping xcompat_old_client_1_0_0_1 ... 
Stopping xcompat_datanode_2         ... 
Stopping xcompat_s3g_1              ... 
Stopping xcompat_new_client_1       ... 
Stopping xcompat_old_client_1_2_1_1 ... 
Stopping xcompat_recon_1            ... 
Stopping xcompat_new_client_1       ... done
Stopping xcompat_old_client_1_1_0_1 ... done
Stopping xcompat_old_client_1_2_1_1 ... done
Stopping xcompat_old_client_1_0_0_1 ... done
Stopping xcompat_om_1               ... done
Stopping xcompat_datanode_3         ... done
Stopping xcompat_s3g_1              ... done
Stopping xcompat_scm_1              ... done
Stopping xcompat_datanode_1         ... done
Stopping xcompat_datanode_2         ... done
Stopping xcompat_recon_1            ... done
Removing xcompat_scm_1              ... 
Removing xcompat_old_client_1_1_0_1 ... 
Removing xcompat_om_1               ... 
Removing xcompat_datanode_3         ... 
Removing xcompat_datanode_1         ... 
Removing xcompat_old_client_1_0_0_1 ... 
Removing xcompat_datanode_2         ... 
Removing xcompat_s3g_1              ... 
Removing xcompat_new_client_1       ... 
Removing xcompat_old_client_1_2_1_1 ... 
Removing xcompat_recon_1            ... 
Removing xcompat_old_client_1_0_0_1 ... done
Removing xcompat_new_client_1       ... done
Removing xcompat_recon_1            ... done
Removing xcompat_datanode_3         ... done
Removing xcompat_old_client_1_1_0_1 ... done
Removing xcompat_om_1               ... done
Removing xcompat_old_client_1_2_1_1 ... done
Removing xcompat_s3g_1              ... done
Removing xcompat_datanode_2         ... done
Removing xcompat_scm_1              ... done
Removing xcompat_datanode_1         ... done
Removing network xcompat_default
Starting cluster with COMPOSE_FILE=old-cluster.yaml:clients.yaml
Removing network xcompat_default
Network xcompat_default not found.
Creating network "xcompat_default" with the default driver
Creating xcompat_new_client_1 ... 
Creating xcompat_datanode_1   ... 
Creating xcompat_datanode_2   ... 
Creating xcompat_datanode_3   ... 
Creating xcompat_scm_1        ... 
Creating xcompat_s3g_1        ... 
Creating xcompat_old_client_1_0_0_1 ... 
Creating xcompat_old_client_1_1_0_1 ... 
Creating xcompat_old_client_1_2_1_1 ... 
Creating xcompat_recon_1            ... 
Creating xcompat_om_1               ... 
Creating xcompat_datanode_1         ... done
Creating xcompat_new_client_1       ... done
Creating xcompat_datanode_3         ... done
Creating xcompat_old_client_1_0_0_1 ... done
Creating xcompat_s3g_1              ... done
Creating xcompat_recon_1            ... done
Creating xcompat_datanode_2         ... done
Creating xcompat_old_client_1_1_0_1 ... done
Creating xcompat_old_client_1_2_1_1 ... done
Creating xcompat_om_1               ... done
Creating xcompat_scm_1              ... done
SECONDS: 30
Retrying connect to server: scm/172.21.0.8:9860. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) Retrying connect to server: scm/172.21.0.8:9860. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) Retrying connect to server: scm/172.21.0.8:9860. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) Retrying connect to server: scm/172.21.0.8:9860. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) Retrying connect to server: scm/172.21.0.8:9860. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) SCM is in safe mode. validated:false, DataNodeSafeModeRule, registered datanodes (=0) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 38
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 43
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 47
SCM is out of safe mode.
Safe mode is off
No OM HA service, no need to wait
2023-05-09 17:27:00,444 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2023-05-09 17:27:00,550 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2023-05-09 17:27:00,550 [main] INFO impl.MetricsSystemImpl: ozone-freon metrics system started
2023-05-09 17:27:00,668 [main] INFO freon.BaseFreonGenerator: Executing test with prefix warmup
2023-05-09 17:27:00,682 [Thread-3] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2023-05-09 17:27:01,446 [main] INFO rpc.RpcClient: Creating Volume: vol1, with hadoop as owner and space quota set to -1 bytes, counts quota set to -1
2023-05-09 17:27:01,654 [main] INFO rpc.RpcClient: Creating Bucket: vol1/bucket1, with Versioning false and Storage Type set to DISK and Encryption set to false 
2023-05-09 17:27:01,688 [Thread-3] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2023-05-09 17:27:02,414 [pool-1-thread-1] WARN impl.MetricsSystemImpl: ozone-freon metrics system already initialized!
2023-05-09 17:27:02,616 [pool-1-thread-1] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
2023-05-09 17:27:02,618 [pool-1-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-5CB870E71041->fb0959ab-8b91-4e89-a246-74d0fde532ed
2023-05-09 17:27:02,620 [pool-1-thread-1] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
2023-05-09 17:27:02,691 [Thread-3] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2023-05-09 17:27:03,691 [Thread-3] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2023-05-09 17:27:04,691 [Thread-3] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2023-05-09 17:27:05,692 [Thread-3] INFO freon.ProgressBar: Progress: 100.00 % (1 out of 1)
2023-05-09 17:27:05,747 [Thread-2] INFO metrics: type=TIMER, name=key-create, count=1, min=3211.945262, max=3211.945262, mean=3211.945262, stddev=0.0, median=3211.945262, p75=3211.945262, p95=3211.945262, p98=3211.945262, p99=3211.945262, p999=3211.945262, mean_rate=0.24749663563220883, m1=0.0, m5=0.0, m15=0.0, rate_unit=events/second, duration_unit=milliseconds
2023-05-09 17:27:05,748 [Thread-2] INFO freon.BaseFreonGenerator: Total execution time (sec): 5
2023-05-09 17:27:05,749 [Thread-2] INFO freon.BaseFreonGenerator: Failures: 0
2023-05-09 17:27:05,749 [Thread-2] INFO freon.BaseFreonGenerator: Successful executions: 1
==============================================================================
xcompat-cluster-1.1.0-client-1.3.0-write :: Write Compatibility               
==============================================================================
Key Can Be Written                                                    | PASS |
------------------------------------------------------------------------------
Dir Can Be Created                                                    | PASS |
------------------------------------------------------------------------------
File Can Be Put                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.1.0-client-1.3.0-write :: Write Compatibility       | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-20.xml
==============================================================================
xcompat-cluster-1.1.0-client-1.3.0-read-1.3.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.1.0-client-1.3.0-read-1.3.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-21.xml
==============================================================================
xcompat-cluster-1.1.0-client-1.1.0-write :: Write Compatibility               
==============================================================================
Key Can Be Written                                                    | PASS |
------------------------------------------------------------------------------
Dir Can Be Created                                                    | PASS |
------------------------------------------------------------------------------
File Can Be Put                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.1.0-client-1.1.0-write :: Write Compatibility       | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-22.xml
==============================================================================
xcompat-cluster-1.1.0-client-1.1.0-read-1.1.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.1.0-client-1.1.0-read-1.1.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-23.xml
==============================================================================
xcompat-cluster-1.1.0-client-1.1.0-read-1.3.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.1.0-client-1.1.0-read-1.3.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-24.xml
==============================================================================
xcompat-cluster-1.1.0-client-1.3.0-read-1.1.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.1.0-client-1.3.0-read-1.1.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-25.xml
Stopping xcompat_om_1               ... 
Stopping xcompat_old_client_1_2_1_1 ... 
Stopping xcompat_recon_1            ... 
Stopping xcompat_old_client_1_1_0_1 ... 
Stopping xcompat_s3g_1              ... 
Stopping xcompat_scm_1              ... 
Stopping xcompat_old_client_1_0_0_1 ... 
Stopping xcompat_datanode_2         ... 
Stopping xcompat_datanode_1         ... 
Stopping xcompat_datanode_3         ... 
Stopping xcompat_new_client_1       ... 
Stopping xcompat_new_client_1       ... done
Stopping xcompat_old_client_1_2_1_1 ... done
Stopping xcompat_old_client_1_0_0_1 ... done
Stopping xcompat_old_client_1_1_0_1 ... done
Stopping xcompat_datanode_3         ... done
Stopping xcompat_s3g_1              ... done
Stopping xcompat_datanode_1         ... done
Stopping xcompat_scm_1              ... done
Stopping xcompat_om_1               ... done
Stopping xcompat_datanode_2         ... done
Stopping xcompat_recon_1            ... done
Removing xcompat_om_1               ... 
Removing xcompat_old_client_1_2_1_1 ... 
Removing xcompat_recon_1            ... 
Removing xcompat_old_client_1_1_0_1 ... 
Removing xcompat_s3g_1              ... 
Removing xcompat_scm_1              ... 
Removing xcompat_old_client_1_0_0_1 ... 
Removing xcompat_datanode_2         ... 
Removing xcompat_datanode_1         ... 
Removing xcompat_datanode_3         ... 
Removing xcompat_new_client_1       ... 
Removing xcompat_new_client_1       ... done
Removing xcompat_old_client_1_1_0_1 ... done
Removing xcompat_s3g_1              ... done
Removing xcompat_old_client_1_2_1_1 ... done
Removing xcompat_old_client_1_0_0_1 ... done
Removing xcompat_datanode_2         ... done
Removing xcompat_datanode_1         ... done
Removing xcompat_recon_1            ... done
Removing xcompat_scm_1              ... done
Removing xcompat_datanode_3         ... done
Removing xcompat_om_1               ... done
Removing network xcompat_default
Starting cluster with COMPOSE_FILE=old-cluster.yaml:clients.yaml
Removing network xcompat_default
Network xcompat_default not found.
Creating network "xcompat_default" with the default driver
Creating xcompat_new_client_1 ... 
Creating xcompat_datanode_1   ... 
Creating xcompat_datanode_2   ... 
Creating xcompat_datanode_3   ... 
Creating xcompat_om_1         ... 
Creating xcompat_old_client_1_1_0_1 ... 
Creating xcompat_scm_1              ... 
Creating xcompat_recon_1            ... 
Creating xcompat_old_client_1_0_0_1 ... 
Creating xcompat_old_client_1_2_1_1 ... 
Creating xcompat_s3g_1              ... 
Creating xcompat_new_client_1       ... done
Creating xcompat_old_client_1_1_0_1 ... done
Creating xcompat_om_1               ... done
Creating xcompat_datanode_3         ... done
Creating xcompat_datanode_1         ... done
Creating xcompat_s3g_1              ... done
Creating xcompat_old_client_1_2_1_1 ... done
Creating xcompat_datanode_2         ... done
Creating xcompat_old_client_1_0_0_1 ... done
Creating xcompat_recon_1            ... done
Creating xcompat_scm_1              ... done
SECONDS: 36
com.google.protobuf.ServiceException: java.net.ConnectException: Call From 90e9b3bb8bcd/172.22.0.9 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.22.0.9:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 90e9b3bb8bcd/172.22.0.9 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.22.0.9:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. SCM is in safe mode. validated:false, DataNodeSafeModeRule, registered datanodes (=1) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 46
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 52
SCM is out of safe mode.
Safe mode is off
No OM HA service, no need to wait
2023-05-09 17:29:45,026 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2023-05-09 17:29:45,164 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2023-05-09 17:29:45,164 [main] INFO impl.MetricsSystemImpl: ozone-freon metrics system started
2023-05-09 17:29:45,339 [main] INFO freon.BaseFreonGenerator: Executing test with prefix warmup
2023-05-09 17:29:45,422 [Thread-3] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2023-05-09 17:29:46,428 [Thread-3] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2023-05-09 17:29:46,433 [main] INFO rpc.RpcClient: Creating Volume: vol1, with hadoop as owner and space quota set to -1 bytes, counts quota set to -1
2023-05-09 17:29:46,565 [main] INFO rpc.RpcClient: Creating Bucket: vol1/bucket1, with Versioning false and Storage Type set to DISK and Encryption set to false 
2023-05-09 17:29:47,232 [pool-2-thread-1] WARN impl.MetricsSystemImpl: ozone-freon metrics system already initialized!
2023-05-09 17:29:47,429 [Thread-3] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2023-05-09 17:29:47,432 [pool-2-thread-1] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
2023-05-09 17:29:48,429 [Thread-3] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2023-05-09 17:29:49,429 [Thread-3] INFO freon.ProgressBar: Progress: 100.00 % (1 out of 1)
2023-05-09 17:29:49,632 [shutdown-hook-0] INFO metrics: type=TIMER, name=key-create, count=1, min=2167.104011, max=2167.104011, mean=2167.104011, stddev=0.0, median=2167.104011, p75=2167.104011, p95=2167.104011, p98=2167.104011, p99=2167.104011, p999=2167.104011, mean_rate=0.3300503366072321, m1=0.0, m5=0.0, m15=0.0, rate_unit=events/second, duration_unit=milliseconds
2023-05-09 17:29:49,633 [shutdown-hook-0] INFO freon.BaseFreonGenerator: Total execution time (sec): 4
2023-05-09 17:29:49,633 [shutdown-hook-0] INFO freon.BaseFreonGenerator: Failures: 0
2023-05-09 17:29:49,633 [shutdown-hook-0] INFO freon.BaseFreonGenerator: Successful executions: 1
==============================================================================
xcompat-cluster-1.2.1-client-1.3.0-write :: Write Compatibility               
==============================================================================
Key Can Be Written                                                    | PASS |
------------------------------------------------------------------------------
Dir Can Be Created                                                    | PASS |
------------------------------------------------------------------------------
File Can Be Put                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.2.1-client-1.3.0-write :: Write Compatibility       | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-26.xml
==============================================================================
xcompat-cluster-1.2.1-client-1.3.0-read-1.3.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.2.1-client-1.3.0-read-1.3.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-27.xml
==============================================================================
xcompat-cluster-1.2.1-client-1.2.1-write :: Write Compatibility               
==============================================================================
Key Can Be Written                                                    | PASS |
------------------------------------------------------------------------------
Dir Can Be Created                                                    | PASS |
------------------------------------------------------------------------------
File Can Be Put                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.2.1-client-1.2.1-write :: Write Compatibility       | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-28.xml
==============================================================================
xcompat-cluster-1.2.1-client-1.2.1-read-1.2.1 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.2.1-client-1.2.1-read-1.2.1 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-29.xml
==============================================================================
xcompat-cluster-1.2.1-client-1.2.1-read-1.3.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.2.1-client-1.2.1-read-1.3.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-30.xml
==============================================================================
xcompat-cluster-1.2.1-client-1.3.0-read-1.2.1 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.2.1-client-1.3.0-read-1.2.1 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-31.xml
Stopping xcompat_s3g_1              ... 
Stopping xcompat_old_client_1_2_1_1 ... 
Stopping xcompat_old_client_1_0_0_1 ... 
Stopping xcompat_recon_1            ... 
Stopping xcompat_scm_1              ... 
Stopping xcompat_datanode_1         ... 
Stopping xcompat_old_client_1_1_0_1 ... 
Stopping xcompat_datanode_3         ... 
Stopping xcompat_om_1               ... 
Stopping xcompat_datanode_2         ... 
Stopping xcompat_new_client_1       ... 
Stopping xcompat_old_client_1_2_1_1 ... done
Stopping xcompat_old_client_1_0_0_1 ... done
Stopping xcompat_old_client_1_1_0_1 ... done
Stopping xcompat_new_client_1       ... done
Stopping xcompat_s3g_1              ... done
Stopping xcompat_om_1               ... done
Stopping xcompat_recon_1            ... done
Stopping xcompat_scm_1              ... done
Stopping xcompat_datanode_3         ... done
Stopping xcompat_datanode_1         ... done
Stopping xcompat_datanode_2         ... done
Removing xcompat_s3g_1              ... 
Removing xcompat_old_client_1_2_1_1 ... 
Removing xcompat_old_client_1_0_0_1 ... 
Removing xcompat_recon_1            ... 
Removing xcompat_scm_1              ... 
Removing xcompat_datanode_1         ... 
Removing xcompat_old_client_1_1_0_1 ... 
Removing xcompat_datanode_3         ... 
Removing xcompat_om_1               ... 
Removing xcompat_datanode_2         ... 
Removing xcompat_new_client_1       ... 
Removing xcompat_datanode_1         ... done
Removing xcompat_old_client_1_2_1_1 ... done
Removing xcompat_om_1               ... done
Removing xcompat_recon_1            ... done
Removing xcompat_datanode_2         ... done
Removing xcompat_datanode_3         ... done
Removing xcompat_old_client_1_0_0_1 ... done
Removing xcompat_new_client_1       ... done
Removing xcompat_old_client_1_1_0_1 ... done
Removing xcompat_scm_1              ... done
Removing xcompat_s3g_1              ... done
Removing network xcompat_default
Running Erasure Coded storage backward compatibility tests.
Removing network xcompat_default
Network xcompat_default not found.
Creating network "xcompat_default" with the default driver
Creating xcompat_old_client_1_1_0_1 ... 
Creating xcompat_s3g_1              ... 
Creating xcompat_old_client_1_0_0_1 ... 
Creating xcompat_scm_1              ... 
Creating xcompat_recon_1            ... 
Creating xcompat_new_client_1       ... 
Creating xcompat_om_1               ... 
Creating xcompat_old_client_1_2_1_1 ... 
Creating xcompat_datanode_1         ... 
Creating xcompat_datanode_2         ... 
Creating xcompat_datanode_3         ... 
Creating xcompat_datanode_4         ... 
Creating xcompat_datanode_5         ... 
Creating xcompat_old_client_1_1_0_1 ... done
Creating xcompat_new_client_1       ... done
Creating xcompat_old_client_1_2_1_1 ... done
Creating xcompat_recon_1            ... done
Creating xcompat_old_client_1_0_0_1 ... done
Creating xcompat_s3g_1              ... done
Creating xcompat_scm_1              ... done
Creating xcompat_datanode_2         ... done
Creating xcompat_datanode_1         ... done
Creating xcompat_om_1               ... done
Creating xcompat_datanode_5         ... done
Creating xcompat_datanode_3         ... done
Creating xcompat_datanode_4         ... done
SECONDS: 66
com.google.protobuf.ServiceException: java.net.ConnectException: Call From 31dddeab4d8d/172.23.0.7 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy21.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.23.0.7:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 31dddeab4d8d/172.23.0.7 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy21.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.23.0.7:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 31dddeab4d8d/172.23.0.7 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy21.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.23.0.7:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:bae4d39a-27fe-4c3e-ae64-251aed666351 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026) , while invoking $Proxy21.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.23.0.7:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:bae4d39a-27fe-4c3e-ae64-251aed666351 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026) , while invoking $Proxy21.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.23.0.7:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5. SCM is in safe mode. validated:false, DataNodeSafeModeRule, registered datanodes (=2) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 86
SCM is out of safe mode.
Safe mode is off
No OM HA service, no need to wait
Generating data locally...   done
Copy data into client containers...   done
tr: write error: Broken pipe
tr: write error
==============================================================================
xcompat-cluster-1.3.0-setup-data :: Test EC backward compatibility            
==============================================================================
Setup Cluster Data                                                    | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-setup-data :: Test EC backward compatibility    | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-32.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.0.0-read-1.3.0 :: Test EC backward compatibi...
==============================================================================
Test Read Key Compat                                                  | PASS |
------------------------------------------------------------------------------
Test Listing Compat                                                   | PASS |
------------------------------------------------------------------------------
Test Info Compat                                                      | PASS |
------------------------------------------------------------------------------
Test FS Compat                                                        | PASS |
------------------------------------------------------------------------------
Test FS Client Can Read Own Writes                                    | PASS |
------------------------------------------------------------------------------
Test Client Can Read Own Writes                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.0.0-read-1.3.0 :: Test EC backward ... | PASS |
6 critical tests, 6 passed, 0 failed
6 tests total, 6 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-33.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.1.0-read-1.3.0 :: Test EC backward compatibi...
==============================================================================
Test Read Key Compat                                                  | PASS |
------------------------------------------------------------------------------
Test Listing Compat                                                   | PASS |
------------------------------------------------------------------------------
Test Info Compat                                                      | PASS |
------------------------------------------------------------------------------
Test FS Compat                                                        | PASS |
------------------------------------------------------------------------------
Test FS Client Can Read Own Writes                                    | PASS |
------------------------------------------------------------------------------
Test Client Can Read Own Writes                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.1.0-read-1.3.0 :: Test EC backward ... | PASS |
6 tests, 6 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-34.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.2.1-read-1.3.0 :: Test EC backward compatibi...
==============================================================================
Test Read Key Compat                                                  | PASS |
------------------------------------------------------------------------------
Test Listing Compat                                                   | PASS |
------------------------------------------------------------------------------
Test Info Compat                                                      | PASS |
------------------------------------------------------------------------------
Test FS Compat                                                        | PASS |
------------------------------------------------------------------------------
Test FS Client Can Read Own Writes                                    | PASS |
------------------------------------------------------------------------------
Test Client Can Read Own Writes                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.2.1-read-1.3.0 :: Test EC backward ... | PASS |
6 tests, 6 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-35.xml
Stopping xcompat_datanode_1         ... 
Stopping xcompat_old_client_1_2_1_1 ... 
Stopping xcompat_datanode_5         ... 
Stopping xcompat_datanode_3         ... 
Stopping xcompat_datanode_2         ... 
Stopping xcompat_om_1               ... 
Stopping xcompat_datanode_4         ... 
Stopping xcompat_recon_1            ... 
Stopping xcompat_new_client_1       ... 
Stopping xcompat_s3g_1              ... 
Stopping xcompat_old_client_1_0_0_1 ... 
Stopping xcompat_scm_1              ... 
Stopping xcompat_old_client_1_1_0_1 ... 
Stopping xcompat_new_client_1       ... done
Stopping xcompat_old_client_1_0_0_1 ... done
Stopping xcompat_old_client_1_2_1_1 ... done
Stopping xcompat_old_client_1_1_0_1 ... done
Stopping xcompat_s3g_1              ... done
Stopping xcompat_recon_1            ... done
Stopping xcompat_om_1               ... done
Stopping xcompat_datanode_3         ... done
Stopping xcompat_datanode_5         ... done
Stopping xcompat_datanode_2         ... done
Stopping xcompat_datanode_1         ... done
Stopping xcompat_datanode_4         ... done
Stopping xcompat_scm_1              ... done
Removing xcompat_datanode_1         ... 
Removing xcompat_old_client_1_2_1_1 ... 
Removing xcompat_datanode_5         ... 
Removing xcompat_datanode_3         ... 
Removing xcompat_datanode_2         ... 
Removing xcompat_om_1               ... 
Removing xcompat_datanode_4         ... 
Removing xcompat_recon_1            ... 
Removing xcompat_new_client_1       ... 
Removing xcompat_s3g_1              ... 
Removing xcompat_old_client_1_0_0_1 ... 
Removing xcompat_scm_1              ... 
Removing xcompat_old_client_1_1_0_1 ... 
Removing xcompat_recon_1            ... done
Removing xcompat_new_client_1       ... done
Removing xcompat_datanode_3         ... done
Removing xcompat_scm_1              ... done
Removing xcompat_datanode_5         ... done
Removing xcompat_s3g_1              ... done
Removing xcompat_old_client_1_1_0_1 ... done
Removing xcompat_old_client_1_0_0_1 ... done
Removing xcompat_old_client_1_2_1_1 ... done
Removing xcompat_om_1               ... done
Removing xcompat_datanode_2         ... done
Removing xcompat_datanode_1         ... done
Removing xcompat_datanode_4         ... done
Removing network xcompat_default
Log:     /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/xcompat/result/log.html
Report:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/xcompat/result/report.html
Output:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat.xml
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-1.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-10.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-11.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-12.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-13.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-14.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-15.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-16.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-17.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-18.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-19.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-2.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-20.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-21.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-22.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-23.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-24.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-25.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-26.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-27.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-28.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-29.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-3.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-30.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-31.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-32.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-33.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-34.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-35.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-4.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-5.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-6.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-7.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-8.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-9.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client.xml'
removed 'xcompat/result/log.html'
removed 'xcompat/result/report.html'
renamed 'xcompat/result/dn-audit-1aa0f3065c07.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/dn-audit-1aa0f3065c07.log'
renamed 'xcompat/result/dn-audit-1f8b2021065f.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/dn-audit-1f8b2021065f.log'
renamed 'xcompat/result/dn-audit-22f32d697d4a.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/dn-audit-22f32d697d4a.log'
renamed 'xcompat/result/dn-audit-24f7a5c2a87a.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/dn-audit-24f7a5c2a87a.log'
renamed 'xcompat/result/dn-audit-3ae47a4ae0b6.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/dn-audit-3ae47a4ae0b6.log'
renamed 'xcompat/result/dn-audit-44c62c16c866.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/dn-audit-44c62c16c866.log'
renamed 'xcompat/result/dn-audit-581e2497ab18.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/dn-audit-581e2497ab18.log'
renamed 'xcompat/result/dn-audit-5c391d488eab.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/dn-audit-5c391d488eab.log'
renamed 'xcompat/result/dn-audit-77695b98f37b.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/dn-audit-77695b98f37b.log'
renamed 'xcompat/result/dn-audit-9c4c4a7036bf.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/dn-audit-9c4c4a7036bf.log'
renamed 'xcompat/result/dn-audit-9e4538c15cbc.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/dn-audit-9e4538c15cbc.log'
renamed 'xcompat/result/dn-audit-9f6b573e4bda.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/dn-audit-9f6b573e4bda.log'
renamed 'xcompat/result/dn-audit-acf8d80ca6b3.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/dn-audit-acf8d80ca6b3.log'
renamed 'xcompat/result/dn-audit-c4199841a55c.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/dn-audit-c4199841a55c.log'
renamed 'xcompat/result/dn-audit-e373952af5cd.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/dn-audit-e373952af5cd.log'
renamed 'xcompat/result/dn-audit-f840af030313.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/dn-audit-f840af030313.log'
renamed 'xcompat/result/dn-audit-fbf6f4c693b1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/dn-audit-fbf6f4c693b1.log'
renamed 'xcompat/result/docker-xcompat-xcompat-write-new_client.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/docker-xcompat-xcompat-write-new_client.log'
renamed 'xcompat/result/om-audit-124c3474f86a.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/om-audit-124c3474f86a.log'
renamed 'xcompat/result/om-audit-5abf24350244.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/om-audit-5abf24350244.log'
renamed 'xcompat/result/om-audit-6ad61eb828b9.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/om-audit-6ad61eb828b9.log'
renamed 'xcompat/result/om-audit-6dafe762cd58.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/om-audit-6dafe762cd58.log'
renamed 'xcompat/result/om-audit-a54d467d995d.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/om-audit-a54d467d995d.log'
renamed 'xcompat/result/s3g-audit-10d723f8c6b4.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/s3g-audit-10d723f8c6b4.log'
renamed 'xcompat/result/s3g-audit-f83a69fa4bc6.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/s3g-audit-f83a69fa4bc6.log'
renamed 'xcompat/result/scm-audit-1f80a59fb5eb.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/scm-audit-1f80a59fb5eb.log'
renamed 'xcompat/result/scm-audit-31dddeab4d8d.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/scm-audit-31dddeab4d8d.log'
renamed 'xcompat/result/scm-audit-3bc9588a47e2.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/scm-audit-3bc9588a47e2.log'
renamed 'xcompat/result/scm-audit-90e9b3bb8bcd.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/scm-audit-90e9b3bb8bcd.log'
renamed 'xcompat/result/scm-audit-a3f5ff3a1c10.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/xcompat/scm-audit-a3f5ff3a1c10.log'
Exception in thread "main" java.net.SocketException: Socket closed
	at java.base/java.net.PlainSocketImpl.socketAccept(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)
	at java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)
	at java.base/java.net.ServerSocket.accept(ServerSocket.java:533)
	at org.apache.hadoop.test.JacocoServer.main(JacocoServer.java:60)
Log:     /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/log.html
Report:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.4.0-SNAPSHOT/compose/result/report.html
